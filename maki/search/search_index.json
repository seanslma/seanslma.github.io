{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>This is <code>maki</code>, my learning notes wiki on different topics from energy modelling to data analysis and programming.</p>"},{"location":"#about-me","title":"About Me","text":"<p>Sean Ma, PhD</p> <p>LinkedIn: https://linkedin.com/in/seanslma</p> <p>WordPress: https://shuiliang0ma.wordpress.com</p>"},{"location":"#purpose-and-scope","title":"Purpose and Scope","text":"<p>The contents in this repository are a collection of knowledge from different sources, including websites with or without a link to them, an improved version of the contents on the websites and my own experiences and practices.</p>"},{"location":"#what-topics-this-contains","title":"What Topics This Contains","text":"<ul> <li>programming languages</li> <li>scriping languages</li> <li>devops</li> <li>cloud</li> <li>data</li> <li>sql</li> <li>ml</li> <li>viz</li> <li>energy</li> <li>optimization</li> </ul>"},{"location":"AI/ChatGPT/","title":"ChatGPT","text":""},{"location":"AI/ChatGPT/#python-api","title":"Python api","text":"<pre><code>import os\nimport openai\nfrom dotenv import load_dotenv, find_dotenv\n\n_ = load_dotenv(find_dotenv()) #read local .env file\nopenai.api_key  = os.getenv('OPENAI_API_KEY')\n\ndef get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=temperature,\n    )\n    return response.choices[0].message[\"content\"]\n</code></pre>"},{"location":"AI/Design/","title":"Design","text":""},{"location":"AI/Design/#httpsdesignermicrosoftcom","title":"https://designer.microsoft.com","text":"<p>A graphic design app that helps you create professional quality social media posts, invitations, digital postcards, graphics, and more.</p>"},{"location":"AI/Design/#httpswwwuizardio","title":"https://www.uizard.io","text":"<p>Try Uizard's AI-powered screenshot scanner, pre-designed templates or design from scratch.</p>"},{"location":"AI/Learn/","title":"Learn","text":""},{"location":"AI/Learn/#mckinseys-top-10-articles","title":"McKinsey's top 10 articles","text":"<ol> <li> <p>What every CEO should know about generative AI \u27a1https://mck.co/3remJyu</p> </li> <li> <p>The State of Organizations 2023: Ten shifts transforming organizations \u27a1https://mck.co/44vtcUl</p> </li> <li> <p>The economic potential of generative AI: The next productivity frontier \u27a1https://mck.co/3XOlhiV</p> </li> <li> <p>Rising CEOs: Lessons from the McKinsey Leadership Forum \u27a1https://mck.co/3rd1j56</p> </li> <li> <p>New leadership for a new era of thriving organizations \u27a1https://mck.co/439ggCv</p> </li> <li> <p>Exploring opportunities in the generative AI value chain \u27a1 https://mck.co/44vqzSx</p> </li> <li> <p>McKinsey Explainer: What is AI? \u27a1https://mck.co/431J6Ve</p> </li> <li> <p>Six CEO priorities for 2023 \u27a1https://mck.co/3Xx194o</p> </li> <li> <p>AI-powered marketing and sales reach new heights with generative AI \u27a1https://mck.co/3NCqNAt</p> </li> <li> <p>McKinsey Explainer: What is business transformation? \u27a1https://mck.co/3Xy1e80</p> </li> </ol>"},{"location":"AI/Notes/","title":"Notes","text":""},{"location":"AI/Notes/#httpstldvio","title":"https://tldv.io","text":"<p>Taking your meeting notes for you</p>"},{"location":"AI/Presentation/","title":"Presentation","text":""},{"location":"AI/Presentation/#httpswwwbeautifulai","title":"https://www.beautiful.ai","text":"<p>Create slides quickly</p>"},{"location":"AI/Task/","title":"Task","text":""},{"location":"AI/Task/#godmode-autogpt","title":"GodMode AutoGPT","text":"<p>Perform tasks for you on autopilot</p>"},{"location":"AI/Video/","title":"Video","text":""},{"location":"AI/Video/#httpswwwheygencom","title":"https://www.heygen.com","text":"<p>AI Spokesperson Video Generator</p>"},{"location":"AI/Web/","title":"Web","text":""},{"location":"AI/Web/#httpsdurableco","title":"https://durable.co","text":"<p>Build a website in 30 seconds with AI</p>"},{"location":"AWS/CDN/","title":"Content Delivery Network","text":"<p>Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data.</p>"},{"location":"Azure/Advisor/","title":"Advisor","text":"<p>Get various recommendations on aspects such as Cost, Security and High Availability.</p>"},{"location":"Azure/AppInsight/","title":"Application Insights","text":""},{"location":"Azure/Azure/","title":"azure","text":"<p>ms learn</p> <p>https://docs.microsoft.com/en-gb/learn/?WT.mc_id=modinfra-12190-thmaure</p> <p>https://docs.microsoft.com/en-us/learn/paths/azure-for-the-data-engineer/</p> <p>user account -&gt; subscription -&gt; resource group -&gt; service</p>"},{"location":"Azure/Env/","title":"Env","text":""},{"location":"Azure/Env/#instal-az-cli","title":"instal az cli","text":"<p>https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-linux?view=azure-cli-latest&amp;pivots=apt <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre></p>"},{"location":"Azure/Learn/","title":"Learn","text":"<p>https://olohmann.github.io/azure-hands-on-labs/labs/01_serverless/serverless.html</p>"},{"location":"Azure/Learn/#sessions","title":"sessions","text":"<p>https://build.microsoft.com/en-US/sessions</p>"},{"location":"Azure/Learn/#how-to-let-web-app-automatically-login-using-rbac","title":"how to let web app automatically login using rbac","text":"<p>service principle, app registration???</p>"},{"location":"Azure/Learn/#how-to-control-who-can-have-access-to-web-api","title":"how to control who can have access to web api","text":"<p>user group, user role??? how to use service account not user to access the web api?</p>"},{"location":"Azure/Learn/#hands-on-lab","title":"Hands On Lab","text":"<p>https://build5nines.com/top-free-microsoft-certification-hands-on-labs/</p>"},{"location":"Azure/Learn/#microsoft-learning","title":"Microsoft Learning","text":"<p>https://github.com/MicrosoftLearning</p>"},{"location":"Azure/Learn/#microsoft-cloud-workshops","title":"Microsoft Cloud Workshops","text":"<p>https://microsoftcloudworkshop.com \\ https://build5nines.com/tag/mcw/</p>"},{"location":"Azure/Learn/#other-labs","title":"Other Labs","text":"<p>https://github.com/microsoft/TechnicalCommunityContent \\ https://azure-samples.github.io/azureiotlabs/</p> <p>https://azurelib.com/azure-data-factory-tutorial/</p> <p>PowerShell in Depth.pdf</p> <p>Database Programming with C#.pdf</p> <p>https://docs.microsoft.com/en-gb/learn/modules/aks-deploy-container-app</p> <p>https://github.com/kromerm/adflab</p>"},{"location":"Azure/Learn/#cloud-sandbox","title":"Cloud sandbox","text":"<p>https://help.acloud.guru/hc/en-us/articles/360001389296-Azure-Cloud-Sandbox</p>"},{"location":"Azure/Learn/#free-ebook","title":"free ebook","text":"<p>Cloud Native Infrastructure with Azure: https://clouddamcdnprodep.azureedge.net/gdc/gdcxZiG7n/original</p>"},{"location":"Azure/ResourceProvider/","title":"Resource Provider","text":"<p>https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/resource-providers-and-types</p> <p>An Azure resource provider is a collection of REST operations that provide functionality for an Azure service.</p>"},{"location":"Azure/ResourceProvider/#register-resource-provider","title":"Register resource provider","text":"<ul> <li>Some are registered by default</li> <li>Other are registered automatically when you take certain actions</li> <li>In some scenarios, a resource provider needs to be registered manually <pre><code>az provider register --namespace Microsoft.KeyVault\n</code></pre></li> </ul>"},{"location":"Azure/ResourceProvider/#show-resource-provider","title":"Show resource provider","text":"<p>Show all resource providers and status <pre><code>az provider list \\\n    --query \"[].{Provider:namespace, Status:registrationState}\" \\\n    --out table\n</code></pre></p> <p>Show all registered resource providers <pre><code>az provider list \\\n    --query \"sort_by([?registrationState=='Registered'].{Provider:namespace, Status:registrationState}, &amp;Provider)\" \\\n    --out table\n</code></pre></p> <p>Show a perticular resource provider <pre><code>az provider show --namespace Microsoft.KeyVault\n</code></pre></p> <p>Show resource provider type <pre><code>az provider show --namespace Microsoft.KeyVault \\\n    --query \"resourceTypes[*].resourceType\" \\\n    --out table\n</code></pre></p> <p>Show resource provider available API versions <pre><code>az provider show --namespace Microsoft.KeyVault \\\n    --query \"resourceTypes[?resourceType=='batchAccounts'].apiVersions | [0]\" \\\n    --out table\n</code></pre></p> <p>Show resource provider supported locations <pre><code>az provider show --namespace Microsoft.KeyVault \\\n    --query \"resourceTypes[?resourceType=='batchAccounts'].locations | [0]\" \\\n    --out table\n</code></pre></p>"},{"location":"Azure/Services/","title":"Services","text":""},{"location":"Azure/Services/#azure-active-directory","title":"Azure Active Directory","text":"<ul> <li>Azure identity system used to define users and groups and provide them permissions to your resources</li> <li>External users who can have access to resources in Azure can also be defined here</li> <li>Create conditions in Conditional Access policies to allow or deny users to log into Azure</li> </ul>"},{"location":"Azure/Services/#azure-blueprints","title":"Azure Blueprints","text":"<ul> <li>Define a repeatable set of Azure resources, that can adhere to an organization\u2019s standards, patterns and requirements</li> <li>Orchestrate deployment of resources such as role assignments, policy assignments, Azure resource manager templates and resource groups</li> </ul>"},{"location":"Azure/Services/#azure-security-center","title":"Azure Security Center","text":"<ul> <li>Infrastructure security management system, to improve the security of Azure based and on-premise resources</li> <li>Support for services such as Azure virtual machines , Function Apps, Azure SQL Server databases</li> <li>Give recommendations on what to do for on-premise Windows and Linux servers, provided Microsoft Monitoring agent is installed</li> <li>Can also helps detect and prevent threats at an infrastructure layer</li> </ul>"},{"location":"Azure/Services/#azure-ad-identity-protection","title":"Azure AD Identity Protection","text":"<ul> <li>Help detect suspicious actions related to user identities<ul> <li>Users with leaked credentials</li> <li>Impossible travel to atypical locations</li> <li>Sign-ins from infected devices, anonymous IP addresses, IP addresses with suspicious activity, unfamiliar locations</li> </ul> </li> </ul>"},{"location":"Azure/Services/#azure-ad-privileged-identity-management","title":"Azure AD Privileged Identity Management","text":"<ul> <li>Help manage, control and monitor access to important resources</li> <li>Provide just-in-time privileged access to Azure AD and Azure resources</li> <li>Provide time-bound access to resources using start and end dates</li> <li>Enforce multi-factor authentication to activate any role</li> <li>Get notifications when privileged roles are activated</li> <li>Conduct access reviews to ensure users still require the roles</li> </ul>"},{"location":"Azure/Services/#azure-firewall","title":"Azure Firewall","text":"<ul> <li>Cloud-based network security service to protect network resources</li> <li>Can filter incoming requests and alert or deny traffic from/to malicious IP addresses and domains</li> <li>With built-in high availability, and can scale automatically based on network traffic flows</li> </ul>"},{"location":"Azure/Services/#azure-ddos-protection","title":"Azure DDoS protection","text":"<ul> <li>Protect against Distributed denial of service (DDoS) attacks</li> </ul>"},{"location":"Azure/Services/#role-based-access-control","title":"Role-based access control","text":"<ul> <li>Assign access to resources in Azure based on roles</li> </ul>"},{"location":"Azure/AAD/Account/","title":"Account","text":""},{"location":"Azure/AAD/Account/#update-password","title":"update password","text":"<pre><code>az login --scope https://management.azure.com//.default\naz login --scope https://management.core.windows.net//.default\n</code></pre>"},{"location":"Azure/AAD/Account/#error-credential-authentication-unavailable","title":"Error: Credential authentication unavailable","text":"<p>Run the command first: <pre><code>az account list -o table\naz account set --subscription &lt;subscription-id&gt;\naz login\n</code></pre></p>"},{"location":"Azure/AAD/Account/#continuous-access-evaluation-resulted-in-challenge-with-result","title":"Continuous access evaluation resulted in challenge with result","text":"<ul> <li><code>InteractionRequired and code: TokenCreatedWithOutdatedPolicies</code></li> <li><code>InteractionRequired and code: TokenIssuedBeforeRevocationTimestamp</code></li> </ul> <p>Solution: <pre><code>az logout\naz login\n</code></pre></p>"},{"location":"Azure/AAD/Account/#login-using-managed-identity","title":"login using managed identity","text":"<p>https://learn.microsoft.com/en-us/cli/azure/authenticate-azure-cli-managed-identity</p> <p>Only works for aad_pod_identity <pre><code>az login --identity      # system-assigned managed identity\naz login --identity --username &lt;client_id|object_id|resource_id&gt; # user-assigned managed identity\n</code></pre></p> <p>login freeze and timeout issue: - https://github.com/Azure/azure-cli/issues/12441 - MSI: Failed to acquire tokens after 12 times - firewall blocked <code>management.azure.com</code></p> <p>Error: <pre><code>curl -s -H \"Metadata: true\" \"http://169.254.169.254/metadata/instance?api-version=2021-02-01\"\nRequest blocked by AAD Pod Identity NMI\n</code></pre> Reason: used workload identity but reuires aad_pod_identity</p> <p>If using workload_identity see the section for workaround <pre><code>az login --federated-token \"$(cat $AZURE_FEDERATED_TOKEN_FILE)\" --service-principal -u $AZURE_CLIENT_ID -t $AZURE_TENANT_ID\n</code></pre></p>"},{"location":"Azure/AAD/ActiveDirectory/","title":"Active directory","text":"<p>Azure Active Directory is now Microsoft Entra ID</p>"},{"location":"Azure/AAD/ActiveDirectory/#groups","title":"Groups","text":"<p>Azure Active Directory (Azure AD) groups are used to manage users that all need the same access and permissions to resources.</p>"},{"location":"Azure/AAD/ActiveDirectory/#activate-an-azure-ad-role-in-pim","title":"Activate an Azure AD role in PIM","text":"<p>https://learn.microsoft.com/en-us/azure/active-directory/privileged-identity-management/pim-how-to-activate-role</p>"},{"location":"Azure/AAD/ActiveDirectory/#grant-has-expired-due-to-it-being-revoked","title":"grant has expired due to it being revoked","text":"<p>This will happen after password change <pre><code>Azure Active Directory error '(invalid_grant) AADSTS50173: \nThe provided grant has expired due to it being revoked, a fresh auth token is needed. \nThe user might have changed or reset their password.\n</code></pre> Solution: delete the cache the credential uses. When solution 3 does not work try this one. <pre><code>%LOCALAPPDATA%\\.IdentityService\\msal.cache\n</code></pre></p>"},{"location":"Azure/AAD/AppRegistration/","title":"App Registration","text":"<p>Azure App registrations in Azure Active Directory (AAD) is a service that allows you to register your applications with AAD.  Once your application is registered, you can use it to authenticate users and access resources on their behalf.</p>"},{"location":"Azure/AAD/AppRegistration/#example","title":"example","text":"<p>When users login to a website and that website will use the user's login username and password to automatically login to the app.</p> <p>The generated app registration credentials (<code>application ID</code> and <code>client secret</code>) will be used to authenticate your application with Azure Active Directory (AAD) and access resources on behalf of users.</p> <p>To use this for exemple in jupyterhub login, we have to create an <code>azuread_application</code>, <code>azuread_application_password</code> and <code>azuread_service_principal</code>.  Then in jupyterhub we use the client_id and client_secret to redirect for authentication.</p>"},{"location":"Azure/AAD/AppRegistration/#types-of-azure-app-registrations","title":"Types of Azure App registrations:","text":"<ul> <li>Web applications: Web applications are applications that are hosted on a web server and can be accessed by users through a web browser.   When a user visits a web application that is registered with AAD, the web application uses the application ID and client secret to authenticate with AAD and obtain an access token. The web application then uses the access token to access resources on behalf of the user.</li> <li>Native client applications: Native client applications are applications that are installed on a user's device, such as a computer or mobile phone.   When a user logs in to a native client application that is registered with AAD, the application uses the application ID and client secret to authenticate with AAD and obtain an access token. The application then uses the access token to access resources on behalf of the user.</li> </ul>"},{"location":"Azure/AAD/AppRegistration/#register-your-application","title":"Register your application","text":"<p>When you register your application with AAD, you need to specify the following information: - A name for your application. - The type of application your application is (web or native client). - The redirect URI for your application. The redirect URI is the URL that users will be redirected to after they authenticate with your application. - The permissions that your application needs to access resources on behalf of users.</p> <p>Once you have registered your application, you will be given an application ID and a client secret.  You can use these credentials to authenticate your application with AAD and access resources on behalf of users.</p>"},{"location":"Azure/AAD/AppRegistration/#where-to-use-it","title":"Where to use it","text":"<p>Azure App registrations are used in a variety of scenarios, such as: - Authenticating users to web applications and native client applications. - Accessing resources on behalf of users, such as Microsoft Graph or Azure resources. - Implementing single sign-on (SSO) for your applications.</p> <p>Here are some of the benefits of using Azure App registrations: - Security: Azure App registrations provides a secure way to authenticate users and access resources on their behalf. - Convenience: Azure App registrations is easy to use and integrates with other Azure services. - Scalability: Azure App registrations can scale to support a large number of users and applications.</p> <p>If you are developing an application that needs to authenticate users or access resources on their behalf, then you should consider registering your application with Azure App registrations.</p>"},{"location":"Azure/AAD/AppRegistration/#secret-update","title":"secret update","text":"<p>https://stackoverflow.com/questions/69045286/invalid-client-secret-is-provided-when-using-correct-and-not-expired-secret</p> <p>If the app registrations secret is created via terraform, a renewed secret from the portal will not work. <pre><code>AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app\n</code></pre> We should update the secret from terraform??? No, got the same error.</p> <p>For ArgoCD, seems this is due to the cache: https://github.com/argoproj/argo-cd/issues/12815</p> <p>The solutions:  - Recreate the secret using terraform code   - both the argocd config and kubernetes secret should be updated   - the base64 coded text might be different from the one generated from terraform code - Solve argocd server cache issue   - Clearing the browser cache fixed it for next login attempt.   - To definitively solve this issue, we've had to restart argocd-server.</p>"},{"location":"Azure/AAD/AppRegistration/#get-secret-id","title":"get secret id","text":"<pre><code>az ad app credential list --id &lt;application_id&gt;\n</code></pre>"},{"location":"Azure/AAD/Group/","title":"Group","text":""},{"location":"Azure/AAD/Group/#group-member-access-to-subscription","title":"group member access to subscription","text":"<p>how to create a security group <code>az-dev-contributor</code> in azure aad to let it's members have access to a azure subscription name <code>az-dev</code>?</p> <p>Steps on how to create a security group in Azure AD and grant its members access to the \"az-dev\" subscription:</p> <p>1. Create the Security Group in Azure AD: - Sign in to the Azure portal using a user account with sufficient permissions to manage Azure AD groups. - Go to Azure Active Directory &gt; Groups. - Click \"New group\". - Choose \"Security\" as the group type. - Provide a name for the group (e.g., \"az-dev-contributor\"). - Add a description if desired. - Select the appropriate membership type (e.g., \"Assigned\"). - Click \"Create\".</p> <p>2. Assign a Role to the Security Group: - Go to Subscriptions &gt; \"az-dev\" subscription. - Click on \"Access control (IAM)\". - Click \"Add\" &gt; \"Add role assignment\". - Select the appropriate role for the security group (e.g., \"Contributor\" for general access   or more specific roles like \"Virtual Machine Contributor\" or \"Storage Blob Data Contributor\" as needed). - Under \"Assign access to\", click \"Select members\". - Search for and select the newly created security group. - Click \"Review + assign\". - Click \"Assign\".</p> <p>3. Add Members to the Security Group: - Go back to Azure Active Directory &gt; Groups &gt; \"az-dev-contributor\" group. - Click on \"Members\". - Click \"Add members\". - Search for and select the users or other groups you want to add as members. - Click \"Select\". - Click \"Add\".</p> <p>Key Points: - Ensure you have the necessary permissions to create groups and assign roles in Azure AD. - Choose the appropriate role based on the level of access you want to grant to the group members. - Consider using more granular roles for specific resource types if needed. - Regularly review and update group memberships and assigned roles to maintain security and compliance. - For more complex scenarios or automation, consider using Azure PowerShell or Azure CLI commands.</p>"},{"location":"Azure/AAD/ManagedIdentity/","title":"Managed Identity","text":"<p>https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview</p> <p>After creating the identity, with the scope of the resource's manager id create roles linked to the identity's principle id, then add the identity to the appilication so the app can access the resources.</p> <ul> <li>Managed identities provide an automatically managed identity in Azure Active Directory for applications to use when connecting to resources that support Azure Active Directory (Azure AD) authentication.</li> <li>Applications can use managed identities to obtain Azure AD tokens without having to manage any credentials.</li> </ul>"},{"location":"Azure/AAD/ManagedIdentity/#system-and-user-types","title":"system and user types","text":"<ul> <li><code>System assigned</code> (1:1):  generated by the service, will be deleted when the parent resource or service is deleted</li> <li><code>User assigned</code> (1:n): created standalone and assigned to services, can be applied and reused with multiple services</li> </ul>"},{"location":"Azure/AAD/ManagedIdentity/#difference-between-service-principal","title":"difference between service principal","text":"<p>https://stackoverflow.com/questions/61322079/difference-between-service-principal-and-managed-identities-in-azure</p> <p>For <code>managed identities</code>, admins do not have to manage credentials, including passwords - so no one knows the credentials. - Managed identities manage the creation and automatic renewal of a service principal on your behalf? - Managed identities are service principals of a special type, which are locked to only be used with Azure resources. - When the managed identity is deleted, the corresponding service principal is automatically removed. - When a User-Assigned or System-Assigned Identity is created, the Managed Identity Resource Provider (MSRP) issues a certificate internally to that identity.</p>"},{"location":"Azure/AAD/ManagedIdentity/#benifit","title":"benifit","text":"<p>https://learningbydoing.cloud/blog/stop-using-client-secrets-start-using-managed-identities/</p>"},{"location":"Azure/AAD/ManagedIdentity/#managed-identity-for-container","title":"Managed identity for container","text":"<p>https://learn.microsoft.com/en-us/azure/container-instances/container-instances-managed-identity</p>"},{"location":"Azure/AAD/ManagedIdentity/#show-the-added-pod-identity","title":"show the added pod-identity","text":"<pre><code>kubectl get azureidentity\n</code></pre>"},{"location":"Azure/AAD/ManagedIdentity/#add-manged-identity-to-aks","title":"add manged identity to aks","text":"<pre><code>az aks pod-identity add --resource-group &lt;rg-name&gt; \\\n\u00a0\u00a0--cluster-name &lt;cluster-name&gt; --namespace &lt;namespace&gt; \\\n  --name csi-to-key-vault --identity-resource-id &lt;managed-identity-resource-id&gt;\n</code></pre>"},{"location":"Azure/AAD/Role/","title":"Role","text":"<p>Roles manage User's access</p>"},{"location":"Azure/AAD/Role/#check-role-assignment","title":"check role assignment","text":"<pre><code>az role assignment list --scope \"/subscriptions/&lt;subscription-id&gt;\" --role \"&lt;role-name&gt;\"\naz role assignment list --scope \"/subscriptions/&lt;subscription-id&gt;\" --assignee &lt;user-principal-name-or-object-id&gt;\n</code></pre>"},{"location":"Azure/AAD/Role/#how-to-setup-the-role-so-the-members-in-a-group-can-have-access-to-the-api-url","title":"How to setup the role so the members in a group can have access to the api url?","text":"<p>After creating the <code>aad group</code>, <code>user assigned identity</code>, and the <code>role assignment</code>,  we should deploy the <code>AzureIdentity</code> and <code>AzureIdentityBinding</code> in aks so the pod can access the azure resources.</p>"},{"location":"Azure/AAD/Role/#user-assigned-identity-and-role-assignment","title":"User Assigned Identity and Role Assignment","text":"<p>In Azure, <code>azurerm_user_assigned_identity</code> and <code>azurerm_role_assignment</code> are two distinct resources  that are commonly used together to grant specific roles to a user-assigned managed identity.</p> <p>The relationship between them is that <code>azurerm_role_assignment</code> is used to grant roles to different types of identities (including user-assigned managed identities created by <code>azurerm_user_assigned_identity</code>).  This allows you to control the permissions and access levels for various resources in your Azure environment.  The <code>principal_id</code> attribute in <code>azurerm_role_assignment</code> is typically the identity to which you want to assign a role,  and this identity can be a user-assigned managed identity among other possibilities.</p>"},{"location":"Azure/AAD/Role/#azurerm_user_assigned_identity","title":"<code>azurerm_user_assigned_identity</code>","text":"<ul> <li>A user-assigned managed identity is an aad identity that can be used to create, manage, and assign to one or more Azure resources.</li> <li>is used to create the user-assigned managed identity, and it provides you with an identity that can be assigned roles.  <pre><code>resource \"azurerm_user_assigned_identity\" \"example\" {\n  name                = \"your-identity-name\"\n  resource_group_name = \"your-resource-group\"\n}\n</code></pre></li> </ul>"},{"location":"Azure/AAD/Role/#azurerm_role_assignment","title":"<code>azurerm_role_assignment</code>","text":"<ul> <li>used to assign a role to a user, group, service principal, or managed identity at a specific scope (such as a resource group or resource).</li> <li>is often used to grant permissions to the user-assigned managed identity created by <code>azurerm_user_assigned_identity</code>.  <pre><code>resource \"azurerm_role_assignment\" \"example\" {\n  principal_id         = azurerm_user_assigned_identity.example.principal_id\n  role_definition_name = \"Contributor\"\n  scope                = \"/subscriptions/your-subscription-id/resourceGroups/your-resource-group\"\n}\n</code></pre> For Key Vault and Blob storage, the role_definition_name and scope are different:</li> <li>key vault:</li> <li><code>Key Vault Secrets User</code></li> <li><code>&lt;key_vault&gt;/secrets/&lt;secret&gt;</code></li> <li>blob storage container:</li> <li><code>Storage Blob Data Contributor</code></li> <li><code>Storage Queue Data Contributor</code></li> <li><code>storage.container[\"&lt;name&gt;\"].resource_manager_id</code></li> </ul>"},{"location":"Azure/AAD/Scope/","title":"Scope","text":"<p>https://learn.microsoft.com/en-us/azure/role-based-access-control/scope-overview</p> <p>Scopes manage Application's access. Scope is the set of resources that access applies to. </p>"},{"location":"Azure/AAD/Scope/#scope-levels","title":"Scope levels","text":"<p>Can specify a scope at four levels:  - management group - subscription - resource group - resource</p>"},{"location":"Azure/AAD/Scope/#add-a-role-asignment-for-a-scope-manually","title":"add a role asignment for a scope manually","text":"<p>https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/howto-assign-access-cli</p> <p>To add a role assignment in Azure for a user-assigned system-managed identity with a scope for a blob storage container,  you can follow these steps (cannot do it in Azure UI as there is no Scope option): - Identify the system-managed identity and get the <code>principal_id</code> - run azcli command</p> <p>add storage account <pre><code>az role assignment create \\\n  --assignee &lt;identity-principal-id&gt; \\\n  --role 'Storage Blob Data Contributor' \\\n  --scope /subscriptions/&lt;subscription-id&gt;/resourceGroups/&lt;resource-group&gt; \\\n    /providers/Microsoft.Storage/storageAccounts/&lt;storage-acctount&gt;\n</code></pre> add blob storage container <pre><code>az role assignment create \\\n  --role \"Storage Blob Data Contributor\" \\\n  --assignee &lt;email&gt; \\\n  --scope \"/subscriptions/&lt;subscription_id&gt;/resourceGroups/&lt;resource-group&gt; \\\n    /providers/Microsoft.Storage/storageAccounts/&lt;storage-account&gt;/blobServices/default/containers/&lt;container&gt;\"\n</code></pre></p> <p>The system-managed identity should now have the assigned role with the specified scope for the blob storage container. It will be able to access the container according to the permissions granted by the assigned role.</p>"},{"location":"Azure/AAD/Scope/#default-scope","title":".default scope","text":"<p>https://dev.to/425show/just-what-is-the-default-scope-in-the-microsoft-identity-platform-azure-ad-2o4d <pre><code>https://graph.microsoft.com/Mail.Read\nhttps://graph.microsoft.com/User.Read\n</code></pre></p> <p>The <code>/.default</code> scope is a shortcut back to the Azure AD v1 behavior (e.g., static consent). There are two extra scenarios where the <code>/.default</code> scope is required (<code>https: //your-app.your-co.com/.default</code>):\u202f - <code>client_credentials</code>: our app is making service-to-service calls or using application-only permissions (also known as application app roles in Azure AD parlance), or - when using the <code>on-behalf-of</code> (OBO) flow, where our API is making calls on behalf of the user to a different API; something like this: client app\u202f--&gt; our API\u202f--&gt;\u202fGraph API.</p>"},{"location":"Azure/AAD/ServicePrincipal/","title":"Service Principal","text":"<p>A service principal (SP) in Azure Active Directory (AD) is a form of security identity: - Create an App registration with a Service Principal - Add app credential as a client secret (password string) or a certificate - By default, service principals have a lifespan of one year before the password expires</p> <p>Admins assign an Azure SP to an object, such as an automated tool, application or VM. Then, they use role-based access controls to manage that object's access to Azure resources, rather than use security credentials within scripts.</p>"},{"location":"Azure/AAD/ServicePrincipal/#issues-risks-and-management-overhead","title":"issues, risks and management overhead","text":"<ul> <li>App credentials has a limited lifetime and requires to be rolled from time to time</li> <li>Expired credentials can lead to downtime for the services utilizing the credential</li> <li>App credentials (especially client secrets) sometimes are directly exposed as clear text to code</li> </ul>"},{"location":"Azure/AAD/ServicePrincipal/#create","title":"create","text":"<pre><code>az ad sp create-for-rbac \\\n  --name &lt;service-principal-name&gt; \\\n  --role reader #Reader role\n\naz ad sp create-for-rbac \\\n  --name &lt;service-principal-name&gt; \\\n  --role Contributor \\\n  --scopes /subscriptions/&lt;subscription-id&gt;\n\naz login --service-principal \\\n  --username &lt;app-id&gt; --password &lt;password&gt; \\\n  --tenant &lt;tenant-id&gt;\n\naz account show #get Azure Subscription ID and name\n</code></pre>"},{"location":"Azure/AAD/Subscription/","title":"Subscription","text":"<p>When a user signs up for a Microsoft cloud service, a new Azure AD tenant is created and the user is made a member of the Global Administrator role.</p> <p>However, when an owner of a subscription joins their subscription to an existing tenant, the owner isn't assigned to the Global Administrator role.</p>"},{"location":"Azure/AAD/Subscription/#change-subscription","title":"change subscription","text":"<p>If resource-group could not be found, change default subscription <pre><code>az login\naz login --tenant tenant.onmicrosoft.com\naz account list --output table #list all subscriptions\naz account set -s &lt;subscription-id&gt;\naz account set --subscription &lt;subscription-id&gt; #set default subscription\n</code></pre></p>"},{"location":"Azure/AAD/Subscription/#role-assignment-with-vs-pro-subscription","title":"Role assignment with VS Pro Subscription","text":"<p>https://stackoverflow.com/questions/67486458/how-can-i-associate-a-subscription-with-a-new-tenant-in-azure</p> <p>Invite a user from new tenant by assigning them a suitable RBAC role in the VS Pro Subscription. - Subscription - Access control (IAM) - Role Assignments - Add &gt; Owner from the new tenant</p>"},{"location":"Azure/AAD/Subscription/#role-assignment","title":"role assignment","text":"<p>https://learn.microsoft.com/en-us/azure/role-based-access-control/role-assignments-portal</p>"},{"location":"Azure/AAD/Subscription/#associate-subscription-to-another-directory","title":"Associate subscription to another directory","text":"<p>https://docs.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-how-subscriptions-associated-directory</p> <p>https://vmlabblog.com/2020/02/how-to-move-an-azure-subscription/</p>"},{"location":"Azure/AAD/Subscription/#transfer-subscription-to-another-directory","title":"Transfer subscription to another directory","text":"<p>https://docs.microsoft.com/en-us/azure/role-based-access-control/transfer-subscription</p>"},{"location":"Azure/AAD/Tenant/","title":"Tenant","text":""},{"location":"Azure/AAD/Tenant/#create-new-tenant","title":"Create new tenant","text":"<p>https://learn.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-access-create-new-tenant - Subscription - Azure active directory - Overview page, select Manage tenants - Create: Azure Active Directory, Orgnisation Name, Initial Domain Name</p>"},{"location":"Azure/AAD/Topic/","title":"topic","text":"<p>what's the relationships between subscription, active directory, resource group?</p> <p>https://emails.microsoft.com/dc/pTEFlGZ9Q3ITrVt7_I2wJfSaXP4fVmu5GQerBk9DuUtPn_NF7SywZiwt2iEQctSTiGdwyghfEFpr0j1GZXklO-RZMhzpN5_YSb4Mi8O6542RfQe3jU-siswhnER1D3o3/MTU3LUdRRS0zODIAAAGJoWLsvzDTkcligDK6G08Ha3drMQfhCQchbPNgpywCzOezYfHmSTtpZsgZsuvRgI6AZKMShBk=</p> <p>https://emails.microsoft.com/dc/pTEFlGZ9Q3ITrVt7_I2wJfSaXP4fVmu5GQerBk9DuUuxNxDsblewhwka4osFJHSTFWcz5VtEqzmffBvjfWT89CPecos5fp-6IogE6IjD2Rm1CzW6JT8PtP5cmS20TKjp/MTU3LUdRRS0zODIAAAGJq-8B696LuFEAGUn3vaSI3ISrtOn19RrmzLiiJQUc16kz5TV1X6QBfrQkmh8CEcSl12FVpz8=</p>"},{"location":"Azure/AAD/aad_pod_identity/","title":"aad_pod_identity","text":"<p>https://azure.github.io/aad-pod-identity/docs/getting-started/installation/#quick-install</p> <p>https://azure.github.io/aad-pod-identity/docs/demo/standard_walkthrough/</p> <p>https://learn.microsoft.com/en-us/azure/aks/use-azure-ad-pod-identity</p>"},{"location":"Azure/AAD/aad_pod_identity/#depreciated","title":"depreciated","text":"<p>https://github.com/Azure/aad-pod-identity</p> <p>New one to use <code>Azure Workload Identity</code>: https://azure.github.io/azure-workload-identity/docs</p>"},{"location":"Azure/AAD/aad_pod_identity/#azureidentity-vs-clusterrole","title":"<code>AzureIdentity</code> vs <code>ClusterRole</code>","text":"<p>AzureIdentityBinding is specific to AKS and is part of the Azure AD Pod Identity (AAD Pod Identity) feature: - Enables secure access to Azure resources from within Kubernetes pods using Azure identities.</p> <p>ClusterRoleBinding is a Kubernetes resource used to bind a Role or ClusterRole to a user, group, or service account within a namespace, specifying the permissions they have: - Defines access within the Kubernetes cluster, such as the ability to list, get, or watch specific resources (pods, services, etc.).</p>"},{"location":"Azure/AAD/aad_pod_identity/#create-azureidentity-and-azureidentitybinding","title":"create <code>AzureIdentity</code> and <code>AzureIdentityBinding</code>","text":"<pre><code>apiVersion: \"aadpodidentity.k8s.io/v1\"\nkind: AzureIdentity\nmetadata:\n  name: \"{{ .Values.azureIdentity.name }}\"\n  namespace: \"{{ .Values.namespace }}\"\nspec:\n  type: 0\n  resourceID: \"{{ .Values.azureIdentity.resourceID }}\"\n  clientID: \"{{ .Values.azureIdentity.clientID }}\"\n---\napiVersion: \"aadpodidentity.k8s.io/v1\"\nkind: AzureIdentityBinding\nmetadata:\n  name: \"{{ .Values.azureIdentity.name }}\"\n  namespace: \"{{ .Values.namespace }}\"\nspec:\n  azureIdentity: \"{{ .Values.azureIdentity.name }}\"\n  selector: \"{{ .Values.azureIdentity.name }}\"\n</code></pre>"},{"location":"Azure/AAD/aad_pod_identity/#reference-the-created-azureidentity","title":"reference the created AzureIdentity","text":"<p>AAD Pod Identity enables Kubernetes applications to access cloud resources securely with Azure Active Directory. <pre><code>metadata:\n  name: demo\n  labels:\n    app: \"{{ .Chart.Name }}\"\n    aadpodidbinding: \"{{ .Values.azureIdentity.name }}\"    \n</code></pre></p>"},{"location":"Azure/AAD/aad_pod_identity/#list-identities","title":"list identities","text":"<pre><code>az identity list [--resource-group]\naz identity create --name --resource-group [--location] [--tags]\naz identity delete [--name] [--ids] [--resource-group]\naz identity show [--name] [--ids] [--resource-group]\naz identity list-operations\naz identity list-resources [--filter]\n                           [--ids]\n                           [--name]\n                           [--orderby]\n                           [--resource-group]\n                           [--skip]\n                           [--skiptoken]\n                           [--top]\n</code></pre>"},{"location":"Azure/AAD/aad_workload_identity/","title":"workload identity","text":""},{"location":"Azure/AAD/aad_workload_identity/#docs","title":"docs","text":"<p>https://azure.github.io/azure-workload-identity/docs/</p> <ul> <li>user managed identity -&gt; federated credential (new) -&gt; service account (azure_identity)</li> <li>service account (azure_identity, azure_identity_binding) -&gt; pod</li> </ul>"},{"location":"Azure/AAD/aad_workload_identity/#migrate-from-pod-identity","title":"Migrate from pod-identity","text":"<p>CLI example: - https://learn.microsoft.com/en-us/azure/aks/workload-identity-migrate-from-pod-identity - https://learn.microsoft.com/en-us/azure/aks/workload-identity-deploy-cluster</p> <p>Others: - https://blog.identitydigest.com/migrate-podid/ - https://www.codit.eu/blog/migrating-to-aad-workload-identity-on-azure-kubernetes-service-aks/?country_sel=be - https://blog.novacare.no/moving-from-aad-pod-identity-to-workload-identity-in-aks/</p> <p>Very good guide with terraform examples: - https://cloudchronicles.blog/blog/A-Step-by-Step-Guide-to-installing-Azure-Workload-Identities-on-AKS</p>"},{"location":"Azure/AAD/aad_workload_identity/#enable-the-oidc-issuer-in-aks","title":"Enable the OIDC Issuer in AKS","text":"<p>https://learn.microsoft.com/en-us/azure/aks/use-oidc-issuer <pre><code># create aks with oidc-issuer enabled\naz aks create -n my_aks -g my_rg --node-count 1 --generate-ssh-keys --enable-oidc-issuer\n\n# enable oidc-issuer in existing aks\naz aks update -n my_aks -g my_rg --enable-oidc-issuer\n# show oidc-issuer url\naz aks show -n my_aks -g my_rg --query \"oidcIssuerProfile.issuerUrl\" -o tsv\n# rotate oidc key\naz aks oidc-issuer rotate-signing-keys -n my_aks -g my_rg\n</code></pre> We can also include <code>--enable-workload-identity</code> without using helm chart to install it separately.</p>"},{"location":"Azure/AAD/aad_workload_identity/#install-azure-workload-identity-webhook-controller","title":"Install azure workload identity webhook controller","text":"<p>https://www.blakyaks.com/resources/using-azure-workload-identity-on-aks</p> <ul> <li>This can be done with <code>--enable-workload-identity</code> when creating aks. </li> <li>But the helm chart allows customization like namespace, tolerations, etc. <pre><code># install workload-identity-webhook\nhelm repo add azure-workload-identity https://azure.github.io/azure-workload-identity/charts\nhelm repo update\nhelm install workload-identity-webhook azure-workload-identity/workload-identity-webhook \\\n  --namepace workload-identity-system \\\n  --create-namespace \\\n  --set azureTenantID=\"${AZURE_TENANT_ID}\"\n\n# check webhook controller manager\nkubectl get po -n workload-identity-system\n</code></pre></li> </ul>"},{"location":"Azure/AAD/aad_workload_identity/#federate-service-account","title":"Federate Service Account","text":"<p>Create <code>federated_identity_credential</code> to link between managed identity and service account. <pre><code>resource \"azurerm_federated_identity_credential\" \"main\" {\n  count               = var.oidc_enabled ? 1 : 0\n  name                = \"${var.aks_name}-ServiceAccount-${var.aks_namespace}-${var.aks_serviceaccount_name}\"\n  resource_group_name = var.resource_group_name\n  parent_id           = azurerm_user_assigned_identity.main.id\n  audience            = var.oidc_audience\n  issuer              = var.oidc_issuer_url \n  subject             = \"system:serviceaccount:${var.aks_namespace}:${var.aks_serviceaccount_name}\"\n}\n</code></pre></p> <pre><code>az identity federated-credential create \\\n    --name ${FEDERATED_IDENTITY_CREDENTIAL_NAME} \\    \n    --resource-group \"${RESOURCE_GROUP}\" \\\n    --identity-name \"${USER_ASSIGNED_IDENTITY_NAME}\" \\\n    --issuer \"${AKS_OIDC_ISSUER}\" \\\n    --subject system:serviceaccount:\"${SERVICE_ACCOUNT_NAMESPACE}\":\"${SERVICE_ACCOUNT_NAME}\" \\\n    --audience api://AzureADTokenExchange\n</code></pre>"},{"location":"Azure/AAD/aad_workload_identity/#replace-azureidentity-and-azureidentitybinding-by-pod-serviceaccount","title":"Replace <code>AzureIdentity</code> and <code>AzureIdentityBinding</code> by pod <code>ServiceAccount</code>","text":"<p>https://blog.novacare.no/moving-from-aad-pod-identity-to-workload-identity-in-aks <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-service-workload-identity\n  namespace: dev\n  annotations:\n    azure.workload.identity/client-id: 00000000-0000-0000-0000-000000000000  \n</code></pre> In deployment replace <pre><code>spec:\n  template:\n    metadata:\n      labels:\n        app: web\n        aadpodidbinding: my-service\n</code></pre> by <pre><code>spec:\n  template:\n    metadata:\n      labels:\n        app: web\n        azure.workload.identity/use: \"true\"\n    spec:\n      serviceAccountName: my-service-workload-identity\n      containers:\n      ...\n</code></pre></p> <p>In pod <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: sample-pod-with-workload-identity\n  namespace: ${SERVICE_ACCOUNT_NAMESPACE}\n  labels:\n    azure.workload.identity/use: \"true\"  # Required. Only pods with this label can use workload identity.\nspec:\n  serviceAccountName: ${SERVICE_ACCOUNT_NAME}\n  containers:\n    - image: &lt;image&gt;\n      name: &lt;containerName&gt;\n</code></pre></p>"},{"location":"Azure/AAD/aad_workload_identity/#mount-key-vault-secret-as-volume","title":"mount key-vault secret as volume","text":"<p><pre><code>Warning  FailedMount  &lt;invalid&gt;  kubelet MountVolume.SetUp failed for volume \"config\" :\nrpc error: code = Unknown desc = failed to mount secrets store objects for pod dev/pod-kv,\nerr: rpc error: code = Unknown desc = failed to mount objects,\nerror: failed to get keyvault client: failed to get authorizer for keyvault client:\nnmi response failed with status code: 404, response body:\ngetting assigned identities for pod dev/pod-kv in CREATED state failed after 16 attempts, retry duration [5]s, error: &lt;nil&gt;.\nCheck MIC pod logs for identity assignment errors\n</code></pre> This indicates that the pod is still using the <code>aad_pod_identity</code> not <code>workload_identity</code>. As this is for using key-vault secret as the mounted volume, in the <code>SecretProviderClass</code> we must update <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\n# This is a SecretProviderClass example using workload identity to access your key vault\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: azure-kvname-wi # needs to be unique per namespace\nspec:\n  provider: azure\n  parameters:\n    usePodIdentity: \"false\"                # Must be false for workload identity\n    clientID: \"${USER_ASSIGNED_CLIENT_ID}\" # Setting this to use workload identity\n    keyvaultName: ${KEYVAULT_NAME}         # Set to the name of your key vault\n    cloudName: \"\"  # [OPTIONAL for Azure] if not provided, the Azure environment defaults to AzurePublicCloud\n    objects:  |\n      array:\n        - |\n          objectName: secret1             # Set to the name of your secret\n          objectType: secret              # object types: secret, key, or cert\n          objectVersion: \"\"               # [OPTIONAL] object versions, default to latest if empty\n        - |\n          objectName: key1                # Set to the name of your key\n          objectType: key\n          objectVersion: \"\"\n    tenantId: \"${IDENTITY_TENANT}\"        # The tenant ID of the key vault\nEOF\n</code></pre> More detail can be found here: https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-identity-access</p>"},{"location":"Azure/AAD/aad_workload_identity/#connection-to-database-timed-out","title":"connection to database timed out","text":"<p>checked the version of <code>azure-identity</code> which is higher then <code>1.13.0</code>.</p> <p>https://github.com/Azure/azure-workload-identity/issues/976 - Upgrading the package \"Microsoft.Data.SqlClient\" fixed the issue</p> <p>https://github.com/Azure/azure-workload-identity/issues/1157 - these links helped me with Python where we now are using ODBC with workload identity</p> <p>details about workload identity with sql connection - https://moimhossain.com/2024/03/29/aks-workload-identity-a-deeper-look - use <code>token</code> as well <pre><code>credential = ClientSecretCredential(tenant_id, client_id, client_secret)    \ntoken_bytes = credential.get_token('https://database.windows.net/.default').token.encode('UTF-16-LE')\ntoken_struct = struct.pack(f'&lt;I{len(token_bytes)}s', len(token_bytes), token_bytes)\nSQL_COPT_SS_ACCESS_TOKEN = 1256\nconn_string = f\"DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={server};DATABASE={database};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30\"\nconn = pyodbc.connect(conn_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct})\n\ncursor = conn.cursor()\ncursor.execute('SELECT @@version').fetchall()\n</code></pre></p> <p>ODBC driver not supporting AKS workload identity - https://techcommunity.microsoft.com/t5/azure-database-support-blog/lesson-learned-384-odbc-driver-not-supporting-aks-workload/ba-p/3858209 - workaround (using <code>token</code>): https://stackoverflow.com/questions/77134053/login-timeout-expired-while-connecting-to-sql-server-using-workload-identity <pre><code>import pyodbc, struct   \nfrom azure.identity import DefaultAzureCredential\n# Get credentials, default to workload identity\ncredential = DefaultAzureCredential()\n# Get token for Azure SQL Database and convert to UTF-16-LE for SQL Server driver\ntoken_bytes = credential.get_token('https://database.windows.net/.default').token.encode('UTF-16-LE')\ntoken_struct = struct.pack(f'&lt;I{len(token_bytes)}s', len(token_bytes), token_bytes)\n# Connect with the token\nSQL_COPT_SS_ACCESS_TOKEN = 1256\n# can also have `encrypt=yes;trustservercertificate=no;connection timeout=30`\n# exclude `authentication=ActiveDirectoryMSI`: cannot use both token and authentication, uid etc.\nconn_string = 'dialect=mssql;SERVER=dbserver.database.windows.net;DATABASE=db;Driver={ODBC Driver 17 for SQL Server}'\nwith pyodbc.connect(conn_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct}) as conn: \n    with conn.cursor() as cursor:\n        cursor.execute(\"SELECT @@version\")\n        rows = cursor.fetchall()\n        for row in rows:\n            print(row)\n</code></pre></p> <p>Note that for <code>turbodbc</code> there is still not a workaround to use azure workload identity.  so perhaps we should use pyodbc with <code>fast_executemany=True</code> for inserting records to db.</p> <p>https://stackoverflow.com/questions/48006551/speeding-up-pandas-dataframe-to-sql-with-fast-executemany-of-pyodbc <pre><code>engine = create_engine(connection_string, fast_executemany=True)\ndf.to_sql('sqlalchemy_test', engine, if_exists='append', index=False)\n</code></pre></p>"},{"location":"Azure/AAD/aad_workload_identity/#do-not-use-token-if-use-userpass","title":"do not use token if use user/pass","text":"<p>Cannot use Access Token with any of the following options: Authentication, Integrated Security, User, Password. (0) (SQLDriverConnect)')</p>"},{"location":"Azure/AAD/aad_workload_identity/#sqlalchemy-for-azure-sql-server","title":"sqlalchemy for azure sql server","text":"<p>https://docs.sqlalchemy.org/en/20/dialects/mssql.html#connecting-to-databases-with-access-tokens</p> <p>Note that the TOKEN_URL<code>is</code>https://database.windows.net/.default<code>, not</code>https://database.windows.net/<code>. Otherwise you will get the error</code>WorkloadIdentityCredential: Microsoft Entra ID error '(invalid_scope) AADSTS70011: The provided request must include a 'scope' input parameter. The provided value for the input parameter 'scope' is not valid. The scope https://database.windows.net/ is not valid.`</p> <p>If include <code>Authentication=ActiveDirectoryMsi</code> will lead to error <code>Cannot use Access Token with any of the following options: Authentication, Integrated Security, User, Password</code>.</p> <pre><code>import struct\nfrom sqlalchemy import create_engine, event\nfrom azure.identity import DefaultAzureCredential\n\nTOKEN_URL = 'https://database.windows.net/.default'  # The token URL for any Azure SQL database\nSQL_COPT_SS_ACCESS_TOKEN = 1256  # Connection option for access tokens, as defined in msodbcsql.h\n\nconnection_string = 'mssql+pyodbc://@my-server.database.windows.net/myDb?driver=ODBC+Driver+17+for+SQL+Server'\nengine = create_engine(connection_string)\n\n# can use sa.event.listen if do not require the decorator\n@event.listens_for(engine, \"do_connect\")\ndef provide_token(dialect, conn_rec, cargs, cparams):\n    # remove the \"Trusted_Connection\" parameter that SQLAlchemy adds\n    cargs[0] = cargs[0].replace(';Trusted_Connection=Yes', '')\n\n    # create token credential\n    azure_credentials = DefaultAzureCredential()\n    raw_token = azure_credentials.get_token(TOKEN_URL).token.encode('utf-16-le')\n    token_struct = struct.pack(f'&lt;I{len(raw_token)}s', len(raw_token), raw_token)\n\n    # apply it to keyword arguments\n    cparams['attrs_before'] = {SQL_COPT_SS_ACCESS_TOKEN: token_struct}\n</code></pre>"},{"location":"Azure/AAD/aad_workload_identity/#error-login-failed-for-user-token-identified-principal","title":"error: <code>Login failed for user '&lt;token-identified principal&gt;'</code>","text":"<p><pre><code>[28000] [Microsoft][ODBC Driver 17 for SQL Server]\n[SQL Server]Login failed for user '&lt;token-identified principal&gt;'. (18456) (SQLDriverConnect)\n</code></pre> We should assign necessary roles like <code>SQL Server Contributor</code> or <code>SQL DB Contributor</code> to the managed identity <pre><code># https://registry.terraform.io/providers/hashicorp/azuread/latest/docs/resources/group_member\nresource \"azuread_group_member\" \"mssql_db_contributor\" {\n  group_object_id  = var.db_contributors_group\n  member_object_id = module.ad_identity.principal_id\n}\n</code></pre></p>"},{"location":"Azure/AAD/aad_workload_identity/#az-login-using-workload-identity","title":"az login using workload identity","text":"<p>https://github.com/Azure/azure-cli/issues/26858</p> <p>workaround: <pre><code>az login --federated-token \"$(cat $AZURE_FEDERATED_TOKEN_FILE)\" --service-principal -u $AZURE_CLIENT_ID -t $AZURE_TENANT_ID\n</code></pre> - <code>AZURE_FEDERATED_TOKEN_FILE</code>: this env var is injected into the pod if you enable workload identity - <code>AZURE_CLIENT_ID</code>: client_id of the managed identity</p>"},{"location":"Azure/ACR/ACR/","title":"Azure Container Registry","text":"<ul> <li>https://azure.microsoft.com/en-us/services/container-registry/</li> <li>https://learn.microsoft.com/en-us/cli/azure/acr?view=azure-cli-latest</li> </ul>"},{"location":"Azure/ACR/ACR/#login","title":"login","text":"<pre><code>az acr login -n &lt;container-registry-name&gt;\n</code></pre>"},{"location":"Azure/ACR/ACR/#list-repos-in-acr","title":"List repos in ACR","text":"<pre><code>az acr repository list --name &lt;acr-name&gt; --output table #list repos\naz acr repository show-tags --name &lt;acr-name&gt; \\\n    --repository sample/hello-world --output table      #list repo tags\n</code></pre>"},{"location":"Azure/ACR/ACR/#run-an-image-from-acr","title":"Run an image from ACR","text":"<p>https://learn.microsoft.com/en-us/learn/modules/publish-container-image-to-azure-container-registry/6-build-run-image-azure-container-registry <pre><code>az acr run \\\n--registry &lt;acr-name&gt; \\\n--cmd '&lt;acr-name&gt;.azurecr.io/sample/hello-world:v1' /dev/null\n</code></pre> <code>cmd</code> docs:</p> <p>https://learn.microsoft.com/en-us/azure/container-registry/container-registry-tasks-reference-yaml#cmd</p> <p>Create a container instantance:</p> <p>https://learn.microsoft.com/en-us/training/modules/create-run-container-images-azure-container-instances/3-run-azure-container-instances-cloud-shell <pre><code>az container create --resource-group az204-aci-rg \\\n    --name mycontainer \\\n    --image mcr.microsoft.com/azuredocs/aci-helloworld \\\n    --ports 80 \\\n    --dns-name-label $DNS_NAME_LABEL --location &lt;myLocation&gt; \\\n</code></pre></p>"},{"location":"Azure/ACR/ACR/#integrate-an-existing-acr-with-existing-aks-clusters","title":"Integrate an existing ACR with existing AKS clusters","text":"<p>https://learn.microsoft.com/en-us/azure/aks/cluster-container-registry-integration?tabs=azure-cli</p> <p>When using Azure Container Registry (ACR) with Azure Kubernetes Service (AKS), you need to establish an authentication mechanism.  You can configure the required permissions between ACR and AKS using the <code>Azure CLI</code>, <code>Azure PowerShell</code>, or <code>Azure portal</code>.  <pre><code>az aks update -n &lt;cluster-name&gt; -g &lt;resource-group&gt; --attach-acr &lt;acr-name/resource-id&gt;\n</code></pre></p>"},{"location":"Azure/ACR/ACR/#remove-the-integration-between-an-acr-and-an-aks-cluster","title":"Remove the integration between an ACR and an AKS cluster","text":"<pre><code>az aks update -n &lt;cluster-name&gt; -g &lt;resource-group&gt; --detach-acr &lt;acr-name/resource-id&gt;\n</code></pre>"},{"location":"Azure/ACR/ACR/#import-an-image-from-docker-hub-into-your-acr","title":"Import an image from docker hub into your ACR","text":"<pre><code>az acr import  -n &lt;acr-name&gt; --source docker.io/library/nginx:latest --image nginx:v1\n</code></pre>"},{"location":"Azure/ACR/ACR/#deploy-docker-image-from-acr-to-aks","title":"Deploy docker image from ACR to AKS","text":"<pre><code>kubectl apply -f &lt;path-to-deployment.yaml&gt;\nkubectl delete deploy &lt;deployment-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Deployment.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  namespace: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: \"&lt;acr-name&gt;.azurecr.io/&lt;namespace&gt;/myapp:latest\"\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            memory: 4Gi\n            cpu: \"1\"\n          limits:\n            memory: 32Gi\n            cpu: \"3\"\n          volumeMounts:\n          - name: cfg\n            mountPath: \"/home/user/.config/cfg1\"\n            readOnly: true\n          env:\n            - name: RESULT_PATH\n              value: \"test\"\n      volumes:\n      - name: db\n        secret:\n          secretName: mysql\n      - name: cfg\n        csi:\n          driver: secrets-store.csi.k8s.io\n          readOnly: true\n          volumeAttributes:\n            secretProviderClass: \"myapp-cfg\"\n</code></pre></p>"},{"location":"Azure/ACR/ACR/#check-arc-health","title":"check arc health","text":"<pre><code>az acr check-health --name &lt;myregistry&gt; --ignore-errors --yes\n</code></pre>"},{"location":"Azure/ACR/ACR/#cache-for-azure-container-registry","title":"Cache for Azure Container Registry","text":""},{"location":"Azure/ACR/Authenticate/","title":"Authenticate","text":"<p>https://devpress.csdn.net/k8s/62ebe99189d9027116a0fbb9.html - push charts from github in CI - pull charts from acr in CD</p>"},{"location":"Azure/ACR/Authenticate/#allow-aks-pull-images-from-acr","title":"allow aks pull images from acr","text":"<p>We must set role assignment <code>AcrPull</code> for aks</p>"},{"location":"Azure/ACR/Authenticate/#connect-issue-using-workload-identity","title":"connect issue using workload identity","text":"<p>Not solved: https://github.com/Azure/azure-cli/issues/28740</p>"},{"location":"Azure/ACR/Authenticate/#ways-of-authentication","title":"ways of authentication","text":"<p>https://learn.microsoft.com/en-us/azure/container-registry/container-registry-authentication?tabs=azure-cli - service principal - managed identity - image pull secret - repository scoped token</p>"},{"location":"Azure/ACR/Authenticate/#user-assigned-managed-identity-to-login-to-acr","title":"user-assigned managed identity to login to acr","text":"<p>https://learn.microsoft.com/en-us/azure/container-registry/container-registry-authentication-managed-identity?tabs=azure-cli <pre><code>az login --identity --username &lt;identity-id&gt;\naz acr login --name my_acr\n</code></pre></p>"},{"location":"Azure/ACR/Authenticate/#login-acr-without-docker","title":"login acr without docker","text":"<pre><code>az login --federated-token \"$(cat $AZURE_FEDERATED_TOKEN_FILE)\" --service-principal -u $AZURE_CLIENT_ID -t $AZURE_TENANT_ID\nacr_token=$(az acr login --name &lt;my-acr&gt; --expose-token --query accessToken -o tsv)\ncurl -H \"Authorization: Bearer $acr_token\" https://&lt;my_acr&gt;.azurecr.io/v2/_catalog # api call using token\n</code></pre>"},{"location":"Azure/ACR/Delete/","title":"Delete","text":"<p>https://learn.microsoft.com/en-us/azure/container-registry/container-registry-delete</p>"},{"location":"Azure/ACR/Delete/#login-and-delete","title":"login and delete","text":"<p>https://stackoverflow.com/questions/70373959/acr-docker-unable-to-get-aad-authorization-tokens-with-message-please-run-az-l <pre><code>acr='my_acr'\necho $(sudo_user_password) | sudo -S az login --service-principal -u xxx -p $(client_secret) --tenant xxx &amp;&amp; \\\necho $(sudo_user_password) | sudo -S az acr login -n myacr.azurecr.io\n\nrepos=('dev/my-app')\ndelete_from=3\n\nfor repo in \"${repos[@]}\"; do\n    tags_to_delete=$(echo $(az acr repository show-tags -n ${acr} --repository ${repo} --orderby time_desc --output tsv) | cut -d ' ' -f${delete_from}-)\n    for tag_to_delete in ${tags_to_delete}; do\n        az acr repository delete --yes -n ${acr} --image ${repo}:${tag_to_delete}\n    done\ndone\n</code></pre></p>"},{"location":"Azure/ACR/Delete/#delete-repo","title":"delete repo","text":"<pre><code>az acr repository delete --name &lt;acr-name&gt; --repository &lt;repo-name&gt;\n</code></pre>"},{"location":"Azure/ACR/Delete/#delete-by-tag","title":"delete by tag","text":"<pre><code>az acr repository delete -n &lt;acr-name&gt; -t &lt;repo-name&gt;:&lt;tag&gt;\naz acr repository delete --name &lt;acr-name&gt; --image &lt;repo-name&gt;:&lt;tag&gt;\n</code></pre>"},{"location":"Azure/ACR/Delete/#delte-digests-by-timestamp","title":"delte digests by timestamp","text":"<pre><code># Change to 'true' to enable image delete\nENABLE_DELETE=false\n\n# Modify for your environment\nREGISTRY=myregistry\nREPOSITORY=myrepository\nTIMESTAMP=2019-04-05\n\n# Loop through command line arguments\nwhile [[ $# -gt 0 ]]; do\n    key=\"$1\"\n    case $key in\n        -d)\n            ENABLE_DELETE=true\n            shift # past argument\n            ;;\n        -r|--registry)\n            REGISTRY=\"$2\"\n            shift # past argument\n            shift # past value\n            ;;\n        -p|--repository)\n            REPOSITORY=\"$2\"\n            shift # past argument\n            shift # past value\n            ;;\n        -t|--timestamp)\n            TIMESTAMP=\"$2\"\n            shift # past argument\n            shift # past value\n            ;;\n        *)    # unknown option\n            echo \"Unknown option: $1\"\n            exit 1\n            ;;\n    esac\ndone \n\n# Delete all images older than specified timestamp\nif [ \"$ENABLE_DELETE\" = true ]\nthen\n    az acr manifest list-metadata --name $REPOSITORY --registry $REGISTRY \\\n    --orderby time_asc --query \"[?lastUpdateTime &lt; '$TIMESTAMP'].digest\" -o tsv \\\n    | xargs -I% az acr repository delete --name $REGISTRY --image $REPOSITORY@% --yes\nelse\n    echo \"No data deleted.\"\n    echo \"Set ENABLE_DELETE=true to enable deletion of these images in $REPOSITORY:\"\n    az acr manifest list-metadata --name $REPOSITORY --registry $REGISTRY \\\n    --orderby time_asc --query \"[?lastUpdateTime &lt; '$TIMESTAMP'].[digest, lastUpdateTime, imageSize, tags[:]]\" -o table \\\n    | tail -n +3 | cut -d ' ' -f1,3- | awk '{split($2,a,\".\"); printf \"%s %s %.2fGiB %s\\n\", $1, a[1], $3/1024/1024/1024, $4 }'\nfi\n</code></pre>"},{"location":"Azure/ACR/Issue/","title":"Common image pull issues","text":"<p>https://azureossd.github.io/2023/08/25/Container-Apps-Troubleshooting-image-pull-errors/</p>"},{"location":"Azure/AKS/ACR/","title":"ACR","text":""},{"location":"Azure/AKS/ACR/#aks-gets-images-from-acr","title":"aks gets images from acr","text":"<p>https://learn.microsoft.com/en-us/troubleshoot/azure/azure-kubernetes/cannot-pull-image-from-acr-to-aks-cluster</p>"},{"location":"Azure/AKS/ACR/#pull-images-from-acr-to-aks","title":"pull images from acr to aks","text":"<p>https://learn.microsoft.com/en-us/azure/aks/cluster-container-registry-integration?tabs=azure-cli</p>"},{"location":"Azure/AKS/ACR/#validate-an-acr-is-accessible-from-an-aks-cluster","title":"Validate an ACR is accessible from an AKS cluster","text":"<pre><code>az aks check-acr --name MyManagedCluster --resource-group MyResourceGroup --acr myacr.azurecr.io\n</code></pre>"},{"location":"Azure/AKS/AKS/","title":"AKS","text":"<ul> <li>Fully managed Kubernetes service on Azure;</li> <li>easy to deploy and manage containerized applications;</li> <li>remove the burden of managing the underlying infrastructure for the Kubernetes deployment.</li> </ul>"},{"location":"Azure/AKS/AKS/#kubernetes","title":"Kubernetes","text":"<ul> <li>an open-source platform used to manage containerized workloads</li> <li>able to provide a DNS name to your container</li> <li>can load balance and distribute network traffic, If there is a high load on containers</li> <li>can restart containers that fail, replace or kill containers</li> <li>store and manage sensitive information such as passwords, OAuth tokens and ssh keys</li> </ul>"},{"location":"Azure/AKS/AKS/#aks-credentials","title":"AKS credentials","text":"<pre><code>az aks list -o table\naz aks get-credentials --resource-group rg01 --name aks01\n</code></pre>"},{"location":"Azure/AKS/AKS/#aks-available-versions","title":"AKS available versions","text":"<pre><code>az aks get-versions -l australiaeast -o table\n</code></pre>"},{"location":"Azure/AKS/AKS/#kubectl-basic","title":"kubectl basic","text":"<pre><code>alias k=kubectl                      #using alias\nkubectl get nodes                    #verify have access\nkubectl create -f aks01.yaml         #launch app\nkubectl get pods                     #check deployment progress\nkubectl get pods --watch             #following pods status\nkubectl get service aks01-front --watch #get load balancer public IP\nkubectl get all                      #see all objects in Kubernetes\nkubectl delete -f aks01.yaml         #delete created objects\n</code></pre>"},{"location":"Azure/AKS/AKS/#upgrade","title":"upgrade","text":"<pre><code>az aks get-upgrades -n &lt;cluster-name&gt; -g &lt;resource-group&gt; -o table                 #list available versions\naz aks upgrade -n &lt;cluster-name&gt; -g &lt;resource-group&gt; --kubernetes-version  1.25.5  #upgrade\n</code></pre>"},{"location":"Azure/AKS/API/","title":"API","text":""},{"location":"Azure/AKS/API/#check-aks-api-version","title":"check aks api version","text":"<p>https://github.com/Azure/AKS/issues/3639 <pre><code>API_VERSION=2022-04-02-preview\naz monitor activity-log list --offset 30d --max-events 10000 --namespace microsoft.containerservice --query \"[?eventName.value == 'EndRequest' &amp;&amp; contains(not_null(httpRequest.uri,''), '${API_VERSION}')]\"\n</code></pre></p>"},{"location":"Azure/AKS/Authorization/","title":"Authorization","text":"<p>https://medium.com/microsoftazure/azure-kubernetes-service-aks-authentication-and-authorization-between-azure-rbac-and-k8s-rbac-eab57ab8345d</p>"},{"location":"Azure/AKS/Authorization/#authentication-and-authorization-in-an-aks-cluster","title":"Authentication and Authorization in an AKS cluster","text":"<ul> <li>Authentication using local accounts for both user and admin access / Authorization using Kubernetes RBAC (default)</li> <li>Authentication using local accounts for both user and admin access/ no authorization mechanism is enabled</li> <li>Authentication using Azure Active directory / Authorization using Kubernetes RBAC only</li> <li>Authentication using Azure Active directory / Authorization using both kubernetes RBAC and Azure RBAC (recommended)</li> </ul>"},{"location":"Azure/AKS/Authorization/#get-kubeconfig","title":"get kubeconfig","text":"<pre><code>az aks get-credentials -g &lt;resource-group&gt; -n &lt;cluster-name&gt;\n</code></pre>"},{"location":"Azure/AKS/Authorization/#install-kubelogin","title":"install kubelogin","text":"<p>https://github.com/Azure/kubelogin <pre><code>curl -LO https://github.com/Azure/kubelogin/releases/latest/download/kubelogin-linux-amd64.zip\nunzip kubelogin-linux-amd64.zip\nsudo mv ./bin/linux_amd64/kubelogin /usr/local/bin/\n\nexport KUBECONFIG=/path/to/kubeconfig\nkubelogin convert-kubeconfig\nkubectl get nodes\n</code></pre></p>"},{"location":"Azure/AKS/Authorization/#getting-credentials-exec-executable-kubelogin-failed-with-exit-code-1","title":"getting credentials: exec: executable kubelogin failed with exit code 1","text":"<p>Reason: not logged in from the terminal. Solution: <code>az aks get-credentials</code> and then run <code>kubelogin convert-kubeconfig</code> and <code>kubectl get nodes</code></p>"},{"location":"Azure/AKS/Certificate/","title":"Certificate","text":""},{"location":"Azure/AKS/Certificate/#error-using-kubectl","title":"error using <code>kubectl</code>","text":"<p><pre><code>Unable to connect to the server:\ntls: failed to verify certificate: x509: certificate has expired or is not yet valid:\ncurrent time 2024-10-29T11:52:50+10:00 is after 2023-10-20T00:44:13Z\n</code></pre> Can access the aks cluster from other machines. Solution: Updated firewall rules and it worked.</p>"},{"location":"Azure/AKS/Certificate/#check-expiration","title":"check expiration","text":"<p>https://github.com/Azure/AKS/issues/2347</p>"},{"location":"Azure/AKS/Certificate/#check-serving-cert","title":"check serving-cert","text":"<pre><code>kubectl get secrets serving-cert -n kube-system -o yaml\n</code></pre>"},{"location":"Azure/AKS/Certificate/#rotate-certificate","title":"rotate certificate","text":"<ul> <li>https://stackoverflow.com/questions/65219904/kubernetes-azures-aks-suddenly-gives-error-kubectl-x509-certificate-has-expi</li> <li>https://learn.microsoft.com/en-us/azure/aks/certificate-rotation</li> </ul> <p>manual certificate rotation <pre><code>az aks rotate-certs -g $RESOURCE_GROUP_NAME -n $CLUSTER_NAME\naz aks get-credentials -g $RESOURCE_GROUP_NAME -n $CLUSTER_NAME --overwrite-existing\n</code></pre></p>"},{"location":"Azure/AKS/Cluster/","title":"Cluster","text":"<p>https://learn.microsoft.com/en-us/azure/aks/start-stop-cluster?tabs=azure-cli</p>"},{"location":"Azure/AKS/Cluster/#stop-cluster","title":"stop cluster","text":"<pre><code>az aks show\naz aks stop --name &lt;cluster-name&gt; --resource-group &lt;resource-group&gt;\n</code></pre>"},{"location":"Azure/AKS/Cluster/#start-cluster","title":"start cluster","text":"<pre><code>az aks start --name &lt;cluster-name&gt; --resource-group &lt;resource-group&gt;\n</code></pre>"},{"location":"Azure/AKS/Cluster/#manual-upgrade","title":"manual upgrade","text":"<pre><code>az aks list -o table\naz aks get-upgrades --resource-group &lt;resource-group&gt; --name &lt;aks-name&gt; -o table\naz aks upgrade --resource-group &lt;resource-group&gt; --name &lt;aks-name&gt; --kubernetes-version &lt;version&gt;\naz aks show --resource-group &lt;resource-group&gt; --name &lt;aks-name&gt; -o table\nkubectl get events -A\n</code></pre>"},{"location":"Azure/AKS/Cluster/#restart-vms-in-scale-set-in-aks-node-pool","title":"restart VMs in scale set in AKS node pool","text":"<pre><code>#get the AKS node resource group name\naz aks show -g &lt;resource-group&gt; -n &lt;cluster-name&gt; --query nodeResourceGroup\n\n#get the scale set info and all the instance id\naz vmss list -g &lt;node-resource-group&gt; --query [].name\naz vmss list-instances -g &lt;node-resource-group -n &lt;vmss-name&gt; -o table\n\n#restart the instance with the instance Id\naz vmss restart -g &lt;node-resource-group&gt; -n &lt;vmss-name&gt; --instance-ids &lt;id&gt;\n</code></pre>"},{"location":"Azure/AKS/ClusterAutoscaler/","title":"Cluster Autoscaler","text":"<p>https://docs.microsoft.com/en-us/azure/aks/scale-cluster?tabs=azure-cli</p> <p>https://www.danielstechblog.io/azure-kubernetes-service-cluster-autoscaler-configurations/</p> <p>https://betterprogramming.pub/build-kubernetes-autoscaling-for-cluster-nodes-and-application-pods-bb7f2d716b07</p> <p>Pod not deleted preventing scaling down on all nodes: https://github.com/kubernetes/autoscaler/issues/248 Fix: After many month of period debugging found the issue to be signal handling. Installed https://github.com/Yelp/dumb-init in all the pods and the issue as cleared. I am closing this.</p>"},{"location":"Azure/AKS/ClusterAutoscaler/#when-nodes-are-scaled-down","title":"when nodes are scaled down","text":"<p>AKS &gt; Monitoring &gt; Metrics &gt; Unneeded Nodes</p>"},{"location":"Azure/AKS/ClusterAutoscaler/#reasons-nodes-not-scaled-down","title":"reasons nodes not scaled down","text":"<p>https://github.com/kubernetes/autoscaler/issues/525</p>"},{"location":"Azure/AKS/ClusterAutoscaler/#autoscaler-logs","title":"autoscaler logs","text":"<pre><code>kubectl -n kube-system logs --follow kube-dns-autoscaler-7xxxxb7-lxxxp\n</code></pre>"},{"location":"Azure/AKS/ClusterAutoscaler/#autoscaler-configmap","title":"Autoscaler configmap","text":"<pre><code>kubectl get configmap cluster-autoscaler-status -n kube-system -o yaml\n</code></pre>"},{"location":"Azure/AKS/ClusterAutoscaler/#cannot-control-which-node-to-delete","title":"cannot control which node to delete","text":"<ul> <li>nodes are being removed starting from the highest IDs</li> <li>You cannot control which node will be removed when scaling down the AKS cluster - the cordoned node will not be selected.</li> </ul> <p>Workaround: first manually scale the nodes to zero then scale back.</p>"},{"location":"Azure/AKS/ClusterAutoscaler/#custom-metrics","title":"custom metrics","text":"<p>If both CPU and memory usages are not high, you might want to consider using custom metrics or external metrics for Horizontal Pod Autoscaler (HPA) in aks. This allows you to scale your application based on metrics other than just CPU and memory.</p> <p>Instrument your application code to expose the relevant metric (in this case, the number of requests) as an endpoint. This can be done by using a metrics library or directly exposing a custom metric through an HTTP endpoint.</p>"},{"location":"Azure/AKS/ClusterAutoscaler/#cluster-autoscaler-settings","title":"Cluster autoscaler settings","text":"<p>https://github.com/Azure/AKS/issues/2766</p> <p>https://docs.microsoft.com/en-us/azure/azure-monitor/autoscale/autoscale-overview</p> <p>Cannot scale down to zero? seems fixed - test it</p>"},{"location":"Azure/AKS/ClusterAutoscaler/#autoscale-pods","title":"Autoscale pods","text":"<p>https://docs.microsoft.com/en-us/azure/aks/tutorial-kubernetes-scale?tabs=azure-cli#autoscale-pods</p> <p>manually scale pods: <pre><code>kubectl scale --replicas=5 deployment/&lt;deployment-name&gt;\n</code></pre></p> <p>Kubernetes supports horizontal pod autoscaling (HPA) to adjust the number of pods in a deployment depending on CPU utilization or other select metrics.</p> <p>To use the autoscaler, all containers in your pods and your pods must have CPU requests and limits defined.</p> <p>If average CPU utilization across all pods exceeds 50% of their requested usage, the autoscaler increases the pods up to a maximum of 10 instances. <pre><code>kubectl autoscale deployment &lt;container-app-name&gt; --cpu-percent=50 --min=3 --max=10\n</code></pre> <pre><code>apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: &lt;container-app-name&gt;-hpa\nspec:\n  minReplicas: 3  # define min replica count\n  maxReplicas: 10 # define max replica count\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: &lt;container-app-name&gt;\n  targetCPUUtilizationPercentage: 50 # target CPU utilization\n</code></pre></p>"},{"location":"Azure/AKS/ClusterAutoscaler/#manually-scale-down-nodes","title":"Manually scale down nodes","text":"<p>https://docs.microsoft.com/en-us/azure/aks/scale-cluster?tabs=azure-cli <pre><code>#show nodepool profiles\naz aks show --resource-group &lt;resource-group&gt; \\\n    --name &lt;cluster-name&gt; --query agentPoolProfiles\n\n#scale nodepool\naz aks scale --resource-group &lt;resource-group&gt; \\\n    --name &lt;cluster-name&gt; --node-count 1 --nodepool-name &lt;nodepool-name&gt;\n\n#can scale user nodepool to zero node\naz aks nodepool scale --resource-group &lt;resource-group&gt; \\\n    --cluster-name &lt;cluster-name&gt; --name &lt;nodepool-name&gt; \\\n    --node-count 0\n\n#disable ca\naz aks nodepool update --resource-group &lt;resource-group&gt; \\\n    --cluster-name &lt;cluster-name&gt; --name &lt;nodepool-name&gt; \\\n    --disable-cluster-autoscaler\n\n#enable ca\naz aks nodepool update --resource-group &lt;resource-group&gt; \\\n    --cluster-name &lt;cluster-name&gt; --name &lt;nodepool-name&gt; \\\n    --enable-cluster-autoscaler --min-count 0 --max-count 4\n\n#change node pool count\naz aks update --resource-group &lt;resource-group&gt; \\\n    --cluster-name &lt;cluster-name&gt; --name &lt;nodepool-name&gt; \\\n    --update-cluster-autoscaler --min-count 0 --max-count 4\n</code></pre></p>"},{"location":"Azure/AKS/ClusterAutoscaler/#metrics","title":"Metrics","text":"<p>https://docs.microsoft.com/en-us/azure/azure-monitor/autoscale/autoscale-common-metrics</p>"},{"location":"Azure/AKS/ClusterAutoscaler/#scale-down-utilization-threshold","title":"scale-down-utilization-threshold","text":"<p>https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler#set-the-cluster-autoscaler-profile-on-an-existing-aks-cluster <pre><code>az aks update \\\n  --resource-group myResourceGroup \\\n  --name myAKSCluster \\\n  --cluster-autoscaler-profile scale-down-utilization-threshold=&lt;percentage value&gt;\n</code></pre></p>"},{"location":"Azure/AKS/ClusterAutoscaler/#prevent-autoscaler-deleting-node-which-runs-a-specific-pod","title":"Prevent autoscaler deleting node which runs a specific pod","text":"<p>https://faun.pub/how-to-make-sure-kubernetes-autoscaler-not-deleting-the-nodes-which-run-a-specific-pod-8df3f2c28c46</p>"},{"location":"Azure/AKS/ClusterAutoscaler/#solution-1","title":"Solution 1:","text":"<p>Can add the annotation to the critical pods or deployments: <pre><code>\"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"false\"\n</code></pre></p> <p>Update a running deployment with the below command: <pre><code>kubectl annotate deployment.apps/efs-provisioner cluster-autoscaler.kubernetes.io/safe-to-evict\": \"false\"\n</code></pre></p>"},{"location":"Azure/AKS/ClusterAutoscaler/#solution-2","title":"Solution 2:","text":"<p>Autoscaler provides an option to exclude nodes from the scale-down process. This annotation is applied to a specific node so make sure the critical pods running on this Kubernetes node. <pre><code>\"cluster-autoscaler.kubernetes.io/scale-down-disabled\": \"true\"\n</code></pre></p>"},{"location":"Azure/AKS/ClusterAutoscaler/#solutions-3","title":"Solutions 3:","text":"<p>If a pod is using local storage AutoScaler will skip that node from deletion. So mounting local storage to a pod also can use as a trick to protect the nodes which run our critical pods.</p>"},{"location":"Azure/AKS/ClusterAutoscalerProfile/","title":"Cluster Autoscaler profile","text":"<p>All the parameters used in the cluster-autoscaler:</p> <p>https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler#using-the-autoscaler-profile</p>"},{"location":"Azure/AKS/ClusterAutoscalerProfile/#reset-cluster-autoscaler-profile-to-default-values","title":"Reset cluster autoscaler profile to default values","text":"<pre><code>az aks update \\\n  --resource-group &lt;resource-group-name&gt; \\\n  --name &lt;cluster-name&gt; \\\n  --cluster-autoscaler-profile \"\"\n</code></pre>"},{"location":"Azure/AKS/ClusterAutoscalerProfile/#set-the-cluster-autoscaler-profile-on-an-existing-aks-cluster","title":"Set the cluster autoscaler profile on an existing AKS cluster","text":"<pre><code>az aks update \\\n  --resource-group &lt;resource-group-name&gt; \\\n  --name &lt;cluster-name&gt; \\\n  --cluster-autoscaler-profile scan-interval=30s\n# scale-down-utilization-threshold=0.5\n</code></pre>"},{"location":"Azure/AKS/Deploy/","title":"deploy app","text":""},{"location":"Azure/AKS/Deploy/#app-deployment","title":"app deployment","text":"<pre><code>#show more details about a deployment\nkubectl describe deployment/redis-master #describe &lt;object&gt; &lt;instance-name&gt;\n#delete current deployment\nkubectl delete deployment/redis-master\n\n#create configmap\nkubectl create configmap example-redis-config --from-file=redis-config\n#delete configmap\nkubectl delete configmap/example-redis-config\n#output object to yaml\nkubectl get -o yaml configmap/example-redis-config\n#open a redis-cli session with the running pod\nkubectl exec -it redis-master-&lt;pod-id&gt; -- redis-cli\nkubectl exec -it redis-master-&lt;pod-id&gt; -- bash\n#create service from yaml\nkubectl apply -f redis-master-service.yaml\n#check service properties\nkubectl get service\n#delete deployment and service\nkubectl delete deployment frontend redis-master redis-replica\nkubectl delete service frontend redis-master redis-replica\n</code></pre>"},{"location":"Azure/AKS/Deploy/#deploy-complex-app-using-helm","title":"deploy complex app using Helm","text":"<p>\"When deploying more complicated applications, across multiple environments (such as dev/test/prod), it can become cumbersome to manually edit YAML files for each environment. This is where the Helm tool comes in.\"</p> <pre><code>#add repo containing stable Helm Charts\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n#install WordPress\nhelm install handsonakswp bitnami/wordpress\n#delete aks\nhelm delete aks01\n#delete pvcs\nkubectl delete pvc --all\n</code></pre>"},{"location":"Azure/AKS/Disk/","title":"Disk","text":"<p>https://techcommunity.microsoft.com/blog/fasttrackforazureblog/everything-you-want-to-know-about-ephemeral-os-disks-and-azure-kubernetes-servic/3565605</p>"},{"location":"Azure/AKS/Disk/#ephemeral-os-disks","title":"Ephemeral OS disks","text":"<ul> <li>created on the local virtual machine (VM) storage and not saved to the remote Azure Storage, as when using managed OS disks</li> <li>work well for stateless workloads</li> <li>lower read/write latency to the OS disk and faster VM reimage</li> </ul>"},{"location":"Azure/AKS/Disk/#managed-os-disks","title":"Managed OS disks","text":"<ul> <li>can set disk size</li> <li>data will be saved to the remote Azure Storage</li> <li>no data loss</li> <li>slower</li> </ul>"},{"location":"Azure/AKS/Disk/#check-pod-disk-usage","title":"check pod disk usage","text":"<p>how to check disk usage: - https://neilcameronwhite.medium.com/under-disk-pressure-34b5ba4284b6 - run into pod and execute <code>du -sh</code> - disks and usage: <code>df -h /var/lib/docker</code> - usage of each folder: <code>du -sh /var/lib/docker/* | sort -h</code> - files that are open: <code>lsof /var/lib/docker/ | grep deleted | head</code> - can use shell to get disk usage for all pods</p>"},{"location":"Azure/AKS/Disk/#increase-node-disk-size","title":"increase node disk size","text":"<ul> <li>Managed disk is slower and has a cost but can specify the size.</li> <li>Ephemeral disk size should not be larger than the <code>temp</code>/<code>cache</code> size of the vm_size</li> <li>Default ephemeral disk size is set to 128GB: https://cloudchronicles.blog/blog/AKS-Best-Practices-Part2-Cost-Efficiency <pre><code>os_disk_type    = \"Ephemeral\"  # {Ephemeral|Managed}\nos_disk_size_gb = 256          # default 128 GB\n</code></pre></li> </ul> <p>The node pool must be recreated: https://github.com/Azure/AKS/issues/610?WT.mc_id=AZ-MVP-5005118</p> <p>solution: - create a temporal node pool - manully if tf does not support - cordon the nodes in that node pool: <code>kubectl cordon aks-agentpool-xxxx-1</code> - delete some pods that should be moved to the new node pool first: <code>kubectl delete po xyz -n namespace</code> - drain all other pods in the nodes: <code>kubectl drain aks-agentpool-xxxx-1 --ignore-daemonsets --delete-local-data</code> - change the disk size, and <code>kubectl uncordon aks-agentpool-xxx-1</code> - drain the nodes in the temporal node pool, and delete the temporal node pool</p>"},{"location":"Azure/AKS/Disk/#freediskspacefailed","title":"FreeDiskSpaceFailed","text":"<p>run <code>kubectl describe node &lt;node-name&gt;</code> the output will be <pre><code>Events:\n  Type     Reason                Age                     Message\n  ----     ------                ----                    -------\n  Warning  FreeDiskSpaceFailed   20m (x2195 over 8d)     Failed to garbage collect required amount of images.\n                                                         Attempted to free 5852591718 bytes, but only found 1164052028 bytes eligible to free.\n  Warning  FreeDiskSpaceFailed   10m                     Failed to garbage collect required amount of images.\n                                                         Attempted to free 11461842534 bytes, but only found 0 bytes eligible to free.\n  Warning  EvictionThresholdMet  9m28s (x20 over 7d17h)  Attempting to reclaim ephemeral-storage\n  Warning  FreeDiskSpaceFailed   5m54s                   Failed to garbage collect required amount of images.\n                                                         Attempted to free 7382013542 bytes, but only found 1000999069 bytes eligible to free.\n  Warning  FreeDiskSpaceFailed   54s                     Failed to garbage collect required amount of images.\n                                                         Attempted to free 7399114342 bytes, but only found 0 bytes eligible to free.\n</code></pre></p> <p>Possible solutions: - find which pod used most disk space - limit resource usage   <pre><code>resources:\nrequests:\n    ephemeral-storage: \"1Gi\"\nlimits:\n    ephemeral-storage: \"2Gi\"\n</code></pre> - adjust Kubernetes eviction and garbage collection settings   <pre><code>evictionHard:\n\"nodefs.available\": \"10%\"\n\"imagefs.available\": \"15%\"\n</code></pre> - enable image garbage collection   <pre><code>--image-gc-high-threshold=85\n--image-gc-low-threshold=80\n</code></pre></p>"},{"location":"Azure/AKS/Docker/","title":"Docker","text":""},{"location":"Azure/AKS/Docker/#self-hosted-tfs-agent","title":"self-hosted tfs agent","text":"<p>Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? - https://github.com/Azure/AKS/issues/2343 - docker build will not function as of aks 1.19 as the container runtime changed from <code>docker</code> to <code>containerd</code> on AKS - alternatives: https://docs.microsoft.com/en-us/azure/aks/cluster-configuration#container-runtime-configuration</p>"},{"location":"Azure/AKS/Docker/#alternative-as-containerd-replaced-docker-in-aks","title":"alternative as <code>containerd</code> replaced <code>docker</code> in aks","text":"<ul> <li>https://juejin.cn/post/6943558078269030408</li> <li>https://blog.alexellis.io/building-containers-without-docker</li> </ul>"},{"location":"Azure/AKS/Docker/#use-buildkit-in-aks","title":"use <code>buildkit</code> in aks","text":"<p>https://medium.com/@aabeing/aks-as-azure-devops-agents-buildkit-5af8e5cd43d1 - details for the deployment of buildkit in aks</p> <p>https://techblog.greeneye.ag/blog/kubernetes-continuous-integration-using-buildkit-buildx-and-docker-regisrty - details to describe how to choose the tools for migrating from aks docker to buildkit - also mentioned tech details such cache etc</p> <p>How to do it: - create a <code>buildkit</code> server - install <code>buildctl</code> in self-hosted tfs agent</p> <pre><code>docker buildx create --driver kubernetes --driver-opt replicas=3 --use\ndocker buildx build -t example.com/foo --push .\n</code></pre> <p>buildkit: https://www.sliceofexperiments.com/p/a-comprehensive-guide-for-the-fastest</p>"},{"location":"Azure/AKS/Docker/#use-buildah-in-aks","title":"use <code>buildah</code> in aks","text":"<ul> <li>use <code>podman</code> / <code>buidah</code>:</li> <li>https://www.reddit.com/r/azuredevops/comments/o5noa8/how_to_build_container_images_from_selfhosted/</li> <li>https://medium.com/@reachpankajdhami/running-azure-devops-self-hosted-agent-on-aks-for-building-docker-images-with-podman-2fa052e6409d</li> </ul> <pre><code># build docker image\nbuildah bud -t example.com/foo:latest .\n# list docker images\nbuildah images\n</code></pre>"},{"location":"Azure/AKS/Docker/#kaniko","title":"kaniko","text":"<ul> <li>slow?</li> </ul>"},{"location":"Azure/AKS/Failure/","title":"AKS Failure","text":""},{"location":"Azure/AKS/Failure/#node-failure","title":"node failure","text":"<pre><code>kubectl get nodes\nkubectl create -f guestbook-all-in-one.yaml\nkubectl get service -w\nkubectl get pods -o wide\n#new cloud shell: hit the guestbook front end every 5 seconds and get the HTML\nwhile true; do\n  curl -m 1 http://&lt;EXTERNAl-IP&gt;/;\n  sleep 5;\ndone\n\nkubectl get pods -o wide -w\n</code></pre>"},{"location":"Azure/AKS/Failure/#out-of-resource-failure","title":"out-of-resource failure","text":"<pre><code>kubectl scale deployment/redis-replica --replicas=10\nkubectl get pods\nkubectl describe pod redis-replica-&lt;pod-id&gt;\nkubectl delete -f guestbook-all-in-one.yaml\n</code></pre>"},{"location":"Azure/AKS/Failure/#storage-mount-issue","title":"storage mount issue","text":"<pre><code>#install WordPress\nkubectl get all\nkubectl get nodes\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install wp bitnami/wordpress\nkubectl get pods -w\n\n#get PersistentVolumeClaims\nkubectl get pvc\n#show actual PVs created\nkubectl get pv\n#get more pv details\nkubectl describe pv &lt;pv name&gt;\n#get IP address\nkubectl get service\n#get usr and pwd\nhelm status wp\necho Username: user\necho Password: $(kubectl get secret --namespace default wp-wordpress\n-o jsonpath=\"{.data.wordpress-password}\" | base64 -d)\n#watch pods\nkubectl get pods -w\n#kill two pods having PVC mounted\nkubectl delete pod --all\n</code></pre>"},{"location":"Azure/AKS/Failure/#node-failure-with-pvc","title":"node failure with pvc","text":"<pre><code>#get the node hosting app\nkubectl get pods -o wide\nkubectl get pods -o wide -w\nkubectl describe pods/wp-wordpress-&lt;pod-id&gt;\n#forcefully remove terminating pod from cluster\nkubectl get pods\nkubectl delete pod wordpress-wp-&lt;pod-id&gt; --force\n#check\nkubectl get pods -w\nkubectl describe pod wp-wordpress-&lt;pod-id&gt;\n#clean up\nhelm delete wp\nkubectl delete pvc --all\nkubectl delete pv --all\n</code></pre>"},{"location":"Azure/AKS/HTTPS/","title":"https","text":"<p>HTTPS makes use of Transport Layer Security (TLS) certificates to encrypt traffic between an end user and a server, or between two servers. TLS is the successor to the Secure Sockets Layer (SSL). The terms TLS and SSL are often used interchangeably.</p>"},{"location":"Azure/AKS/HTTPS/#azure-app-gateway-as-ingress","title":"Azure App Gateway as ingress","text":"<p>An ingress in Kubernetes is an object that is used to route HTTP and HTTPS traffic from outside the cluster to services in a cluster.</p> <p>Azure application gateway can be used as an ingress for Kubernetes by using the Application Gateway Ingress Controller (AGIC). Configuring AGIC can either uses Helm or aks add-on.</p>"},{"location":"Azure/AKS/HTTPS/#create-app-gateway","title":"create app gateway","text":"<pre><code>#create resource group\naz group create -n agic -l westus2\n#create public IP with a DNS name\naz network public-ip create -n agic-pip \\\n   -g agic --allocation-method Static --sku Standard \\\n   --dns-name \"&lt;your unique DNS name&gt;\"\n#create virtual network\naz network vnet create -n agic-vnet -g agic \\\n\u00a0\u00a0--address-prefix\u00a0192.168.0.0/24\u00a0--subnet-name\u00a0agic-subnet\u00a0\\\n\u00a0\u00a0--subnet-prefix\u00a0192.168.0.0/24\n#create app gateway\naz network application-gateway create -n agic -l westus2 \\\n  -g agic --sku Standard_v2 --public-ip-address agic-pip \\\n  --vnet-name agic-vnet --subnet agic-subnet\n</code></pre>"},{"location":"Azure/AKS/HTTPS/#setup-up-agic","title":"setup up AGIC","text":"<pre><code>#enable integration between cluster and app gateway\nappgwId=$(az network application-gateway \\\n  show -n agic -g agic -o tsv --query \"id\")\naz aks enable-addons -n handsonaks \\\n  -g rg-handsonaks -a ingress-appgw \\\n  --appgw-id $appgwId\n#peer app gateway network with AKS network\nnodeResourceGroup=$(az aks show -n handsonaks \\\n  -g rg-handsonaks -o tsv --query \"nodeResourceGroup\")\naksVnetName=$(az network vnet list \\\n\u00a0\u00a0-g\u00a0$nodeResourceGroup\u00a0-o\u00a0tsv\u00a0--query\u00a0\"[0].name\")\n\naksVnetId=$(az network vnet show -n $aksVnetName \\\n  -g $nodeResourceGroup -o tsv --query \"id\")\naz network vnet peering create \\\n  -n AppGWtoAKSVnetPeering -g agic \\\n  --vnet-name agic-vnet --remote-vnet $aksVnetId \\\n  --allow-vnet-access\n\nappGWVnetId=$(az network vnet show -n agic-vnet \\\n  -g agic -o tsv --query \"id\")\naz network vnet peering create \\\n  -n AKStoAppGWVnetPeering -g $nodeResourceGroup \\\n  --vnet-name $aksVnetName --remote-vnet $appGWVnetId --allow-vnet-\naccess\n</code></pre>"},{"location":"Azure/AKS/HTTPS/#add-ingress-rule","title":"add ingress rule","text":"<pre><code>#launch app\nkubectl create -f guestbook-all-in-one.yaml\n#create ingest\nkubectl apply -f simple-frontend-ingress.yaml\n#verify no external ip\nkubectl get service\n</code></pre>"},{"location":"Azure/AKS/HTTPS/#add-tls-to-ingress","title":"add TLS to ingress","text":"<pre><code>#install cert-manager\nkubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.2.0/cert-manager.yaml\n#install certificate issuer\nubectl create -f certificate-issuer.yaml\n#create TLS certificate and secury ingress\nkubectl apply -f ingress-with-tls.yaml\nkubectl\u00a0get\u00a0certificate\nkubectl\u00a0get\u00a0certificaterequest\nkubectl\u00a0describe\u00a0certificaterequest\n\n#clean up\nkubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.1.0/cert-manager.yaml\naz aks disable-addons -n handsonaks \\\n  -g rg-handsonaks -a ingress-appgw\n</code></pre>"},{"location":"Azure/AKS/Issue/","title":"Issue","text":""},{"location":"Azure/AKS/Issue/#unable-to-connect-to-the-server-dial-tcp-lookup-no-such-host","title":"Unable to connect to the server: dial tcp: lookup : no such host","text":"<p>The <code>Private Cluster</code> option is enabled while creating the AKS cluster. <pre><code>az aks show -n &lt;aks-name&gt; -g &lt;resource-group-name&gt; | grep private\n</code></pre></p>"},{"location":"Azure/AKS/Issue/#category-clienterror-code-upgradefailed-subcode-draindidnotcomplete","title":"Category: ClientError; Code: UpgradeFailed; SubCode: DrainDidNotComplete","text":"<p>https://learn.microsoft.com/en-us/troubleshoot/azure/azure-kubernetes/error-code-poddrainfailure</p> <p>Reason: If the <code>Allowed Disruption</code> value is 0, the node drain will <code>fail</code> during the upgrade process.</p> <p>Solution: Note that all the solution will not work if the app has been deployed via argocd helm charts. - Enable pods to drain <pre><code>kubectl edit pdb &lt;pdb-name&gt; -n &lt;pdb-namespace&gt; #then change allowedDisruptions\n</code></pre> - Back up, delete, and redeploy the PDB <pre><code>kubectl get pdb &lt;pdb-name&gt; -n &lt;pdb-namespace&gt; -o yaml &gt; pdb_backup.yaml\nkubectl delete pdb &lt;pdb-name&gt; -n &lt;pdb-namespace&gt;\nkubectl apply -f pdb_backup.yaml\n</code></pre> - Delete the pods that can't be drained</p>"},{"location":"Azure/AKS/Learn/","title":"Learn","text":"<ul> <li>https://github.com/HoussemDellai/docker-kubernetes-course/tree/main</li> </ul>"},{"location":"Azure/AKS/Learn/#best-practices","title":"best practices","text":"<p>https://cloudchronicles.blog/blog/AKS-Best-Practices-Part2-Cost-Efficiency/</p>"},{"location":"Azure/AKS/Links/","title":"Links","text":""},{"location":"Azure/AKS/Links/#blog-about-aks","title":"blog about aks","text":"<p>https://pixelrobots.co.uk/tag/azure/</p>"},{"location":"Azure/AKS/Links/#tutorials","title":"tutorials","text":"<p>https://docs.microsoft.com/en-us/azure/aks/quickstart-helm?tabs=azure-cli</p>"},{"location":"Azure/AKS/Logs/","title":"Logs","text":""},{"location":"Azure/AKS/Logs/#retrieve-cluster-autoscaler-logs-and-status","title":"Retrieve cluster autoscaler logs and status","text":"<p>https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler#using-the-autoscaler-profile</p> <p>https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler#retrieve-cluster-autoscaler-logs-and-status</p>"},{"location":"Azure/AKS/Logs/#connect-to-node","title":"connect to node","text":"<p>https://learn.microsoft.com/en-us/azure/aks/node-access</p> <p>Connect to Azure Kubernetes Service (AKS) cluster nodes for maintenance or troubleshooting</p>"},{"location":"Azure/AKS/Logs/#get-kubelet-logs","title":"get kubelet logs","text":"<p>https://learn.microsoft.com/en-us/azure/aks/kubelet-logs#get-kubelet-logs</p> <p>Get kubelet logs from Azure Kubernetes Service (AKS) cluster nodes</p>"},{"location":"Azure/AKS/MaintConfig/","title":"Maintenance Configure","text":""},{"location":"Azure/AKS/MaintConfig/#aks-maintainance-configuration","title":"aks maintainance configuration","text":"<pre><code>az aks maintenanceconfiguration list -g &lt;resource-group-name&gt; --cluster-name &lt;cluster-name&gt;\n</code></pre>"},{"location":"Azure/AKS/ManagedIdentity/","title":"Managed identity","text":""},{"location":"Azure/AKS/ManagedIdentity/#service-principal","title":"Service principal","text":"<p>not recommended: https://learn.microsoft.com/en-us/azure/aks/kubernetes-service-principal?tabs=azure-cli</p>"},{"location":"Azure/AKS/ManagedIdentity/#managed-identity_1","title":"managed identity","text":"<p>https://learn.microsoft.com/en-us/azure/aks/use-managed-identity</p> <p>Check type of managed identity the cluster is used: <pre><code>az aks show --name myAKSCluster --resource-group myResourceGroup --query identity.type --output tsv \n</code></pre> - If the cluster is using a managed identity, the value of the type property will be either <code>SystemAssigned</code> or <code>UserAssigned</code>. - If the cluster is using a service principal, the value of the type property will be <code>null</code>. </p> <p>Get the identity resource id: <pre><code>az aks show --name myAKSCluster --resource-group myResourceGroup --query \"identity\"\n</code></pre></p>"},{"location":"Azure/AKS/Monitor/","title":"Monitor","text":""},{"location":"Azure/AKS/Monitor/#azure-monitor","title":"azure monitor","text":"<ul> <li>Best to enable azure monitor.</li> <li>It provides valuable insights into the health and performance of your aks resources.</li> <li>The specific cost will depend on the amount of data you collect and the retention period you choose.</li> </ul>"},{"location":"Azure/AKS/Monitor/#commands-for-monitoring","title":"Commands for monitoring","text":"<pre><code>kubectl get &lt;resource type&gt; &lt;resource name&gt;\nkubectl describe &lt;resource type&gt; &lt;resource name&gt;\nkubectl logs &lt;pod name&gt;\n</code></pre>"},{"location":"Azure/AKS/Monitor/#kubectl-get","title":"kubectl get","text":"<p>Lists resources such as pods, ReplicaSets, ingresses, nodes, deployments, secrets, and so on. <pre><code>#show all deployments, ReplicaSets, pods, and services\nkubectl get all\n#get pods status\nkubectl get pods\n#output with extra columns\nkubectl get pods -o wide\n</code></pre></p>"},{"location":"Azure/AKS/Monitor/#kubectl-describe","title":"kubectl describe","text":"<p>It contains the details of the object itself, as well as any recent events related to that object. <pre><code>#just pods\nkubectl describe pods\n#a particular pod\nkubectl describe pod/&lt;pod-name&gt; #or use space: kubectl describe pod &lt;pod-name&gt;\n#get all events in a cluster\nkubectl get events\n</code></pre></p>"},{"location":"Azure/AKS/Monitor/#debug-app","title":"debug app","text":""},{"location":"Azure/AKS/Monitor/#image-pull-error","title":"image pull error","text":"<pre><code>#create error\nkubectl edit deployment/frontend\nkubectl get pods\n#get full error details\nkubectl describe pods/&lt;failed pod name&gt;\n</code></pre>"},{"location":"Azure/AKS/Monitor/#app-error","title":"app error","text":"<pre><code>kubectl get service\n#scale down frontend\nkubectl scale --replicas=1 deployment/frontend\n#launch bash shell on pod\nkubectl exec -it &lt;frontend-pod-name&gt; -- bash\n#install vim\napt update\napt install -y vim\n#update guestbook.php\nvim guestbook.php\n#get logs\nkubectl logs &lt;frontend-pod-name&gt;\n#get live log stream\nkubectl logs &lt;pod-name&gt; -f\n#solve error\nkubectl delete pod &lt;podname&gt;\n</code></pre>"},{"location":"Azure/AKS/Monitor/#probe","title":"probe","text":"<p>A liveness probe monitors the availability of an application while it is running. If a liveness probe fails, Kubernetes will restart your pod.</p> <p>A readiness probe monitors when your application becomes available. If a readiness probe fails, Kubernetes will not send any traffic to the unready pods.</p> <p>218</p>"},{"location":"Azure/AKS/Network/","title":"Network","text":""},{"location":"Azure/AKS/Network/#outbound-connections","title":"outbound connections","text":"<p>https://learn.microsoft.com/en-us/troubleshoot/azure/azure-kubernetes/basic-troubleshooting-outbound-connections</p> <p>Tools: - <code>kubectl</code> connect to the cluster - <code>apt-get</code> handle packages - <code>curl</code> - <code>host</code> DNS lookups - <code>netcat</code> (nc) TCP connections - <code>traceroute</code> print the trace of routing packets to the network host</p>"},{"location":"Azure/AKS/Network/#check-dns-resolution","title":"Check DNS resolution","text":"<p>If we cannot install the tools, we have to create a pod with the tools already being installed: https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/#create-a-simple-pod-to-use-as-a-test-environment <pre><code>kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml\n</code></pre></p> <p>create a test pod and install test packages <pre><code>kubectl run -it --rm aks-ssh --namespace &lt;namespace&gt; --image=debian:stable\napt-get update -y\napt-get install dnsutils -y\napt-get install curl -y\napt-get install netcat -y\n</code></pre></p>"},{"location":"Azure/AKS/Network/#check-if-cluster-can-reach-the-endpoint","title":"Check if cluster can reach the endpoint","text":"<pre><code>kubectl run -it --rm aks-ssh --namespace &lt;namespace&gt; --image=debian:stable\napt-get update -y\napt-get install traceroute -y\napt-get install netcat -y\n\n# Check the route to the endpoint\ntraceroute -T microsoft.com -m 50 -p 443\n\n# Check whether the desired port is open on the remote host\nnc -z -v microsoft.com 443\n\n# Check the HTTP response code\ncurl -Iv https://microsoft.com\n\n# Check whether we can connect to any other endpoint\ncurl -Iv https://kubernetes.io\n\n# Use -k or --insecure to ignore SSL certificate validation errors\ncurl -Ik https://google.com\n</code></pre>"},{"location":"Azure/AKS/NodePool/","title":"Node Pool","text":"<p>https://learn.microsoft.com/en-us/azure/aks/use-multiple-node-pools</p>"},{"location":"Azure/AKS/NodePool/#abort-operation-on-node-pool","title":"abort operation on node pool","text":"<pre><code>az aks nodepool operation-abort --name &lt;node-pool&gt; \\\n  --cluster-name &lt;cluster-name&gt; --resource-group &lt;resource-group&gt;\n</code></pre>"},{"location":"Azure/AKS/NodePool/#abort-operation-on-managed-cluster","title":"abort operation on managed cluster","text":"<pre><code>az aks operation-abort --name &lt;cluster-name&gt; --resource-group &lt;resource-group&gt;\n</code></pre>"},{"location":"Azure/AKS/NodePool/#spot-node-pool","title":"spot node pool","text":"<p>To schedule a pod to run on a Spot node, the pod should define - a <code>toleration</code> that corresponds to the <code>kubernetes.azure.com/scalesetpriority=spot:NoSchedule</code> taint and - a <code>node affinity</code> that corresponds to the <code>kubernetes.azure.com/scalesetpriority=spot</code> label <pre><code>spec:\n  containers:\n  - name: spot-example\n  tolerations:\n  - key: \"kubernetes.azure.com/scalesetpriority\"\n    operator: \"Equal\"\n    value: \"spot\"\n    effect: \"NoSchedule\"\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: \"kubernetes.azure.com/scalesetpriority\"\n            operator: In\n            values:\n            - \"spot\"\n</code></pre></p>"},{"location":"Azure/AKS/PipelineAgent/","title":"Pipeline agent","text":""},{"location":"Azure/AKS/PipelineAgent/#self-hosted-windows-agents","title":"Self-hosted Windows agents","text":"<p>https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/v2-windows?view=azure-devops</p>"},{"location":"Azure/AKS/PrivateCluster/","title":"Private Cluster","text":""},{"location":"Azure/AKS/PrivateCluster/#access-a-public-url-from-a-private-aks-cluster","title":"access a public URL from a private AKS cluster","text":"<p>https://www.linkedin.com/pulse/how-access-public-url-from-private-aks-cluster-naninga-karunaratne/</p> <p>https://www.reddit.com/r/AZURE/comments/13p79ml/public_access_to_api_running_within_a_private_aks/</p>"},{"location":"Azure/AKS/Scale/","title":"Builing scalable apps","text":"<p>Scale app using Horizontal Pod Autoscaler (HPA). Scale the cluster using cluster autoscaler.</p>"},{"location":"Azure/AKS/Scale/#scale-app","title":"scale app","text":"<p>Scale dimensions include the number of pods a deployment has, and the number of nodes in the cluster.</p> <pre><code>kubectl edit service frontend  #edit service\nkubectl get service -w         #watch service\n</code></pre>"},{"location":"Azure/AKS/Scale/#scale-pods","title":"scale pods","text":""},{"location":"Azure/AKS/Scale/#mannual-scale-pods","title":"mannual scale pods","text":"<pre><code>kubectl get pods\nkubectl get pods -o wide                       #show Ip and running status\nkubectl scale deployment/frontend --replicas=6 #add additional pods to the deployment\n</code></pre>"},{"location":"Azure/AKS/Scale/#auto-scale-pods-using-hpa","title":"auto scale pods using hpa","text":"<pre><code>#hap.ymal\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: frontend-scaler\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: frontend\n  minReplicas: 1\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 50\n</code></pre> <pre><code>kubectl create -f hpa.yaml  #create hpa\nkubectl get hpa             #check hpa\nkubectl get pods -w         #watch pods\n\n#install and run hey (tiny program sending loads to web app) from cloud shell\nexport GOPATH=~/go\nexport PATH=$GOPATH/bin:$PATH\ngo get -u github.com/rakyll/hey\nhey -z 20m http://&lt;external-ip&gt; #create up to 20 million connections\n\nkubectl describe hpa  #closer look of hpa\nkubectl get hpa -w    #check hpa\n\n#clean up resources\nkubectl delete -f hpa.yaml\nkubectl delete -f guestbook-all-in-one.yaml\n</code></pre>"},{"location":"Azure/AKS/Scale/#scale-cluster","title":"scale cluster","text":"<pre><code>kubectl create -f guestbook-all-in-one.yaml          #launch guestbook app\nkubectl scale deployment redis-replica --replicas 5  #mannually scale out redis-replica\nkubectl get pods                                     #check pods\n\n#azure cli configure cluster autoscaler\naz aks nodepool update --enable-cluster-autoscaler \\\n  -g rg01 --cluster-name myaks \\\n  --name agentpool --min-count 1 --max-count 2\n\nkubectl get nodes -w  #watch nodes\nkubectl get pods      #watch pods\n\n#clean up resources\nkubectl delete -f guestbook-all-in-one.yaml\n#disable cluster autoscaler\naz aks nodepool update --disable-cluster-autoscaler \\\n  -g rg01 --cluster-name myaks --name agentpool\n#scale cluster to 2 nodes\naz aks nodepool scale --node-count 2 -g rg01 \\\n  --cluster-name handsonaks --name agentpool\n</code></pre>"},{"location":"Azure/AKS/Upgrade/","title":"upgrade app","text":""},{"location":"Azure/AKS/Upgrade/#changing-yaml-file","title":"changing yaml file","text":"<pre><code>code guestbook-all-in-one.yaml              #edit yaml\nkubectl apply -f guestbook-all-in-one.yaml  #apply changes\nkubectl get service                         #get service public IP\ncode guestbook-all-in-one.yaml              #change version\nkubectl apply -f guestbook-all-in-one.yaml \\\n  &amp;&amp; kubectl get pods -w                    #apply and check\nkubectl get events | grep ReplicaSet        #show rolling update strategy\nkubectl get replicaset                      #verify\nkubectl rollout history deployment frontend #show rollout history\nkubectl rollout undo deployment frontend    #rollback deployment\nkubectl get replicaset                      #verify\nkubectl delete -f guestbook-all-in-one.yaml #clean up\n</code></pre>"},{"location":"Azure/AKS/Upgrade/#using-kubectl-edit","title":"using kubectl edit","text":"<p>This will not work in an automated environment. <pre><code>git reset --hard                            #undo changes\nkubectl create -f guestbook-all-in-one.yaml #deploy app\nkubectl edit service frontend               #edit service\nkubectl get svc -w                          #watch service\n</code></pre></p>"},{"location":"Azure/AKS/Upgrade/#using-kubectl-patch","title":"using kubectl patch","text":"<p>This can make automated changes, when don't have access to the original YAML file, e.g. in a script or in a continuous integration/ continuous deployment system.</p>"},{"location":"Azure/AKS/Upgrade/#yaml-patch-file","title":"yaml patch file","text":"<pre><code>#create ymal file\ncode frontend-image-patch.yaml\n#yaml file\nspec:\n  template:\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google-samples/gb-frontend:v3\n#apply the patch\nkubectl patch deployment frontend \\\n  --patch \"$(cat frontend-image-patch.yaml)\"\n#verify changes\nkubectl describe deployment frontend\n</code></pre>"},{"location":"Azure/AKS/Upgrade/#json-inline","title":"json inline","text":"<pre><code>kubectl patch deployment frontend \\\n--patch='\n{\n  \"spec\": {\n    \"template\": {\n      \"spec\": {\n        \"containers\": [{\n          \"name\": \"php-redis\",\n          \"image\": \"gcr.io/google-samples/gb-frontend:v4\"\n        }]\n      }\n    }\n  }\n}\n'\n#remove app from cluster\nkubectl delete -f guestbook-all-in-one.yaml\n</code></pre>"},{"location":"Azure/AKS/Upgrade/#using-helm","title":"using Helm","text":"<pre><code>#force an update of the image of the MariaDB container\nhelm install wp bitnami/wordpress\n#check current image version\nkubectl describe statefulset wp-mariadb | grep Image\n#get MariaDB passwords from secrets in aks\nkubectl get secret wp-mariadb -o yaml\n#get decoded password\necho \"&lt;password&gt;\" | base64 -d\n#get WordPress password\nkubectl get secret wp-wordpress -o yaml\necho \"&lt;WordPress password&gt;\" | base64 -d\n\n#update image tag with Helm and watch pods change\nhelm upgrade wp bitnami/wordpress \\\n  --set mariadb.image.tag=10.5.8-debian-10-r44\\\n  --set mariadb.auth.password=\"&lt;decoded password&gt;\" \\\n  --set mariadb.auth.rootPassword=\"&lt;decoded password&gt;\" \\\n  --set wordpressPassword=\"&lt;decoded password&gt;\" \\\n  &amp;&amp; kubectl get pods -w\n\n#show new image version\nkubectl describe pod wp-mariadb-0 | grep Image\n\n#clean up\nhelm delete wp\nkubectl delete pvc --all\nkubectl delete pv --all\n</code></pre>"},{"location":"Azure/AKS/VolumeBlob/","title":"Volume blob stoarge","text":""},{"location":"Azure/AKS/VolumeBlob/#nfs-30","title":"nfs 3.0","text":"<p>https://learn.microsoft.com/en-us/azure/storage/blobs/network-file-system-protocol-support-how-to</p> <p><code>No such file or directory</code>: - wrong container name or - the account isn't enabled for NFS 3.0</p> <p>To mount a container by using NFS 3.0: - you must create a storage account - you can't enable existing accounts</p> <p>Storage account features: - Hierarchical namespace: enabled - NFS V3: enabled</p>"},{"location":"Azure/AKS/VolumeBlob/#mount-azure-blob-storage-as-pv","title":"mount azure blob storage as pv","text":"<ul> <li>https://medium.com/@er.singh.nitin/mount-azure-blob-storage-to-aks-pod-358ee55040ac</li> <li>https://learn.microsoft.com/en-us/azure/aks/azure-csi-blob-storage-provision?tabs=mount-nfs%2Csecret</li> <li>https://learn.microsoft.com/en-us/troubleshoot/azure/azure-kubernetes/storage/mounting-azure-blob-storage-container-fail</li> </ul>"},{"location":"Azure/AKS/VolumeBlob/#blob-storage-csi-driver","title":"blob storage csi driver","text":"<p>enable/disable csi driver on aks <pre><code>az aks create --enable-blob-driver -n &lt;cluster-name&gt; -g &lt;resource-group-name&gt; #new aks\naz aks update --enable-blob-driver -n &lt;cluster-name&gt; -g &lt;resource-group-name&gt; #existing aks\naz aks update --disable-blob-driver -n &lt;cluster-name&gt; -g &lt;resource-group-name&gt; #disable\n</code></pre> Once we have enabled the driver, we should see two <code>StorageClass</code>es created in our cluster: <pre><code>azureblob-nfs-premium\nazureblob-fuse-premium\n</code></pre></p> <p>List all the CSI drivers available on the worker nodes <pre><code>kubectl describe csinodes\n</code></pre></p> <p>List <code>StorageClass</code>es available in aks <pre><code>kubectl get storageclass\n</code></pre></p>"},{"location":"Azure/AKS/VolumeBlob/#use-managed-identity-to-access-blob-storage-from-aks-pod","title":"use managed identity to access blob storage from aks pod","text":"<p>https://www.youtube.com/watch?v=azBHvHKsMhM</p> <p>https://github.com/HoussemDellai/docker-kubernetes-course/tree/main/47_blob_fuse_msi - Create <code>managed identity</code> - Assign <code>rbac role</code> - Attach <code>managed identity</code> to AKS VMSS - Configure <code>pv</code> with managed identity   <pre><code>csi:\n  volumeAttributes:\n    AzureStorageAuthType: msi  # key, sas, msi, spn\n    AzureStorageIdentityResourceID: $IDENTITY_ID\n</code></pre></p>"},{"location":"Azure/AKS/VolumeBlob/#create-persistent-volume","title":"create persistent volume","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  annotations:\n    pv.kubernetes.io/provisioned-by: blob.csi.azure.com\n  name: pv-blob\nspec:\n  storageClassName: azureblob-nfs-premium\n  persistentVolumeReclaimPolicy: Retain\n  capacity:\n    storage: 1Pi\n  accessModes:\n    - ReadWriteMany\n  mountOptions:\n    - nconnect=4\n  csi:\n    driver: blob.csi.azure.com\n    volumeHandle: &lt;storage-account&gt;-&lt;container-name&gt;\n    volumeAttributes:\n      resourceGroup: &lt;storage-resource-group&gt; # unique id\n      storageAccount: &lt;storage-account&gt;\n      containerName: &lt;container-name&gt;\n      AzureStorageAuthType: msi  # key, sas, msi, spn\n      AzureStorageIdentityResourceID: $IDENTITY_ID\n      protocol: nfs\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-blob\nspec:\n  volumeName: pv-blob\n  storageClassName: azureblob-nfs-premium\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"Azure/AKS/VolumeBlob/#create-app-pod","title":"create app pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-blob\n  namespace: retail\n  labels:\n    aadpodidbinding: managed-identity-name\nspec:\n  nodeSelector:\n    \"kubernetes.io/os\": linux\n  containers:\n    - name: nginx-blob\n      image: mcr.microsoft.com/oss/nginx/nginx:1.17.3-alpine\n      command:\n        - \"/bin/bash\"\n        - \"-c\"\n        - |\n          while true; do sleep 3600; done\n      volumeMounts:\n        - name: blob-storage\n          mountPath: /home/user/data\n          readOnly: false\n  volumes:\n    - name: blob-storage\n      persistentVolumeClaim:\n        claimName: pvc-blob\n</code></pre>"},{"location":"Azure/AKS/VolumeFile/","title":"Volume files storage","text":""},{"location":"Azure/AKS/VolumeFile/#mount-azure-file-storage-as-pv","title":"mount azure file storage as pv","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/aks/azure-csi-files-storage-provision</li> <li>https://github.com/HoussemDellai/aks-file-share</li> </ul>"},{"location":"Azure/AKS/VolumeFile/#pv-and-pvc","title":"pv and pvc","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-azurefile\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteMany\n  azureFile:\n    secretName: azure-secret\n    SecretNamespace: &lt;namespace&gt;\n    shareName: &lt;share-name (must already exist in the storage account)&gt;\n    readOnly: false\n  mountOptions:\n    - dir_mode=0777\n    - file_mode=0777\n    - uid=1000\n    - gid=1000\n    - mfsymlinks\n    - nobrl\n  persistentVolumeReclaimPolicy: Retain  \n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-azurefile\n  namespace: &lt;namespace&gt;\nspec:\n  accessModes:\n    - ReadWriteMany  \n  resources:\n    requests:\n      storage: 5Gi  \n  storageClassName: azurefile\n</code></pre>"},{"location":"Azure/AKS/VolumeFile/#create-a-secret-in-aks","title":"create a secret in aks","text":"<pre><code>kubectl create secret generic &lt;secret-name&gt; --from-literal=azurestorageaccountname=filestoragename --from-literal=azurestorageaccountkey=filestoragekey\n</code></pre>"},{"location":"Azure/AKS/VolumeFile/#test-pod","title":"test pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - image: mcr.microsoft.com/oss/nginx/nginx:1.15.5-alpine\n    name: my-pod-name\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 250m\n        memory: 256Mi\n    volumeMounts:\n      - name: azure\n        mountPath: /home/user/azure\n  volumes:\n  - name: azure\n    persistentVolumeClaim:\n      claimName: pvc-azurefile\n</code></pre>"},{"location":"Azure/AKS/VolumeFile/#custom-storage-class","title":"custom storage class","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: my-azurefile\nprovisioner: file.csi.azure.com\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\nmountOptions:\n  - dir_mode=0640  # default 0777\n  - file_mode=0640 # default 0777\n  - uid=0\n  - gid=0\n  - mfsymlinks\n  - cache=strict # https://linux.die.net/man/8/mount.cifs\n  - nosharesock\nparameters:\n  skuName: Standard_LRS\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-azurefile\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: my-azurefile\n  resources:\n    requests:\n      storage: 100Gi\n</code></pre>"},{"location":"Azure/AKS/VolumeFile/#private-azure-files-storage-private-endpoint","title":"private Azure Files storage (private endpoint)","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: private-azurefile-csi\nprovisioner: file.csi.azure.com\nallowVolumeExpansion: true\nparameters:\n  resourceGroup: &lt;resourceGroup&gt;\n  storageAccount: &lt;storageAccountName&gt;\n  server: &lt;storageAccountName&gt;.file.core.windows.net \nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nmountOptions:\n  - dir_mode=0777\n  - file_mode=0777\n  - uid=0\n  - gid=0\n  - mfsymlinks\n  - cache=strict  # https://linux.die.net/man/8/mount.cifs\n  - nosharesock  # reduce probability of reconnect race\n  - actimeo=30  # reduce latency for metadata-heavy workload\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: private-azurefile-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: private-azurefile-csi\n  resources:\n    requests:\n      storage: 100Gi\n</code></pre>"},{"location":"Azure/AKS/VolumeFile/#nfs-file-shares","title":"NFS file shares","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: azurefile-csi-nfs\nprovisioner: file.csi.azure.com\nallowVolumeExpansion: true\nparameters:\n  protocol: nfs\nmountOptions:\n  - nconnect=4\n  - rsize=262144\n  - wsize=262144\n</code></pre>"},{"location":"Azure/AKS/VolumeFile/#issue","title":"issue","text":"<p>mount a file storage: - https://stackoverflow.com/questions/74825923/aks-kubernetes-pod-volume-mounting-failing-even-after-pv-is-bound-and-attached - https://learn.microsoft.com/en-us/troubleshoot/azure/azure-kubernetes/storage/fail-to-mount-azure-file-share</p>"},{"location":"Azure/AKV/KeyVault/","title":"Key Vault","text":"<p>Azure Key Vault works on a <code>per-region</code> basis. If the app deploed in multile regions, multiple instances of key vault should be provisioned.</p>"},{"location":"Azure/AKV/KeyVault/#basic","title":"basic","text":"<pre><code>az config set defaults.location=westus2\naz group create --name &lt;your-resource-group&gt;\naz keyvault create \\\n  --resource-group &lt;your-resource-group&gt; \\\n  --name &lt;your-key-vault&gt;\n\naz keyvault secret set \\\n  --vault-name &lt;unique-keyvault-namee&gt;  \n  --name \"&lt;keyvault-secret-name&gt;\" \\\n  --value \"mysecretpassword\" \\\n</code></pre>"},{"location":"Azure/AKV/KeyVault/#save-secret-to-file","title":"Save secret to file","text":"<pre><code>az keyvault secret download \\\n  --vault-name &lt;keyvault-name&gt; \\\n  --name &lt;secret-name&gt; \\\n  --file &lt;filepath&gt;\n</code></pre>"},{"location":"Azure/AKV/KeyVault/#add-multi-line-secret","title":"Add multi-line secret","text":"<pre><code>az keyvault secret set \\\n  --vault-name \"&lt;keyvault-name&gt;\" \\\n  --name \"&lt;secret-name&gt;\" \\\n  --file \"secretfile.txt\"\naz keyvault secret set-attributes \\\n  --vault-name \"&lt;keyvault-name&gt;\" \\\n  --name \"&lt;secret-name&gt;\" \\\n  --content-type 'application/json'\n</code></pre>"},{"location":"Azure/AKV/KeyVault/#use-secret-in-aks","title":"use secret in aks","text":"<p>https://shailender-choudhary.medium.com/access-secrets-from-azure-key-vault-in-azure-kubernetes-service-e8efffe49427 Secret Provider Class (secret.yml) <pre><code>#using system-assigned identity to access key vault\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: my-secret-csi\n  namespace: dev\nspec:\n  provider: azure\n  parameters:\n    usePodIdentity: \"false\"\n    useVMManagedIdentity: \"true\"      #Set to true for using managed identity\n    userAssignedIdentityID: \"\"        #If empty, then defaults to use clientId of the system assigned identity on the VM\n    keyvaultName: my-kv-name\n    cloudName: \"AzurePublicCloud\"     #[OPTIONAL for Azure] if not provided, the Azure environment defaults to AzurePublicCloud\n    objects:  |\n      array:\n        - |\n          objectName: database--mysql\n          objectType: secret          # object types: secret, key, or cert\n          objectAlias: \"mysql.json\"   # [OPTIONAL] name of the secret\n          objectVersion: \"\"           # [OPTIONAL] object versions, default to latest if empty\n    tenantId: XXXXXXXXXXXX            # The tenant ID of the key vault\n</code></pre></p> <p>job.yml <pre><code>ok\n</code></pre></p>"},{"location":"Azure/AKV/KeyVault/#when-updated-will-take-effect","title":"when updated will take effect","text":"<p>When the secret/key is updated in external secrets store after the initial pod deployment, the updated secret will be periodically updated in the pod mount and the Kubernetes Secret.</p> <p>Depending on how the application consumes the secret data: - Mount Kubernetes secret as a volume: Use auto rotation feature + Sync K8s secrets feature in Secrets Store CSI Driver, application will need to watch for changes from the mounted Kubernetes Secret volume. When the Kubernetes Secret is updated by the CSI Driver, the corresponding volume contents are automatically updated. - Application reads the data from container\u2019s filesystem: Use rotation feature in Secrets Store CSI Driver, application will need to watch for the file change from the volume mounted by the CSI driver. - Using Kubernetes secret for environment variable: The pod needs to be restarted to get the latest secret as environment variable. Use something like https://github.com/stakater/Reloader to watch for changes on the synced Kubernetes secret and do rolling upgrades on pods.</p> <p>Additional configuration: https://learn.microsoft.com/en-us/azure/azure-arc/kubernetes/tutorial-akv-secrets-provider#additional-configuration-options</p>"},{"location":"Azure/AKV/Kubernetes/","title":"Kubernetes","text":""},{"location":"Azure/AKV/Kubernetes/#secrets-store-csi-driver-provider-azure","title":"secrets-store-csi-driver-provider-azure","text":"<p>https://github.com/Azure/secrets-store-csi-driver-provider-azure</p> <p>Azure Key Vault provider for Secret Store CSI driver allows you to get secret contents stored in Azure Key Vault instance and use the Secret Store CSI driver interface to mount them into Kubernetes pods.</p>"},{"location":"Azure/AKV/Kubernetes/#troubleshoot-key-vault-csi-secrets-store-csi-driver","title":"troubleshoot key-vault-csi-secrets-store-csi-driver","text":"<p>https://learn.microsoft.com/en-us/troubleshoot/azure/azure-kubernetes/extensions/troubleshoot-key-vault-csi-secrets-store-csi-driver</p> <p>check secrets store provider logs <pre><code>kubectl get pods \\\n    --selector 'app in (csi-secrets-store-provider-azure, secrets-store-provider-azure)' \\\n    --all-namespaces \\\n    --output wide\nkubectl logs &lt;provider-pod-name&gt; --since=1h | grep ^E\n</code></pre></p> <p>Check Secrets Store CSI driver logs <pre><code>kubectl get pods --selector app=secrets-store-csi-driver --all-namespaces --output wide\nkubectl logs &lt;driver-pod-name&gt; --container secrets-store --since=1h | grep ^E\n</code></pre></p>"},{"location":"Azure/AKV/Kubernetes/#keyvault-to-aks-container","title":"keyvault to aks container","text":"<p>https://medium.com/@bashaus/3-4-configuring-key-vault-to-expose-environment-variables-to-azure-kubernetes-services-48b633ec9e67</p> <p>To establish a connection between a Key Vault and an AKS container, we need to configure the following resources: - Azure <code>Key Vault</code> (with some keys, secrets or certificates). - A <code>servicve principal</code>using Role-Based Access Control (RBAC). - A <code>SecretProviderClass</code> \u2014 a Kubernetes resource (which requires installation) that describes instructions for how keys/secrets/certificates can be pulled from a Key Vault and stored as Secrets. - A <code>Secret</code> \u2014 a Kubernetes resource (which comes out of the box) that stores secrets in Kubernetes for use by other resources \u2014 it will be automatically generated by the <code>ServiceProviderClass</code>. - A deployment or <code>pod</code> to consume the Secret.</p>"},{"location":"Azure/AKV/Kubernetes/#mount-secret-as-env-var","title":"mount secret as env var","text":"<pre><code>containers:\n  - name: tfs-agent\n    env:\n      - name: ENV_VAR_K8S_SECRET\n        valueFrom:\n          secretKeyRef:\n            name: k8s_secret\n            key: name\n</code></pre>"},{"location":"Azure/AKV/Kubernetes/#mount-azure-keyvault-secret-as-env-var","title":"mount azure keyvault secret as env var","text":"<p>https://serverfault.com/questions/1075149/aks-with-azure-key-vault-env-variables-dont-load</p> <p>secrets.yaml <pre><code>apiVersion: secrets-store.csi.x-k8s.io/v1alpha1\nkind: SecretProviderClass\nmetadata:\n  name: azure-kv-secrets\n  namespace: dev\nspec:\n  provider: azure\n  parameters:\n    usePodIdentity: \"true\"\n    keyvaultName: \"my-keyvault\"                  \n    objects:  |\n      array:\n        - |\n          objectName: my-db-user\n          objectType: secret\n          #ObjectAlias: user.json\n          objectVersion: \"\"\n        - |\n          objectName: my-db-pass\n          objectType: secret\n          #ObjectAlias: pass.json\n          objectVersion: \"\"\n    tenantId: \"&lt;tenantID&gt;\"\n</code></pre></p>"},{"location":"Azure/AKV/Kubernetes/#azure-secretproviderclass-not-create-k8s-secret","title":"azure SecretProviderClass not create k8s secret","text":"<p>https://github.com/Azure/secrets-store-csi-driver-provider-azure/issues/714 - set <code>secrets-store-csi-driver.syncSecret.enabled=true</code> when installing the driver and provider with helm - install both driver and provider: The charts in this repo <code>https://github.com/Azure/secrets-store-csi-driver-provider-azure/tree/master/charts/csi-secrets-store-provider-azure</code> have the driver charts as dependency and will install the driver and provider - must mount the azure key-vault secret to a path in the pod to force creating the k8s secret: <code>https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/configurations/sync-with-k8s-secrets/#how-to-sync-mounted-content-with-kubernetes-secret</code></p>"},{"location":"Azure/AKV/Python/","title":"Python","text":""},{"location":"Azure/AKV/Python/#import","title":"import","text":"<pre><code>from azure.identity import DefaultAzureCredential\nfrom azure.keyvault.secrets import SecretClient\nfrom azure.core.exceptions import (    \n    ServiceRequestError,\n    ResourceNotFoundError,\n)\n</code></pre>"},{"location":"Azure/AKV/Python/#get-keyvault-secret","title":"get keyvault secret","text":"<p>https://stackoverflow.com/questions/64613699/how-to-add-azure-python-sdk-exceptions-to-try-except-statements <pre><code>def get_keyvault_secret(\n    vault_url: str,\n    secret_name: str,\n) -&gt; dict:\n    credential = DefaultAzureCredential()\n    client = SecretClient(vault_url=vault_url, credential=credential)\n\n    data = None\n    try:\n        secret = client.get_secret(secret_name)\n        data = json.loads(secret.value)\n    except ServiceRequestError:\n        msg = f'Error connecting to Azure Key Vault: {vault_url}'\n        warnings.warn(msg, RuntimeWarning)\n    except ResourceNotFoundError:\n        msg = (\n            f'Secret name `{secret_name}` not '\n            f'found in Azure Key Vault `{vault_url}`'\n        )\n        warnings.warn(msg, RuntimeWarning) \n    return data\n</code></pre></p>"},{"location":"Azure/ARM/ARM/","title":"ARM","text":"<p>Azure resource manager template.</p>"},{"location":"Azure/ARM/ARM/#storage-account","title":"storage account","text":"<pre><code>{\n   \"type\": \"Microsoft.Storage/storageAccounts\",\n   \"name\": \"[variables('storageAccountName')]\",\n   \"apiVersion\": \"2023-04-01\",\n   \"location\": \"[parameters('location')]\",\n   \"sku\": {\n      \"name\": \"Standard_LRS\"\n   },\n   \"kind\": \"Storage\",\n   \"properties\": {}\n}\n</code></pre>"},{"location":"Azure/ARM/ARM/#virtual-network","title":"virtual network","text":"<pre><code>{\n   \"apiVersion\": \"2023-04-01\",\n   \"type\": \"Microsoft.Network/virtualNetworks\",\n   \"name\": \"[variables('virtualNetworkName')]\",\n   \"location\": \"[parameters('location')]\",\n   \"properties\": {\n      \"addressSpace\": {\n         \"addressPrefixes\": [\n            \"[variables('addressPrefix')]\"\n         ]\n      },\n      \"subnets\": [\n         {\n            \"name\": \"[variables('subnetName')]\",\n            \"properties\": {\n               \"addressPrefix\": \"[variables('subnetPrefix')]\"\n            }\n         }\n      ]\n   }\n}\n</code></pre>"},{"location":"Azure/ARM/ARM/#public-ip-address","title":"public ip address","text":"<pre><code>{\n   \"apiVersion\": \"2023-04-01\",\n   \"type\": \"Microsoft.Network/publicIPAddresses\",\n   \"name\": \"[variables('publicIPAddressName')]\",\n   \"location\": \"[parameters('location')]\",\n   \"properties\": {\n      \"publicIPAllocationMethod\": \"Dynamic\",\n      \"dnsSettings\": {\n         \"domainNameLabel\": \"[parameters('dnsLabelPrefix')]\"\n      }\n   }\n}\n</code></pre>"},{"location":"Azure/ARM/ARM/#network-interface","title":"network interface","text":"<pre><code>{\n   \"apiVersion\": \"2023-04-01\",\n   \"type\": \"Microsoft.Network/networkInterfaces\",\n   \"name\": \"[variables('nicName')]\",\n   \"location\": \"[parameters('location')]\",\n   \"dependsOn\": [\n      \"[resourceId('Microsoft.Network/publicIPAddresses/', variables('publicIPAddressName'))]\",\n      \"[resourceId('Microsoft.Network/virtualNetworks/', variables('virtualNetworkName'))]\"\n   ],\n   \"properties\": {\n      \"ipConfigurations\": [\n         {\n            \"name\": \"ipconfig1\",\n            \"properties\": {\n               \"privateIPAllocationMethod\": \"Dynamic\",\n               \"publicIPAddress\": {\n                  \"id\": \"[resourceId('Microsoft.Network/publicIPAddresses',variables('publicIPAddressName'))]\"\n               },\n               \"subnet\": {\n                  \"id\": \"[variables('subnetRef')]\"\n               }\n            }\n         }\n      ]\n   }\n}\n</code></pre>"},{"location":"Azure/ARM/ARM/#virtual-machine","title":"virtual machine","text":"<pre><code>{\n   \"apiVersion\": \"2023-04-01\",\n   \"type\": \"Microsoft.Compute/virtualMachines\",\n   \"name\": \"[variables('vmName')]\",\n   \"location\": \"[resourceGroup().location]\",\n   \"tags\": {\n      \"displayName\": \"VirtualMachine\"\n    },\n   \"dependsOn\": [\n      \"[concat('Microsoft.Storage/storageAccounts/', variables('storageAccountName'))]\",\n      \"[concat('Microsoft.Network/networkInterfaces/', variables('nicName'))]\"\n   ],\n   \"properties\": {\n      \"hardwareProfile\": { \"vmSize\": \"[variables('vmSize')]\" },\n      \"availabilitySet\": {\n         \"id\": \"[resourceId('Microsoft.Compute/availabilitySets', parameters('adAvailabilitySetName'))]\"\n       },\n      \"osProfile\": {\n         \"computerName\": \"[variables('vmName')]\",\n         \"adminUsername\": \"[parameters('adminUsername')]\",\n         \"adminPassword\": \"[parameters('adminPassword')]\"\n       },\n      \"storageProfile\": {\n         \"imageReference\": {\n            \"publisher\": \"[variables('imagePublisher')]\",\n            \"offer\": \"[variables('imageOffer')]\",\n            \"sku\": \"[parameters('windowsOSVersion')]\",\n            \"version\": \"latest\"\n         },\n         \"osDisk\": { \"createOption\": \"FromImage\" },\n         \"copy\": [\n            {\n               \"name\": \"dataDisks\",\n               \"count\": 2,\n               \"input\": {\n                  \"lun\": \"[copyIndex('dataDisks')]\",\n                  \"createOption\": \"Empty\",\n                  \"diskSizeGB\": \"512\",\n                  \"name\": \"[concat(variables('vmName'), '-datadisk', copyIndex('dataDisks'))]\"\n                }\n             }\n          ]\n        },\n         \"networkProfile\": {\n         \"networkInterfaces\": [\n             {\n               \"id\": \"[resourceId('Microsoft.Network/networkInterfaces', variables('nicName'))]\"\n             }\n          ]\n       }\n    }\n }\n</code></pre>"},{"location":"Azure/Architecture/LandingZone/","title":"Landng Zones","text":"<p>https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/landing-zone/</p> <p>An Azure landing zone is a blueprint that defines a secure and well-architected foundation for deploying your workloads in Microsoft Azure. It's essentially a set of design principles and best practices that help you organize your Azure environment effectively.</p>"},{"location":"Azure/Architecture/LandingZone/#deployment-tools","title":"Deployment tools","text":"<ul> <li>For most organizations, Azure Landing Zone Accelerators (portal, Bicep, or Terraform) are the best option due to their ease of use and reduced risk.</li> <li>If you have a strong IaC background and require a highly customized landing zone, consider custom IaC templates.</li> <li>Manual deployment is generally not recommended due to the complexity and potential for errors.</li> </ul>"},{"location":"Azure/Architecture/LandingZone/#azure-landing-zone-characteristics","title":"Azure landing zone characteristics","text":"<ul> <li> <p>Multi-subscription Architecture: It utilizes multiple Azure subscriptions to isolate and manage different aspects of your cloud environment. This helps with organization, security, and access control. There are typically two types of subscriptions:</p> <ul> <li>Platform Landing Zones: These contain shared services used by various workloads across your organization.</li> <li>Application Landing Zones: These are dedicated subscriptions for deploying specific applications or workloads.</li> </ul> </li> <li> <p>Scalability and Modularity: The architecture is designed to be scalable and modular. This means you can easily add new resources or services as your cloud adoption grows, and individual components can be modified to meet specific needs.</p> </li> <li> <p>Security and Governance:  Security and governance are central principles in an Azure landing zone. The architecture incorporates features like Azure Policy, Role-Based Access Control (RBAC), and Azure Blueprints to enforce security best practices, manage access controls, and ensure compliance with regulations.</p> </li> <li> <p>Standardized Infrastructure: The landing zone approach promotes a standardized way to deploy and manage your Azure resources. This simplifies operations, reduces the risk of errors, and makes it easier to manage your cloud environment at scale.</p> </li> </ul>"},{"location":"Azure/Architecture/LandingZone/#benefits-of-using-an-azure-landing-zone","title":"Benefits of using an Azure landing zone","text":"<ul> <li>Improved Security and Governance:  By following best practices and enforcing policies, landing zones help you maintain a secure and compliant cloud environment.</li> <li>Simplified Management: Standardized infrastructure and centralized management tools make it easier to manage and monitor your Azure resources.</li> <li>Scalability and Agility: The modular design allows you to easily scale your cloud environment and deploy new workloads quickly.</li> <li>Cost Optimization: Landing zones can help you optimize your cloud costs by promoting efficient resource utilization and avoiding unnecessary configurations.</li> </ul>"},{"location":"Azure/Automation/Account/","title":"Account","text":"<p>https://blog.johnfolberth.com/deploying-azure-automation-account-and-runbooks-via-terraform/</p> <ul> <li>In Azure Automation no components talk to each other directly</li> <li>Automation account is managed by a management service</li> <li>The management service is a single point of contact for all activities within Azure Automation</li> <li>All requests from the portal are sent to the automation management service</li> </ul>"},{"location":"Azure/Automation/Account/#run-as-account","title":"Run As account","text":"<ul> <li>Azure Automation accounts do not have access to any resources by default</li> <li>Run As account is one way to provide access to subscriptions and the resources</li> </ul>"},{"location":"Azure/Automation/Runbook/","title":"Runbook","text":"<p>https://learn.microsoft.com/en-us/azure/automation/manage-runbooks</p>"},{"location":"Azure/Automation/Webhook/","title":"Webhook","text":"<p>Webhooks accept endpoint URLs containing custom logic automatically passing in the necessary parameters, and then execute the login available therein</p>"},{"location":"Azure/AzCLI/AzCopy/","title":"AzCopy","text":""},{"location":"Azure/AzCLI/AzCopy/#move-files-in-blob-storage-using-azcopy","title":"move files in blob storage using AzCopy","text":"<p>https://gist.github.com/ehrnst/bb63af04541060af7bdde66f926ac849</p>"},{"location":"Azure/AzCLI/AzCopy/#copy-new-files-only","title":"copy new files only","text":""},{"location":"Azure/AzCLI/AzureCLI/","title":"Azure CLI","text":"<p>https://docs.microsoft.com/en-us/cli/azure/vm?view=azure-cli-latest</p>"},{"location":"Azure/AzCLI/AzureCLI/#install","title":"install","text":"<p>https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-linux?pivots=apt</p> <p>ubuntu <pre><code># remove old version 2.0.81 package\nsudo apt remove azure-cli -y &amp;&amp; sudo apt autoremove -y\n# install the latest version\ncurl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre></p>"},{"location":"Azure/AzCLI/AzureCLI/#upgrade","title":"upgrade","text":"<pre><code>az version\naz upgrade -y\naz extension update --name aks-preview\n</code></pre> <p>if have legacy kubectl must be migrate to the new repo first: https://kubernetes.io/blog/2023/08/15/pkgs-k8s-io-introduction/</p>"},{"location":"Azure/AzCLI/AzureCLI/#new-line","title":"new line","text":"<ul> <li>bash: use backslash \\</li> <li>pwsh: use a backtick `</li> <li>batch: use accent circonflexe ^</li> </ul>"},{"location":"Azure/AzCLI/AzureCLI/#list-all-subscriptions","title":"List all subscriptions","text":"<pre><code>az account list --output table\naz account set --subscription &lt;subscription-id&gt; #set default subscription\n</code></pre>"},{"location":"Azure/AzCLI/AzureCLI/#list-all-regions","title":"List all regions","text":"<pre><code>az account list-locations --output table #list all locations\naz vm list-sizes --location \"westus\"     #list available vm sizes in a specific location\n</code></pre>"},{"location":"Azure/AzCLI/AzureCLI/#resource-group","title":"resource group","text":"<pre><code>az group list\naz group list -o table\naz group create --name rg01 --location australiaeast\naz group delete --name rg01\n</code></pre>"},{"location":"Azure/AzCLI/AzureCLI/#service-principal","title":"service principal","text":"<pre><code>$sp = az ad sp create-for-rbac --name &lt;sp-name&gt; | ConvertFrom-Json\naz ad sp create-for-rbac --name &lt;sp-name&gt; --role Contributor --scopes /subscriptions/&lt;subscription-id&gt;\n</code></pre>"},{"location":"Azure/AzCLI/AzureCLI/#storage-account","title":"storage account","text":"<pre><code>#create\naz storage account create --name &lt;storage-account-name&gt; `\n--resource-group rg01 --location australiaeast --sku Standard_RAGRS --kind StorageV2\n\n#delete\naz storage account delete --name &lt;storage-account-name&gt; --resource-group &lt;resource-group&gt;\n</code></pre>"},{"location":"Azure/AzCLI/AzureCLI/#key-vault","title":"key vault","text":"<pre><code>az keyvault create --location $region --name &lt;kv-name&gt; `\n--resource-group &lt;resource-group&gt; --enabled-for-template-deployment true\n\naz keyvault secret set --name &lt;secret-name&gt; --value $sp.password --vault-name &lt;vault-name&gt;\naz keyvault secret set --name &lt;secret-name&gt; --value \"A string value\" --vault-name &lt;vault-name&gt;\n\n#allow pipeline to access the keyvault\naz keyvault set-policy --name &lt;name&gt; --spn $spIdUri --secret-permissions get list\n</code></pre>"},{"location":"Azure/AzCLI/AzureCLI/#connect-to-vm","title":"Connect to VM","text":"<pre><code>Get-AzRemoteDesktopFile -ResourceGroupName \"RgName\" -Name \"VmName\" -Launch #connect to VM using PowerShell\nGet-AzRemoteDesktopFile -ResourceGroupName \"RgName\" -Name \"VmName\" -LocalPath \"C:\\Path\\to\\folder\" #save RDP file for future use\n</code></pre>"},{"location":"Azure/AzCLI/Feature/","title":"Feature","text":""},{"location":"Azure/AzCLI/Feature/#show-feature","title":"show feature","text":"<pre><code>az feature show -n AutoUpgradePreview --namespace Microsoft.ContainerService\n</code></pre>"},{"location":"Azure/AzCLI/Feature/#register-feature","title":"register feature","text":"<pre><code>az feature register --namespace Microsoft.ContainerService -n AutoUpgradePreview\naz feature registration show --provider-namespace Microsoft.ContainerService -n AutoUpgradePreview\n</code></pre>"},{"location":"Azure/AzCLI/Install/","title":"Install","text":""},{"location":"Azure/AzCLI/Install/#specific-version","title":"specific version","text":"<p>https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-linux?pivots=apt</p> <p>check available versions: <pre><code>apt-cache policy azure-cli\n</code></pre></p> <p>Install <pre><code>AZ_VER=2.51.0              # Azure CLI version\nAZ_DIST=$(lsb_release -cs) # currently installed distribution\n# Install a specific version\nsudo apt-get install azure-cli=${AZ_VER}-1~${AZ_DIST}\n</code></pre></p>"},{"location":"Azure/AzCLI/RestAPI/","title":"Rest API","text":"<p>https://learn.microsoft.com/en-us/cli/azure/use-azure-cli-rest-command?tabs=bash</p>"},{"location":"Azure/AzCLI/RestAPI/#using-rest-api-to-patch-blockblobstorage-property","title":"Using rest api to patch blockblobstorage property","text":"<p>In windows terminal, we escape the double quote by backslash <pre><code>az rest --method get --url xxx\n\naz rest --method patch\n--url https://management.azure.com/subscriptions/subscription_id/resourceGroups/resource_group_id/providers/Microsoft.Storage/storageAccounts/block_blob_storage_name?api-version=2023-05-01\n--body \"{\\\"properties\\\": {\\\"accessTier\\\": null}}\"\n</code></pre></p> <p>in linux <pre><code>--body '{\"properties\": {\"accessTier\": null}}'\n</code></pre></p>"},{"location":"Azure/AzPwsh/Blob/","title":"Blob","text":""},{"location":"Azure/AzPwsh/Blob/#move-files-within-blob-storage-based-on-dates","title":"move files within blob storage based on dates","text":"<pre><code># Set storage account name, container name, destination root folder and number of days to keep old blobs\n$accountName = \"storage-account-name\"\n$containerName = \"container-name\"\n$destinationRootFolder = \"\"\n$daysToKeep = 5\n\n# Create a storage context with the given account name\n$context = New-AzStorageContext -StorageAccountName $accountName -UseConnectedAccount\n\n# Get all the blobs from the container using the storage context\n$blobs = Get-AzStorageBlob -Context $context -Container $containerName\n\n# Loop through each blob.\nforeach ($blob in $blobs) {\n    # Get the name of the blob\n    $blobName = $blob.Name\n\n    # Extract the timestamp string from the blob name using a regular expression\n    $blobTimestampString = ($blobName -split '[-.]')[1]\n\n    # Convert the timestamp string to a datetime object using the specified format\n    $blobTimestamp = [datetime]::ParseExact($blobTimestampString, \"yyyyMMddHHmmss\", $null)\n\n    # Calculate the age of the blob in days\n    $ageInDays = (Get-Date) - $blobTimestamp\n\n    # Check if the blob is older than the specified number of days\n    if ($ageInDays.Days -gt $daysToKeep) {\n        # Construct the destination folder path using the year and month from the blob timestamp\n        $destinationFolder = Join-Path $destinationRootFolder ($blobTimestamp.ToString(\"yyyy/MM\"))\n\n        # Construct the destination blob name by appending the folder path to the original blob name\n        $destinationBlobName = Join-Path $destinationFolder $blobName\n\n        # Move the blob to the destination using the storage context and output a message\n        Write-Output \"Moving blob '$blobName' to '$destinationBlobName'\"\n        Move-AzStorageBlob -Context $context -Container $containerName -Blob $blobName -Destination $destinationBlobName\n    }\n}\n</code></pre>"},{"location":"Azure/AzPwsh/Error/","title":"Error","text":""},{"location":"Azure/AzPwsh/Error/#trouble-shooting","title":"Trouble shooting","text":"<p>https://learn.microsoft.com/en-us/powershell/azure/troubleshooting?view=azps-8.3.0</p>"},{"location":"Azure/AzPwsh/Error/#error-sharedtokencachecredential-authentication-unavailable","title":"Error: SharedTokenCacheCredential authentication unavailable","text":"<p>Run the command first: <pre><code>Connect-AzAccount -Tenant &lt;tenant-id&gt;\n</code></pre></p>"},{"location":"Azure/AzPwsh/Error/#error-command-was-found-in-the-module-azaccounts-but-the-module-could-not-be-loaded","title":"Error: command was found in the module 'Az.Accounts', but the module could not be loaded","text":"<p>have both the Az and AzureRM PowerShell modules installed on the same Windows-based system.</p> <p>AzureRM PowerShell modules will be retired on 29 February 2024. Delete AzureRM: <pre><code>Uninstall-AzureRm\n</code></pre></p> <p>module <code>Az.Compute</code> same error:</p> <p><pre><code>The 'Get-AzComputeResourceSku' command was found in the module 'Az.Compute', but the module could not be loaded. For more information, run 'Import-Module Az.Compute'.\n</code></pre> Solution: run <code>Import-Module Az.Compute</code> to show the error details and solution.</p>"},{"location":"Azure/AzPwsh/Install/","title":"Install","text":""},{"location":"Azure/AzPwsh/Install/#uninstall-azurerm","title":"uninstall AzureRM","text":"<p>AzureRM and AZ cannot be co-exist: <code>We do not support having both the AzureRM and Az PowerShell modules installed in Windows PowerShell 5.1 at the same time</code> <pre><code>Uninstall-AzureRm #prefered method?\nUninstall-Module -Name AzureRM -AllVersions\n</code></pre></p>"},{"location":"Azure/AzPwsh/Install/#install-az","title":"install AZ","text":"<pre><code>Install-Module -Name Az -Repository PSGallery -Force\nUpdate-Module -Name Az -Force\n</code></pre>"},{"location":"Azure/AzPwsh/Logs/","title":"Logs","text":"<p>The location of PowerShell logs is dependent on the target platform.</p> <p>On Linux, PowerShell logs to <code>syslog</code> and rsyslog.conf can be used.</p> <p>https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_logging_non-windows?view=powershell-7.2</p>"},{"location":"Azure/AzPwsh/Logs/#find","title":"find","text":""},{"location":"Azure/AzPwsh/Subscription/","title":"subscription","text":""},{"location":"Azure/AzPwsh/Subscription/#subscription_1","title":"subscription","text":"<pre><code>Get-AzSubscription\nGet-AzSubscription | Select-Object SubscriptionName, SubscriptionId\n</code></pre>"},{"location":"Azure/AzPwsh/Subscription/#connect","title":"connect","text":"<pre><code>Connect-AzAccount\nSet-AzContext -SubscriptionId 12345678-90ab-cdef-ghij-klmnopqrstuv\n</code></pre>"},{"location":"Azure/Blog/PrivateDNSZone/","title":"Private DNS Zone","text":"<p>how to solve domain name could not be resolved</p>"},{"location":"Azure/Blog/PrivateDNSZone/#relationship-from-on-prem-to-azure-cloud","title":"relationship from on-prem to azure cloud","text":""},{"location":"Azure/Blog/PrivateDNSZone/#connection-from-on-prem-to-azure-via-npn-and-private-link","title":"connection from on-prem to azure via npn and private link","text":"<ul> <li>firewall?</li> <li>inside the cloud?</li> <li>DNS works? deleted? how to test DNS resolver?</li> </ul>"},{"location":"Azure/Compute/Function/","title":"Function","text":""},{"location":"Azure/Compute/Function/#type","title":"type","text":"<ul> <li>On-demand functions</li> <li>Scheduled functions</li> <li>Event-based functions</li> </ul>"},{"location":"Azure/Compute/Function/#runtime","title":"runtime","text":"<ul> <li>Functions are hosted within Azure App Service</li> <li>App Service loads the Azure runtime</li> <li>On arrival of a request or the occurrence of a trigger, App Service </li> <li>loads the incoming payload</li> <li>reads the function.json file to find the function's bindings and trigger</li> <li>maps the incoming data to incoming parameters, and </li> <li>invokes the function with parameter values</li> <li>Once the function completes its execution, the value is passed back to the runtime by way of an outgoing parameter defined as a binding in the function.json file</li> <li>The function runtime returns the values to the caller </li> </ul>"},{"location":"Azure/Compute/Function/#binding","title":"binding","text":"<p>Binding is the process of creating a connection between - the incoming data and  - the Azure function along with  - mapping the data types</p> <p>Connection type - a single direction from runtime to function and vice versa or  - multi directional: transmit data between runtime and function in both directions - </p>"},{"location":"Azure/Compute/Function/#trigger","title":"trigger","text":"<p>Trigger can  - invoke a function based on external events - pass the incoming data, payload, and metadata to the function</p>"},{"location":"Azure/Compute/Function/#function-proxies","title":"Function Proxies","text":"<p>can be used to call multiple functions with with a single function URL </p>"},{"location":"Azure/Compute/Function/#durable-functions","title":"Durable Functions","text":"<p>output can be saved to local variable and the state is preserved</p>"},{"location":"Azure/Compute/VM/","title":"VM","text":"<p>https://learn.microsoft.com/en-us/azure/virtual-machines/dedicated-host-memory-optimized-skus</p> <p>https://www.cbtnuggets.com/blog/certifications/microsoft/how-to-choose-the-best-virtual-machine-for-your-workload-in-azure</p>"},{"location":"Azure/Compute/VM/#pricing","title":"pricing","text":"<p>https://azure.microsoft.com/en-ca/pricing/details/virtual-machines/linux/</p>"},{"location":"Azure/Compute/VM/#vm-cpu-quota","title":"vm cpu quota","text":"<p>https://learn.microsoft.com/en-us/azure/virtual-machines/linux/quotas</p> <p>Subscription &gt; Settings &gt; Usage + quotas <pre><code>az vm list-usage --location \"australiaeast\" -o table\n</code></pre></p>"},{"location":"Azure/Compute/VM/#how-to-select-the-vm-series","title":"how to select the VM series","text":"<ul> <li>D-Series: CRM systems, more extensive databases, web-servers with medium web traffic, small production apps, desktop virtualization, e-commerce applications, enterprise-level applications</li> <li>E-Series: SAP Hana, Netweaver, large databases, data warehousing, BI apps, analytical apps, apps that perform heavy processing like financial or weather apps, apps that require heavy in-memory use</li> </ul>"},{"location":"Azure/Data/Data/","title":"Data","text":""},{"location":"Azure/Data/Data/#data-factory","title":"data factory","text":"<p>ingest</p>"},{"location":"Azure/Data/Data/#data-lake-storage","title":"data lake storage","text":"<p>store</p>"},{"location":"Azure/Data/Data/#data-bricks","title":"data bricks","text":"<p>prepare</p>"},{"location":"Azure/Data/Data/#synapse","title":"synapse","text":"<p>serve</p>"},{"location":"Azure/Data/Data/#machine-learning","title":"machine learning","text":"<p>app</p>"},{"location":"Azure/Data/DataFactory/","title":"Data Factory","text":""},{"location":"Azure/Data/DataFactory/#terms","title":"Terms","text":"<ul> <li>Activities are individual tasks</li> <li>Pipelines are composed of groups of activities and are responsible for bringing activities together</li> <li>Datasets are the sources and destinations of data</li> <li>Linked services contain the connection and connectivity information for datasets and are utilized by individual tasks for connection</li> <li>Integration runtime is the main engine running the data factory</li> </ul>"},{"location":"Azure/Data/DataLakeStorage/","title":"Data Lake Storage","text":""},{"location":"Azure/Data/Databricks/","title":"Databricks","text":"<p>Databricks is pretty much <code>managed Apache Spark</code>.</p> <p>Azure Databricks provides data science, engineering, and analytical teams with a single platform for big data processing and machine learning.</p> <p>Azure Databricks is optimized for three specific types of data workload and associated user personas: - Data Science and Engineering - Machine Learning - SQL (premium tier only)</p>"},{"location":"Azure/Data/Databricks/#spark","title":"Spark","text":"<p>Apache Spark clusters are groups of computers that are treated as a single computer and handle the execution of commands issued from notebooks. - Work - Jobs - Stages - Tasks</p> <p>The Databricks appliance is deployed into Azure as a managed resource group within your subscription. This resource group contains - the driver and worker VMs for your clusters, along with - other required resources, including   - a virtual network,   - a security group, and   - a storage account.</p> <p>Internally, AKS is used to run the Azure Databricks control-plane and data-planes via containers.</p>"},{"location":"Azure/Data/Databricks/#setup","title":"Setup","text":"<ul> <li>create databrcks</li> <li>create data lake storage gen2</li> <li>config storage access scope   https://learn.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes \\   By using Azure Key Vault and Databricks Scope, we can access our storage without hard-coding secure information. \\   https://learn.microsoft.com/en-us/azure/databricks/dbfs/mounts</li> <li>create compute cluster (ML runtime)</li> <li>create notebook: attach cumpute cluster to notebook</li> </ul>"},{"location":"Azure/Data/DeltaLake/","title":"DeltaLake","text":"<p>Delta Lake is a storage layer that adds relational database semantics to Spark-based data lake processing.</p>"},{"location":"Azure/Data/DeltaLake/#managed-and-external-tables","title":"managed and external tables","text":"<p>Tables in a Spark catalog, including Delta Lake tables, can be managed or external.</p> <p>managed - managed table is defined without a specified location, and the data files are stored within the storage used by the metastore. - Dropping the table not only removes its metadata from the catalog, but also deletes the folder in which its data files are stored.</p> <p>external - external table is defined for a custom file location, where the data for the table is stored. - The metadata for the table is defined in the Spark catalog. - Dropping the table deletes the metadata from the catalog, but doesn't affect the data files</p>"},{"location":"Azure/Data/DeltaLake/#create-tables-from-dataframe","title":"create tables from dataframe","text":"<pre><code>df.write.format('delta').saveAsTable('managed_table')\ndf.write.format('delta').option('path', '/mydata').saveAsTable('external_table')\n</code></pre>"},{"location":"Azure/Data/DeltaLake/#create-tables-from-sql","title":"create tables from sql","text":"<pre><code>spark.sql(\"CREATE TABLE MyExternalTable USING DELTA LOCATION '/mydata'\")\n\n%sql\nCREATE TABLE MyExternalTable\nUSING DELTA\nLOCATION '/mydata'\n</code></pre>"},{"location":"Azure/Data/DeltaLake/#table-schema","title":"table schema","text":"<p>define the table schema by specifying the column <code>names</code>, <code>types</code>, and <code>nullability</code> as part of the CREATE TABLE statement. <pre><code>%sql\n\nCREATE TABLE ManagedSalesOrders\n(\n    Orderid INT NOT NULL,\n    OrderDate TIMESTAMP NOT NULL,\n    CustomerName STRING,\n    SalesTotal FLOAT NOT NULL\n)\nUSING DELTA\n</code></pre></p>"},{"location":"Azure/Data/HDInsights/","title":"HDInsights","text":"<p>Hadoop service provided by Azure</p>"},{"location":"Azure/Data/HDInsights/#hadoop-core","title":"Hadoop core","text":"<ul> <li>HDFS: Hadoop Distributed File System is a file system for the storage of big data</li> <li>MapReduce: Process and collate data in HDFS</li> <li>YARN: job scheduling</li> </ul>"},{"location":"Azure/Data/IoT/","title":"IoT","text":"<p>Azure IoT Hub is Azure's core IoT service and platform.</p>"},{"location":"Azure/Data/IoT/#connectivity","title":"Connectivity","text":"<p>connection being made between a device and the IoT service - event - service bus - external data source</p>"},{"location":"Azure/Data/IoT/#identity","title":"Identity","text":"<p>identification of the device and allowing to send device telemetry using an authentication process</p>"},{"location":"Azure/Data/IoT/#capture","title":"Capture","text":"<p>device telemetry is captured and received by the IoT service</p>"},{"location":"Azure/Data/IoT/#ingestion","title":"Ingestion","text":"<p>IoT service ingests the device telemetry</p>"},{"location":"Azure/Data/IoT/#storage","title":"Storage","text":"<p>device telemetry is stored - sql database - cosmos db - table/blob storage - external data source - time series insights</p>"},{"location":"Azure/Data/IoT/#transformation","title":"Transformation","text":"<p>telemetry data is transformed for further processing</p>"},{"location":"Azure/Data/IoT/#analytics","title":"Analytics","text":"<p>find patterns, anomalies, and insights from transformed data - machine learning - stream analytics - HDInsight - Data factory - Databricks</p>"},{"location":"Azure/Data/IoT/#presentation","title":"Presentation","text":"<p>insights are shown as dashboards and reports - power bi - app service - notification hub - mobile service - logic apps</p>"},{"location":"Azure/DevOps/Agent/","title":"Agent","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/agents?view=azure-devops&amp;tabs=yaml%2Cbrowser</p>"},{"location":"Azure/DevOps/Agent/#self-hosted-agent","title":"self-hosted agent","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/docker?view=azure-devops#linux env vars: - <code>AZP_URL</code>: The URL of the Azure DevOps or Azure DevOps Server instance - <code>AZP_TOKEN</code>: Personal Access Token (PAT) with Agent Pools (read, manage) scope, created at AZP_URL - <code>AZP_AGENT_NAME</code>: Agent name (default value: the container hostname) - <code>AZP_POOL</code>: Agent pool name (default value: Default) - <code>AZP_WORK</code>: Work directory (default value: _work)</p>"},{"location":"Azure/DevOps/Agent/#aks-self-hosted-agent","title":"aks self-hosted agent","text":"<p>https://medium.com/@muppedaanvesh/azure-devops-self-hosted-agents-on-kubernetes-part-1-aa91e7912f79</p> <p>https://ghoshasish99.medium.com/azure-devops-self-hosted-agents-on-kubernetes-51685fde9a14</p> <p>docker build: - deregister the agent</p>"},{"location":"Azure/DevOps/Agent/#how-to-add-a-build-agent-with-azure-container-instances","title":"How to add a build agent with Azure Container Instances","text":"<p>https://www.vivienfabing.com/azure-devops/2019/06/20/azure-pipelines-how-to-add-a-build-agent-with-azure-container-instances-part-2-custom-agent.html</p>"},{"location":"Azure/DevOps/Agent/#azure-scale-set-agent","title":"azure scale-set agent","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/scale-set-agents?view=azure-devops</p>"},{"location":"Azure/DevOps/Agent/#firewall","title":"firewall","text":"<p>https://learn.microsoft.com/en-us/azure/devops/server/admin/setup-secure-sockets-layer?view=azure-devops-2022 - It tries to connect to VSTS on <code>HTTPS</code> i.e <code>port 443</code></p>"},{"location":"Azure/DevOps/Agent/#install-vsts-agent-in-docker","title":"install vsts-agent in docker","text":"<pre><code># vsts-agent\nARG AGENT_VER=4.248.1\nRUN \\\n    # https://github.com/Microsoft/azure-pipelines-agent/releases\n    url=\"https://vstsagentpackage.azureedge.net/agent/${AGENT_VER}/vsts-agent-linux-x64-${AGENT_VER}.tar.gz\" &amp;&amp; \\\n    mkdir /home/user/agent &amp;&amp; \\\n    curl -sL $url | tar -xz -C /home/user/agent &amp;&amp; \\\n    chown --recursive user:user /home/user/agent &amp;&amp; \\\n    # install dependencies explicitly instead of using agent/bin/installdependencies.sh\n    apt-get install --yes liblttng-ust1 libkrb5-3 libicu70 zlib1g\n</code></pre>"},{"location":"Azure/DevOps/Artifacts/","title":"Azure Artifacts","text":""},{"location":"Azure/DevOps/Boards/","title":"Azure Boards","text":"<p>Used to plan, manage, and track work across your entire team based on product backlog, sprint backlog, and task boards.</p>"},{"location":"Azure/DevOps/Build/","title":"Build","text":""},{"location":"Azure/DevOps/Build/#self-vs-ms-hosted-build-agents","title":"self vs ms hosted build agents","text":"<p>https://luke.geek.nz/azure/hosted-agents-container-apps-job/</p>"},{"location":"Azure/DevOps/ContainerRegistry/","title":"Container Registry","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/ecosystems/containers/publish-to-acr?view=azure-devops&amp;tabs=javascript%2Cportal%2Cmsi - Create an Azure Container Registry - Set up a self-hosted agent on an Azure VM - Set up the managed service identity - Create a Docker Registry service connection - Build and publish your image to Azure Container Registry</p>"},{"location":"Azure/DevOps/DevOpsWiki/","title":"Azure DevOps Wiki","text":""},{"location":"Azure/DevOps/Learn/","title":"Learn","text":""},{"location":"Azure/DevOps/Learn/#devops-sample","title":"devops sample","text":"<p>https://github.com/HoussemDellai/azure-devops-pipelines-samples/tree/main</p>"},{"location":"Azure/DevOps/Repos/","title":"Azure Repos","text":""},{"location":"Azure/DevOps/Repos/#work-steps","title":"work steps","text":"<ul> <li>clone repo to local machine: <code>git clone https://dev.azure.com/{organization}/{project}/_git/{repo-name}</code></li> <li>create a new branch: <code>git checkout -b your-feature-branch</code></li> <li>commit changes: <code>git add . &amp;&amp; git commit -m \"test\" &amp;&amp; git push origin your-feature-branch</code></li> <li>create a pr and merge to main</li> <li>sync local branch: <code>git checkout main &amp;&amp; git pull origin main</code></li> <li>delete local branch: <code>git branch -d your-feature-branch</code></li> <li>delete remote branch: <code>git push origin --delete your-feature-branch</code></li> </ul>"},{"location":"Azure/DevOps/Repos/#git-clone","title":"git clone","text":"<p>Error <pre><code>fatal: Cannot determine the organization name for this 'dev.azure.com' remote URL.\nEnsure the `credential.useHttpPath` configuration value is set, or\nset the organization name as the user in the remote URL '{org}@dev.azure.com'.\n</code></pre></p> <p>Solution:  setting can be specific to Azure Repos <pre><code>git config --global credential.https://dev.azure.com.useHttpPath true\n</code></pre></p>"},{"location":"Azure/DevOps/ServiceConnection/","title":"Service Connection","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/library/service-endpoints?view=azure-devops&amp;tabs=yaml</p>"},{"location":"Azure/DevOps/TestPlans/","title":"Azure Test Plans","text":""},{"location":"Azure/DevOps/pages/","title":"pages","text":"<p>https://azuredevopslabs.com/</p>"},{"location":"Azure/Func/Functions/","title":"Functions","text":""},{"location":"Azure/Func/Functions/#install-python-package","title":"install python package","text":"<pre><code>pip install azure-functions\n</code></pre>"},{"location":"Azure/Func/Functions/#install-the-azure-functions-core-tools","title":"Install the Azure Functions Core Tools","text":"<p>to allow local debugging - install from terminal using <code>npm install -g azure-functions-core-tools</code>?</p>"},{"location":"Azure/Func/Functions/#example","title":"example","text":"<p>https://medium.com/@saurabh.dasgupta1/developing-and-deploying-a-python-azure-function-step-by-step-83c5066a2531</p> <p>https://learn.microsoft.com/en-us/azure/azure-functions/functions-run-local - create local project: This will create a folder called <code>MyProjFolder</code> <pre><code>func init MyProjFolder --worker-runtime python --model V2\n</code></pre> - create a <code>timer trigger</code> function   <pre><code>func new --template \"Timer Trigger\" --name my_timer_trigger\n</code></pre></p>"},{"location":"Azure/Func/Functions/#issues","title":"issues","text":"<p>\"Create in Azure portal\" has been disabled for consumption type.</p>"},{"location":"Azure/Func/learn/","title":"Azure Functions","text":"<p>https://learn.microsoft.com/en-gb/azure/azure-functions</p>"},{"location":"Azure/ML/AutoML/","title":"Automated ML","text":"<p>automatically run all models and provid the scores for each model and also show the best.</p>"},{"location":"Azure/ML/AutoML/#models","title":"models","text":"<p>AutoML's forecasting regression models assume that all features provided by the user are <code>known into the future</code>, at least up to the forecast horizon.</p>"},{"location":"Azure/ML/AutoML/#rerun-failed-experiment","title":"rerun failed experiment","text":"<pre><code>from azureml.core import Workspace, Experiment\n\n# Load the workspace and experiment objects\nworkspace = Workspace.from_config()\nexperiment = Experiment(workspace, \"experiment-name')\n\n# Get the failed runs\nfailed_runs = experiment.get_runs(type='Failed')\n\n# Re-run the experiment\nfor failed_run in failed_runs:\n    new_run = failed_run.re_run()\n</code></pre>"},{"location":"Azure/ML/AutoML/#deploy-using-python","title":"deploy using python","text":"<p>https://towardsdatascience.com/how-to-deploy-scikit-learn-models-to-azure-container-instances-a0a59d0d07a1</p> <ul> <li>Setup Training Environment from terminal   <pre><code>conda create -n ml-10 python=3.10\nconda activate ml-10\n#install packahes\nazureml-core==1.39\npandas==1.3.5\nscikit-learn==0.23.2\nmlflow==1.24.0\ncloudpickle==2.0.0\npsutil==5.9.0  \n</code></pre></li> <li>get ml ws config   <pre><code># config.json\n  {\n      \"subscription_id\": \"subscription-id\",\n      \"resource_group\": \"resource-group-name\",\n      \"workspace_name\": \"workspace-name\"\n  }\n</code></pre></li> </ul>"},{"location":"Azure/ML/AutoML/#experiment","title":"experiment","text":"<pre><code>import time\nfrom azureml.core import Workspace, Experiment\n\nautoml_settings = {\n    \"name\": f'auto_feature_engineering_{time.time()}',\n    \"task\": \"regression\",\n    \"iterations\": 10,\n    \"iteration_timeout_minutes\": 10,    \n    \"max_cores_per_iteration\": 1,\n    \"max_concurrent_iterations\": 10,\n    \"primary_metric\": 'r2_score',    \n    \"experiment_exit_score\": 0.985,\n    \"debug_log\": f'automl_errors{time.time()}.log',\n    \"verbosity\": logging.ERROR,\n}\n# Local compute\nautoml_config = AutoMLConfig(\n   preprocess=False,\n   X=X_train,\n   y=y_train,\n   X_valid=X_valid,\n   y_valid=y_valid,\n   path=project_folder,\n   **automl_settings,\n)\n\n# Training the model\nexperiment = Experiment(ws, experiment_name)\nlocal_run = experiment.submit(automl_config, show_output=True)\n</code></pre>"},{"location":"Azure/ML/ML/","title":"Machine Learning","text":""},{"location":"Azure/ML/ML/#automated-ml","title":"Automated ML","text":""},{"location":"Azure/ML/Pipeline/","title":"Pipeline","text":""},{"location":"Azure/ML/Pipeline/#pipeline-using-ui","title":"pipeline using UI","text":"<ul> <li>join data</li> <li>clean data</li> <li>split data</li> <li>models etc</li> </ul>"},{"location":"Azure/ML/Pipeline/#pipeline-using-code","title":"pipeline using code","text":"<p>using code to create the pipeline in notebook.</p>"},{"location":"Azure/ML/learn/","title":"Learn","text":"<p>https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/machine-learning</p>"},{"location":"Azure/ML/learn/#ml-studio","title":"ML Studio","text":"<p>https://www.red-gate.com/simple-talk/cloud/data-science/azure-machine-learning-introduction-part-1-overview-and-prep-work/</p> <p>https://github.com/PracticalAutomatedMachineLearning/Azure/tree/master/notebook</p>"},{"location":"Azure/ML/learn/#python-sdk","title":"Python sdk","text":"<p>https://learn.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py</p> <p>https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-pipeline-python-sdk?view=azureml-api-2</p>"},{"location":"Azure/ML/learn/#overview","title":"OVERVIEW","text":"<ul> <li>Azure Machine Learning documentation: https://learn.microsoft.com/en-us/azure/machine-learning/?view=azureml-api-2</li> <li>How Azure Machine Learning works: resources and assets: https://learn.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-v2?view=azureml-api-2&amp;tabs=sdk&amp;source=docs</li> <li>What is an Azure Machine Learning workspace?: https://learn.microsoft.com/en-us/azure/machine-learning/concept-workspace?view=azureml-api-2</li> <li>What is an Azure Machine Learning compute instance?: https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-instance?view=azureml-api-2</li> <li>What is Azure Machine Learning CLI and Python SDK v2?: https://learn.microsoft.com/en-us/azure/machine-learning/concept-v2?view=azureml-api-2</li> <li>Tutorial: Create resources you need to get started: https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources?view=azureml-api-2</li> <li>QuickStart: Get started with Azure Machine Learning: https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-azure-ml-in-a-day?view=azureml-api-2</li> <li>Azure Machine Learning glossary: https://learn.microsoft.com/en-us/azure/machine-learning/azure-machine-learning-glossary?view=azureml-api-2 </li> </ul>"},{"location":"Azure/ML/learn/#data-connections","title":"DATA &amp; CONNECTIONS","text":"<ul> <li>Data concepts in Azure Machine Learning: https://learn.microsoft.com/en-us/azure/machine-learning/concept-data?view=azureml-api-2</li> <li>Train models with Azure Machine Learning datasets: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-with-datasets?view=azureml-api-1</li> <li>Data ingestion with Azure Data Factory: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf?view=azureml-api-1&amp;viewFallbackFrom=azureml-api-2 </li> <li>Execute Azure Machine Learning pipelines in Azure Data Factory and Synapse Analytics: https://learn.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service?view=azureml-api-1</li> <li>Create external data connections (preview): https://learn.microsoft.com/en-us/azure/machine-learning/how-to-connection?view=azureml-api-2&amp;tabs=cli</li> <li>What is \"human data\" and why is it important to source responsibly?: https://learn.microsoft.com/en-us/azure/machine-learning/concept-sourcing-human-data?view=azureml-api-2</li> </ul>"},{"location":"Azure/ML/learn/#automl","title":"AUTOML","text":"<ul> <li>What is automated machine learning (AutoML)?: https://learn.microsoft.com/en-us/azure/machine-learning/concept-automated-ml?view=azureml-api-2</li> <li>Tutorial: Train a classification model with no-code AutoML in the Azure Machine Learning studio: https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-first-experiment-automated-ml?view=azureml-api-2</li> <li>Tutorial: Train an object detection model with AutoML and Python: https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-auto-train-image-models?view=azureml-api-2&amp;tabs=cli</li> </ul>"},{"location":"Azure/ML/learn/#compute","title":"COMPUTE:","text":"<p>\u2022   Model training on serverless compute (preview): https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-serverless-compute?view=azureml-api-2&amp;tabs=python \u2022   Distributed training with Azure Machine Learning: https://learn.microsoft.com/en-us/azure/machine-learning/concept-distributed-training?view=azureml-api-2</p>"},{"location":"Azure/ML/learn/#designer-pipelines","title":"DESIGNER &amp; PIPELINES","text":"<ul> <li>What are Azure Machine Learning pipelines?: https://learn.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines?view=azureml-api-2</li> <li>What is an Azure Machine Learning component?: https://learn.microsoft.com/en-us/azure/machine-learning/concept-component?view=azureml-api-2</li> <li>What is Azure Machine Learning designer(v2)?: https://learn.microsoft.com/en-us/azure/machine-learning/concept-designer?view=azureml-api-2</li> <li>Create and run machine learning pipelines using components with the Azure Machine Learning studio: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipelines-ui?view=azureml-api-2</li> <li>Create and run machine learning pipelines using components with the Azure Machine Learning SDK v2: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python?view=azureml-api-2</li> </ul>"},{"location":"Azure/ML/learn/#responsible-ai","title":"RESPONSIBLE AI","text":"<ul> <li>What is Responsible AI?: https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai?view=azureml-api-2</li> <li>Generate a Responsible AI insights in the studio UI: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-insights-ui?view=azureml-api-2</li> <li>Assess AI systems by using the Responsible AI dashboard: https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?view=azureml-api-2</li> <li>Share Responsible AI insights using the Responsible AI scorecard (preview): https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-scorecard?view=azureml-api-2</li> </ul>"},{"location":"Azure/ML/learn/#architecture","title":"ARCHITECTURE","text":"<ul> <li>Artificial intelligence (AI) architecture design: https://learn.microsoft.com/en-us/azure/architecture/ai-ml/</li> <li>Tutorial: How to create a secure workspace with a managed virtual network: https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-create-secure-workspace?view=azureml-api-2</li> </ul>"},{"location":"Azure/ML/learn/#code-experiences-samples","title":"CODE-EXPERIENCES &amp; SAMPLES","text":"<ul> <li>Cross-compatible platform tools: </li> <li>Azure Machine Learning studio</li> <li>Python SDK (v2)</li> <li>Azure CLI (v2))</li> <li>Azure Resource Manager REST APIs</li> <li>Azure SDK Releases: https://azure.github.io/azure-sdk/releases/latest/all/python.html</li> <li>Azure ML Package client library for Python - version 1.11.1: https://learn.microsoft.com/en-us/python/api/overview/azure/ai-ml-readme?view=azure-python</li> <li>azure-ai-ml Package: https://learn.microsoft.com/en-us/python/api/azure-ai-ml/?view=azure-python</li> <li>Azure Machine Learning examples: https://github.com/Azure/azureml-examples/tree/main</li> <li>Tutorials: https://github.com/Azure/azureml-examples/tree/main/tutorials</li> <li>Set up Visual Studio Code desktop with the Azure Machine Learning extension (preview): https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-vs-code?view=azureml-api-2</li> </ul>"},{"location":"Azure/ML/learn/#mlops","title":"MLOPS","text":"<ul> <li>MLOps: Model management, deployment, and monitoring with Azure Machine Learning: https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment?view=azureml-api-2</li> <li>Git integration for Azure Machine Learning: https://learn.microsoft.com/en-us/azure/machine-learning/concept-train-model-git-integration?view=azureml-api-2&amp;tabs=python</li> <li>Set up MLOps with Azure DevOps: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-mlops-azureml?view=azureml-api-2&amp;tabs=azure-shell</li> <li>Azure MLOps (v2) Solution Accelerator: https://github.com/Azure/mlops-v2</li> </ul>"},{"location":"Azure/Manage/Blueprint/","title":"Blueprint","text":""},{"location":"Azure/Manage/Lock/","title":"Lock","text":"<p>For resources in production environments that should not be modified or deleted accidentally.</p> <p>Azure locks can: - lock resources such that they cannot be deleted, even for owner access - lock resources such that they can neither be deleted nor have their configuration modified</p> <p>Locks can be applied at the levels of  - subscription - resource group - management group - individual resource</p>"},{"location":"Azure/Manage/Lock/#cli-create-lock-to-resource","title":"cli create lock to resource","text":"<pre><code>az lock create --name LockSite --lock-type CanNotDelete \\\n --resource-group &lt;resource-group&gt; --resource-name &lt;resource-name&gt; \\\n --resource-type Microsoft.Web/sites\n</code></pre>"},{"location":"Azure/Manage/Lock/#cli-create-lock-to-resource-group","title":"cli create lock to resource group","text":"<pre><code>az lock create --name LockGroup --lock-type CanNotDelete \\ \n  --resource-group &lt;resource-group&gt;\n</code></pre>"},{"location":"Azure/Manage/RBAC/","title":"RBAC","text":"<p>RBAC means the assigning of permissions to identities within a scope such as management group, subscription, resource group, and individual resources.</p>"},{"location":"Azure/Monitor/Alert/","title":"Alert","text":""},{"location":"Azure/Monitor/Alert/#log-analytics-alert-actions","title":"Log analytics alert actions","text":"<ul> <li>Email/SMS/push/voice notification</li> <li>Webhooks to run external process</li> <li>Azure Functions</li> <li>Logic Apps</li> <li>Email Azure Resource Manager Role</li> <li>Secure webhook</li> <li>Automation runbooks</li> <li>Sending information to ITSM systems</li> </ul>"},{"location":"Azure/Monitor/Log/","title":"Log","text":"<p>Type of logs: - Metrics: view entire metrics for all Azure resources - Activity Logs: look at all control plane activities, e.g., the person shutdown a virtual machine - Service Health: view service related issues, can also create a service health alert - Diagnostic Logs: provide telemetry information about the operations of resources that are inherent to the resources - Application Logs: Direct logs from various resources such as Azure VMs onto the Logs by creating a Log Analytics workspace to store the logs</p>"},{"location":"Azure/Monitor/Log/#set-table-level-read-access","title":"Set table-level read access","text":"<p>https://learn.microsoft.com/en-us/azure/azure-monitor/logs/manage-access?tabs=portal#set-table-level-read-access</p>"},{"location":"Azure/Monitor/Monitor/","title":"Monitor","text":"<p>Azure resources related to monitoring are  - Azure Monitor,  - Azure Application Insights, and  - Log Analytics</p>"},{"location":"Azure/Monitor/Monitor/#azure-monitor","title":"Azure Monitor","text":"<ul> <li>A central tool and resource that allows to monitor an Azure subscription</li> <li>as a dashboard and management resource for all other monitoring capabilities</li> <li>provides management features for activity logs, operation logs, diagnostic settings, metrics, Application Insights, and Log Analytics</li> </ul>"},{"location":"Azure/Monitor/Monitor/#azure-application-insights","title":"Azure Application Insights","text":"<ul> <li>get metrics, logs, and other telemetry information from custom applications </li> <li>provides rich reporting, dashboarding, and analytics capabilities to get insights from incoming data and act on them</li> </ul>"},{"location":"Azure/Monitor/Monitor/#azure-log-analytics","title":"Azure Log Analytics","text":"<ul> <li>centralized process of logs and generate insights and alerts</li> <li>get activity logs, diagnostic logs, application logs, event logs, and custom logs</li> </ul>"},{"location":"Azure/Network/Architecture/","title":"Architecture","text":""},{"location":"Azure/Network/Architecture/#configure-domain-name-servers-settings-in-azure","title":"Configure domain name servers settings in Azure","text":"<p>https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-virtual-networks/6-exercise-configure-domain-name-servers-configuration-azure</p> <p>configure DNS name resolution for Contoso Ltd: - create a private DNS zone named contoso.com - link VNets for registration and resolution - create two virtual machines</p> <p>[private dns zone] -&gt; [vnet link] -&gt; [vnet] -&gt; [subnet: vm1, vm2]</p>"},{"location":"Azure/Network/Architecture/#on-prem-to-azure-cloud","title":"on-prem to azure cloud","text":"<pre><code>[on-prem]\n    -- private link, express route --&gt;\n[private dns resolver]\n    -- vnet peering --&gt;\n[vnet]\n</code></pre>"},{"location":"Azure/Network/CDN/","title":"Content Delivery Network","text":"<p>An ideal service for web applications. Can distribute your websites content to users across the world. - users are directed on various Edge servers by the CDN service - edge servers will get website content and cache frequently accessed content - give all users a seamless experience (reduce response time - network latency), as the Edge servers are located across the world</p>"},{"location":"Azure/Network/DNS/","title":"DNS","text":"<p>Resources in a virtual network can communicate with each other by using  - IP addresses, and  - names that can be easily remembered, and do not change</p>"},{"location":"Azure/Network/DNS/#public-dns-service","title":"Public DNS service","text":""},{"location":"Azure/Network/DNS/#private-dns-service","title":"Private DNS service","text":""},{"location":"Azure/Network/DNS/#internal-dns-service","title":"Internal DNS service","text":"<ul> <li>only works in the same vnet</li> <li>namespace: <code>.internal.cloudapp.net</code>. For example <code>my-vm.internal.cloudapp.net</code></li> </ul>"},{"location":"Azure/Network/DNS/#azure-private-dns-zone","title":"Azure Private DNS Zone","text":"<p>They are global in scope, so can access them from - any region, - any subscription, - any VNet, and - any tenant.</p> <p>A Private DNS Zone in Azure is a feature that allows you to create a <code>custom DNS namespace</code> for your virtual networks in Azure.</p> <p>It provides <code>name resolution</code> within your virtual network, enabling you to use custom domain names for your resources while keeping the DNS queries and responses <code>within the Azure network</code>.</p>"},{"location":"Azure/Network/DNS/#azure-private-dns-zone-virtual-network-link","title":"Azure Private DNS Zone Virtual Network Link","text":"<p>An Azure Private DNS Zone Virtual Network Link is a resource that connects a <code>virtual network</code> within Azure to a specific <code>Azure Private DNS zone</code>.</p> <p>It essentially bridges the gap between private DNS and your virtual network, allowing resources within the virtual network to resolve host names defined in the private zone.</p>"},{"location":"Azure/Network/DNS/#hybrid-dns-resolution","title":"Hybrid DNS resolution","text":"<p>https://learn.microsoft.com/en-us/azure/dns/private-resolver-hybrid-dns</p>"},{"location":"Azure/Network/DNS/#azure-dns-private-resolver","title":"Azure DNS Private Resolver","text":"<p>A service can resolve on-premises DNS queries for Azure DNS private zones.  - fully manged - reduced costs - high availability - can be easily integrated with DevOps workflows</p>"},{"location":"Azure/Network/DNS/#dns-forwarding-ruleset","title":"DNS forwarding ruleset","text":"<p>A group of rules that specify one or more custom DNS servers to answer queries for specific DNS namespaces.</p>"},{"location":"Azure/Network/DNS/#private-dns-best-practices","title":"Private DNS best practices","text":"<p>https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/private-link-and-dns-integration-at-scale - private DNS zones live in the hub - an Azure policy automatically creates dnsZoneGroups for private endpoints - for DNS resolution, spokes point to either a custom DNS or Azure Private Resolver in the hub</p>"},{"location":"Azure/Network/DNS/#migrate-private-dns-zone","title":"Migrate private DNS zone","text":"<p>multiple-azure-private-dns-zones: https://serverfault.com/questions/1104244/multiple-azure-private-dns-zones</p> <p>Not to have two different private DNS zones for the same Azure service tied with VNET links to the same VNE: <pre><code>Creating multiple zones with the same name for different virtual networks\nwould need manual operations to merge the DNS records.\n</code></pre></p> <p>Also from here: https://learn.microsoft.com/en-us/answers/questions/561794/vnet-link-to-multiple-private-dns-zones</p> <p>Creating multiple zones with the same name for different virtual networks would need manual operations to merge the DNS records. This is a known limitation and can be found documented in the below article: https://learn.microsoft.com/en-us/azure/private-link/private-endpoint-dns#virtual-network-workloads-without-custom-dns-server</p> <p>If we need to migrate the private DNS zone to another private DNS zone.  We first create the new DNS zone then delete the old one - this will not work. - Need to restore the deleted old one - When the info in the old DNS merged to the new one, we can then delete the old one</p>"},{"location":"Azure/Network/DNSRecord/","title":"DNS Record","text":"<p>https://learn.microsoft.com/en-us/azure/dns/dns-zones-records</p>"},{"location":"Azure/Network/DNSRecord/#record-types","title":"Record types","text":"<ul> <li><code>apex record</code> (<code>@</code>) is a DNS record at the root (or apex) of a DNS zone. </li> <li><code>A record</code> (short for Address record) is a type of DNS (Domain Name System) record   that maps a hostname (or domain name) to its corresponding <code>IPv4</code> address</li> <li><code>AAAA records</code> (for IPv6 addresses)</li> <li><code>CNAME records</code> (for creating aliases)</li> <li><code>MX records</code> (for email servers)</li> </ul>"},{"location":"Azure/Network/DNSRecord/#record-sets","title":"Record sets","text":"<p>Used to create more than one DNS record with a given name and type. </p> <p>For example, suppose a web site is hosted on two different IP addresses. The website requires two different A records, one for each IP address.</p> <p>The <code>SOA</code> and <code>CNAME</code> record types can only contain a single record.</p>"},{"location":"Azure/Network/IP/","title":"IP","text":"<p>IP can be static or dynamic. Dynamic IP is released when the resource is stopped or deleted.</p>"},{"location":"Azure/Network/IP/#resources-can-associate-with-a-public-ip-address","title":"resources can associate with a public IP address","text":"<ul> <li>Virtual machine network interfaces</li> <li>Virtual machine scale sets</li> <li>Public Load Balancers</li> <li>Virtual Network Gateways (VPN/ER)</li> <li>Network Address Translation (NAT) Gateways</li> <li>Application Gateways</li> <li>Azure Firewall</li> <li>Bastion Host</li> <li>Route Server</li> </ul>"},{"location":"Azure/Network/Learn/","title":"Learn","text":"<ul> <li>https://learn.microsoft.com/en-us/training/paths/architect-network-infrastructure/?source=recommendations</li> <li>https://github.com/HoussemDellai/azure-network-course</li> </ul>"},{"location":"Azure/Network/Learn/#vnet","title":"VNet","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/virtual-network/virtual-networks-overview</li> <li>https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-virtual-networks/</li> <li>https://learn.microsoft.com/en-us/training/modules/introduction-to-azure-virtual-networks/2-explore-azure-virtual-networks</li> </ul>"},{"location":"Azure/Network/Learn/#filter-network-traffic-with-a-network-security-group-using-the-azure-portal","title":"Filter network traffic with a network security group using the Azure portal","text":"<p>https://learn.microsoft.com/en-us/azure/virtual-network/tutorial-filter-network-traffic - vnet - asg - nsg - vms</p>"},{"location":"Azure/Network/NSG/","title":"NSG","text":"<p>A network security group contains security rules that allow or deny   - inbound network traffic to, or  - outbound network traffic from several types of Azure resources.</p> <p>For each rule, you can specify - source and destination - port - protocol</p>"},{"location":"Azure/Network/Project/","title":"Project","text":""},{"location":"Azure/Network/Project/#access-apps-in-subscription-a-aks-from-subscription-b-aks-pod","title":"access apps in subscription-a aks from subscription-b aks pod","text":"<ul> <li>peer vnet</li> <li>firewall rules</li> </ul>"},{"location":"Azure/Network/SubNet/","title":"Subnet","text":"<ul> <li>Subnets provide isolation within a virtual network</li> <li>Subnet IP range should not be too large or too small</li> </ul>"},{"location":"Azure/Network/SubNet/#filter-network-traffic-between-subnets","title":"Filter network traffic between subnets","text":"<ul> <li><code>Network security groups</code> and <code>application security groups</code> can contain multiple inbound and outbound security rules.</li> <li>These rules enable you to filter traffic to and from resources by source and destination <code>IP address</code>, <code>port</code>, and <code>protocol</code>. </li> <li>A <code>network virtual appliance</code> is a VM that performs a network function, such as a firewall, WAN optimization, or other network function.</li> </ul>"},{"location":"Azure/Network/VNet/","title":"VNet","text":"<ul> <li>a logical representation of a network used to isolate and securely connect resources </li> <li>contained within a resource group and is hosted within a region</li> <li>cannot span multiple regions but can span all datacenters within a region</li> </ul>"},{"location":"Azure/Network/VNet/#connection","title":"Connection","text":"<ul> <li>resources within the same region and subscription </li> <li>VNet peering</li> <li>resources within the same (different) region in another subscription</li> <li>VNet peering</li> <li>Gateway: extra charges but allow encryption</li> <li>to on-premises resources</li> <li>Point-to-site VPN</li> <li>Site-to-site VPN  </li> <li>ExpressRoute: not through public internet but costly</li> </ul>"},{"location":"Azure/Network/VNet/#communicate-with-the-internet","title":"Communicate with the internet","text":"<ul> <li><code>outbound</code> to the internet, by default, can be managed by <code>public IP</code>, <code>NAT gateway</code>, or <code>public load balancer</code></li> <li><code>inbound</code> to a resource by assigning a <code>public IP address</code> or <code>public load balancer</code></li> </ul>"},{"location":"Azure/Network/VNet/#communicate-between-azure-resources","title":"Communicate between Azure resources","text":"<ul> <li>VNet</li> <li>connect VMs</li> <li>connect other Azure Resources, such as App Service Environment, Azure Kubernetes Service, and Azure Virtual Machine Scale Sets</li> <li>VNet service endpoint</li> <li>used to connect to other Azure resource types, such as Azure SQL databases and storage accounts</li> <li>services and VMs within the same VNet can communicate directly and securely with each other in the cloud</li> <li>VNet peering</li> <li>can connect virtual networks to each other by using virtual peering</li> </ul>"},{"location":"Azure/Network/VNet/#communicate-with-on-premises-resources","title":"Communicate with on-premises resources","text":"<ul> <li>Point-to-site VPN</li> <li>Site-to-site VPN</li> <li>Azure ExpressRoute</li> </ul>"},{"location":"Azure/Network/VNet/#filter-network-traffic","title":"Filter network traffic","text":"<p>filter network traffic between <code>subnets</code> using any combination of  - network security groups and - network virtual appliances like - firewalls, gateways, proxies, and Network Address Translation (NAT) services</p>"},{"location":"Azure/Network/VNet/#route-network-traffic","title":"Route network traffic","text":"<ul> <li>Azure routes traffic between <code>subnets</code>, <code>connected virtual networks</code>, <code>on-premises networks</code>, and the <code>Internet</code>, by default</li> <li>You can implement <code>route tables</code> or <code>border gateway protocol</code> (BGP) routes to override the default routes Azure creates</li> </ul>"},{"location":"Azure/Network/VNet/#ip-address-space","title":"IP address space","text":"<p>can be used: <pre><code>10.0.0.0 - 10.255.255.255 (10/8 prefix)\n172.16.0.0 - 172.31.255.255 (172.16/12 prefix)\n192.168.0.0 - 192.168.255.255 (192.168/16 prefix)\n</code></pre> Cannot be used: <pre><code>224.0.0.0/4 (Multicast)\n255.255.255.255/32 (Broadcast)\n127.0.0.0/8 (Loopback)\n169.254.0.0/16 (Link-local)\n168.63.129.16/32 (Internal DNS)\n</code></pre></p>"},{"location":"Azure/Network/VPNGateway/","title":"VPN Gateway","text":"<ul> <li>A VPN gateway is a type of Virtual Network Gateway</li> <li>VPN gateways are deployed in Azure virtual networks</li> <li>All transferred data is encrypted in a private tunnel as it crosses the internet</li> </ul>"},{"location":"Azure/Pipeline/Agent/","title":"Agent","text":"<p>Agent can be run as either a service or an interactive process</p>"},{"location":"Azure/Pipeline/Agent/#microsoft-hosted-agent","title":"Microsoft-hosted agent","text":"<ul> <li>The virtual machine is discarded after pipeline run</li> <li>A Microsoft-hosted agent has job time limits</li> <li>Can take longer to start your build - Sometimes take several minutes for an agent to be allocated</li> </ul>"},{"location":"Azure/Pipeline/Agent/#self-hosted-agent","title":"Self-hosted agent","text":"<p>https://github.com/Microsoft/azure-pipelines-agent/releases - Set up and manage on your own - Might need to clear the env after each run - A self-hosted agent doesn't have job time limits - Can run incremental builds - doesn't clean the repo or do a clean build so the builds will typically run faster</p>"},{"location":"Azure/Pipeline/Agent/#how-to-create-a-self-hosted-agent-docker-file","title":"How to create a self-hosted agent docker file?","text":"<p>Under root user: - install self-signed certificates - install docker and docker compose - install azure-pipeline-agent: https://github.com/Microsoft/azure-pipelines-agent/releases - install azure cli</p> <p>Under normal user: - install conda and mamba if build python packages - intsall custom packages - setup agent envs - run the agent from <code>agent/start.sh</code></p>"},{"location":"Azure/Pipeline/Agent/#personal-access-token-pat","title":"Personal access token (PAT)","text":"<p>The pat will be used when run <code>config.sh</code> in agent folder: - PAT is the only scheme that works with Azure Pipelines - PAT is used only when registering the agent and not for succeeding communication - Create a PAT: User settings &gt; Security &gt; Personal access tokens &gt; New Token - scope: Agent Pools (read &amp; manage) - user much have admin right in: https://dev.azure.com/org-name/_settings/agentpools?poolId=1&amp;_a=roles</p> <p>Check pat expiration: https://gist.github.com/ydkn/e4ee3da80b2d9f8714faa395c9c554ed</p>"},{"location":"Azure/Pipeline/Agent/#manage-agent-pools","title":"Manage agent pools","text":"<p>https://microsoftlearning.github.io/AZ400-DesigningandImplementingMicrosoftDevOpsSolutions/Instructions/Labs/AZ400_M03_L04_Configuring_Agent_Pools_and_Understanding_Pipeline_Styles.html</p>"},{"location":"Azure/Pipeline/Agent/#self-hosted-linux-agents","title":"Self-hosted Linux agents","text":"<p>https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/v2-linux?view=azure-devops - Settings: {Name: Agent name, Scope (custom defined): Agent Pools, Permissions: read and manage} - Config: Azure DevOps &gt; Organization settings &gt; Pipelines: Agent pools &gt; Add pool {Self-hosted} - Agent pools &gt; Jobs &gt; New Agent &gt; download and run config.sh to config agent - Server-url: https://dev.azure.com/{your-organization} - Run agent: ./run.sh --once; As a service: sudo ./svc.sh install/start/status/stop/uninstall - Remove agent: ./config.sh remove</p>"},{"location":"Azure/Pipeline/Caching/","title":"Pipeline caching","text":"<p>https://docs.microsoft.com/en-us/azure/devops/pipelines/release/caching?view=azure-devops#use-cache-task</p>"},{"location":"Azure/Pipeline/CondaBuild/","title":"Conda build","text":"<p>https://github.com/MicrosoftDocs/pipelines-anaconda/blob/master/azure-pipelines.yml</p>"},{"location":"Azure/Pipeline/Deployment/","title":"Deployment","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/process/deployment-jobs?view=azure-devops</p>"},{"location":"Azure/Pipeline/Deployment/#checkout-repo","title":"checkout repo","text":"<p>A deployment job doesn't automatically <code>clone the source repo</code>. You can checkout the source repo within your job with checkout: self.  Deployment jobs only support one checkout step. <pre><code>- stage: Deploy\n  displayName: Deploy Job\n  dependsOn: Build\n\n  jobs: \n  - deployment: Deploy\n    displayName: Build Job Stage  \n    environment: Dev\n    pool:\n      name: my-agent     \n    strategy:\n      runOnce:\n        deploy:\n          steps:\n            - checkout: self \n</code></pre></p>"},{"location":"Azure/Pipeline/Deployment/#push-image-to-aks","title":"push image to aks","text":"<p>https://learn.microsoft.com/en-us/azure/aks/devops-pipeline?pivots=pipelines-yaml</p>"},{"location":"Azure/Pipeline/Deployment/#clear-workspace","title":"clear workspace","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/process/phases?view=azure-devops&amp;tabs=yaml#workspace</p> <p>Self-hosted agent will not automatically clear all the <code>workspace</code> directory <pre><code>  jobs:\n  - deployment: MyDeploy\n    pool:\n      vmImage: 'ubuntu-latest'\n    workspace:\n      clean: all\n    environment: staging\n</code></pre></p> <p>May delete needed files, can delete the files manually <pre><code>  - powershell: |\n      Remove-Item *.whl\n    displayName: Delete whl files\n</code></pre></p>"},{"location":"Azure/Pipeline/Issue/","title":"Issue","text":""},{"location":"Azure/Pipeline/Issue/#build-not-triggered","title":"build not triggered","text":"<p>Check triggers (maybe overwritten by UI or had connection issues): <pre><code>Edit &gt; three-dots &gt; Triggers\n</code></pre></p>"},{"location":"Azure/Pipeline/Issue/#serice-connection-pat-expired","title":"serice connection pat expired","text":"<p>error <pre><code>An error occurred while fetching the YAML file 'azure-pipelines.yml' in the repository's default branch and latest build branches: master.\nError on last attempt: File azure-pipelines.yml not found in repository dev/my-task at version/branch master.\n</code></pre> solution: check the project service connection and update the pat (from github)</p>"},{"location":"Azure/Pipeline/Job/","title":"Job","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/process/phases?view=azure-devops&amp;tabs=yaml</p>"},{"location":"Azure/Pipeline/ParamVar/","title":"ParamVar","text":""},{"location":"Azure/Pipeline/ParamVar/#parameter-and-variable","title":"Parameter and Variable","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/security/inputs?view=azure-devops</p>"},{"location":"Azure/Pipeline/ParamVar/#parameters","title":"Parameters","text":"<ul> <li>Pipeline parameters can't be changed by a pipeline while it's running.</li> <li>Parameters have data types such as number and string, and they can be restricted to a subset of values.</li> <li>The setup ensures that the pipeline won't take arbitrary data.</li> </ul>"},{"location":"Azure/Pipeline/ParamVar/#variables","title":"Variables","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops&amp;tabs=yaml%2Cbatch</p> <p>In YAML pipelines, we can set variables at the root, stage, and job level.</p> <ul> <li>Variables can be a convenient way to collect information from the user up front.</li> <li>We can also use variables to pass data from step to step within a pipeline.</li> <li>Newly created variables are read-write by default.</li> </ul> <pre><code>variables:\n  a: ${{ variables.var }} #evaluated when the YAML file is compiled into a plan\n  b: $(var)               #evaluated at runtime before a task executes\n  b: $[variables.var]     #evaluated at runtime\n</code></pre> <ul> <li>compile-time expression <code>${{ &lt;expression&gt; }}</code>: have access to <code>parameters</code> and <code>statically defined variables</code></li> <li>runtime expression <code>$[ &lt;expression&gt; ]</code>: have access to <code>more variables</code> but <code>no parameters</code></li> </ul>"},{"location":"Azure/Pipeline/Parameter/","title":"Azure Pipeline Parameters","text":""},{"location":"Azure/Pipeline/Parameter/#purpose","title":"Purpose","text":"<p>Parameters are used to define values that are passed to the pipeline at runtime. They are typically used to make your pipelines more flexible and reusable by allowing you to pass in different values when triggering the pipeline.</p>"},{"location":"Azure/Pipeline/Parameter/#types","title":"Types","text":"<ol> <li>Pipeline Parameters: Passed when the pipeline is manually triggered or via an API call.</li> <li>Template Parameters: Used in pipeline templates to allow customization when templates are referenced.</li> </ol>"},{"location":"Azure/Pipeline/Parameter/#scope","title":"Scope","text":"<ul> <li>Pipeline-Level: Accessible throughout the pipeline, including all jobs and steps.</li> <li>Template-Level: Scoped to the specific template where they are defined.</li> </ul>"},{"location":"Azure/Pipeline/Parameter/#syntax","title":"Syntax","text":"<ul> <li>YAML: <code>${{ parameters.parameterName }}</code></li> <li>Usage in Templates: Can define default values and types for parameters.</li> </ul>"},{"location":"Azure/Pipeline/Parameter/#example","title":"Example","text":"<pre><code>parameters:\n  env: 'dev'\n\njobs:\n- job: Deploy\n  steps:\n  - script: |\n      echo Deploying to ${{ parameters.env }}\n    displayName: 'Deploy to Environment'\n    condition: eq('${{ parameters.env }}', 'prd')\n</code></pre>"},{"location":"Azure/Pipeline/Parameter/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Must be defined at the beginning of the YAML pipeline or template.</li> <li>Values are provided when the pipeline is triggered.</li> <li>Can be used to parameterize templates, making them reusable with different inputs.</li> <li>Values are typically static during the pipeline run but can be used to control the flow and behavior of the pipeline based on the input.</li> </ul>"},{"location":"Azure/Pipeline/Parameter/#summary","title":"Summary","text":"<p>Variables: - Usage: Store and manage values that can be used across steps and jobs. Can be dynamic and changed during runtime. - Scope: Pipeline, job, or step level. - Syntax: <code>$(variableName)</code> in YAML and <code>$VARIABLE_NAME</code> in scripts.</p> <p>Parameters: - Usage: Pass values into the pipeline or templates at runtime. Make pipelines and templates flexible and reusable. - Scope: Pipeline or template level. - Syntax: <code>${{ parameters.parameterName }}</code> in YAML.</p>"},{"location":"Azure/Pipeline/Performance/","title":"Performance","text":""},{"location":"Azure/Pipeline/Performance/#only-rebuild-changed-porjects","title":"Only rebuild changed porjects","text":"<p>https://timdeschryver.dev/blog/how-to-make-your-azure-devops-ci-cd-pipeline-faster?#the-end-result</p> <p>The solution to a faster CI/CD pipeline is to only build and deploy the projects that are affected in the last commit. The most important thing is to determine what projects are modified within the last commit and to assign these modified projects to an environment variable.</p>"},{"location":"Azure/Pipeline/Performance/#dependency-caching-in-python-ci-pipeline","title":"Dependency Caching in Python CI pipeline","text":"<p>https://stackoverflow.com/questions/69542082/dependency-caching-in-python-ci-pipeline-in-azure-devops</p> <p>https://medium.com/@andre.gensler/guide-how-to-speed-up-your-python-continuous-integration-pipeline-in-azure-devops-using-dependency-916d9cd792a0</p> <p>To also cache the installation rather just the wheels, we need to use virtual env and cache the entire environment.</p>"},{"location":"Azure/Pipeline/PipBuild/","title":"Pip build","text":"<pre><code>variables:\n  docker_file: './docker/linux/repo.docker'\n  artifact_feed: 'my/artifact_feed'\n  docker_reg_service_conn: 'docker_registry_service_connection'\n  docker_image_repository: 'my/app'\n  conda_env: 'env'\n  tag: '$(Build.BuildId)'\n\ntrigger:\n- main\n\nresources:\n- repo: self\n\nstages:\n- stage: Build\n  displayName: Build package stage\n  jobs:\n  - job:\n    displayName: Build pip whl\n    pool:\n      vmImage: 'ubuntu-latest'\n    steps:\n\n    - task: UsePythonVersion@0\n      displayName: Use python version\n      inputs:\n        versionSpec: '3.9'\n        addToPath: true\n        architecture: 'x64'\n\n    - task: PipAuthenticate@0\n      displayName: Configure artifact feed authentication\n      inputs:\n        artifactFeeds: ${{ variables.artifact_feed }}\n\n    - bash: |\n        printenv\n      displayName: Print environment variables\n\n    - bash: |\n        python -m venv $(Agent.TempDirectory)/venv\n        source $(Agent.TempDirectory)/venv/bin/activate\n        python -m pip install --upgrade pip twine\n      displayName: Create virtual environment\n\n    - bash: |\n        source $(Agent.TempDirectory)/venv/bin/activate\n        pip wheel $(Build.SourcesDirectory) --no-deps -w $(Agent.TempDirectory)/pkg --no-cache-dir --verbose\n      displayName: Build wheel file\n      # Run from within tmp_dir - setuptools places build files in cur_dir and does not remove them after build\n      workingDirectory: $(Agent.TempDirectory)\n\n    - bash: |\n        source $(Agent.TempDirectory)/venv/bin/activate\n        pip install `cd  $(Agent.TempDirectory)/pkg/ &amp;&amp; realpath *`[test] --no-cache-dir --verbose\n      displayName: Install built package\n\n    - task: PublishBuildArtifacts@1\n      displayName: Publish artifacts\n      inputs:\n        pathToPublish: $(Agent.TempDirectory)/pkg\n        artifactName: pkg\n\n- stage: BuildDocker\n  displayName: Build docker image stage\n  jobs:\n  - job:\n    displayName: Build python docker image\n    pool:\n      name: linux-dev\n    steps:\n\n    - task: DownloadPipelineArtifact@2\n      displayName: Download pipeline artifacts\n      inputs:\n        artifact: 'pkg'\n        path: $(Build.SourcesDirectory)\n\n    - task: Docker@2\n      displayName: Build docker image\n      inputs:\n        command: build\n        repository: $(docker_image_repository)\n        containerRegistry: $(docker_reg_service_conn)\n        dockerfile: $(docker_file)\n        arguments: '--platform linux/amd64 --build-arg NAME=${{ variables.conda_env }}'\n        buildContext: .\n        tags: |\n          $(tag)\n          latest\n\n    - task: Docker@2\n      displayName: Push docker image to container registry\n      inputs:\n        command: push\n        repository: $(docker_image_repository)\n        containerRegistry: $(docker_reg_service_conn)\n        tags: |\n          $(tag)\n          latest\n</code></pre>"},{"location":"Azure/Pipeline/Pipeline/","title":"Azure Pipeline","text":"<p>https://levelup.gitconnected.com/up-and-running-with-azure-kubernetes-service-aks-and-devops-pipelines-deployment-40f054071477</p> <p>https://learn.microsoft.com/en-us/azure/aks/devops-pipeline?pivots=pipelines-yaml</p> <p>https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/parameters?view=azure-pipelines</p> <p>AZ-400: Implement CI with Azure Pipelines and GitHub Actions\\ https://docs.microsoft.com/learn/paths/az-400-implement-ci-azure-pipelines-github-actions/</p> <p>https://docs.microsoft.com/en-us/azure/devops/pipelines/repos/github?view=azure-devops&amp;tabs=yaml</p> <p>https://azuredevopslabs.com/labs/vstsextend/azurekeyvault/</p> <p>https://devblogs.microsoft.com/devops/demystifying-service-principals-managed-identities/</p>"},{"location":"Azure/Pipeline/Pipeline/#agent","title":"Agent","text":"<p>An agent is an installable software that runs a build or deployment job.</p>"},{"location":"Azure/Pipeline/Pipeline/#artifact","title":"Artifact","text":"<p>An artifact is a collection of files or packages published by a build. Artifacts are made available for next tasks, such as distribution or deployment.</p>"},{"location":"Azure/Pipeline/Pipeline/#build","title":"Build","text":"<p>A build represents one execution of a pipeline. It collects the logs associated with running the steps and the results of running tests.</p>"},{"location":"Azure/Pipeline/Pipeline/#change-pipeline-name","title":"Change pipeline name","text":"<ul> <li>Select pipeline and click e<code>Edit</code></li> <li>Click the <code>vertical ellipse</code>, in the upper right corner to the right of the Run button</li> <li>Select either <code>Triggers</code> or <code>Variables</code> which will bring up the Visual Designer</li> <li>Select the YAML tab</li> <li>Specify the new build pipeline name</li> </ul>"},{"location":"Azure/Pipeline/Pipeline/#job","title":"Job","text":"<p>A build contains one or more jobs. Most jobs run on an agent. A job represents an execution boundary of a set of steps. All the steps run together on the same agent.</p> <p>Types of jobs: - Agent pool jobs: run on an agent that is part of an agent pool - Container jobs: run in a container on an agent part of an agent pool - Deployment group jobs: run on systems in a deployment group - Agentless jobs: run directly on the Azure DevOps - also often called Server Jobs</p>"},{"location":"Azure/Pipeline/Pipeline/#azure-cli-devops-extension","title":"Azure CLI DevOps Extension","text":"<p>https://adamtheautomator.com/azure-devops/#logging-in-with-the-azure-cli <pre><code>az extension add --name azure-devops\naz devops configure --defaults organization=https://&lt;website&gt;\n\naz devops project create --name &lt;project-name&gt;\naz devops configure --defaults project=&lt;project-name&gt;\n</code></pre></p>"},{"location":"Azure/Pipeline/Pipeline/#bash-task","title":"Bash task","text":"<p>https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/utility/bash?view=azure-devops</p>"},{"location":"Azure/Pipeline/Pipeline/#anaconda-pipeline","title":"Anaconda pipeline","text":"<p>https://docs.microsoft.com/en-us/azure/devops/pipelines/ecosystems/anaconda?view=azure-devops&amp;tabs=ubuntu</p>"},{"location":"Azure/Pipeline/Python/","title":"Python","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/ecosystems/python?view=azure-devops</p> <p>https://medium.com/@anthonypjshaw/azure-pipelines-with-python-by-example-aa65f4070634</p>"},{"location":"Azure/Pipeline/Python/#use-python-version","title":"Use Python Version","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/tool/use-python-version?view=azure-devops&amp;viewFallbackFrom=vsts <pre><code>- task: UsePythonVersion@0\n  displayName: Use python version\n  inputs:\n    versionSpec: '3.9'  #python version\n    addToPath: true     #prepend the retrieved Python version to the PATH environment variable\n    architecture: 'x64' #options: x86 (windows only), x64 (this argument applies only on windows agents)\n</code></pre></p>"},{"location":"Azure/Pipeline/Python/#package-python-pip-authenticate","title":"Package: Python Pip Authenticate","text":"<p>Must run before use pip to download python distributions to an authenticated package source such as Azure Artifacts.</p> <p>https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/package/pip-authenticate?view=azure-devops <pre><code>- task: PipAuthenticate@1\n  displayName: Pip artifact feeds authenticate\n  inputs:\n    artifactFeeds: 'proj1/myFeed1, myFeed2' #list of feed names, can use ${{ variables.artifact_feed }}\n\n# Use command line tool to 'pip install'\n- script: |\n    pip install myPackage\n</code></pre></p>"},{"location":"Azure/Pipeline/Python/#publish-build-artifacts","title":"Publish Build Artifacts","text":"<p>Publish build artifacts to Azure Pipelines, TFS, or a file share.</p> <p>https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/utility/publish-build-artifacts?view=azure-devops <pre><code>- task: PublishBuildArtifacts@1\ndisplayName: Publish build artifacts\ninputs:\n    pathToPublish: $(Agent.TempDirectory)/pkg\n    artifactName: pkg\n</code></pre></p>"},{"location":"Azure/Pipeline/Python/#download-pipeline-artifacts","title":"Download Pipeline Artifacts","text":"<p>Download pipeline artifacts from earlier stages in this pipeline, or from another pipeline.</p> <p>https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/utility/download-pipeline-artifact?view=azure-devops <pre><code>- task: DownloadPipelineArtifact@2\n  displayName: Download pipeline artifacts\n  inputs:\n    artifact: 'pkg'\n    path: $(Build.SourcesDirectory)\n</code></pre></p>"},{"location":"Azure/Pipeline/Python/#docker","title":"Docker","text":"<p>Build and push Docker images to any container registry by using a Docker registry service connection.</p> <p>https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/build/docker?view=azure-devops&amp;tabs=yaml <pre><code>- task: Docker@2\n  displayName: Login to ACR\n  inputs:\n    command: login\n    containerRegistry: dockerRegistryServiceConnection1\n- task: Docker@2\n  displayName: Login to Docker Hub\n  inputs:\n    command: login\n    containerRegistry: dockerRegistryServiceConnection2\n- task: Docker@2\n  displayName: Build a Docker image\n  inputs:\n    command: build\n    repository: $(imageRepository)\n    containerRegistry: $(dockerRegistryServiceConnection)\n    dockerfile: $(dockerfilePath)\n    arguments: '--platform linux/amd64 --build-arg ubuntu_version=20.04 --build-arg image_version=1.0.0'\n    tags: |\n        $(tag)\n        latest\n- task: Docker@2\n  displayName: Push an image to container registry\n  inputs:\n    command: push\n    repository: $(imageRepository)\n    containerRegistry: $(dockerRegistryServiceConnection)\n    tags: |\n        $(tag)\n        latest\n</code></pre></p>"},{"location":"Azure/Pipeline/Task/","title":"Task","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/process/tasks?view=azure-devops&amp;tabs=yaml</p>"},{"location":"Azure/Pipeline/Template/","title":"Template","text":"<p>https://docs.microsoft.com/en-us/azure/devops/pipelines/process/templates?view=azure-devops</p>"},{"location":"Azure/Pipeline/Template/#order","title":"order","text":"<p>script(bash, task etc.) must in the first line. generally in this order: - script - displayName - inputs - condition</p>"},{"location":"Azure/Pipeline/Template/#template-file","title":"template file","text":"<pre><code>#pipeline-template.yml\nparameters:\n- name: test\n  type: bool\n  default: False\nvariables:\n- name: check\n  value: ok\nstages:\n- stage: buildstage\n  pool:\n    vmImage: windows-latest\n  jobs:\n  - job: buildjob\n    steps:\n    - script: echo Building\n      displayName: 'Base: Build'\n      condition: and(succeeded(), or(eq(variables['check'], 'ok'), eq('${{ parameters.test }}', True)))\n</code></pre>"},{"location":"Azure/Pipeline/Template/#pipeline-file","title":"pipeline file","text":"<pre><code>#azure-pipeline.yml\ntrigger:\n  - master\n  - refs/tags/*\npr:\n  - master\npool:\n  vmImage: 'linux-latest'\nvariables:\n  var1: v1\nresources:\n  repositories:\n    - repository: templates\n      type: github\n      name: &lt;namespace&gt;/&lt;pipeline-template&gt;\n      endpoint: unknown\n      ref: refs/heads/&lt;branch-name&gt;\njobs:\n  - template: jobs/pipeline-template.yml@templates\n    parameters:\n      test: True\n</code></pre>"},{"location":"Azure/Pipeline/Trigger/","title":"Trigger","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/build/triggers?view=azure-devops</p>"},{"location":"Azure/Pipeline/VarSet/","title":"##vso[task.setvariable variable=release;isReadOnly=true]$release","text":"<p>The Azure Pipelines command <code>echo \"##vso[task.setvariable variable=release;isReadOnly=true]$release\"</code> is used to set a pipeline variable in a specific way. </p>"},{"location":"Azure/Pipeline/VarSet/#syntax-and-components","title":"Syntax and Components","text":"<ol> <li><code>##vso[task.setvariable]</code>:</li> <li> <p>This is an Azure Pipelines logging command used to set or update variables during a pipeline run. It communicates directly with the Azure DevOps agent to change variable values.</p> </li> <li> <p><code>variable=release</code>:</p> </li> <li> <p>This specifies the name of the variable you want to set or update. In this case, the variable name is <code>release</code>.</p> </li> <li> <p><code>isReadOnly=true</code>:</p> </li> <li> <p>This option marks the variable as read-only. Once a variable is set with <code>isReadOnly=true</code>, it cannot be changed by subsequent steps in the pipeline. This is useful to ensure that critical variables remain constant once they've been set.</p> </li> <li> <p><code>$release</code>:</p> </li> <li>This represents the value to assign to the <code>release</code> variable. The <code>$release</code> notation implies that the value is taken from a previously defined variable or expression.</li> </ol>"},{"location":"Azure/Pipeline/VarSet/#example-scenario","title":"Example Scenario","text":"<ul> <li>Purpose: You want to set a pipeline variable named <code>release</code> to a certain value and ensure that this variable is immutable (read-only) for the remainder of the pipeline execution.</li> <li>Value: The value assigned to the <code>release</code> variable is derived from another variable or is dynamically generated during the pipeline execution.</li> </ul>"},{"location":"Azure/Pipeline/VarSet/#how-it-works","title":"How It Works","text":"<ol> <li>Execution: During a pipeline run, the command <code>echo \"##vso[task.setvariable variable=release;isReadOnly=true]$release\"</code> is executed by the Azure Pipelines agent.</li> <li>Setting Variable: The <code>task.setvariable</code> command sets the value of the <code>release</code> variable to the value specified by <code>$release</code>.</li> <li>Read-Only: By specifying <code>isReadOnly=true</code>, the <code>release</code> variable is locked, preventing any further changes to it.</li> </ol>"},{"location":"Azure/Pipeline/VarSet/#example-usage-in-a-pipeline","title":"Example Usage in a Pipeline","text":"<pre><code>jobs:\n- job: SetVariable\n  steps:\n  - script: |\n      # Define a value for the variable\n      release_value=\"v1.2.3\"\n\n      # Set the variable 'release' and make it read-only\n      echo \"##vso[task.setvariable variable=release;isReadOnly=true]$release_value\"\n    displayName: 'Set release variable'\n\n  - script: |\n      # Use the read-only variable\n      echo \"The release version is $(release)\"\n    displayName: 'Display release variable'\n</code></pre>"},{"location":"Azure/Pipeline/VarSet/#key-points","title":"Key Points","text":"<ul> <li>Persistence: The variable <code>release</code> will be available to subsequent steps in the pipeline, but it cannot be modified after it's set as read-only.</li> <li>Scope: The variable <code>release</code> will only be available in the pipeline run in which it was set and won't persist across different pipeline runs.</li> </ul> <p>This command is a powerful way to manage and secure variable values within Azure Pipelines, ensuring that critical data remains unchanged throughout the pipeline's execution.</p>"},{"location":"Azure/Pipeline/VarSystem/","title":"Azure Pipelines System Variables","text":"<p>Azure Pipelines provides a set of predefined system variables that you can use in your pipelines. These variables are automatically populated with information about the pipeline execution environment, agent, build, and more.</p>"},{"location":"Azure/Pipeline/VarSystem/#build-information","title":"Build Information","text":"<ul> <li><code>Build.BuildId</code>: The ID of the build.</li> <li><code>Build.BuildNumber</code>: The build number.</li> <li><code>Build.DefinitionName</code>: The name of the build pipeline.</li> <li><code>Build.DefinitionId</code>: The ID of the build pipeline.</li> <li><code>Build.QueuedBy</code>: The name of the person who queued the build.</li> <li><code>Build.SourceBranch</code>: The source branch of the build (e.g., <code>refs/heads/main</code>).</li> <li><code>Build.SourceBranchName</code>: The name of the source branch (e.g., <code>main</code>).</li> <li><code>Build.SourceVersion</code>: The commit ID of the source branch.</li> <li><code>Build.Repository.Name</code>: The name of the repository.</li> <li><code>Build.Repository.Provider</code>: The type of repository (e.g., <code>TfsGit</code>, <code>GitHub</code>).</li> <li><code>Build.RequestedFor</code>: The name of the person who requested the build.</li> </ul>"},{"location":"Azure/Pipeline/VarSystem/#miscellaneous","title":"Miscellaneous","text":"<ul> <li><code>Build.ArtifactStagingDirectory</code>: The directory used to stage build artifacts.</li> <li><code>Build.BinariesDirectory</code>: The directory where binaries are placed.</li> <li><code>Build.SourcesDirectory</code>: The directory where source code is downloaded.</li> <li><code>Build.Repository.Uri</code>: The URL of the repository.</li> </ul>"},{"location":"Azure/Pipeline/VarSystem/#agent-information","title":"Agent Information","text":"<ul> <li><code>Agent.OS</code>: The operating system of the agent (e.g., <code>Windows_NT</code>, <code>Linux</code>, <code>Darwin</code> for macOS).</li> <li><code>Agent.Name</code>: The name of the agent.</li> <li><code>Agent.MachineName</code>: The name of the machine where the agent is running.</li> <li><code>Agent.Version</code>: The version of the agent.</li> <li><code>Agent.ToolsDirectory</code>: The directory where tools are installed on the agent.</li> <li><code>Agent.TempDirectory</code>: The temporary directory used by the agent.</li> <li><code>Agent.BuildDirectory</code>: The build directory used by the agent.</li> </ul>"},{"location":"Azure/Pipeline/VarSystem/#pipeline-information","title":"Pipeline Information","text":"<ul> <li><code>System.DefinitionId</code>: The ID of the pipeline definition.</li> <li><code>System.DefinitionName</code>: The name of the pipeline definition.</li> <li><code>System.TeamProject</code>: The name of the project.</li> <li><code>System.TeamFoundationCollectionUri</code>: The URL of the Azure DevOps organization.</li> <li><code>System.TeamFoundationServerUri</code>: The URL of the Team Foundation Server (if using TFS).</li> <li><code>System.JobId</code>: The ID of the job currently running.</li> <li><code>System.StageName</code>: The name of the current stage in the pipeline.</li> <li><code>System.PhaseName</code>: The name of the current phase in the pipeline.</li> <li><code>System.PreviousSuccessfulBuildId</code>: The ID of the previous successful build.</li> <li><code>System.PullRequest.PullRequestId</code>: The ID of the pull request (for PR builds).</li> </ul>"},{"location":"Azure/Pipeline/VarSystem/#release-information","title":"Release Information","text":"<ul> <li><code>Release.ReleaseId</code>: The ID of the release.</li> <li><code>Release.ReleaseName</code>: The name of the release.</li> <li><code>Release.EnvironmentName</code>: The name of the release environment.</li> <li><code>Release.ReleaseDescription</code>: The description of the release.</li> <li><code>Release.DefinitionId</code>: The ID of the release definition.</li> <li><code>Release.DefinitionName</code>: The name of the release definition.</li> </ul>"},{"location":"Azure/Pipeline/Variable/","title":"Variable","text":""},{"location":"Azure/Pipeline/Variable/#predefined-variable","title":"Predefined variable","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/build/variables?view=azure-devops&amp;tabs=yaml - Agent.TempDirectory - Build.SourcesDirectory - Build.BuildId</p>"},{"location":"Azure/Pipeline/Variable/#user-defined-variable","title":"user defined variable","text":"<pre><code>variables:\n  python_path: 'C:/Python/Python39/'\n  artifact_feed: 'my/my_artifact_feed'\n  fail_on_no_coverage: true\n  dockerRegistryServiceConnection: 'dev'\n  imageRepository: my/app'\n  dockerfilePath: './docker/Dockerfile'\n  tag: '$(Build.BuildId)'\n</code></pre>"},{"location":"Azure/Pipeline/Variable/#azure-pipeline-variables","title":"Azure Pipeline Variables","text":"<p>In Azure Pipelines, both variables and parameters are used to pass and manage values, but they serve different purposes and have different use cases.</p>"},{"location":"Azure/Pipeline/Variable/#purpose","title":"Purpose","text":"<p>Variables are used to store and manage values that you can use throughout your pipeline.  They can be defined at various scopes and are typically used for storing values that might change during the pipeline execution, such as build numbers, branch names, or environment-specific settings.</p>"},{"location":"Azure/Pipeline/Variable/#types","title":"Types","text":"<ol> <li>Predefined System Variables: Provided by Azure Pipelines, such as <code>Build.BuildId</code>, <code>Agent.OS</code>, and <code>System.TeamProject</code>.</li> <li>User-defined Variables: Defined by the user in the YAML file or the pipeline UI.</li> </ol>"},{"location":"Azure/Pipeline/Variable/#scope","title":"Scope","text":"<ul> <li>Pipeline-Level: Accessible throughout the entire pipeline.</li> <li>Job-Level: Scoped to a specific job.</li> <li>Step-Level: Scoped to a specific step.</li> </ul>"},{"location":"Azure/Pipeline/Variable/#syntax","title":"Syntax","text":"<ul> <li>YAML: <code>${{ variables['variableName'] }}</code> or <code>$(variableName)</code></li> <li>Script: <code>$VARIABLE_NAME</code> (when passed as environment variables)</li> </ul>"},{"location":"Azure/Pipeline/Variable/#example","title":"Example","text":"<pre><code>variables:\n  buildConfiguration: 'Release'\n  myVariable: 'Hello'\n\nsteps:\n- script: |\n    echo $(myVariable)\n    echo $(buildConfiguration)\n  condition: eq(variables['Agent.OS'], 'windows_nt')\n  displayName: 'Print Variables'\n</code></pre>"},{"location":"Azure/Pipeline/Variable/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Can be overridden by pipeline runs or deployment configurations.</li> <li>Can be set in different places: YAML files, the Azure Pipelines UI, or as part of pipeline runs.</li> <li>Values can be dynamic and change during runtime.</li> <li>Can be used in conditional statements, to set environment variables, or to configure task inputs.</li> </ul>"},{"location":"Azure/Route/Balancer/","title":"Balancer","text":""},{"location":"Azure/Route/Balancer/#front-door","title":"Front Door","text":"<p>Like Application Gateway, but with the difference being in the scope.</p>"},{"location":"Azure/Route/Balancer/#traffic-manager","title":"Traffic Manager","text":"<p>Use the Domain Name Service (DNS) to redirect requests to an appropriate endpoint determined by the health and configuration of the endpoint.</p>"},{"location":"Azure/Route/Balancer/#load-balancer","title":"Load Balancer","text":"<p>Work at the infrastructure level.</p>"},{"location":"Azure/Route/Balancer/#application-gateway","title":"Application Gateway","text":"<p>Have more information compared to Azure load balancers in order to make decisions on request routing and load balancing between servers.</p>"},{"location":"Azure/SQLServer/SQLServer/","title":"SQL Server","text":""},{"location":"Azure/SQLServer/SQLServer/#connect-to-postgres-sql-via-azcli","title":"connect to postgres SQL via azcli","text":"<p>https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/connect-azure-cli <pre><code>az postgres flexible-server connect -n &lt;servername&gt; -u &lt;username&gt; -p \"&lt;password&gt;\" -d &lt;databasename&gt;\n</code></pre></p>"},{"location":"Azure/Security/Bastion/","title":"Bastion","text":"<p>Azure Bastion uses your browser to connect to VMs in your virtual network over  - secure shell (SSH) or - remote desktop protocol (RDP) by using their private IP addresses. The VMs don't need public IP addresses, client software, or special configuration.</p> <p>Azure Bastion is a fully managed service that provides  - more secure and seamless Remote Desktop Protocol (RDP) and  - Secure Shell Protocol (SSH) access to virtual machines (VMs) without any exposure through public IP addresses</p>"},{"location":"Azure/Security/Credential/","title":"Credential","text":""},{"location":"Azure/Security/Credential/#this-request-is-not-authorized-to-perform-this-operation-using-this-permission","title":"This request is not authorized to perform this operation using this permission","text":"<ul> <li>The Azure Blob stoarge path might not be correct.</li> </ul>"},{"location":"Azure/Security/Credential/#check-which-credential-is-used-by-python","title":"Check which credential is used by python","text":"<pre><code>import sys\nimport logging\nfrom azure.identity import DefaultAzureCredential\n\nlogger = logging.getLogger('azure.identity')\nlogger.setLevel(logging.INFO)\n\nhandler = logging.StreamHandler(stream=sys.stdout)\nformatter = logging.Formatter('[%(levelname)s] %(name)s: %(message)s')\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n\ncredential = DefaultAzureCredential()\n</code></pre> <p>If the message is like <pre><code>[INFO] azure.identity.aio._credentials.environment: No environment configuration found.\n[INFO] azure.identity.aio._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n</code></pre> but still crashed in jupyter notebook when get files from Azure Blob Storage via <code>adlfs</code> <pre><code>ClientAuthenticationError: DefaultAzureCredential failed to retrieve a token from the included credentials.\nAttempted credentials:\n    EnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\n    ManagedIdentityCredential: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.\n    SharedTokenCacheCredential: SharedTokenCacheCredential authentication unavailable. No accounts were found in the cache.\n    AzureCliCredential: Azure CLI not found on path\n    AzurePowerShellCredential: Failed to invoke PowerShell.\n</code></pre> Restart <code>jupyter lab</code> server. </p>"},{"location":"Azure/Security/Credential/#azure-chainedtokencredential-fails-after-password-change","title":"Azure ChainedTokenCredential Fails after Password Change","text":"<pre><code>SharedTokenCacheCredential: Azure Active Directory error '(invalid_grant) AADSTS50173: \nThe provided grant has expired due to it being revoked, a fresh auth token is needed. \nThe user might have changed or reset their password.\n</code></pre> <p>Solution 1, delete the cache the credential uses.  When solution 3 does not work try this one. <pre><code>%LOCALAPPDATA%\\.IdentityService\\msal.cache\n</code></pre></p> <p>Solution 2,  <pre><code>credential = DefaultAzureCredential(exclude_shared_token_cache_credential=True)\n</code></pre></p> <p>Solution 3, Clear Azure credential cache using az cli or powershell use az cli <pre><code>az account clear\n</code></pre> use powershell - Install <code>Az-Cmdlets</code> for powershell - Run <code>Clear-AzContext</code> to clear cached credentials in powershell</p>"},{"location":"Azure/Security/Credential/#add-credentials-via-client-apps-registration","title":"Add credentials via client app's registration","text":"<p>Let container access the azure blob storage via <code>EnvironmentCredential</code>: https://learn.microsoft.com/en-us/dotnet/api/azure.identity.environmentcredential?view=azure-dotnet</p> <p>https://learn.microsoft.com/en-us/azure/active-directory/develop/quickstart-configure-app-access-web-apis#add-credentials-to-your-web-application</p>"},{"location":"Azure/Security/Firewall/","title":"Azure Firewall","text":""},{"location":"Azure/Security/NSG/","title":"Network Security Group (NSG)","text":"<p>NSGs are applied at the virtual network subnet level or directly to individual network interfaces.</p>"},{"location":"Azure/Security/PrivateLink/","title":"Private Link","text":"<p>https://learn.microsoft.com/en-us/training/modules/introduction-azure-private-link/2-what-is-azure-private-link</p>"},{"location":"Azure/Security/Security/","title":"Security","text":""},{"location":"Azure/Security/Security/#category","title":"category","text":"<ul> <li>storage security: data encryption, client-side encryption, key-vault</li> <li>network security: vpn, network security groups</li> <li>identity/access control: rbac, multi-factor authentication, password policies</li> <li>monitoring processes: storage analytics and security logs</li> </ul>"},{"location":"Azure/Security/Security/#ensure-azure-container-app-is-not-accecible-from-public-internet","title":"Ensure azure container app is not accecible from public internet","text":"<p>To avoid deploying an Azure Container App to the public internet, you can implement various security measures and configurations to restrict access to your container app. Here's a general outline of steps you can take: - Disable Public Endpoints: Make sure that you haven't configured any public endpoints or exposed unnecessary ports that could potentially lead to public internet access. - Use Private Endpoints: Azure Private Link allows you to access services privately over an Azure Virtual Network. You can create a private endpoint for your Azure Container App, which will enable you to connect to the container app over a private IP address from within your virtual network, rather than over the public internet. - Network Security Groups (NSGs): Use NSGs to control inbound and outbound traffic to your container app. Configure NSGs to allow traffic only from specific IP ranges or virtual networks that need access, and deny all other traffic. - Firewalls: Utilize Azure Firewall or third-party firewalls to control and filter traffic to and from your container app. Configure rules to allow only necessary traffic while blocking everything else. - Service Tags: Azure provides service tags for various Azure services, including Azure Container Registry and Azure Storage. You can configure NSG rules to allow traffic only from these service tags, ensuring that your container app can interact with these services securely. -  VNet Integration: If your container app needs to access other Azure services, you can integrate your virtual network with those services using Azure VNet Integration. This will allow your app to communicate privately with those services without exposing them to the public internet. - Authentication and Authorization: Implement strong authentication mechanisms for your container app. Use Azure Active Directory (AAD) or other identity providers to manage access. Additionally, implement role-based access control (RBAC) to restrict access to specific users or roles. - SSL/TLS Encryption: If your container app serves content over HTTPS, use SSL/TLS certificates to encrypt communication between clients and the app. This ensures data confidentiality and integrity. - Monitoring and Logging: Implement robust monitoring and logging to detect any unauthorized access attempts or unusual activities. Azure Monitor and Azure Security Center can help you with this. - Regular Updates and Security Patches: Keep your container app and its underlying components up to date with the latest security patches to prevent vulnerabilities that could be exploited. - Penetration Testing: Conduct regular penetration testing to identify potential security weaknesses in your setup and address them before they can be exploited.</p>"},{"location":"Azure/Security/SecurityCenter/","title":"Security Center","text":""},{"location":"Azure/Storage/Access/","title":"Access","text":""},{"location":"Azure/Storage/Access/#anonymous-access-disabled-by-default","title":"anonymous access (disabled by default)","text":"<p>https://learn.microsoft.com/en-gb/azure/storage/blobs/anonymous-read-access-configure?tabs=portal#allow-or-disallow-public-read-access-for-a-storage-account</p>"},{"location":"Azure/Storage/Access/#cross-tenant-replication-disabled-by-default","title":"cross-tenant replication (disabled by default)","text":"<p>https://learn.microsoft.com/en-gb/azure/storage/blobs/object-replication-prevent-cross-tenant-policies?tabs=portal</p>"},{"location":"Azure/Storage/Access/#sas-token","title":"sas token","text":"<p>Permissions for a directory, container, or blob:  https://learn.microsoft.com/en-us/rest/api/storageservices/create-service-sas?redirectedfrom=MSDN</p> <p>how to create a sas token: https://learn.microsoft.com/en-gb/rest/api/storageservices/delegate-access-with-shared-access-signature</p>"},{"location":"Azure/Storage/BlobStorage/","title":"Blob Storage","text":"<p>https://github.com/Azure-Samples/AzureStorageSnippets/blob/master/blobs/howto/python/blob-devguide-py/blob-devguide-blobs.py</p> <p>https://learn.microsoft.com/en-us/samples/azure/azure-sdk-for-python/storage-blob-samples/</p>"},{"location":"Azure/Storage/BlobStorage/#when-to-use","title":"when to use","text":"<ul> <li>Ideal for storing unstructured data, like media files (videos, images, audio), logs, scientific data, and archives.</li> <li>Data is stored as objects called blobs, which can be very large (up to petabytes).</li> <li>Access is typically programmatic through code or APIs.</li> <li>Offers various access tiers for cost optimization based on access frequency.</li> </ul>"},{"location":"Azure/Storage/BlobStorage/#list-blobs","title":"list blobs","text":"<p>https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-list-python <pre><code>ContainerClient.list_blobs      #name and metadata, tags, and other information associated with each blob\nContainerClient.list_blob_names #only blob name\nContainerClient.walk_blobs      #hierarchical listing\n</code></pre></p> <p>list blob storage containers with container resource_manager_id <pre><code>az storage container list --account-name &lt;storage-account-name&gt; \\\n    --auth-mode login -o json\naz storage container list --account-name &lt;storage-account-name&gt; \\\n    --auth-mode login --query \"[].{Name:name, ResourceId:id}\"\naz storage container list --account-name &lt;storage-account-name&gt; \\\n    --account-key &lt;storage-account-key&gt; --query \"[].{Name:name, ResourceId:id}\"\n</code></pre></p> <p>example <pre><code>from azure.identity import DefaultAzureCredential\nfrom azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, BlobLeaseClient, BlobPrefix, ContentSettings\n\naccount_name = '&lt;account_name&gt;'\ncontainer_name = '&lt;container_name&gt;'\n\naccount_url = f\"https://{account_name}.blob.core.windows.net\"\ncredential = DefaultAzureCredential()\n\n# Create the BlobServiceClient object\nblob_service_client = BlobServiceClient(account_url, credential=credential)\ncontainer_client = blob_service_client.get_container_client(container=container_name)\n\nm = 0\nn = 0\nfor blob in container_client.walk_blobs(name_starts_with='2021/', delimiter='/'):\n    if isinstance(blob, BlobPrefix):\n        m += 1\n        if m &lt; 4:\n            print(f'hierarchical: {blob.name}')\n        #list_blobs_hierarchical(container_client, prefix=blob.name)\n    else:\n        n += 1\n        if n &lt; 4:\n            print(f'normal blob: {blob.name}')\n</code></pre></p>"},{"location":"Azure/Storage/BlobStorage/#blob-size-in-a-folder","title":"blob size in a folder","text":"<pre><code>total_size = 0\nfor blob in container_client.walk_blobs(name_starts_with='2021/', delimiter='/'):\n    total_size += blob.size\nprint(f'Total size: {total_size / 1024 / 1024} MB')\n</code></pre>"},{"location":"Azure/Storage/BlobStorage/#move-blob","title":"move blob","text":"<p>limitations: creation_time and last_modified cannot be preserved and cannot be updated</p> <p>can we do this? No! 'BlobClient' object has no attribute 'set_blob_properties' <pre><code># Set the creation time of the destination blob to match the creation time of the source blob\ndest_blob_properties = dest_blob_client.get_blob_properties()\ndest_blob_properties.creation_time = blob_properties.creation_time\ndest_blob_client.set_blob_properties(blob_properties=dest_blob_properties)\n</code></pre></p> <pre><code>def move_blob(\n    container_client: ContainerClient,\n    source_blob_fullpath: str,\n    dest_blob_path: str,\n):\n    \"\"\"\n    Move blob file to another folder in the same blob container\n    \"\"\"\n    # Make sure source blob exists\n    source_blob = container_client.get_blob_client(blob=source_blob_fullpath)\n    if source_blob.exists():\n        # Lease source blob during copy to prevent other clients from modifying it\n        lease = BlobLeaseClient(client=source_blob)\n        lease.acquire(-1) # Create an infinite lease\n\n        # Get source blob properties\n        source_blob_properties = source_blob.get_blob_properties()\n\n        # Copy blob\n        blob_filename = source_blob_fullpath.rsplit('/', 1)[-1]\n        dest_blob = container_client.get_blob_client(blob=f'{dest_blob_path}/{blob_filename}')\n        dest_blob.start_copy_from_url(source_url=source_blob.url)\n\n        # Break source blob lease\n        if source_blob_properties.lease.state == \"leased\":\n            lease.break_lease()\n\n        # Delete source blob\n        source_blob.delete_blob()\n\n        return source_blob_properties\n</code></pre>"},{"location":"Azure/Storage/BlobStorage/#blob-with-python","title":"blob with Python","text":"<pre><code>import os, uuid\nfrom azure.storage.blob import BlobServiceClient, ContainerClient, BlobClient, __version__\n\n#create BlobServiceClient\nblobsvc = BlobServiceClient.from_connection_string('blob connection string')\n\n#create container with a unique name\ncontainer_name = str(uuid.uuid4())\ncontainer = blobsvc.create_container(container_name)\n\n#create blob client\nclient = client_blobsvc.get_blob_client(container=container_name, blob=filename)\n\n#upload file to blob\nwith open(f'{filename}.txt', \"rb\") as data:\n    client.upload_blob(data)\n\n#list blobs in container\nblob_list = container.list_blobs()\nfor blob in blob_list:\n    print(\"\\t\" + blob.name)\n\n#download blob to a local file\nwith open(download_file, \"wb\") as file:\n    file.write(client.download_blob().readall())\n\n#deleting blob container\ncontainer.delete_container()\n</code></pre>"},{"location":"Azure/Storage/BlobStorage/#download-file","title":"download file","text":"<pre><code>from azure.identity import DefaultAzureCredential\nfrom azure.storage.blob import BlobServiceClient\n\ncontainer_name = 'my-container'\nstorage_account_name = 'my-storage-account'\nparquet_file_path = 'path/from/container/root.parquet'\n\naccount_url = f\"https://{storage_account_name}.blob.core.windows.net\"\ncredential = DefaultAzureCredential()\n\ntry:\n    blob_service_client = BlobServiceClient(account_url=account_url, credential=credential)\n    blob_client = blob_service_client.get_blob_client(container_name, parquet_file_path)\n\n    # Download the Parquet file\n    with open('c:/data/downloaded_file.parquet', 'wb') as f:\n        blob_data = blob_client.download_blob()\n        blob_data.readinto(f)\n    print('File downloaded successfully.')\nexcept Exception as e:\n    print(f\"Error downloading the file: {e}\")\n</code></pre>"},{"location":"Azure/Storage/BlobStorage/#upload-file","title":"upload file","text":"<pre><code>import uuid\ndef file_to_blob(local_filepath, blob_filepath, chunk_size = 4 * 1024 * 1024):\n    \"\"\"\n    Upload file to blob storage\n    \"\"\"\n    try:\n        blob_client = container_client.get_blob_client(blob_filepath)\n        block_list = []\n        with open(local_filepath,'rb') as f:\n            while True:\n                chunk = f.read(chunk_size)\n                if not chunk:\n                    break\n                blk_id = str(uuid.uuid4())\n                blob_client.stage_block(block_id=blk_id, data=chunk)\n                block_list.append(BlobBlock(block_id=blk_id))\n        blob_client.commit_block_list(block_list)\n    except Exception as exc:\n        print('Upload file error')\n</code></pre>"},{"location":"Azure/Storage/BlobStorage/#deltalake","title":"deltalake","text":"<pre><code>import duckdb\nimport pandas as pd\nfrom deltalake import DeltaTable\nfrom azure.identity import DefaultAzureCredential\n\ncontainer_name = 'my-container-name'\nstorage_account_name = 'my-storage-account-name'\ncredential = DefaultAzureCredential()\ndef get_dataframe(\n    path: str, # not include container name\n    query: str=None,\n) -&gt; pd.DataFrame:\n    token = credential.get_token(\"https://storage.azure.com/.default\").token\n    delta_table_path = f'abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{path}'\n    delta_table = DeltaTable(delta_table_path, storage_options={'bearer_token':token}).to_pyarrow_dataset()\n    with duckdb.connect() as conn:\n        conn.execute(\"SET timezone = 'UTC'\")  # force UTC timezone in DuckDB\n        conn.register(\"delta_table\", delta_table) # register as a view\n        if not query:\n            query = 'select * from delta_table'\n        results = conn.execute(query).df()\n    return results\n</code></pre>"},{"location":"Azure/Storage/CLI/","title":"CLI","text":""},{"location":"Azure/Storage/CLI/#storage-account","title":"storage account","text":"<pre><code># list storage accounts\naz storage account list -o table\n# show storage account properties\naz storage account show --name &lt;account-name&gt; --resource-group &lt;resource-group&gt;\n# get storage account id\naz storage account show --name &lt;account-name&gt; --resource-group &lt;resource-group&gt; --query id\n# check has public access or not\naz storage account show --name &lt;account-name&gt; --resource-group &lt;resource-group&gt; --subscription &lt;sid&gt;--query allowBlobPublicAccess\n# update storage account property\naz storage account update --name &lt;account-name&gt; --resource-group &lt;resource-group&gt; --allow-cross-tenant-replication false\n# list role assignments\naz role assignment list --scope &lt;storage-account-id&gt; --query [].id\n</code></pre>"},{"location":"Azure/Storage/CLI/#storage-container","title":"storage container","text":"<pre><code># list storage containers\naz storage container list --account-name &lt;account-name&gt; -o table --auth-mode login\n# show storage container properties\naz storage container show --name &lt;container-name&gt; --account-name &lt;storage-account&gt; --auth-mode login\n# list role assignment\naz role assignment list --scope &lt;storage-account-id&gt;/blobServices/default/containers/&lt;container-name&gt;\n</code></pre>"},{"location":"Azure/Storage/FileStorage/","title":"File Storage","text":""},{"location":"Azure/Storage/FileStorage/#when-to-use","title":"when to use","text":"<ul> <li>Designed for storing structured data, like documents, application data, and user folders.</li> <li>Presents a familiar file system interface, allowing users to mount the storage as a network drive on virtual machines or locally.</li> <li>Supports common file protocols like SMB for easy access.</li> <li>Ideal for sharing files among applications and users.</li> </ul>"},{"location":"Azure/Storage/FileStorage/#mount-fileshare-as-local-drive","title":"mount fileshare as local drive","text":"<p>https://learn.microsoft.com/en-us/azure/storage/files/storage-files-quick-create-use-windows - data storage -&gt; file shares -&gt; connect</p>"},{"location":"Azure/Storage/IO/","title":"IO","text":""},{"location":"Azure/Storage/IO/#adlfs","title":"adlfs","text":"<p>cavaet: The <code>AzureBlobFileSystem</code> in <code>adlfs</code> does not support concurrent read/write.  <pre><code>from adlfs.spec import AzureBlobFileSystem\nfrom azure.identity.aio import DefaultAzureCredential\n\nfs = AzureBlobFileSystem(\n    account_name=account_name,\n    credential=DefaultAzureCredential(),\n)\n\nwith fs.open(path) as f:\n    df = pd.read_parquet(path=f, columns=columns)\n</code></pre></p> <p>If use <code>ManagedIdentityCredential</code> will get this error: ManagedIdentityCredential authentication unavailable, no response from the IMDS endpoint.</p>"},{"location":"Azure/Storage/IO/#azure-storage-blob-read-parquet-file-to-df","title":"Azure Storage Blob: Read parquet file to df","text":"<pre><code># option 1\nblob_name = 'dev/data.parquet'\nblob_client = container.get_blob_client(blob=blob_name)\nstream_downloader = blob_client.download_blob()\n\nstream = BytesIO()\nstream_downloader.readinto(stream)\ndf = pd.read_parquet(stream, engine='pyarrow')\n\n# option 2\nstream_downloader = container_client.download_blob(blob_name)\nstream = BytesIO(stream_downloader.readall())\ndf = pd.read_parquet(stream, engine='pyarrow')\n</code></pre>"},{"location":"Azure/Storage/IO/#azure-storage-blob-write-df-to-parquet-file","title":"Azure Storage Blob: Write df to parquet file","text":"<p>Method 1: using pandas and adlfs (seems not working 100% when file too large), also adlfs not support concurrent read/write <pre><code>from adlfs import AzureBlobFileSystem\n# create file system\nfs = AzureBlobFileSystem(\n    account_name = '&lt;AZURE_STORAGE_ACCOUNT&gt;',\n    credential = DefaultAzureCredential(),\n)\n# read parquet from azure blob storage\nwith fs.open(filepath) as f:\n    df = pd.read_parquet(f)\ndf = pq.read_table(filepath, filesystem=fs).to_pandas() #another way\n# write parquet to azure blob storage\nwith fs.open(filepath, mode='wb') as f:\n    df.to_parquet(f)\n</code></pre></p> <p>Method 2: convert df to bytes first https://stackoverflow.com/questions/54664712/how-to-store-pandas-dataframe-data-to-azure-blobs-using-python</p> <ul> <li>use <code>io.BytesIO</code></li> <li>Apache Arrow <code>BufferOutputStream</code>: writes to the stream without the overhead of going through Python, less copies are made and the GIL is released <pre><code>import io\nimport pandas as pd\nfrom azure.identity import DefaultAzureCredential\nfrom azure.storage.blob import BlobServiceClient\n\n# get blob client\ncredential = DefaultAzureCredential()\nblob_service = BlobServiceClient(\n    account_url=\"https://&lt;my_account_name&gt;.blob.core.windows.net\",\n    credential=credential,\n)\nblob_client = blob_service.get_blob_client(\n    container=container_name,\n    blob=blob_path,\n)\n\n# upload file: method 1\nparquet_bytes = io.BytesIO()\ndf.to_parquet(parquet_bytes, engine='pyarrow')\nparquet_bytes.seek(0)  # change the stream position back to the beginning after writing\nblob_client.upload_blob(data=parquet_bytes)\n\n# upload file: method 2\ntable = pa.Table.from_pandas(df)\nbuf = pa.BufferOutputStream()\npq.write_table(table=table, where=buf, compression='zstd')\nblob = buf.getvalue()\nbuf = pa.py_buffer(blob) #buf.to_pybytes() will make a copy of the data\nwith fs.open(filepath, mode='wb') as f:\n    f.write(buf)\n</code></pre></li> </ul>"},{"location":"Azure/Storage/Logs/","title":"Logs","text":""},{"location":"Azure/Storage/Logs/#enable-logs","title":"enable logs","text":"<p>https://learn.microsoft.com/en-us/azure/storage/common/manage-storage-analytics-logs?tabs=azure-portal</p>"},{"location":"Azure/Storage/Logs/#download-logs","title":"download logs","text":"<p>https://learn.microsoft.com/en-us/azure/storage/common/manage-storage-analytics-logs?tabs=azure-portal#download-storage-logging-log-data</p>"},{"location":"Azure/Storage/Logs/#analyze-logs","title":"analyze logs","text":"<p>https://learn.microsoft.com/en-us/troubleshoot/azure/azure-storage/troubleshoot-latency-storage-analytics-logs</p>"},{"location":"Azure/Storage/Logs/#why-the-folder-logs-is-empty-in-my-azure-blob-storage-account","title":"why the folder $logs is empty in my azure blob storage account?","text":"<p>If the <code>$logs</code> folder in your Azure Blob Storage account is empty, it could be due to several reasons. Here are some common explanations: - Logging is Disabled: By default, logging for Azure Blob Storage is not enabled. If you haven't configured logging for your storage account, no log files will be generated, and the <code>$logs</code> container will remain empty. - Log Retention Policy: Even if you have enabled logging, log files may be automatically deleted based on your retention policy settings. Check your storage account's logging settings and retention policy to ensure that logs are being retained for a duration that suits your needs. - Log Settings: It's possible that you have configured logging but with specific filters that are not generating any log entries. Check your logging settings to make sure you are capturing the events and operations you are interested in. - Delayed Logging: Sometimes, it can take a little time for log data to appear in the <code>$logs</code> container. Logs may not be generated instantly, and there could be a delay in their availability. - Permissions: Ensure that you have the necessary permissions to access the <code>$logs</code> container. If your account doesn't have the correct permissions, you won't be able to see the log files.</p> <p>To address these issues, you can: - Verify that logging is enabled for your storage account. - Check the retention policy settings to ensure logs are kept for an adequate duration. - Review your logging configuration to ensure it captures the necessary events. - Wait for some time to see if logs appear with a slight delay. - Confirm that you have the required permissions to access the <code>$logs</code> container.</p> <p>Additionally, keep in mind that Azure services and configurations may change over time, so it's a good practice to refer to the Azure portal and documentation for the most up-to-date information regarding your Azure Blob Storage account and its logs.</p>"},{"location":"Azure/Storage/Network/","title":"Network Security","text":"<p>https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security?tabs=azure-portal</p>"},{"location":"Azure/Storage/Network/#nework-rules","title":"nework rules","text":"<p>https://learn.microsoft.com/en-us/cli/azure/storage/account/network-rule?view=azure-cli-latest</p>"},{"location":"Azure/Storage/Performance/","title":"Performance","text":"<p>https://learn.microsoft.com/en-us/troubleshoot/azure/azure-storage/troubleshoot-storage-performance</p>"},{"location":"Azure/Storage/Performance/#why-sometimes-the-blob-storage-read-can-be-more-then-10-times-slower","title":"why sometimes the blob storage read can be more then 10 times slower???","text":"<p>not helpful: https://learn.microsoft.com/en-us/answers/questions/829113/sometimes-download-from-azure-blob-storage-is-very</p>"},{"location":"Azure/Storage/Performance/#metrics","title":"metrics","text":"<p>https://learn.microsoft.com/en-gb/azure/storage/common/storage-metrics-migration?WT.mc_id=Portal-Microsoft_Azure_Storage</p> <ul> <li><code>SuccessE2ELatency</code> and <code>SuccessServerLatency</code> metrics show the average time the storage service or API operation type is taking to process requests.</li> <li><code>SuccessE2ELatency</code> is a measure of end-to-end latency that includes the time taken to read the request and send the response in addition to the time taken to process the request (therefore, it includes network latency once the request reaches the storage service).</li> <li><code>SuccessServerLatency</code> is a measure of just the processing time and therefore excludes any network latency related to communicating with the client.</li> <li><code>Egress</code> and <code>Ingress</code> metrics show the total amount of data, in bytes, coming in to and going out of your storage service or through a specific API operation type.</li> <li><code>Transactions</code> metric shows the total number of requests that the storage service of API operation is receiving. Transactions is the total number of requests the storage service receives.</li> </ul>"},{"location":"Azure/Storage/Storage/","title":"Storage","text":""},{"location":"Azure/Storage/Storage/#storage-types","title":"Storage types","text":"<ul> <li>Azure Blob storage:</li> <li>for unstructured data such as documents, image</li> <li>Azure File storage:</li> <li>shared storage based on the SMB protocol, can be mounted as networkdrive</li> <li>accessible by pods on multiple nodes simultaneously</li> <li>Azure Disk Storage:</li> <li>block-level storage for Azure Virtual Machines</li> <li>mounted as ReadWriteOnce, only available to a single node</li> <li>Azure Table storage: NoSQL key-attribute data store for structured data</li> <li>Azure Queue storage: for storing large numbers of messages, message can be up to 64 KB in size</li> </ul>"},{"location":"Azure/Storage/Storage/#message-queue","title":"Message queue","text":"<p>Azure Service Bus - provides support for 256 KB messages - can store messages for an unlimited period - higher cost and latency</p> <p>Azure Queue storage  - provides support for 64 KB messages - can store messages for 7 days</p>"},{"location":"Azure/Synapse/Pipeline/","title":"Pipeline","text":""},{"location":"Azure/Synapse/Pipeline/#paralell-limitation","title":"paralell limitation","text":"<p>we do need to specify the max parallel run number. Cannot run all jobs in real paralell mode - need to limit the number of jobs and there is the timeout limit in the automation pipeline.</p>"},{"location":"Azure/Synapse/Pipeline/#parameter-and-variable","title":"parameter and variable","text":"<p>https://learn.microsoft.com/en-us/azure/data-factory/concepts-parameters-variables</p> <p>parameters - defined for the whole pipeline  - constant during a pipeline run - can read them during a pipeline run but unable to modify them - access parameter value: <code>@pipeline().parameters.&lt;parameter-name&gt;</code></p> <p>variables - can be set at the start of a pipeline,  - read and modified during a pipeline run through a Set Variable activity - not thread safe, e.g. accessed from within a parallel iteration activity - access variable value: <code>@variables('&lt;variable-name&gt;')</code></p>"},{"location":"Azure/Synapse/Pipeline/#activity-user-property","title":"activity user property","text":"<p>https://www.techtalkcorner.com/monitor-azure-data-factory-user-properties/ - Easily monitor pipelines, complements perfectly with Azure Data Factory annotations - Find errors faster so you can solve them  - Enhance the governance of pipelines and datasets </p>"},{"location":"Azure/Synapse/Pipeline/#execute-pipeline","title":"execute pipeline","text":"<p>https://learn.microsoft.com/en-us/azure/data-factory/control-flow-execute-pipeline-activity</p>"},{"location":"Azure/Synapse/Pipeline/#execution-and-triggers","title":"execution and triggers","text":"<p>https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers</p>"},{"location":"Azure/Synapse/Pipeline/#forloop-call-different-pipelines","title":"ForLoop call different pipelines","text":"<p>https://www.purplefrogsystems.com/2022/04/how-to-parameterise-the-execute-pipeline-activity-in-azure-synapse-analytics/</p> <p>Call different pipelines with different parameters</p>"},{"location":"Azure/Synapse/Studio/","title":"Synapse Studio","text":""},{"location":"Azure/Synapse/Synapse/","title":"Synapse","text":"<p>Synapse Analytics is <code>managed SQL Data Warehouse</code>.</p> <p>A limitless analytics service that brings together - data integration, - enterprise data warehousing, and - big data analytics.</p> <p>https://azure.microsoft.com/en-us/resources/data-engineer-learning-journey/</p>"},{"location":"Azure/Synapse/Workspace/","title":"Synapse Workspace","text":"<p>https://microsoftlearning.github.io/mslearn-synapse/Instructions/Labs/01-Explore-Azure-Synapse.html</p>"},{"location":"Azure/Synapse/Workspace/#create-azure-synapse-analytics-workspace","title":"Create Azure Synapse Analytics workspace","text":"<p>Install <pre><code>Install-Module Az.Resources  \nInstall-Module Az.Storage   #Get-AzStorageAccount, Set-AzStorageBlobContent\nInstall-Module Az.Sql\n</code></pre></p> <p>Install sqlcmd and bcp (bulk copy program): <pre><code>Install-Module -Name SqlServer #does not help\ncurl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/20.04/prod.list | sudo tee /etc/apt/sources.list.d/msprod.list\nsudo apt update &amp;&amp; sudo apt install mssql-tools unixodbc-dev\n</code></pre> Set the <code>sqlcmd</code> and <code>bcp</code> path - see <code>Linux/PowerShell/Creating a PowerShell profile</code>.</p> <p><code>Connect-AzAccount #login</code></p> <p>Then run pwsh to register resource providers: - Microsoft.Synapse - Microsoft.Sql - Microsoft.Storage - Microsoft.Compute</p> <p>And create: - Resource group - Synapse Analytics workspace     - Datalake storage account gen2         - Apache Spark pools     - Dedicated SQL pools      - Data Explorer pools</p> <ul> <li>Pause the Data Explorer Pool</li> <li>Grant permissions on the datalake storage account</li> <li>Create the SQL database</li> <li>Pause the SQL Pool</li> </ul>"},{"location":"Azure/Synapse/Workspace/#injest-data-with-a-pipeline","title":"Injest data with a pipeline","text":""},{"location":"Azure/Synapse/Workspace/#use-a-serverless-sql-pool-to-analyze-data","title":"Use a serverless SQL pool to analyze data","text":""},{"location":"Azure/Synapse/Workspace/#use-a-spark-pool-to-analyze-data","title":"Use a Spark pool to analyze data","text":""},{"location":"Azure/Synapse/Workspace/#use-a-dedicated-sql-pool-to-query-a-data-warehouse","title":"Use a dedicated SQL pool to query a data warehouse","text":""},{"location":"Azure/Synapse/Workspace/#explore-data-with-a-data-explorer-pool","title":"Explore data with a Data Explorer pool","text":""},{"location":"Coding/Notepad%2B%2B/","title":"Notepad++","text":""},{"location":"Coding/Notepad%2B%2B/#shortcut-keys","title":"shortcut keys","text":"<ul> <li><code>Ctrl + L</code> cut current line</li> </ul>"},{"location":"Coding/Notepad%2B%2B/#select-column-to-file-end","title":"select column to file end","text":"<ul> <li>use the Begin/End Select feature on the Edit menu [select start]</li> <li>Alt + Left Click to select end line and column</li> <li>use the Begin/End Select feature on the Edit menu [select end]</li> </ul>"},{"location":"Coding/Notepad%2B%2B/#regex","title":"regex","text":""},{"location":"Coding/Notepad%2B%2B/#replace-empty-lines","title":"replace empty lines","text":"<pre><code>type ^\\R ( for exact empty lines) or\n^\\h*\\R ( for empty lines with blanks, only).\nLeave the Replace with zone empty.\n</code></pre>"},{"location":"Coding/Notepad%2B%2B/#replace-float","title":"replace float","text":"<pre><code>(\\d+(\\.\\d+)?) -&gt; ,\\1,\n</code></pre>"},{"location":"Coding/Notepad%2B%2B/#match-commas","title":"match commas","text":"<pre><code>^[^,\\n]*((,[^,\\n]*){3}$)\n#matches a single comma followed by zero or more\n#  characters that are not comma or newline 3 times\n(,[^,\\n]*){3}\n</code></pre>"},{"location":"Coding/Notepad%2B%2B/#match-lines-start-end-with","title":"match lines start-end with","text":"<pre><code>&lt;\\!\\-\\-(.*?)\\-\\-&gt;\nstart((.|\\n|\\r|\\r\\n)*?)end\nstart(((.|\\n|\\r|\\r\\n)(?!start))*?)end #correct\n\nstart((.|\\n|\\r|\\r\\n)(?!end))*?V056(.|\\n|\\r|\\r\\n)*?end\n#(.|\\n|\\r|\\r\\n) means \"any char (new line included)\"\n#(?!end) means \"not followed by end\"\n#*? means \"match as short as possible\" (while * means \"match as long as possible\")\n</code></pre>"},{"location":"Coding/Notepad%2B%2B/#gurobi-lp-format-using-notepad-regexp","title":"gurobi lp format using notepad++ regexp","text":"<p>requirements: - add <code>\\r\\n</code> after <code>:</code> - add <code>\\r\\n</code> before <code>+,-,&gt;=,&lt;=,=</code> - ignore <code>+,-</code> etc.</p> <p>solution: - find: <code>(:\\s)|(?&lt;!\\s{3})(\\s[+\\-&lt;&gt;=]=?\\s)</code> - repl: <code>(?1\\1\\r\\n   )(?2\\r\\n  \\2)</code></p>"},{"location":"Coding/Standard/","title":"standard","text":"<p>code guid: - decouple components - separate general functionality from specific functionality (create utility) - make code reusable (make abstract classes)</p>"},{"location":"Coding/Teams/","title":"Teams","text":""},{"location":"Coding/Teams/#inline-code-block","title":"inline code block","text":"<p>A pair of single backticks is for inline code.</p>"},{"location":"Coding/Teams/#multiline-code-block","title":"multiline code block","text":"<ul> <li>On a new line type three backticks, then give space,</li> <li>then type or paste the multi line of code or text.</li> <li>Then type enter twice to come out of the block.</li> </ul>"},{"location":"Coding/Teams/#delete-files-from-chat","title":"delete files from chat","text":"<p>must delet them from OneDrive: https://onedrive.live.com</p>"},{"location":"Coding/Algorithm/BloomFilter/","title":"Bloom filter","text":"<p>https://brilliant.org/wiki/bloom-filter/</p> <p>A space-efficient probabilistic technique to test if an element is a member of a set.</p>"},{"location":"Coding/Bash/Alias/","title":"Alias","text":"<p>Bash alias does not directly accept parameters. We have to create a function.</p>"},{"location":"Coding/Bash/Alias/#create-a-permanent-alias","title":"create a permanent alias","text":"<pre><code>vi ~/.bash_aliases               #edit\nalias my_alias='sudo -- sh -c \"/root/bin/chk_disk &amp;&amp; dnf update\"' #add alias\nsource ~/.bash_aliases           #active alias\n</code></pre>"},{"location":"Coding/Bash/Array/","title":"Array","text":""},{"location":"Coding/Bash/Array/#create-an-array","title":"create an array","text":"<pre><code>a=(first second third)\necho $a      #first\necho ${a[0]} #first\n</code></pre>"},{"location":"Coding/Bash/Bash/","title":"bash","text":"<p>book: bash cookbook</p> <p>review and note ch1-5: 1-181</p> <p>https://linuxize.com/post/bash-case-statement/</p>"},{"location":"Coding/Bash/Bash/#ch1-intro","title":"Ch1 intro","text":""},{"location":"Coding/Bash/Bash/#add-dir-to-path-variable","title":"add dir to PATH variable","text":"<pre><code>export PATH=\"$PATH:~/script\"\n</code></pre>"},{"location":"Coding/Bash/Bash/#change-script-file-permission","title":"change script file permission","text":"<pre><code>chmod u+x script1.sh #change\nls -l script1.sh #check\n</code></pre>"},{"location":"Coding/Bash/Bash/#run-by-special-shell","title":"run by special shell","text":"<pre><code>rbash script_name.sh\nsh script_name.sh\nbash -x script_name.sh\n</code></pre> <p>The specified shell will start as a subshell of your current shell and execute the script. This is done when you want the script to start up with specific options or under specific conditions which are not specified in the script.</p> <p>If you don't want to start a new shell but execute the script in the current shell, you source it: <pre><code>source script_name.sh\n</code></pre></p>"},{"location":"Coding/Bash/Bash/#ch2-write-and-debug","title":"Ch2 write and debug","text":"<p>Entire script: traces of each command plus its arguments are printed to standard before execution: <pre><code>bash -x script1.sh\n</code></pre></p> <p>Debugging part(s) of the script, they can be added between <pre><code>set -x #activate debugging from here\nw\nset +x #stop debugging from here\n</code></pre></p> <p>Other options <pre><code>set -f #disable file name generation using metacharacters\nset -v #print shell input lines as they are read\nset -x #print command traces before executing command\n</code></pre></p> <p>They can also be added to the first line: <pre><code>#!/bin/bash -xv\n</code></pre></p>"},{"location":"Coding/Bash/Bash/#ch3-bash-env","title":"Ch3 bash env","text":""},{"location":"Coding/Bash/Bashrc/","title":".bashrc","text":""},{"location":"Coding/Bash/Bashrc/#bashrc-vs-bashenv","title":"<code>.bashrc</code> vs <code>.bashenv</code>","text":"<p>Both <code>.bashrc</code> and <code>.bashenv</code> are configuration files used with the Bash shell, but they have distinct purposes in terms of when they are read.</p>"},{"location":"Coding/Bash/Bashrc/#bashrc-interactive-non-login-shells","title":"<code>.bashrc</code> - Interactive Non-Login Shells","text":"<ul> <li>Executed whenever you start a new interactive shell that is not a login shell. </li> <li>Interactive shells are those you typically use when working on your terminal.</li> <li>Login shells occur when you first log in to a system through SSH or the physical console.</li> <li>Commonly used for:<ul> <li>Aliases: Shortcuts for frequently used commands</li> <li>Functions: Reusable blocks of commands</li> <li>PS1 customization: Defining how your bash prompt looks</li> <li>History settings: Controlling how Bash remembers past commands</li> </ul> </li> </ul>"},{"location":"Coding/Bash/Bashrc/#bashenv-less-common","title":"<code>.bashenv</code> (Less Common)","text":"<ul> <li>Not as widely used as <code>.bashrc</code>.</li> <li>Executed whenever an interactive shell is started, regardless of login or non-login status. </li> <li>Typically used for:<ul> <li>Environment variables: Settings that affect how programs behave (e.g., PATH for finding executables)</li> </ul> </li> </ul>"},{"location":"Coding/Bash/Bracket/","title":"Bracket","text":"<p>https://stackoverflow.com/questions/2188199/how-to-use-double-or-single-brackets-parentheses-curly-braces</p>"},{"location":"Coding/Bash/Bracket/#test-construct","title":"<code>[]</code> Test construct","text":"<p><code>[</code> is just a different name for the <code>test</code> command, and <code>]</code> is required as the last parameter, for readability and aesthetic reasons. <pre><code>x=2\nv=x\nif [ $v == x ] ; then echo yes ; else echo no ; fi\n# if test $v == x ; then echo yes ; else echo no ; fi\nyes\n</code></pre></p>"},{"location":"Coding/Bash/Bracket/#extended-test-construct","title":"<code>[[]]</code> Extended test construct","text":"<pre><code>v=x\nif [[ $v == 1 ]] ; then echo yes ; else echo no ; fi\nno\n</code></pre> <ul> <li>Using the [[ ... ]] test construct, rather than [ ... ] can prevent many logic errors in scripts</li> <li>the &amp;&amp;, ||, &lt;, and &gt; operators work within a [[ ]] test, despite giving an error within a [ ] construct</li> <li>Arithmetic evaluation of octal / hexadecimal constants takes place automatically within a [[ ... ]] construct</li> </ul>"},{"location":"Coding/Bash/Bracket/#double-parentheses-arithmetic-operations","title":"<code>(())</code> Double Parentheses: arithmetic operations","text":"<pre><code>(( i = 78 ))\ni = $(( 20 + 5 ))\n(( i++ ))\n</code></pre>"},{"location":"Coding/Bash/Bracket/#var","title":"<code>${var}</code>","text":"<p>${var} is just a disambiguation mechanism, so <code>${var}text</code> can be different to <code>$vartext</code>.</p> <pre><code>echo Variable: ${var}1\nvar: x1\n\n{ date; top -b -n1 | head ; } &gt;logfile\n</code></pre>"},{"location":"Coding/Bash/Bracket/#command","title":"<code>$(command)</code>","text":"<p>$(command) is a modern synonym for <code>command</code> which stands for command substitution; it means run command and put its output here. <pre><code>round_k() {\n    echo $(( ($1 + 500) / 1000 * 1000 ))\n}\nx=600\ny=$(round_k $x)\necho $y\n</code></pre></p>"},{"location":"Coding/Bash/Bracket/#x","title":"<code>($x)</code>","text":"<p>treat as array init by parentheses, each word an element</p>"},{"location":"Coding/Bash/DateTime/","title":"Datetime","text":""},{"location":"Coding/Bash/DateTime/#basic","title":"basic","text":"<pre><code>$(date \"+%F %H:%M:%S\"); #datatime\n\n# 2 days ago\n$(date -d \"2 days ago\" +\"%Y-%m-%d\"); #current date -2 days, not supported by all platforms\necho $(date -d@\"$(($(date +%s) - $((2*86400))))\" +\"%Y-%m-%d\") #more basic\n\n# 2 days later\n$(date --date='+2 days' +%F); #current date +2 days in format yyyy-mm-dd\n$(date -d@\"$(( $(date +%s) + 2 * 86400 ))\" +\"%Y-%m-%d\"); #stripped more basic version\n</code></pre>"},{"location":"Coding/Bash/DateTime/#day-of-month","title":"day of month","text":"<pre><code>if [[ $(date +%d) == 1 ]]; then r=360; else r=30; fi\nif [[ $(date +%d) == 1 &amp;&amp; $(( $(date +%m)%2 )) == 1 ]]; then r=180; fi #month is odd\n</code></pre>"},{"location":"Coding/Bash/DateTime/#example-of-scripts-for-binsh","title":"example of scripts for <code>/bin/sh</code>","text":"<p>How to change scripts to work in sh</p> <ol> <li><code>numdays=$(($NDAYS-1));</code> This line tries to subtract 1 from the value of <code>NDAYS</code>, but it seems that the shell is interpreting <code>$NDAYS</code> as a string rather than a number. To fix this, you can use double parentheses for arithmetic expansion:</li> <li>Replace: <code>numdays=$(($NDAYS-1));</code></li> <li> <p>With: <code>numdays=$((NDAYS-1));</code></p> </li> <li> <p><code>if [[ $(date +%d) == 10 ]]; then numdays=$((30*6-1)); fi;</code> This is to check if the current day is the 10th day of the month. However, the date format <code>+%d</code> returns the day of the month with leading zeros (e.g., <code>07</code> instead of <code>7</code>). To fix this, you can use the <code>-eq</code> operator for integer comparison and remove the leading zeros:</p> </li> <li>Replace: <code>if [[ $(date +%d) == 10 ]]; then numdays=$((30*6-1)); fi;</code></li> <li> <p>With: <code>if [ $(date +%e) -eq 10 ]; then numdays=$((30*6-1)); fi;</code></p> </li> <li> <p><code>if [[ $(date +%d) == 10 &amp;&amp; $(( $(date +%m)%2 )) == 1 ]]; then numdays=$((30*13-1)); fi;</code> Here, we check if it's the 10th day of the month and if the current month is an odd number. However, you should ensure that each opening <code>[</code> has a corresponding closing <code>]</code>:</p> </li> <li>Replace: <code>if [[ $(date +%d) == 10 &amp;&amp; $(( $(date +%m)%2 )) == 1 ]]; then numdays=$((30*13-1)); fi;</code></li> <li>With: <code>if [ $(date +%e) -eq 10 ] &amp;&amp; [ $(( $(date +%-m) % 2 )) -eq 1 ]; then numdays=$((30*13-1)); fi;</code></li> </ol>"},{"location":"Coding/Bash/EnvVar/","title":"Env var","text":""},{"location":"Coding/Bash/EnvVar/#set-env-var","title":"set env var","text":"<pre><code>export v=x\necho $v\n</code></pre>"},{"location":"Coding/Bash/EnvVar/#set-env-var-from-command-output","title":"set env var from command output","text":"<pre><code>MY_ENV=\"$(my_command \"$my_par\")\"\n</code></pre>"},{"location":"Coding/Bash/Error/","title":"Error","text":""},{"location":"Coding/Bash/Error/#exit-if-has-errors","title":"exit if has errors","text":"<pre><code>set -eoux pipefail;\n</code></pre>"},{"location":"Coding/Bash/FileEdit/","title":"File Edit","text":""},{"location":"Coding/Bash/FileEdit/#merge-and-grep-files-to-new-file","title":"merge and grep files to new file","text":"<pre><code>cat ./*.txt | grep 'my-test-name' &gt; test.txt\n</code></pre>"},{"location":"Coding/Bash/FileEdit/#merge-two-yml-files","title":"merge two yml files","text":"<pre><code>cat file1.yml &lt;(echo \"---\") file2.yml &gt; merged.yml\n</code></pre>"},{"location":"Coding/Bash/FileIO/","title":"File IO","text":""},{"location":"Coding/Bash/FileIO/#read-file-first-line-to-var-dat","title":"read file first line to var <code>dat</code>","text":"<pre><code>read -r dat &lt; \"$HOME/data.txt\"\n</code></pre>"},{"location":"Coding/Bash/Filepath/","title":"Filepath","text":""},{"location":"Coding/Bash/Filepath/#strip-off-leading-path","title":"strip off leading path","text":"<pre><code>${fp##*/}\n</code></pre>"},{"location":"Coding/Bash/Function/","title":"Function","text":"<p>Unlike functions in \u201creal\u201d programming languages, Bash functions don\u2019t allow you to return a value when called. When a bash function completes, its return value is the status of the last statement executed in the function, 0 for success and non-zero decimal number in the 1 - 255 range for failure.</p>"},{"location":"Coding/Bash/Function/#create-a-function","title":"create a function","text":"<p>put the defined function in <code>.bashrc</code> and run <code>source ~/.bashrc</code>. <pre><code>myfun() {\n    mv \"$1\" \"$1.bak\"\n    cp \"$2\" \"$1\"\n}\n</code></pre></p>"},{"location":"Coding/Bash/Function/#how-to-return-a-value","title":"how to return a value","text":"<p>using <code>echo</code> to return a value from a function <pre><code>myfun() {\n    echo $(( $1+1 ))\n}\n</code></pre></p>"},{"location":"Coding/Bash/LoopCase/","title":"Loop and Case","text":""},{"location":"Coding/Bash/LoopCase/#each-line-in-a-variable","title":"each line in a variable","text":"<pre><code>list=$'One\\ntwo\\nthree\\nfour'\necho \"$list\"\n\nwhile IFS= read -r line; do\n    echo \"... $line ...\"\ndone &lt;&lt;&lt; \"$list\"\n\n#one line\necho \"$list\" | while IFS= read -r line; do echo $line; done\n</code></pre>"},{"location":"Coding/Bash/LoopCase/#while","title":"while","text":"<pre><code>#arithmetic condition\nwhile (( n &lt; m )); do\n    ...\n    let n++\ndone\n\n#file system condition\nwhile [ -z \"$fn\" ]; do\n    ...\ndone\n\n#from input\nwhile read line; do\n    process $line\ndone\n\n#from file\nwhile read line; do\n    process $line\ndone &lt; file.input\n\n#cat to standard output\ncat file.input |\nwhile read line; do\n    process $line\ndone\n</code></pre>"},{"location":"Coding/Bash/LoopCase/#for-with-count","title":"for with count","text":"<pre><code>for (( i=0 ; i &lt; 10 ; i++ )); do\n    echo $i\ndone\n\nfor (( i=0, j=0 ; i+j &lt; 10 ; i++, j++ )); do\n    echo $(( i*j ))\ndone\n</code></pre>"},{"location":"Coding/Bash/LoopCase/#for-with-seq","title":"for with seq","text":"<pre><code>#`$()` runs the command in subshell and\n#returns the result with the newlines replaced by whitespace\nfor v in $(seq 1 .1 1.2) ; do\n    echo $v\ndone\n\n#preferred: can run `seq` in parallel with `while`\nseq 1 .1 1.2 |\nwhile read v ; do\n    echo $v\ndone\n</code></pre>"},{"location":"Coding/Bash/LoopCase/#case","title":"case","text":"<ul> <li><code>;;</code>  break</li> <li><code>;;&amp;</code> continue matching next patterns</li> <li><code>;&amp;</code>  fall-through, run next regardless of matching, useful for version patch <pre><code>case $fn in\n    *.gif) gif2jpg $fn\n        ;;\n    *.tif | *.TIFF) tif2jpg $fn\n        ;;\n    *) printf \"File not supported: %s\" $fn\n        ;;\nesac\n</code></pre></li> </ul> <p>case $version in     1.0) update to version 2         ;&amp;  #fall-through     2.0) update to version 3         ;&amp;  #fall-through     *) printf \"Nothing to update. It's already the latest version: %s\" $version         ;; esac ```</p>"},{"location":"Coding/Bash/Operator/","title":"Operator","text":""},{"location":"Coding/Bash/Operator/#and-and-or","title":"AND and OR","text":"<p>No <code>short circuits</code> but <code>&amp;&amp;</code>and <code>||</code> have short circuits - <code>-a</code>: [ -r \"$FN\" -a ( -f \"$FN\" -o -p \"$FN\" ) ] - <code>-o</code>: [ -z \"$V1\" -o -z \"${V2:=YIKES}\" ]</p>"},{"location":"Coding/Bash/Operator/#comparison-operators","title":"Comparison operators","text":"<p>https://tldp.org/LDP/abs/html/comparison-ops.html</p> <p>Always put the $var in quotes - <code>-n</code> is not null - <code>-z</code> is null - has zero length Using <code>-n</code> without quoting, will return true even it's empty</p> <ul> <li><code>==</code> string comparison</li> <li><code>-eq</code> numerical comparison</li> </ul> <p>If always use the math-style symbols for comparison: - do numerical tests with the <code>double-parentheses</code> syntax - do string comparisons with the <code>double-square-brackets</code> syntax</p>"},{"location":"Coding/Bash/Operator/#arithmetic-operators","title":"Arithmetic operators","text":"<p><code>=, +=, -=, *=, /=, &amp;=, \\=, ^=, &lt;&lt;=, &gt;&gt;=</code> Integer arithmetic: <pre><code>let 'i += j' 'j *= i'\nlet n+='i**2 + 1'\nn=$(( n + i**2 + 1 ))\n</code></pre></p>"},{"location":"Coding/Bash/Operator/#file-testing-unary-operators","title":"File testing unary operators","text":"<p>Exists <pre><code>-d File is a directory\n-f File is a regular file\n-e File exists\n-s File has a size greater than zero\n</code></pre></p> <p>Read/write etc <pre><code>-r File is readable\n-w File is writable\n-x File is executable\n-N File has been modified since it was last read\n</code></pre></p> <p>Types <pre><code>-b File is a block special device (for files like /dev/hda1)\n-c File is character special (for files like /dev/tty)\n-S File is a socket\n-p File is a named pipe\n-h File is a symbolic link (same as -L )\n-L File is a symbolic link (same as -h )\n</code></pre></p> <p>bit set <pre><code>-k File has its sticky bit set\n-u File has its set-user-ID (setuid) bit set\n-g File has its set-group-ID (setgid) bit set\n-O File is owned by the effective user ID\n-G File is owned by the effective group ID\n</code></pre></p>"},{"location":"Coding/Bash/Operator/#file-testing-binary-operators","title":"File testing binary operators","text":"<pre><code>[ file_1 -nt file_2 ] #file_1 is newer than file_2\n[ file_1 -ot file_2 ] #file_1 is older than file_2\n[ file_1 -ef file_2 ] #file_1 and file_2 have the same device or inode numbers\n</code></pre>"},{"location":"Coding/Bash/Parameter/","title":"Parameter","text":"<p>Cheat sheet: https://devhints.io/bash</p>"},{"location":"Coding/Bash/Parameter/#special-parameter","title":"Special parameter","text":"<pre><code>$#  #number of arguments\n$@  #array-like construct of all positional parameters, {$1, $2, ...}\n$*  #IFS expansion of all positional parameters, $1 $2, ...\n$0  #filename of shell script\n$1  #positional parameters, 1, 2, ...\n$$  #pid of shell\n$!  #pid of last background task\n$?  #exit status of most recent foreground pipeline (last command)\n$_  #last argument of previous command\n</code></pre>"},{"location":"Coding/Bash/Parameter/#default-value","title":"Default value","text":"<pre><code>${var:-x} #default value is x\n</code></pre>"},{"location":"Coding/Bash/Parameter/#shift-one-parameter","title":"shift one parameter","text":"<pre><code>while (( $# &gt; 0 )); do\n    case $1 in\n        [0-9]*) LEN=$1\n        ;;\n        -c) shift; CHAR=${1:--}\n        ;;\n        *) printf 'usage: %s [-c X] [#]\\n' ${0##*/} &gt;&amp;2 #strip off leading path part\n        exit 2\n        ;;\n    esac\n    shift #drop 2st param so $# to be one smaller\ndone\n</code></pre>"},{"location":"Coding/Bash/PatternMatch/","title":"Pattern Match","text":""},{"location":"Coding/Bash/PatternMatch/#wildcard","title":"wildcard","text":"<p><pre><code>if [[ \"$fn\" == *.jpg ]]\n</code></pre> - <code>*</code> to match any number of characters - <code>?</code> to match a single character - <code>[]</code> for including a list of possible characters</p>"},{"location":"Coding/Bash/PatternMatch/#shell-options","title":"shell options","text":"<p><pre><code>shopt -s extglob\nif [[ \"$fn\" == *.@(jpg|jpeg) ]]\n</code></pre> <code>extglob</code> option deals with extended pattern matching (or globbing) and <code>shopt -s nocasematch</code> for case-insensitive match. - <code>@(...)</code> only one occurrence - <code>+(...)</code> one or more occurrences - <code>?(...)</code> zero or one occurrence - <code>*(...)</code> zero or more occurrences - <code>!(...)</code> not this, but anything else</p>"},{"location":"Coding/Bash/PatternMatch/#regular-expression","title":"regular expression","text":"<p>Filename example: <code>Ludwig Van Beethoven - 05 - \"Coriolan\" Overture, Op. 12.ogg</code> <pre><code>for fn in *\ndo\n    if [[ \"$fn\" =~ \"([[:alpha:][:blank:]]*)- ([[:digit:]]*) - (.*)$\"\n    then\n        echo Track ${BASH_REMATCH[2]} is ${BASH_REMATCH[3]}\n        mv \"$fn\" \"Track${BASH_REMATCH[2]}\"\n    fi\ndone\n</code></pre> - <code>${BASH_REMATCH[0]}</code> entire string matched by the regular expression - <code>${BASH_REMATCH[1]}</code> any subexpressions, 1 ... n</p>"},{"location":"Coding/Bash/Select/","title":"Select","text":"<p><code>select</code> statement can create simple character-based screen menu. You select the number and the value for the number will be recorded.</p> <pre><code>files=$(ls)\nselect fn in $files; do\n    echo File: $fn\n    #do something\ndone\n</code></pre>"},{"location":"Coding/Bash/Setting/","title":"setting","text":""},{"location":"Coding/Bash/Setting/#make-bash-shell-safe","title":"make bash shell safe","text":"<p>avoid pipeline errors passing silently <pre><code>set -eoux pipefail\n</code></pre> https://sipb.mit.edu/doc/safe-shell/#:~:text=set%20%2Do%20pipefail%20causes%20a,command%20in%20a%20pipeline%20errors.</p> <p>https://gist.github.com/vncsna/64825d5609c146e80de8b1fd623011ca</p> <ul> <li>e: if a command fails, set <code>-e</code> will make the whole script exit, instead of just resuming on the next line.</li> <li>o: set <code>-o pipefail</code> causes a pipeline to produce a failure return code if any command errors. Normally, pipelines only return a failure if the last command errors.</li> <li>u: treat unset variables as an error, and immediately exit.</li> <li>x: enable a mode of the shell where all executed commands are printed to the terminal.</li> </ul>"},{"location":"Coding/Bash/StrFmt/","title":"String Format","text":""},{"location":"Coding/Bash/StrFmt/#convert-to-lowercase","title":"convert to lowercase","text":"<pre><code>${var,,}\n# Example: rename files\nfor fn in *.JPG; do\n    mv \"$fn\" \"${fn,,}\"\ndone\n</code></pre>"},{"location":"Coding/Bash/StrFmt/#convert-to-uppercase","title":"convert to uppercase","text":"<pre><code>${var^^}\n</code></pre>"},{"location":"Coding/Bash/StrFmt/#swap-case","title":"swap case","text":"<pre><code>${var~~}\n</code></pre>"},{"location":"Coding/Bash/StrFmt/#convert-to-camel-case","title":"convert to camel case","text":"<pre><code>while read txt; do\n    a=($txt)      #treat as array init by parentheses, each word an element\n    echo ${a[@]^} #[@] references all elements at once, and ^ converts first character to uppercase\ndone\n</code></pre>"},{"location":"Coding/Bash/StrFmt/#printf-format","title":"printf format","text":"<p><code>printf \"%-10.10s [%8d]:\", i, v[i]</code> - <code>%-10.10s</code> left align and pad to 10 characters but also truncate at 10 characters - <code>%8d</code> assure the integer is printed in an 8-character field (right aligned)</p>"},{"location":"Coding/Bash/String/","title":"String","text":""},{"location":"Coding/Bash/String/#quotes-or-not","title":"Quotes or not","text":"<p>Quote it if it can - be empty - contain spaces (or any whitespace) - contain special characters (wildcards) - not require word splitting and wildcard expansion</p> <p>Not quoting strings with spaces - often leads to the shell breaking apart a single argument into many</p>"},{"location":"Coding/Bash/String/#single-quotes","title":"Single quotes","text":"<p>Single quotes protect the text between them verbatim. - want to suppress interpolation and special treatment of backslashes</p>"},{"location":"Coding/Bash/String/#double-quotes","title":"Double quotes","text":"<p>Double quotes are suitable when variable interpolation is required or single quotes are required in the string. - want to suppress word splitting and globbing - ant the literal to be treated as a string, not a regex</p>"},{"location":"Coding/Bash/String/#process-logs-to-csv","title":"Process logs to csv","text":"<pre><code># extract log info to csv\nlog_file=\"$1.log\"\ncsv_file=\"$1.csv\"\n\n# write csv header\necho \"ts,dept,type,value\" &gt; $csv_file\n\n# extract the desired columns from each row of the log file\ncat $log_file | while read line; do\n    text=$(echo \"$line\" | cut -d \"|\" -f 1,7)                               #get columns 1 and 7\n    ts=$(echo \"$text\" | cut -d \"|\" -f 1 | awk '{$1=$1};1' | sed \"s/,/./g\") #remove space and replace , by .\n    dept=$(echo \"$text\" | grep -oP '(?&lt;=\\[)[^\\]]+' | head -1)              #get dept in [dept]\n    last_col=$(echo \"$text\" | rev | cut -d \"]\" -f 1 | rev)                 #get last col\n    if [[ $last_col == *\"Dept started\"* ]]; then                           #is dept row\n        typ=\"task\"                                                         #get string after 'Dept: '\n        val=\\\"$(echo \"$last_col\" | grep -oP '(?&lt;=Dept: ).*' | head -1 | sed \"s/\\\"/'/g\")\\\"\n    elif [[ $last_col == *\"Finished getting\"* ]]; then                     #get string between two strings\n        typ=\\\"$(echo \"$last_col\" | grep -oP '(?&lt;=Finished getting ).*(?= in)' | head -1)\\\"\n        val=$(echo \"$last_col\" | grep -oP '(?&lt;=in ).*(?= seconds.)' | head -1)\n    else\n        continue\n    fi\n    echo \"$ts,$dept,$typ,$val\" &gt;&gt; $csv_file\ndone\n</code></pre>"},{"location":"Coding/Bash/Sudo/","title":"Sudo","text":""},{"location":"Coding/Bash/Sudo/#sudo-with-echo","title":"sudo with echo","text":"<p><code>sudo echo 7 &gt; /proc/fs/cifs/cifsFYI)</code> - only the echo command runs with elevated privileges due to the sudo prefix - the file redirection (&gt;) is still executed by your shell before sudo is applied, and it's done with your regular user's permissions</p> <p>Solution 1: Use sudo with a shell command <pre><code>sudo sh -c 'echo 7 &gt; /proc/fs/cifs/cifsFYI'\n</code></pre></p> <p>Solution 2: Use sudo with tee <pre><code>echo 7 | sudo tee /proc/fs/cifs/cifsFYI\n</code></pre></p>"},{"location":"Coding/Bash/awk/","title":"awk","text":""},{"location":"Coding/Bash/awk/#read-awk-command-from-file","title":"read awk command from file","text":"<pre><code>awk -f func.awk\n</code></pre>"},{"location":"Coding/Bash/awk/#print-selected-fields","title":"print selected fields","text":"<ul> <li><code>$0</code> is for the entire line</li> <li><code>NF</code> number of fields <pre><code>awk '{print $1}'      #select 1st field\nawk '{print $1, $NF}' #select 1st and last fields\n</code></pre></li> </ul>"},{"location":"Coding/Bash/awk/#reverse-words-in-line","title":"reverse words in line","text":"<pre><code>awk '{for (i=NF; i&gt;=0; i--) {printf \"%s \", $i;} printf \"\\n\"}'\n</code></pre>"},{"location":"Coding/Bash/awk/#sum-of-a-field","title":"sum of a field","text":"<ul> <li><code>END</code> will only execute once</li> <li><code>/^total/{next}</code> exclude first line starts with <code>total</code> <pre><code>ls -l | awk '/^total/{next} {sum += $5}; END {print sum}'\n</code></pre></li> </ul>"},{"location":"Coding/Bash/awk/#group-another-field-by-one-field","title":"group another field by one field","text":"<ul> <li>delimiter is space</li> <li>awk <code>associate array</code> is a dictionary</li> <li><code>NF &gt; 8</code> only when the number of fields &gt; 8 <pre><code>awk -F\" \" 'NF &gt; 8 {v[$1] += $2}; END {for (k in v) {printf \"%s:%d\\n\", k, v[k]}}'\n</code></pre></li> </ul>"},{"location":"Coding/Bash/awk/#select-table-rows-based-on-columns","title":"select table rows based on columns","text":"<p>Regex patterns should be put in <code>//</code> and not quoted. Include the header as well. <pre><code>#MaxDataDiskCount MemoryInMb Name NumberOfCores OsDiskSizeInMb ResourceDiskSizeInMb\naz vm list-sizes --location \"australiaeast\" -o table \\\n  | awk '(($3 == \"Name\" || $3 ~ /^Standard_DS/) \\\n      &amp;&amp; ($2 == \"MemoryInMb\" || $2 &gt;= 32000) \\\n      &amp;&amp; ($4 == \"NumberOfCores\" || ($4 &gt;= 4 &amp;&amp; $4 &lt;= 20)))'\n</code></pre></p> <p>Convert string to number input: path_to_my_file.txt <pre><code>NAME                    CPU(cores)   MEMORY(bytes)\ndev-dashboard-6570x79   2m           533Mi\ndev-task-734449-2h882   1096m        4442Mi\ndev-task-734449-d5k9m   1m           4531Mi\ndev-task-734449-d996d   37m          6232Mi\n</code></pre> code: select names starting with <code>dev-task</code> and cpu &gt; 1000m. <code>$2 + 0</code> will convert string to number. <pre><code>pods_to_delete=$(kubectl top po -n dev \\\n| awk '$1 ~ /^dev-task/ &amp;&amp; $2 ~ /m/ { sub(\"m\", \"\", $2); if ($2 + 0 &gt; 1000) print $1 }')\n\n# test\nawk '$1 ~ /^dev-task/ &amp;&amp; $2 ~ /m/ { sub(\"m\", \"\", $2); if ($2 + 0 &gt; 1000) print $1 }' \"path_to_my_file.txt\"\n</code></pre></p>"},{"location":"Coding/Bash/curl/","title":"curl","text":""},{"location":"Coding/Bash/curl/#download-to-current-folder","title":"download to current folder","text":"<p><code>-O</code> keep remote filename <pre><code>curl -O https://dev.com/api/hello.cpp\n</code></pre></p> <p>Sending a request with HEAD method (<code>-I</code>) to see the redirected url <pre><code>curl -LI https://dev.com/api/hello.cpp\n</code></pre></p> <p>Sometimes downloadd file has 0 size. Need to tell <code>curl</code> to follow HTTP redirection by using <code>-L/--location</code> <pre><code>curl -LO https://dev.com/api/hello.cpp\n</code></pre></p>"},{"location":"Coding/Bash/curl/#get-with-header","title":"get with header","text":"<pre><code>curl 'https://my.api/hello?id=1&amp;name=dev' -H 'Accept: text/csv'\ncurl 'https://my.api/hello?id=1&amp;name=dev' -H 'Accept: text/csv' -o my.csv\ncurl `https://my.api/hello -H \"X-Custom-Header: value\" -H \"Content-Type: application/json\"\n</code></pre>"},{"location":"Coding/Bash/curl/#post-with-header","title":"post with header","text":"<pre><code>curl -k -X POST https://dev.com/api/hello -H 'Content-Type: application/json' -d '{ \"date\": \"2021-11-01\", \"value\": 12 }'\n</code></pre>"},{"location":"Coding/Bash/cut/","title":"cut","text":"<p>cut parts of lines from specified files or piped data and print the result to standard output.</p>"},{"location":"Coding/Bash/cut/#basic-options","title":"basic options","text":"<ul> <li><code>-f</code> (--fields): 1,3 [select 1 and 3]; -4 [select 1 to 4]</li> <li><code>-c</code> (--characters): -c4- [select from 4th character to end]</li> <li><code>-b</code> (--bytes): -b1 [select 1st byte]</li> </ul>"},{"location":"Coding/Bash/cut/#other-options","title":"other options","text":"<ul> <li><code>-d</code> delimiter specified instead of default \u201cTAB\u201d</li> <li><code>--output-delimiter</code> specify output delimiter, default to input delimiter</li> <li><code>-s</code> (--only-delimited) not print lines without delimiters</li> <li><code>--complement</code> exclude selected</li> </ul>"},{"location":"Coding/Bash/cut/#get-second-field","title":"get second field","text":"<pre><code>&gt; echo '192.168.0.2 #test' | cut -d'#' -f2 #eq: awk -F'#' '{print $2}'\ntest\n</code></pre>"},{"location":"Coding/Bash/cut/#get-field-based-on-delimiter","title":"get field based on delimiter","text":"<p>Select field 4 <pre><code>&gt; echo '\"title\":\"Math &amp; Physics\"' | cut -d '\"' -f 4\nMath &amp; Physics\n</code></pre></p> <p>Delimiter has multiple spaces <pre><code>tr -s ' ' | cut -d ' ' -f 8\n</code></pre> use <code>tr</code> command along with squeeze option (<code>-s</code> flag ) to convert all multiple consecutive spaces to a single space.</p>"},{"location":"Coding/Bash/cut/#get-value-in-square-brackets","title":"get value in square brackets","text":"<pre><code>&gt; echo 'dev:[1.0.0]' | cut -d'[' -f2 | cut -d']' -f1\n1.0.0\n</code></pre>"},{"location":"Coding/Bash/grep/","title":"grep","text":""},{"location":"Coding/Bash/grep/#search-in-files","title":"search in files","text":"<pre><code>grep error *.log                #list files and lines with `error`\ngrep error ../tst/*.log */*.log #from multiple folders\ngrep -h error *.log             #exclude filenames\ngrep -c error *.log             #count matches in each file (include zero match)\ngrep -l error *.log             #list files with matches\ngrep -i error *.log             #case insensitive search\ngrep -v error *.log             #exclude lines with `error`\ngrep '[0-9]\\{4\\}-\\{0,1\\}[0-9]\\{2\\}-\\{0,1\\}[0-9]\\{2\\}' file #find a date\n</code></pre>"},{"location":"Coding/Bash/grep/#delete-files-with-matches-confirm-before-delete","title":"delete files with matches (confirm before delete)","text":"<p>search all files in the current folder, not subfolders <pre><code>rm -i $(grep -l 'debug' * )\n</code></pre></p>"},{"location":"Coding/Bash/grep/#switch-based-on-match","title":"switch based on match","text":"<p>if multiple files provided, stop search after found 1st <pre><code>if grep -q error tst.log ; then\n    echo \"found\"\nelse\n    echo \"not found\"\nfi\n</code></pre></p> <p>search all files <pre><code>if grep error tst.log &gt; /dev/null ; then\n    echo \"found\"\nelse\n    echo \"not found\"\nfi\n</code></pre></p>"},{"location":"Coding/Bash/ldd/","title":"ldd","text":"<p>Show shared objects and the dependencies in an executable.</p>"},{"location":"Coding/Bash/ldd/#example","title":"example","text":"<p><pre><code>sudo ldd /bin/bash\n</code></pre> - <code>-v</code> Verbose mode, print all information - <code>-u</code> Show unused direct dependencies - <code>-d</code> Execute data relocation and shows missing ELF objects - <code>-r</code> Execute data and function relocation and shows missing ELF objects and functions</p>"},{"location":"Coding/Bash/ldd/#ldconfig","title":"ldconfig","text":"<p>configure dynamic linker run-time bindings</p>"},{"location":"Coding/Bash/sed/","title":"sed","text":"<p>Perform basic text transformations on an input stream, in some ways similar to an editor. - <code>-i</code> edit files in-place instead of printing to standard output - <code>-n</code> suppress output</p>"},{"location":"Coding/Bash/sed/#replace","title":"replace","text":"<p><pre><code>s/regexp/replacement/[flags]\n</code></pre> replace <code>{\"title\":\"</code> with nothing: <code>sed 's/{\"title\":\"//g'</code></p>"},{"location":"Coding/Bash/sed/#case-convert","title":"case convert","text":"<pre><code>sed 's/.*/\\L&amp;/'  #to lower\nsed 's/.*/\\U&amp;/'  #to upper\n\nsed 'y/'$(printf \"%s\" {A..Z} \"/\" {a..z} )'/'\nsed 'y/'$(printf \"%s\" {a..z} \"/\" {A..Z} )'/'\n</code></pre>"},{"location":"Coding/Bash/sed/#print-selected-lines","title":"print selected lines","text":"<p>only print 1st line in file1 and last line in last file <pre><code>sed -n '1p;$p' one.txt two.txt three.txt\n</code></pre></p>"},{"location":"Coding/Bash/sed/#delete-selected-lines","title":"delete selected lines","text":"<p>deletes lines 2 to 5 in the input <pre><code>sed '2,5d' input.txt &gt; output.txt\n</code></pre></p>"},{"location":"Coding/Bash/sed/#delete-matching-lines-and-replace","title":"delete matching lines and replace","text":"<p>delete lines matching regex /^foo/, and replace <code>hello</code> with <code>world</code> <pre><code>seq 2 | sed -e 1ahello -e 2ifoo | sed '/^foo/d;s/hello/world/'\nseq 2 | sed -e 1ahello | sed -e '2ahahahah' -e s/hello/world/\n</code></pre></p>"},{"location":"Coding/Bash/sed/#select-lines-with-step-number","title":"select lines with step number","text":"<pre><code>seq 10 | sed -n '1~2p' #1,3,...\nseq 10 | sed -n '0~4p' #4,8,...\n</code></pre>"},{"location":"Coding/Bash/sed/#select-lines-based-on-address","title":"select lines based on address","text":"<pre><code>sed '2s/hello/world/'      #only replace on line 2\nsed '2,5s/hello/world/'    #only replace lines 2-5\nsed '2,5!s/hello/world/'   #only replace lines not 2-5\nsed '/foo/s/hello/world/'  #only replace lines contain foo\nsed '/foo/!s/hello/world/' #only replace lines not contain foo\n</code></pre>"},{"location":"Coding/Bash/sed/#select-lines-with-regex-match","title":"select lines with regex match","text":"<pre><code>sed -n '/bash$/p' #print lines end with `bash`\n\n#escape regex char, all match lines start with `/usr/home/`\nsed -n '/^\\/usr\\/home\\//p'\nsed -n '\\%^/usr/home/%p'\nsed -n '\\;^/usr/home/;p'\n</code></pre>"},{"location":"Coding/Bash/sort/","title":"sort","text":"<p>options - <code>-r</code> reverse the order - <code>-f</code> ignore case - <code>-n</code> sort data as number - <code>-u</code> unique, remove duplicates - <code>-t</code> field separator - <code>-k</code> sort via a key</p>"},{"location":"Coding/Bash/sort/#sort-ip-address","title":"sort ip address","text":"<p>192.168.0.2: sort from first field to last field as number <pre><code>sort -t . -k 1,1n -k 2,2n -k 3,3n -k 4,4n ip.list\n</code></pre></p>"},{"location":"Coding/Bash/tail/","title":"tail","text":""},{"location":"Coding/Bash/tail/#drop-first-line","title":"drop first line","text":"<pre><code>tail -n +2 a.csv\n</code></pre>"},{"location":"Coding/Bash/wget/","title":"wget","text":"<p><code>wget &lt;OPTION&gt; &lt;URL&gt;</code></p>"},{"location":"Coding/Bash/wget/#download-to-current-working-folder","title":"download to current working folder","text":"<pre><code>wget http://mirror.ctan.org/&lt;package_path+package_name.zip&gt;\n</code></pre>"},{"location":"Coding/Bash/wget/#download-to-a-target-folder","title":"download to a target folder","text":"<pre><code>wget -O $HOME/Download http://mirror.ctan.org/&lt;package_path+package_name.zip&gt;\n</code></pre>"},{"location":"Coding/Bash/wget/#unpack-the-archive","title":"unpack the archive","text":"<pre><code>mkdir -p /path/to/directory\nunzip filename.zip -d /path/to/directory\ntar -xvf filename.tar --C /path/to/directory\ntar -xvf filename.tar --directory /path/to/directory\n</code></pre>"},{"location":"Coding/Bash/wget/#dos","title":"dos","text":"<pre><code>@echo off\n\nset wget=\"wget.exe\"\nset despath=\"C:/Files\"\nset website=\"https://www.example.com/data/\"\n\n%wget% -P %despath% -A .zip -N -nd -r -l1  %website%\n%wget% -P %despath% -A .ZIP -N -nd -r -l1  %website%\n\n@pause\n</code></pre> <p> -P destination path -A extension -N Turn on time-stamping -r Turn on recursive retrieving -l1 One level depth -nd Do not create a hierarchy of directories when retrieving recursively <p>Note: extension is case sensitive. Extension .ZIP should also be considered </p>"},{"location":"Coding/Batch/CMD/","title":"cmd","text":"<p>https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/xcopy</p>"},{"location":"Coding/Batch/CMD/#read-4-cols-of-csv","title":"read 4 cols of csv","text":"<pre><code>for /f \"usebackq tokens=1-4 delims=,\" %%a in (\"file.csv\") do (\n    echo %%a %%b %%c %%d\n)\n</code></pre>"},{"location":"Coding/Batch/CMD/#input-parameter","title":"input parameter","text":"<pre><code>set inp=x\nIF [%1]==[] (\n  set inp=s\n)\n</code></pre>"},{"location":"Coding/Batch/CMD/#if-else","title":"if else","text":"<p>python --version &gt;nul 2&gt;&amp;1 &amp;&amp; ( python test.py ) || ( py test.py )</p>"},{"location":"Coding/Batch/CMD/#list-all-files-in-a-folder","title":"list all files in a folder","text":"<p>dir /b         #include folders   dir /b /a-d    #exclude folders   dir /b /a-d /s #exclude folders but include files in folders   dir /b /a-d | findstr /I /R \"^xyz*.pdf\"</p>"},{"location":"Coding/Batch/CMD/#output-to-console-and-file","title":"output to console and file","text":"<p>run.bat &gt;run.log | type run.log   run.bat | findstr /R \"^xyz???.*\" &gt;&gt; run.log 2&gt;&amp;1</p>"},{"location":"Coding/Batch/CMD/#ouput-taskmgr-to-csv-file","title":"ouput taskmgr to csv file","text":"<p>tasklist /FO csv &gt; \"%userprofile%\\desktop\\tasks.csv\"</p>"},{"location":"Coding/Batch/CMD/#rename-all-files-in-a-folder","title":"rename all files in a folder","text":"<pre><code>rem in cmd replace %% with %\nfor /f \"tokens=*\" %%a in ('dir /b') do ren \"%%a\" \"00_%%a\"\n\nfor %%f in (*.csv) do rename \"%%f\" \"%%~nf_old.csv\"\n\nrem cmd\nfor %f in (*.csv) do rename \"%f\" \"%~nf_old.csv\"\n</code></pre>"},{"location":"Coding/Batch/CMD/#kill-task","title":"kill task","text":"<pre><code>tasklist | findstr /I /c:\"mysql\"\ntasklist /v | findstr /I /c:\"mysql\"\n\ntaskkill /F /PID pid_number\ntaskkill /F /IM \"process name\"\nstart explorer\n</code></pre>"},{"location":"Coding/Batch/CMD/#delete-recycle-bin-data","title":"delete recycle bin data","text":"<pre><code>rd /s c:\\$Recycle.Bin\n</code></pre>"},{"location":"Coding/Batch/CMD/#delete-hidden-file-on-shared-drive","title":"delete hidden file on shared drive","text":"<pre><code>rem show hidden files\ndir \"networkdrive\" /A:H /B\nrem delete hidden file\ndel /A:H \"hiddenfile\"\n</code></pre>"},{"location":"Coding/Batch/CMD/#disk-performance-check","title":"disk performance check","text":"<pre><code>winsat disk -drive g\n</code></pre>"},{"location":"Coding/Batch/CMD/#change-path","title":"change path","text":"<p>setx path \"%path%;C:\\yourFolder\" Windows resource kit tools 'pathman.exe':   pathman /au c:\\Programs\\Python35 #add a path   pathman /ru c:\\Programs\\Python35 #del a path</p>"},{"location":"Coding/Batch/CMD/#escape","title":"escape \"","text":"<p>use \"\" to represent \"</p>"},{"location":"Coding/Batch/CMD/#change-service-account","title":"change service account","text":"<pre><code>SC.EXE CONFIG MyServiceName obj= \"Local Service\" password= \"\"\n</code></pre>"},{"location":"Coding/Batch/CMD/#parallel-run-from-cmd","title":"Parallel run from cmd","text":"<pre><code>rem %~dp0 is the bat file path\nstart \"\" %~dp0\\%dir%1\\My.exe 1\nstart \"\" %~dp0\\%dir%2\\My.exe 2\nstart \"\" %~dp0\\%dir%3\\My.exe 3\n</code></pre>"},{"location":"Coding/Batch/CMD/#event","title":"event","text":"<p>https://www.petri.com/managing-command-line-event-logs\\ wevtutil epl nam c:\\machine.svr.com.evt /r:machine.svr.com /u: /p:"},{"location":"Coding/Batch/Copy/","title":"copy","text":"<pre><code>@echo off\n\nset file=\"SampleFile.csv\"\nset desdir=\n\nIf not exist %file% (\n    echo Error: Input file does not exist %file%\n    goto TheEnd\n)\n\nif \"%desdir%\"==\"\" (\n    Set desdir=%~dp0\n)\necho Source file: %file%\necho Destination dir: %desdir%\necho.\n\nREM type for filename with spaces\nfor /f \"delims=\" %%a in ('type %file%') do (\n    If not exist %%a (\n        echo Error: File does not exist \"%%a\"\n        goto TheEnd\n    )\n    echo f | xcopy \"%%a\" \"%desdir%/%%~nxa\" /Y /I /Q &gt; nul\n    echo Copied file: %%~nxa\n)\n\n:TheEnd\necho.\necho all done!\n</code></pre>"},{"location":"Coding/Batch/Env/","title":"Env","text":""},{"location":"Coding/Batch/Env/#set-env-var","title":"set env var","text":"<p>do not put the value in quotes <pre><code>set NAME=my-pkg\n</code></pre></p>"},{"location":"Coding/Batch/FileSys/","title":"File sys","text":""},{"location":"Coding/Batch/FileSys/#list-file-names-in-a-folder-to-file","title":"list file names in a folder to file","text":"<pre><code>dir &lt;path&gt; /b &gt; \"files.txt\"\n</code></pre>"},{"location":"Coding/Batch/FileSys/#search-for-a-file","title":"search for a file","text":"<pre><code>dir /s tnsnames.ora\n</code></pre>"},{"location":"Coding/Batch/IO/","title":"IO","text":""},{"location":"Coding/Batch/IO/#read-first-line","title":"read first line","text":"<p>limits: - max line length of 1021 bytes, not including EOL - file must use Windows style EOL of CarriageReturn LineFeed - trailing control characters will be stripped from the line</p> <pre><code>set /p line=&lt; file.txt\necho %line%\necho %line:~3% ::skip first 3 characters\n</code></pre>"},{"location":"Coding/Batch/IO/#read-from-second-line","title":"read from second line","text":"<pre><code>@echo off\nsetlocal EnableDelayedExpansion\n\nset file=myfile.txt\nset curdir=%~dp0\n\nset /p line1=&lt; %file%\n\nset i=0\necho %date% %time% ln: %line1%\nfor /f %%a in (%file%) do (\n    set /a i=!i!+1\n    if !i! GTR 1 (\n        echo !date! !time!  !i! ln: %%a\n    )\n)\n\necho %date% %time% all done!\n</code></pre>"},{"location":"Coding/Batch/IO/#read-csv","title":"read csv","text":"<pre><code>:readcsv\n    set i=0\n    for /f \"usebackq tokens=1-2 delims=,\" %%a in (\"file.csv\") do (\n        set /a i=!i!+1\n        if !i! GTR 1 exit /b\n        echo %%a %%b\n    )\n    exit /b\n</code></pre>"},{"location":"Coding/Batch/Learn/","title":"Website","text":"<p>https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/call</p> <p>https://www.tutorialspoint.com/batch_script/batch_script_functions_with_parameters.htm</p>"},{"location":"Coding/Batch/Priority/","title":"priority","text":"<p>^Priority Level ID ^Priority Level Name ^ |256 |Realtime | |128 |High | |32768 |Above normal | |32 |Normal | |16384 |Below normal | |64 |Low |</p>"},{"location":"Coding/Batch/Priority/#change-priority-of-running-process-in-command-prompt","title":"Change Priority of Running Process in Command Prompt","text":"<pre><code>wmic process where name=\"mspaint.exe\" CALL setpriority \"Above normal\"\n</code></pre>"},{"location":"Coding/Batch/Priority/#start-application-with-a-set-priority-in-command-prompt","title":"Start Application with a Set Priority in Command Prompt","text":"<pre><code>start \"\" /AboveNormal /B \"C:\\Windows\\System32\\mspaint.exe\" &gt; out.txt\n</code></pre>"},{"location":"Coding/Batch/Priority/#set-the-io-priority-of-a-process","title":"set the IO priority of a process","text":"<p>It looks like the \"real\" way to set the IO priority of a process is using NtSetInformationProcess with the ProcessIoPriority information class. Unfortunately this API is undocumented, but you can see it in action by attaching a debugger to taskeng.exe and breaking in ExeTask::GetYourPrioritiesStraight.</p> <p>I believe the PROCESS_INFORMATION_CLASS value for ProcessIoPriority is 33 (0x21), and the priority values are as follows: - Very Low: 0 - Low: 1 - Normal: 2 - High: 3 or above?</p>"},{"location":"Coding/Batch/Robocopy/","title":"robocopy","text":"Option Description /s Copies subdirectories. Note that this option excludes empty directories /e Copies subdirectories. Note that this option includes empty directories /b Copies files in Backup mode /v produce Verbose output, showing skipped files /np not show percentage progress of each file copy /mov Moves files and deletes them from the source after they are copied /purge Deletes destination files and directories that no longer exist in the source /r:[n] number of retries /w:[t] wait time in sec between retries /mt:[n] multi-threaded switch /log:[file] log file path /xa:SH exclude hidden system files /xd folder exclude the folder"},{"location":"Coding/Batch/Robocopy/#mirror-a-folder-without-deleting-destination-files","title":"mirror a folder without deleting destination files","text":"<pre><code>Robocopy \"C:\\src\" \"F:\\bak\" /s /r:5 /w:15 /mt:4 /np /log:backup.log\n</code></pre>"},{"location":"Coding/Batch/Service/","title":"service","text":""},{"location":"Coding/Batch/Service/#change-service-log-on","title":"change service log on","text":"<p>Change service from 'Local System' account and 'Network Service', you need to clear the password and when you click apply it will fill the password automatically</p>"},{"location":"Coding/Batch/Service/#service-control","title":"service control","text":"<pre><code>@echo off\n@echo off\n\nSET username=xxx@svr.com\nSET password=xxx\n\nset test_dir=client_test\nset test_log_dir=client_test_logs\n\nfor /f %%a in (list.txt) do (\n    echo ''''%%a\n    net use \"\\\\%%a\" %password% /user:%username%\n    ping /n 1 /w 1500 %%a | (findstr /r /c:\"[0-9]*ms\" &gt; nul)\n    if %errorlevel% neq 0 (\n        echo NOT EXIST: %%a\n    ) else (\n        if \"%1\"==\"copy\" (\n            xcopy %test_dir% \\\\%%a\\c$\\%test_dir%\\ /i /s /e /y\n        ) else if \"%1\"==\"install\" (\n            sc \\\\%%a create MyService binpath= \\\\%%a\\c$\\%test_dir%\\myservice.exe\n        ) else if \"%1\"==\"start\" (\n            sc \\\\%%a start MyService\n        ) else if \"%1\"==\"stop\" (\n            sc \\\\%%a stop MyService\n        ) else if \"%1\"==\"uninstall\" (\n            sc \\\\%%a delete MyService\n        ) else if \"%1\"==\"delete\" (\n            rmdir /s /q \\\\%%a\\c$\\%test_dir%\n        ) else if \"%1\"==\"query\" (\n            sc \\\\%%a query MyService\n        ) else if \"%1\"==\"copylog\" (\n            copy /y \\\\%%a\\c$\\%test_dir%\\test.log %test_log_dir%\\log_%%a.log\n        ) else if \"%1\"==\"rmlog\" (\n            rmdir /s /q \\\\%%a\\c$\\%test_dir%\\test.log\n        ) else (\n            REM default case...\n        )\n    )\n    net use \"\\\\%%a\" /delete\n)\n\necho all done!\n</code></pre>"},{"location":"Coding/Batch/Time/","title":"time","text":"<pre><code>for /f \"skip=1\" %%x in ('wmic os get localdatetime') do if not defined MyDate set MyDate=%%x\nfor /f %%x in ('wmic path win32_localtime get /format:list ^| findstr \"=\"') do set %%x\nset fmonth=00%Month%\nset fday=00%Day%\nset today=%Year%%fmonth:~-2%%fday:~-2%\n</code></pre> <pre><code>@echo off\nset day=0\necho &gt;\"%temp%\\%~n0.vbs\" s=DateAdd(\"d\",%day%,now) : d=weekday(s)\necho&gt;&gt;\"%temp%\\%~n0.vbs\" WScript.Echo year(s)^&amp; right(100+month(s)-1,2)^&amp; right(100+day(s),2)\nfor /f %%a in ('cscript /nologo \"%temp%\\%~n0.vbs\"') do set \"result=%%a\"\ndel \"%temp%\\%~n0.vbs\"\nset \"YYYY=%result:~0,4%\"\nset \"MM=%result:~4,2%\"\nset \"DD=%result:~6,2%\"\nif \"%MM%\"==\"00\" set \"MM=12\" &amp; set /a YYYY=YYYY-1\nset \"data=%yyyy%%mm%\"\n\necho last year/month was \"%data%\"\npause\n</code></pre>"},{"location":"Coding/Batch/Time/#date","title":"date","text":"<pre><code>@echo off\n\nset _date=\"\"\ncall:isodtfunc _date\necho %_date%\ngoto:eof\n\n:isodtfunc\n:: use wmic to retrieve date and time\nfor /f \"skip=1 tokens=1-6\" %%g in ('wmic path win32_localtime get day^,hour^,minute^,month^,second^,year /format:table') do (\n  if \"%%~l\"==\"\" goto s_done\n  set _yyyy=%%l\n  set _mm=00%%j\n  set _dd=00%%g\n  set _hour=00%%h\n  set _minute=00%%i\n)\n:s_done\n\n:: pad digits with leading zeros\nset _mm=%_mm:~-2%\nset _dd=%_dd:~-2%\nset _hour=%_hour:~-2%\nset _minute=%_minute:~-2%\n\n:: display the date/time\nset %~1=%_yyyy%%_mm%%_dd%%_hour%%_minute%\ngoto:eof\n\n:datefunc\nsetlocal\nrem use findstr to strip blank lines from wmic output\nfor /f \"usebackq skip=1 tokens=1-3\" %%g in (`wmic path win32_localtime get day^,month^,year ^| findstr /r /v \"^$\"`) do (\n  set _day=00%%g\n  set _month=00%%h\n  set _year=%%i\n)\nrem pad day and month with leading zeros\nset _day=%_day:~-2%\nset _month=%_month:~-2%\nrem output format required is yyyy-mm-dd\nset %~1=%_year%-%_month%-%_day%\nendlocal\ngoto:eof\n</code></pre>"},{"location":"Coding/Batch/Tip/","title":"tip","text":""},{"location":"Coding/Batch/Tip/#escape-char","title":"escape char","text":"<p>^(^), ^&lt;^&gt;</p>"},{"location":"Coding/Batch/Tip/#variables-in-iffor-block","title":"variables in if/for block","text":"<p>Environment variables in batch files are expanded when a line is parsed. The whole blocks delimited by parentheses (such as if) counts as a \"line\" or command. So the variables defined in the if/for block are replaces by their values before the block is run.</p> <p>To solve this we should enable delayed expansion:  setlocal enabledelayedexpansion and use !your_variable!</p>"},{"location":"Coding/Batch/Watch/","title":"watch","text":""},{"location":"Coding/Batch/Watch/#send-email","title":"send email","text":"<pre><code>@echo off\nsetlocal EnableDelayedExpansion\n\nif exist \"1~%\" (\n    echo I'm OK    \n) else (\n    ::send email and attach file\n    set subject=Error happened\n    set mail_body=Line 1.^&lt;br /^&gt;Line 2 ^(example^)     \n    call :SendEmail \"!subject!\", \"!mail_body!\", \"2~%\"\n)\necho done\nexit /b\n\n:SendEmail\n    SETLOCAL\n    set mail_from=from@example.com\n    set mail_to=to@example.com\n    powershell -command \"Send-MailMessage -BodyAsHtml -From '%mail_from%' -To '%mail_to%' -SmtpServer 'mail.xx.yy' -Subject '%~1' -Body '%~2' -Attachments '%~3'\"\n    exit /b\n</code></pre>"},{"location":"Coding/Batch/Watch/#machine-watch","title":"machine watch","text":"<pre><code>@echo off\nset ip=\\\\ip.xx.yy\\c$\nif exist %ip% (\n    echo %ip% is OK\n) else (\n    goto SendEmail\n)\nexit /b\n\n:SendEmail\n    set mail_fr=my@example.com\n    set mail_to=my@gmail.com\n    set subject=From %COMPUTERNAME%\n    set mail_body=MACHINE is down\n    powershell -windowstyle hidden -command \"Send-MailMessage -From '%mail_fr%' -To '&lt;%mail_to%&gt;' -Subject '%subject%' -SmtpServer  'mail.xx.yy' -BODY '%mail_body%'\"\n    exit /b\n</code></pre>"},{"location":"Coding/Batch/other/","title":"Other","text":""},{"location":"Coding/Batch/other/#other","title":"other","text":"<pre><code>rem Output file names in dir to file\ndir /b &gt; f.txt\n\nrem Get the bat file path\nset bat_dir=%~dp0\n\nrem Get the bat file directory and join with file name\nset file_path=\"%~dp0Test.py\"\n\nrem Get the first parameter, the file path\nset workbook=\"%~1\"\n\nrem Use a variable\necho %file_path%\n\nrem get filename from path\nUse %~nxf for &lt;filename&gt;.&lt;extension&gt;\n\nrem check remote logged user\nquery user /server:machine.svr.com\nwmic.exe /node:\"machine.svr.com\" computersystem get username\n\nrem check methods from dll, open VS console prompt\ndumpbin /exports \"C:\\xx\\yy.dll\"\n\nrem Kill a process by name\ntaskkill /f /im My.exe\n\nrem Find a specific task\ntasklist |find /i \"chrome\"\n\nrem Move files\nrobocopy \"C:\\src_dir\" \"C:\\des_dir\" /move\n\nrem Delete files\ndel \"C:\\my_dir\\*.*\"\n</code></pre>"},{"location":"Coding/Blog/InstallWSL/","title":"How to install and setup WSL on Windows","text":"<p>Windows Subsystem for Linux (WSL) is one of the best options if you need both Windows and Linux at the same time but do not want to setup a dual-boot. WSL allows you to install a Linux distribution like Ubuntu, Debian, OpenSUSE, etc and you can use Linux applications and tools directly on Windows.</p>"},{"location":"Coding/Blog/InstallWSL/#prerequisites","title":"Prerequisites","text":"<p>Before install WSL you must ensure your Windows machine has been setup appropriately. - Generally you must be running Windows 10 version 2004 and higher or Windows 11 to use the installation commands below. Otherwise you need to install the WSL manually. - Another important thing is that the <code>Virtualization</code> on your machine must be enabled. If not sure, you can open the task manager and click the <code>Performance</code> tab to check. If it shows your machine is running as <code>virtual machine</code> rather than <code>virtualization</code>, you must enable the <code>virtualization</code>. - Also, your Windows features like <code>Virtual Machine Platform</code> and <code>Windows Subsystem for Linux</code> must be enabled.</p> <p>If your are not sure how to enable the required features, you can use a search engine or ChatGPT and Bard to help you.</p>"},{"location":"Coding/Blog/InstallWSL/#wsl-installation-via-commands","title":"WSL installation via commands","text":"<p>Once the prerequisites are met, WSL can be installed easily using the following commands: <pre><code># Show available Linux distributions\nwsl -l -o\n# Install a distribution such as `wsl --install -d Ubuntu-22.04`\nwsl --install -d &lt;Distribution Name&gt;\n# Show installed Linux distributions\nwsl -l -v\n</code></pre></p>"},{"location":"Coding/Blog/InstallWSL/#configuration-to-limit-wsl-memory-usage","title":"Configuration to limit WSL memory usage","text":"<p>If you do not limit the amount of memory your WSL can use, it might use most of the memory available on your machine. This might slow down your Windows applications.</p> <p>To limit the memory that can be used by WSL, you can setup it in your WSL configuration file by appending (lines must be ended with the Unix newline terminator): <pre><code>[wsl2]\nmemory=4GB\n</code></pre> Note that the WSL configuration file usually can be found at <code>C:\\Users\\&lt;your-user-name&gt;\\.wslconfig</code>.</p> <p>After the configuration setting update, run the bellow commands: <pre><code># Run it in Windows terminal\nwsl --shutdown # Restart WSL\n# Run it in WSL terminal\nfree -h --giga # Check total memory\n</code></pre></p>"},{"location":"Coding/Blog/InstallWSL/#commands-to-release-disk-space-back","title":"Commands to release disk space back","text":"<p>After using WSL for a while you might find that your hard disk is full and WSL uses most capacity. This is because WSL2 will not automatically release used disk space so we have to do it manually.</p> <p>There are two options to release the disk space used by WSL.</p>"},{"location":"Coding/Blog/InstallWSL/#compact-option-in-diskpart","title":"compact option in diskpart","text":"<p>This option uses the <code>compact</code> command in <code>diskpart</code> to release the disk space. Please remember to change the filepath accordingly. <pre><code># Run them in Windows terminal\nwsl --shutdown\ndiskpart # Will open window Diskpart\n\n# Then run them in the opened Diskpart window\nselect vdisk file=\"C:\\Users\\&lt;your-user-name&gt;\\AppData\\Local\\Packages\\CanonicalGroupLimited.Ubuntu20.04onWindows_79rhkp1fndgsc\\LocalState\\ext4.vhdx\"\nattach vdisk readonly\ncompact vdisk\ndetach vdisk\nexit\n</code></pre></p>"},{"location":"Coding/Blog/InstallWSL/#optimize-vhd","title":"optimize-vhd","text":"<p>The command <code>optimize-vhd</code> is only available in Windows 10 Pro with Hyper-v feature installed. <pre><code>cd C:\\Users\\&lt;your-user-name&gt;\\AppData\\Local\\Packages\\CanonicalGroupLimited.Ubuntu20.04onWindows_79rhkp1fndgsc\\LocalState\nwsl --shutdown\noptimize-vhd -Path .\\ext4.vhdx -Mode full\n</code></pre></p> <p>After the installation of WSL, you can do many things like in Linux. Here are a few examples: - Install other applications such as Docker, Azure CLI etc. - Setup crontab to run scheduled tasks. - Files can be copied between Windows and WSL as well. For example, copy a file from <code>C:/test/data.csv</code> to your current folder you can run <code>cp /mnt/c/test/data.csv ./data.csv</code>.</p>"},{"location":"Coding/C%23/CppAPI/","title":"Call c/c++ dll from c#/vb.net","text":""},{"location":"Coding/C%23/CppAPI/#string-parameter","title":"string parameter","text":"<pre><code>//void foo(const char *par);\n&lt;DllImport(MyDLL, CallingConvention:=CallingConvention.Cdecl, CharSet:=CharSet.Unicode)&gt;\nShared Sub foo(&lt;MarshalAsAttribute(UnmanagedType.LPStr)&gt; ByVal par As String)\nEnd Sub\n</code></pre>"},{"location":"Coding/C%23/CppAPI/#return-string","title":"return string","text":"<p> Shared Function foo() As IntPtr End Function Dim pRet As IntPtr = foo() Dim sRet As String = Marshal.PtrToStringAnsi(pRet) Dim sRet As String = Marshal.PtrToStringAuto(pRet) \u2018 Unix <p>\u201d vector parameter void foo(const int par[]);  Shared Sub foo(ByVal par() As Integer) End Sub <p>\u201d structure parameter \u2018typedef struct { \u2018 int x; \u2018 double y[2]; \u2018} struc1;</p> <p> Structure struc1 Public x As Integer [MarshalAs(UnmanagedType.ByValArray, SizeConst:=2)] Public y() as Double End Structure <p>\u2018void foo(struc1 *par);  Shared Sub foo(ByRef par As struc1) End Sub <p>Dim myStruc as struc1 = new struc1() myStruc.x = 1 myStruc.y = new Double(1) myStruc.y(0) = 1.0 myStruc.y(1) = 4.0 foo(myStruc)</p> <p>\u201d return structure \u2018struc1* foo(void);  Shared Function foo() As IntPtr End Function Dim pRet = foo() <p>\u201d callback function \u2018void setCallback(int (func)(void handle, const char *msg));  Delegate Function MyCallBack(ByVal handle As IntPtr,  ByVal msg As StringBuilder) As Integer  Shared Sub setCallBack(ByVal callbackfunc As MyCallBack) End Sub Private Function MyCallBackFunc(ByVal handle As IntPtr, ByVal msg As StringBuilder) As Integer Console.WriteLine(msg.ToString.Trim) Return 1 End Function logFunc = New MyCallBack(AddressOf MyCallackFunc) setCallBack(logFunc) <p>Please also check for other examples at: https://docs.microsoft.com/en-us/dotnet/framework/interop/marshaling-classes-structures-and-unions</p>"},{"location":"Coding/C%23/Learn/","title":"Learn","text":"<p>https://learn.microsoft.com/en-us/dotnet/csharp/</p> <p>https://github.com/Almantask/CSharp-From-Zero-To-Hero/wiki/Summary</p> <p>https://github.com/exercism/csharp</p> <p>https://github.com/KevinDockx/CSharp10DesignPatterns</p>"},{"location":"Coding/C%23/Project/","title":"Project","text":""},{"location":"Coding/C%23/Project/#create-an-api-template-using-swagger","title":"create an api template using swagger","text":""},{"location":"Coding/C%23/VSCode/","title":"VS Code","text":""},{"location":"Coding/C%23/VSCode/#install-package","title":"install package","text":"<pre><code>dotnet add package Microsoft.Extensions.Configuration.Json --version 6.0.0\ndotnet add package Microsoft.Extensions.Configuration.Binder --version 6.0.0\n</code></pre>"},{"location":"Coding/C%23/VSCode/#vs-build-tools","title":"vs build tools","text":"<p>it's free for comercial use: \\ https://social.msdn.microsoft.com/Forums/en-US/0daefd5d-8d5a-499c-bd16-c3e7aba68c4d/is-msbuild-free-for-commercial-use?forum=msbuild</p> <p>https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;rel=16</p> <p>msbuild app.csproj -t:HelloWorld -p:Configuration=Release \\ msbuild app.csproj -p:Configuration=Release</p>"},{"location":"Coding/C%23/cli/","title":"CLI","text":"<pre><code>class CliArgs\n{\n    string _dir;\n    public string Dir =&gt;_dir; \n\n    DateTime _date;\n    public DateTime Date =&gt; _date;\n\n    public int ParseArgs(string[] args)\n    {\n        int ret = 0;\n\n        List&lt;string&gt; pars;\n        Dictionary&lt;string, List&lt;string&gt;&gt; clipars;\n        ret = CliParams(args, out clipars);\n\n        //dir\n        if (clipars.TryGetValue(\"-dir\", out pars) &amp;&amp; par.Count &gt; 0) {\n            _dir = pars[0].Trim();\n            if (string.IsNullOrEmpty(_dir)) {\n                return Log.Err($\"'-dir' parameter is not a directory\");\n            }\n        }\n        else {\n            return Log.Err($\"'-dir' parameter is required but not provided\");\n        }\n\n        //date\n        if (clipars.TryGetValue(\"-date\", out pars) &amp;&amp; par.Count &gt; 0) {\n            var par = pars[0].Trim();\n            if (string.IsNullOrEmpty(par)) {\n                _date = DateTime.MinValue;\n            }\n            else {\n                _date = DateTime.ParseExact(par, \"yyyy-MM-dd HH:mm:ss\", CultureInfo.InvariantCulture);\n            }\n        }\n        else {\n            _date = DateTime.MinValue;\n        }\n\n        return ret;\n    }\n\n    int CliParams(string[] args, out Dictionary&lt;string, List&lt;string&gt;&gt; clipars)\n    {\n        int ret = 0;\n        cli = null;\n\n        int i = 0;\n        int n = args.Length;\n        clipars = new Dictionary&lt;string, List&lt;string&gt;&gt;();\n        while (i &lt; n) {\n            if (args[i][0] != '-') {\n                return Log.Err($\"Arg '{args[i]}' unexpected - should be an -option?\");\n            }\n\n            var opt = args[i];\n            var pars = new List&lt;string&gt;();\n            while (++i != n) {\n                if (args[i].Length &gt; 0 &amp;&amp; args[i][0] == '-')\n                    break;\n                pars.Add(args[i]);\n            }\n            clipars[opt] = pars;\n        }\n        return ret;\n    }\n}\n</code></pre>"},{"location":"Coding/C%23/config/","title":"config","text":"<p>\\ should be the first section in \\"},{"location":"Coding/C%23/err/","title":"error","text":""},{"location":"Coding/C%23/err/#map-hresult-and-exception","title":"Map HRESULT and Exception","text":"<p>https://docs.microsoft.com/en-us/dotnet/framework/interop/how-to-map-hresults-and-exceptions?view=netframework-4.8</p>"},{"location":"Coding/C%23/err/#error-class-not-registered","title":"Error: Class not registered","text":"<p>DAO = Microsoft.Office.Interop.Access.Dao</p> <p>When running a Visual Studio application, the default Common Language Runtime (CLR) platform in the Visual Studio is set to Any CPU. This default must be changed on 64-bit computers to compile the assemblies with the 32-bit, x86 compatible CLR.</p>"},{"location":"Coding/C%23/err/#net-framework-48-not-showing-in-vs","title":".NET Framework 4.8 not showing in VS","text":"<p>install .NET Framework 4.8 Developer Pack and restart VS</p>"},{"location":"Coding/C%23/err/#this-application-requires-one-of-the-following-versions-of-net-framework","title":"This application requires one of the following versions of .NET Framework","text":"<p>but the .Net version exists. Solution: the .NET version in the app.config file is not correct</p>"},{"location":"Coding/C%23/fmt/","title":"format","text":"<pre><code>//format\nstring msg = $\"Delayed time: {hrs:D2}:{mts:D2}:{sec:00.000}\";\n\n//StopWatch Elapsed to hh:mm:ss.fff\nstring time = $\"elapsed time: {sw.Elapsed:hh\\\\:mm\\\\:ss\\\\.ffff}\";\n\n//Utc to AEST Brisbane time\nvar aestZone = TimeZoneInfo.FindSystemTimeZoneById(\"E. Australia Standard Time\");\ndate = TimeZoneInfo.ConvertTimeFromUtc(utcTime, aestZone);\n\n//integer left pad spaces\n$\"{i,15:D}\"\n//double left pad spaces\n$\"{v,15:N4}\"\n</code></pre>"},{"location":"Coding/C%23/io/","title":"io","text":"<p>if you use StreamWriter and StreamReader without the FileStream, FileStream will be automatically created. But when you created FileStream by yourself, you have options to specify file mode and share access.</p>"},{"location":"Coding/C%23/io/#system-shutdown","title":"system shutdown","text":"<p>StreamWriter Writing NUL characters at the end of the file when system shutdown abruptly</p>"},{"location":"Coding/C%23/io/#write-to-file","title":"write to file","text":"<pre><code>const int BufferSize = 65536;  // 64 kB\nvar writer = new StreamWriter(\"filename\", true, Encoding.UTF8, BufferSize);\n\nusing (var writer = new StreamWriter(filepath, true, Encoding.UTF8, 65536)) {\n    for (int i = 1; i &lt; repeats; i++) {\n        writer.WriteLine(\"Some line of text\");\n    }\n}\n</code></pre>"},{"location":"Coding/C%23/io/#write-to-zipped-csv-file","title":"write to zipped csv file","text":"<pre><code>using(var zf = ZipFile.Open(\"file.zip\", ZipArchiveMode.Create)) {\n   var ze = zf.CreateEntry(\"file.csv\");\n   using(var zs = ze.Open()) {\n      using(var sw = new StreamWriter(zs, Encoding.UTF8)) {\n         sw.WriteLine(\"Name,Address,Email\")\n      }\n   }\n}\n</code></pre>"},{"location":"Coding/C%23/io/#concurrent-write-and-read-file","title":"concurrent write and read file","text":"<pre><code>var writer = File.Open(\"C:\\\\f.txt\", FileMode.Create, FileAccess.Write, FileShare.ReadWrite));\nvar reader = new StreamReader(File.Open(\"C:\\\\f.txt\", \n                                      FileMode.Open, FileAccess.Read, FileShare.ReadWrite)));\n</code></pre>"},{"location":"Coding/C%23/io/#append-to-log-file","title":"append to log file","text":"<pre><code>public static void ToFile(string msg, bool writeline = false)\n{\n    using (var w = File.AppendText(_logpath)) {\n        if (!writeline) {\n            w.Write(msg);\n        }\n        else {\n            w.WriteLine(msg);\n        }\n    }\n}\n</code></pre>"},{"location":"Coding/C%23/io/#color-console","title":"color console","text":"<pre><code>public static void ConsoleColorWLine(string msg, ConsoleColor? fgc = null, ConsoleColor? bgc = null)\n{\n    ConsoleColorWrite($\"{msg}\\n\", fgc, bgc);\n}\n\npublic static void ConsoleColorWrite(string msg, ConsoleColor? fgc = null, ConsoleColor? bgc = null)\n{\n    var fgColor = Console.ForegroundColor;\n    if (fgc.HasValue) {\n        Console.ForegroundColor = fgc.Value;\n    }\n    var bgColor = Console.BackgroundColor;\n    if (bgc.HasValue) {\n        Console.BackgroundColor = bgc.Value;\n    }\n    Console.Write(msg);\n    if (fgc.HasValue) {\n        Console.ForegroundColor = fgColor;\n    }\n    if (bgc.HasValue) {\n        Console.BackgroundColor = bgColor;\n    }\n}\n</code></pre>"},{"location":"Coding/C%23/json/","title":"json","text":"<p>The data contract class must have the same name as in the json results for custom datetime conversion:</p> <pre><code>using System.Net.Http;\nusing System.Net.Http.Headers;\nusing Newtonsoft.Json;\n\npublic class MyRes \n{\n    HttpClient _sol;\n\n    public MyRes()\n    {\n        var hdl = new HttpClientHandler() { UseDefaultCredentials = true };\n        _sol = new HttpClient(hdl);\n        _sol.Timeout = TimeSpan.FromMinutes(Config.ApiTimeout);\n        _sol.BaseAddress = new Uri(Config.ApiBase + \"/\");\n        _sol.DefaultRequestHeaders.Accept.Clear();\n\n        //add an accept header for JSON format\n        _sol.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application/json\"));\n    }\n\n    public List&lt;ResInfo&gt; GetResList(string urn)\n    {\n        var res = _sol.GetAsync(urn).Result;\n        if (res.IsSuccessStatusCode) {\n            //simple\n            //return res.Content.ReadAsAsync&lt;List&lt;ResInfo&gt;&gt;().Result;\n            //with custom date converter\n            string jsonContent = res.Content.ReadAsStringAsync().Result;\n            return JsonConvert.DeserializeObject&lt;List&lt;ResInfo&gt;&gt;(jsonContent);\n        }\n        else {\n            Log.Err($\"Connection failed. Status Code: {res.StatusCode}. Reason Phrase: {res.ReasonPhrase}\\n  url: {_baseAddress + urn}\");\n        }\n        return new List&lt;ResInfo&gt;();\n    }\n\n    /*\n    [{ResID:1, ResName:\"John\", ResDate:\"2021-09-01T12:09:08\"},{...}]\n    */\n    [DataContract]\n    public class ResInfo\n    {\n        [DataMember(Order = 1)] public string ResID { get; set; }\n        [DataMember(Order = 2)] public string ResName { get; set; }\n        [JsonConverter(typeof(CustomDateTimeConverter))]\n        [DataMember(Order = 3)] public DateTime ResDate { get; set; }\n    }\n}\n\npublic class CustomDateTimeConverter : Newtonsoft.Json.Converters.IsoDateTimeConverter\n{\n    public CustomDateTimeConverter()\n    {\n        base.DateTimeFormat = \"yyyy-MM-ddTHH:mm:ss\";\n    }\n}\n</code></pre>"},{"location":"Coding/C%23/opcxmlda/","title":"OpcXmlDa","text":"<p>we should first call Subscribe then periodically call SubscriptionPolledRefresh. The handle provided by Subscribe will be used as input in SubscriptionPolledRefresh. If the parameter SubscriptionPingRate in Subscribe is set too low, the handle will become invalid when call SubscriptionPolledRefresh so you always get invalid handle error.</p> <p>The SubscriptionPingRate is the requested rate in milliseconds (ms) that the server should reevaluate the existence of the client. If the client has not had any communication in the specified period, then the Server is free to clean up all resources associated with that client for this Subscription.</p> <p>The server should attempt to honor the client\u2019s request, but it may reevaluate the existence of the client at a rate faster than the SubscriptionPingRate based on its own implementation, and resource constraints. If the SubscriptionPingRate is 0, then the server will use its own algorithm to reevaluate the existence of the client. It is highly recommended that clients always specify a non-zero ping rate since specifying zero will allow the server to choose a ping rate that the client will not have knowledge of and may be inappropriate.</p>"},{"location":"Coding/C%23/opcxmlda/#cisco-anyconnect-mobile-vpn","title":"Cisco anyconnect mobile vpn","text":"<p>must stop the vpnui process otherwise when you log off, the vpn will be disconnected because this ui is started by a local user. to avoid this issue, we have to ensure the vpn is connected via vpnagent the service that is shared by all users.</p>"},{"location":"Coding/C%23/opcxmlda/#check-vpn-status","title":"check vpn status","text":"<pre><code>netsh interface show interface\n</code></pre>"},{"location":"Coding/C%23/opcxmlda/#web-reference","title":"web reference","text":"<p>Add --&gt;  Web Reference --&gt; Service Reference --&gt; Advanced --&gt; Address (URL)</p>"},{"location":"Coding/C%23/other/","title":"other","text":""},{"location":"Coding/C%23/other/#doc","title":"doc","text":"<p>https://docs.microsoft.com/en-us/dotnet/standard/events/observer-design-pattern</p>"},{"location":"Coding/C%23/other/#check-dllexe-net-version","title":"check dll/exe .net version","text":"<pre><code>$path = \"C:\\scripts\\my.dll\"\n[Reflection.Assembly]::ReflectionOnlyLoadFrom($path).CustomAttributes |\nWhere-Object {$_.AttributeType.Name -eq \"TargetFrameworkAttribute\" } |\nSelect-Object -ExpandProperty ConstructorArguments |\nSelect-Object -ExpandProperty value\n</code></pre>"},{"location":"Coding/C%23/other/#data-exchange-with-matlab","title":"data exchange with matlab","text":"<p>https://au.mathworks.com/help/matlab/ref/putfullmatrix.html</p> <p>System.Net.Http.Formatting nuget package has been replaced by Microsoft.AspNet.WebApi.Client</p>"},{"location":"Coding/C%23/parallel/","title":"Parallel","text":"<pre><code>using System.Threading.Tasks;\nParallel.For(0, lst.Count, idx =&gt; {\n    MyFunc(par1, par2, lst[idx]);\n});\n</code></pre>"},{"location":"Coding/C%23/perf/","title":"perf","text":"<p>StringBuilder is actually a linked list of blocks, default with size from 16, 32, ....  To avoid the multiple blocks, we should set the capacity to the possibly largest value we need.</p>"},{"location":"Coding/C%23/service/","title":"service","text":"<p>if a service depends on another service, it might fail to start as the other service is not started yet. Perhaps need to consider using auto (DELAYED START) or using C# to start it if it stopped.</p>"},{"location":"Coding/C%23/service/#service_1","title":"service","text":"<p>Right click the 'service.cs' file and add installer.</p> <p>install_service.bat <pre><code>@echo off\n\nset exe=\"C:\\WINDOWS\\Microsoft.NET\\Framework\\v2.0.50727\\InstallUtil.exe\"\nset svi=\"C:\\DemoService\\MyService.exe\"\n\nrem install\n%exe% %svi%\n\nrem uninstall\nrem %exe% /u %svi%\n\necho all done!\n@pause\n</code></pre></p>"},{"location":"Coding/C%23/service/#event-log","title":"event log","text":"<pre><code>//set log\nlog = new System.Diagnostics.EventLog();\nlog.Source = \"MyService\";\nlog.Log = \"MyLog\";\n\n//source needs to be created if it doesn't exist\n((ISupportInitialize)log).BeginInit();\nif (!EventLog.SourceExists(log.Source)) {\n    EventLog.CreateEventSource(log.Source, log.Log);\n}\n((ISupportInitialize)log).EndInit();\n\n//then use it\nlog.WriteEntry(\"My Eventlog Info message.\", EventLogEntryType.Information);\nlog.WriteEntry(\"My Eventlog Err message.\", EventLogEntryType.Error);\n</code></pre>"},{"location":"Coding/C%23/service/#control-service","title":"control service","text":"<pre><code>using System.ServiceProcess;\n\nvar svc = new ServiceController(svcName);\nvar timeout = TimeSpan.FromMilliseconds(timeoutMilliseconds);\nsvc.Start();\nsvc.WaitForStatus(ServiceControllerStatus.Running, timeout);\nsvc.Refresh();\n\nif (svc.Status==ServiceControllerStatus.Running) svc.Stop();\nif (svc.Status==ServiceControllerStatus.Stopped) svc.Start();\n</code></pre>"},{"location":"Coding/C%23/Data/DataFrame/","title":"DataFrame","text":"<p>C# DataFrame and Xplot.Ploty</p> <p>https://dzone.com/articles/getting-started-with-c-dataframe-and-xplotploty</p>"},{"location":"Coding/C%23/Datetime/Datetime/","title":"DateTime","text":""},{"location":"Coding/C%23/Datetime/Datetime/#datetime-format","title":"datetime format","text":"<pre><code>Console.WriteLine($\"Current date time is: {DateTime.Now:yyyy-MM-dd HH:mm:ss}\");\n</code></pre>"},{"location":"Coding/C%23/Datetime/Datetime/#timzezone","title":"timzezone","text":"<pre><code>var aestZone = TimeZoneInfo.FindSystemTimeZoneById(\"E. Australia Standard Time\");\nvar dtnow = TimeZoneInfo.ConvertTimeFromUtc(DateTime.UtcNow, aestZone);\n</code></pre>"},{"location":"Coding/C%23/Deploy/Build/","title":"Build","text":""},{"location":"Coding/C%23/Deploy/Build/#build-a-c-app","title":"build a c# app","text":"<p><code>dotnet publish</code> is a .NET CLI command used for publishing a .NET application.  - <code>publish</code> is the command used for publishing a .NET application.   It compiles the application and its dependencies, and it prepares it for deployment. - The <code>Release configuration</code> typically includes optimizations and is suitable for deployment in a production environment.   Other common configurations include Debug for development and debugging. <pre><code>set -eoux pipefail\nunset VERSION\n\ndotnet publish -c Release ./src/Dev.Sales.csproj\n\nmkdir -p \"${PREFIX}/lib/dotnet/shared/dev\"\ncp -r \"${SRC_DIR}/src/bin/Release/net8.0/publish/\" \"${PREFIX}/lib/dotnet/shared/dev/dev-sales/\"\n</code></pre></p>"},{"location":"Coding/C%23/Deploy/Docker/","title":"Docker","text":""},{"location":"Coding/C%23/Deploy/Docker/#net-sdk","title":".NET SDK","text":"<p>The .NET SDK (Software Development Kit) is primarily used for building, developing, and running .NET applications.</p> <p>To install dotnet-sdk:  <pre><code>RUN apt-get update \\\n    &amp;&amp; apt-get install --yes dotnet-sdk-8.0\nENV dotnet_sdk=\"true\"\n</code></pre></p>"},{"location":"Coding/C%23/Feature/cs13/","title":"cs13","text":"<p>https://learn.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-13</p>"},{"location":"Coding/C%23/File/CSV/","title":"CSV","text":""},{"location":"Coding/C%23/File/CSV/#read-csv-in-zip-file","title":"Read CSV in Zip file","text":"<pre><code>public void ReadCsvInZip(string dirout, string zipfile)\n{\n    string file = Path.Combine(dirout, Path.GetFileNameWithoutExtension(zipfile) + \".csv\");\n    using (var sw = new StreamWriter(file)) {\n        using (ZipArchive zip = ZipFile.OpenRead(zipfile)) {\n            foreach (var entry in zip.Entries) {\n                Console.WriteLine($\"  {entry.Name}\");\n                using (var sr = new StreamReader(entry.Open())) {\n                    while (sr.Peek() &gt;= 0) {\n                        var line = sr.ReadLine();\n                        if (line.Length &gt; 0) {\n                            sw.WriteLine(line);\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"Coding/C%23/File/CSV/#read-specific-col-in-row","title":"Read specific col in row","text":"<pre><code>string GetColInRow(string line, int icol)\n{\n    int i = -1;\n    int j = -1;\n    int k = -1;\n    while (++j &lt; line.Length) {\n        switch (line[j]) {\n            case ',':\n                ++k;\n                if (k == icol) {\n                    return line.Substring(i + 1, j - i - 1);\n                }\n                i = j;\n                break;\n            case '\"':\n                MoveToEndQuote(ref j, line);\n                break;\n            default:\n                break;\n        }\n    }\n    if (k == icol - 1) {\n        return line.Substring(i + 1, j - i - 1);\n    }\n    return null; //not found\n}\n\nvoid MoveToEndQuote(ref int j, string line)\n{\n    while (++j &lt; line.Length) {\n        if (line[j] == '\"') {\n            if (line[j - 1] != '\"') {\n                if (j + 1 == line.Length) {\n                    break;\n                }\n                else if (line[j + 1] != '\"') {\n                    break;\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"Coding/C%23/File/Parquet/","title":"Parquet","text":""},{"location":"Coding/C%23/File/Parquet/#apache-arrow","title":"Apache Arrow","text":""},{"location":"Coding/C%23/File/Parquet/#read-csv-to-parquet","title":"Read csv to parquet","text":"<pre><code>using Apache.Arrow;\n\nvar schema = new Schema.Builder()\n    .Field(\"id\", ArrowTypeId.Int32)\n    .Field(\"name\", ArrowTypeId.String)\n    .Field(\"age\", ArrowTypeId.Int32)\n    .Build();\n\nusing (var csvDataReader = new CsvDataReader(\"path/to/csv/file.csv\", Encoding.UTF8, schema))\nusing (var parquetWriter = new ParquetWriter(\"path/to/parquet/file.parquet\", schema) {\n    parquetWriter.WriteTable(Table.FromReader(csvDataReader));\n}\n</code></pre>"},{"location":"Coding/C%23/LINQ/Linq/","title":"LINQ","text":""},{"location":"Coding/C%23/LINQ/Linq/#ienumerable-vs-iqueryable","title":"IEnumerable vs IQueryable <p>Both will be deferred execution. However, there are significant differences.</p> <p>IEnumerable is for LINQ-to-object, so all objects matching the original query will have to be loaded into memory from the database. Then the following refined queries will be based on the data in the memory.  For repeated filtering on the original results, it is better to use IEnumerable to avoid several roundtrips to the database. <p>IQueryable is the interface that allows LINQ-to-SQL to work. If you further refine your query on an IQueryable, that merged query will be executed in the database, if possible. IQueryable will use expression tree so the code might execute much faster but underlying data provider might not support it.","text":""},{"location":"Coding/C%23/LINQ/Linq/#example","title":"example <pre><code>class Customer\n{\n    public int Id;\n    public string Name;\n    public string Region;\n    public float[] Value;\n}\n\n//get iterations into different lists\nvar lst = new List&lt;Customer&gt;();\nvar qry = lst.GroupBy(r =&gt; new {\n    r.Name,\n    r.Region,\n}).Select(grp =&gt; new {\n    Key = grp.Key,\n    IDs = grp.Select(x =&gt; x.Id).ToList(),\n}).ToList();\n\nvar recordLst = new List&lt;ClientRecord&gt;();\nfor (int idx = 0; idx &lt; qry.Count; ++idx) {\n    var res = qry[idx];\n    var key = res.Key;\n    List&lt;int&gt; ids = res.IDs;\n    var firstCustomer = lst[ids[0]];\n    var record = new ClientRecord() { Id = idx, Name = key.Namea, Region = key.Region, Ids = ids, FirstCustomer = firstCustomer };\n    recordLst.Add(record);\n}\n</code></pre>","text":""},{"location":"Coding/C%23/LINQ/Linq/#datatable","title":"DataTable <pre><code>var dt1 = new DataTable();\ndt1.Columns.Add(\"id\", typeof(string));\ndt1.Columns.Add(\"name\", typeof(string));\ndt1.Columns.Add(\"stock\", typeof(int));\n\nvar dt2 = new DataTable();\ndt2.Columns.Add(\"id\", typeof(string));\ndt2.Columns.Add(\"price\", typeof(double));\n\nvar dta = from r1 in dt1.AsEnumerable()\n          join r2 in dt2.AsEnumerable()\n          on dt1.Field&lt;string&gt;(\"id\") equals dt2.Field&lt;string&gt;(\"id\")\n          select new {\n                r1.id,\n                r1.name,\n                r1.stock,\n                r2.price\n          };\n\n var res = from d in dta\n           group d by new { d.id, d.name } into grp\n           select new {\n                 id = grp.Key.id,\n                 name = grp.Key.name,\n                 sumOfStock = grp.Sum(g =&gt; g.stock),\n                 avgOfPrice = grp.Average(g =&gt; g.price)\n           };\nvar dtn = res.CopyToDataTable();\n</code></pre>","text":""},{"location":"Coding/C%23/NuGet/NuGet/","title":"NuGet","text":""},{"location":"Coding/C%23/NuGet/NuGet/#install-package-from-nuget-console","title":"install package from nuget console","text":"<pre><code>Install-Package Apache.Arrow\nInstall-Package Apache.Arrow.Parquet\n</code></pre>"},{"location":"Coding/C%23/Report/Learn/","title":"Learn","text":""},{"location":"Coding/C%23/Report/Learn/#data-visulization","title":"data visulization","text":"<p>https://github.com/swharden/Csharp-Data-Visualization</p>"},{"location":"Coding/C%23/Report/Spreadsheet/","title":"Spreadsheet","text":""},{"location":"Coding/C%23/Report/Spreadsheet/#syncfusion-essential-studio","title":"Syncfusion Essential Studio","text":"<p>https://ej2.syncfusion.com/home/</p> <p>To create a website showing a table like Excel with filters and formats, you can use a C# library such as Syncfusion Essential Studio or Telerik UI for ASP.NET AJAX.</p> <p>Syncfusion Essential Studio and Telerik UI for ASP.NET AJAX both provide powerful Grid controls that allow you to create Excel-like tables with filters and formats.  These Grid controls are easy to use and provide a number of features that make them ideal for creating web applications, including: - Filtering: You can allow users to filter the data in the Grid by any column. Filters can be applied using a variety of operators, such as equals, contains, and greater than. - Formatting: You can format the data in the Grid using a variety of options, such as font, color, and alignment. You can also format individual cells or rows. - Exporting: You can allow users to export the data in the Grid to a variety of formats, such as Excel, PDF, and CSV.</p> <p>To use a C# Grid control to create a website showing a table like Excel with filters and formats, you would follow these steps: - Add the Syncfusion Essential Studio or Telerik UI for ASP.NET AJAX NuGet package to your project. - Create a new ASP.NET web application. - Add a Grid control to your web page. - Bind the Grid control to the data that you want to display in the table. - Configure the Grid control's filtering and formatting options. - Run the web application and you should see a table that looks like Excel with filters and formats.</p> <p>Here is an example of how to create a website showing a table like Excel with filters and formats using Syncfusion Essential Studio: <pre><code>using Syncfusion.JavaScript.Controls.Grid;\n\npublic partial class Default : System.Web.UI.Page\n{\n    protected void Page_Load(object sender, EventArgs e)\n    {\n        // Create a Syncfusion Grid control\n        SfGrid grid = new SfGrid();\n\n        // Bind the grid to the data\n        grid.DataSource = GetData();\n\n        // Configure the grid's filtering and formatting options\n        grid.AllowFiltering = true;\n        grid.AllowFormatting = true;\n\n        // Render the grid to HTML\n        string htmlTable = grid.Render();\n\n        // Write the HTML table to the response\n        Response.Write(htmlTable);\n    }\n\n    private DataTable GetData()\n    {\n        // Create a DataTable\n        DataTable dataTable = new DataTable();\n\n        // Add columns to the DataTable\n        dataTable.Columns.Add(\"Name\");\n        dataTable.Columns.Add(\"Age\");\n        dataTable.Columns.Add(\"Occupation\");\n\n        // Add rows to the DataTable\n        dataTable.Rows.Add(\"John Doe\", 30, \"Software Engineer\");\n        dataTable.Rows.Add(\"Jane Doe\", 25, \"Doctor\");\n        dataTable.Rows.Add(\"Peter Parker\", 20, \"Student\");\n\n        return dataTable;\n    }\n}\n</code></pre></p> <p>This code will create a simple website with a table that shows the names, ages, and occupations of three people.  The table will have filters and formats that allow users to filter the data and format the cells.</p> <p>You can customize the Grid control to meet your specific needs. For example, you can add more columns to the table, change the order of the columns,  and add different types of filters and formats. You can also use CSS to style the table.</p> <p>Syncfusion Essential Studio and Telerik UI for ASP.NET AJAX also provide a number of other components for creating web applications,  such as charts, menus, and calendars. You can use these components to create a complete and professional web application.</p>"},{"location":"Coding/C%23/Web/Bootstrapper/","title":"Bootstrapper","text":"<p>A bootstrapper is a piece of code responsible for initializing and setting up the necessary components of an application before it runs.  This could include configuring services, setting up dependency injection, or performing other initialization tasks.</p>"},{"location":"Coding/C%23/Web/Durandal/","title":"Durandal","text":"<p>Durandal is a JavaScript framework designed to simplify the process of building <code>single-page applications</code> (SPAs).  It follows the MVVM (Model-View-ViewModel) architectural pattern and is intended to work seamlessly with other popular web technologies like Knockout.js, RequireJS, and jQuery.</p> <p>Durandal was gained popularity in the early 2010s.  However, it's essential to note that the JavaScript ecosystem has evolved, and other modern frameworks like React, Angular, and Vue.js have become more prominent in recent years.  Durandal's usage may have diminished compared to its earlier days. Developers often choose frameworks based on their specific project requirements and preferences.</p> <p>Key features and components of Durandal include: - Composition: Durandal provides a composition system that facilitates the organization and assembly of   views, view models, and templates. This helps in creating modular and maintainable SPAs. - Router: Durandal includes a client-side router that enables navigation within the application without requiring full page reloads.   It supports hash-based URLs and allows for the creation of distinct \"routes\" within the application. - View-Model Binding: Durandal leverages Knockout.js for data-binding between views and view models.   This allows for automatic synchronization between the user interface and underlying data. - Lifecycle Management: Durandal helps manage the lifecycle of views and view models.   It provides hooks for various lifecycle events, allowing developers to execute custom logic during different phases of an SPA's lifecycle. - Modular Development: Durandal encourages modular development by supporting the use of AMD (Asynchronous Module Definition) through RequireJS.   This allows developers to organize their code into manageable modules, promoting code reusability and maintainability. - Plugins: Durandal is extensible and supports the use of plugins to enhance its functionality.   Developers can create or use existing plugins to add features such as dialogs, widgets, and other reusable components. - Convention over Configuration: Durandal follows the principle of convention over configuration,   making it easy to set up projects with sensible defaults while still allowing for customization when needed.</p>"},{"location":"Coding/C%2B%2B/basic/","title":"basic","text":""},{"location":"Coding/C%2B%2B/basic/#stdmove","title":"std::move","text":"<p>using a move constructor, a std::vector could just copy its internal pointer to data to the new object, leaving the moved object in an incorrect state, avoiding to copy all data.</p>"},{"location":"Coding/C%2B%2B/basic/#local-and-global-static-variables","title":"local and global static variables","text":"<ul> <li>The name of the local is only accessible within the function, and has no linkage</li> <li>Local is initialized the first time execution reaches the definition, not necessarily during the program's initialisation phases</li> </ul>"},{"location":"Coding/C%2B%2B/basic/#const-member-functions","title":"Const member functions","text":"<p>A \"const function\", denoted with the keyword const after a function declaration, makes it a compiler error for this class function to change a member variable of the class.</p> <p>A function becomes const when const keyword is used in function\u2019s declaration. The idea of const functions is not allow them to modify the object on which they are called. </p> <pre><code>#include&lt;iostream&gt; \nusing namespace std; \n\nclass Test { \n    int value; \npublic: \n    Test(int v = 0) {value = v;}       \n    // we get compiler error if we add a line like \"value = 100;\" in this function\n    int getValue() const {return value;}   \n}; \n\nint main() { \n    Test t(20); \n    cout &lt;&lt; t.getValue(); \n    return 0; \n} \n</code></pre>"},{"location":"Coding/C%2B%2B/basic/#mutable-keyword","title":"mutable keyword","text":"<p>The mutable keyword is a way to pierce the const veil you drape over your objects. If you have a const reference or pointer to an object, you cannot modify that object in any way except when and how it is marked mutable.</p> <p>With your const reference or pointer you are constrained to:   only read access for any visible data members   permission to call only methods that are marked as const. The mutable exception makes it so you can now write or set data members that are marked mutable. That's the only externally visible difference.</p> <p>Internally those const methods that are visible to you can also write to data members that are marked mutable. Essentially the const veil is pierced comprehensively. It is completely up to the API designer to ensure that mutable doesn't destroy the const concept and is only used in useful special cases. The mutable keyword helps because it clearly marks data members that are subject to these special cases.</p> <p>In practice you can use const obsessively throughout your codebase (you essentially want to \"infect\" your codebase with the const \"disease\"). In this world pointers and references are const with very few exceptions, yielding code that is easier to reason about and understand. For a interesting digression look up \"referential transparency\".</p> <p>Without the mutable keyword you will eventually be forced to use const_cast to handle the various useful special cases it allows (caching, ref counting, debug data, etc.). Unfortunately const_cast is significantly more destructive than mutable because it forces the API client to destroy the const protection of the objects (s)he is using. Additionally it causes widespread const destruction: const_casting a const pointer or reference allows unfettered write and method calling access to visible members. In contrast mutable requires the API designer to exercise fine grained control over the const exceptions, and usually these exceptions are hidden in const methods operating on private data.</p>"},{"location":"Coding/C%2B%2B/basic/#size_t","title":"size_t","text":"<p>size_t operation can be dangerous</p> <p>unsigned int vs int comparison forces conversion to unsigned</p> <pre><code>size_t i = 2;\nsize_t j = 4;\nauto k = i - j; //a large positive value\n</code></pre>"},{"location":"Coding/C%2B%2B/compile/","title":"compile","text":""},{"location":"Coding/C%2B%2B/compile/#dumpbin","title":"DumpBin","text":"<pre><code>@echo off\n\nset \"dir=C:\\Users\\name\\Desktop\"\nset dumpbin=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\\Tools\\MSVC\\14.16.27023\\bin\\Hostx64\\x64\\dumpbin.exe\n\"%dumpbin%\" /EXPORTS /ALL /OUT:\"%dir%\\vpnapi_dump.txt\" \"%dir%\\vpnapi.lib\"\n\n\"%dumpbin%\" /DEPENDENTS MathClient.exe\n\n@pause\n</code></pre>"},{"location":"Coding/C%2B%2B/compile/#dll-search-order","title":"DLL Search Order","text":"<p>https://docs.microsoft.com/en-us/windows/win32/dlls/dynamic-link-library-search-order</p>"},{"location":"Coding/C%2B%2B/compile/#disable-avx","title":"disable AVX","text":"<p>\\NotSet</p>"},{"location":"Coding/C%2B%2B/container/","title":"container","text":""},{"location":"Coding/C%2B%2B/container/#array","title":"array","text":"<p>Unlike std::vector, the size of the std::array object is fixed. If the container size is fixed, then the std::array container can be used first. In addition, since std::vector is automatically expanded, when a large amount of data is stored, and the container is deleted, the container does not automatically return the corresponding memory of the deleted element. In this case, you need to manually run shrink_to_fit() to release this part of the memory.</p> <pre><code>std::vector&lt;int&gt; v;\nv.push_back(1);\nv.push_back(2);\nv.clear();\nstd::cout &lt;&lt; \"size:\" &lt;&lt; v.size() &lt;&lt; std::endl; // output 0\nstd::cout &lt;&lt; \"capacity:\" &lt;&lt; v.capacity() &lt;&lt; std::endl; // output 8\n//additional memory can be returned to the system via the shrink_to_fit() call\nv.shrink_to_fit();\n</code></pre> <p>Using std::array instead of traditional array can make the code more \u201cmodern\u201d and encapsulate some manipulation functions, such as getting the array size and checking if it is not empty, and also using the standard friendly.</p> <pre><code>std::array&lt;int, 4&gt; arr = {1, 2, 3, 4};\n\narr.empty(); // check if container is empty\narr.size(); // return the size of the container\n\n// iterator support\nfor (auto &amp;i : arr)\n{\n    // ...\n}\n\n// use lambda expression for sort\nstd::sort(arr.begin(), arr.end(), [](int a, int b) { return b &lt; a; });\n\n// array size must be constexpr\nconstexpr int len = 4;\nstd::array&lt;int, len&gt; arr = {1, 2, 3, 4};\nvoid foo(int *p, int len) { return; }\n\nstd::array&lt;int, 4&gt; arr = {1,2,3,4};\n\n// C-stype parameter passing\n// foo(arr, arr.size()); // illegal, cannot convert implicitly\nfoo(&amp;arr[0], arr.size());\nfoo(arr.data(), arr.size());\n\n// use std::sort\nstd::sort(arr.begin(), arr.end()); \n</code></pre>"},{"location":"Coding/C%2B%2B/datetime/","title":"DateTime","text":"<p>https://www.tutorialspoint.com/cplusplus/cpp_date_time.htm</p>"},{"location":"Coding/C%2B%2B/debug/","title":"debug","text":""},{"location":"Coding/C%2B%2B/debug/#show-pointer-array","title":"show pointer array","text":"<p>var, n</p>"},{"location":"Coding/C%2B%2B/err/","title":"error","text":""},{"location":"Coding/C%2B%2B/err/#this-function-or-variable-may-be-unsafe-consider-using-localtime_s-instead","title":"This function or variable may be unsafe. Consider using localtime_s instead","text":"<p>add _CRT_SECURE_NO_WARNINGS to Preprocessor Definitions</p>"},{"location":"Coding/C%2B%2B/err/#modify-a-standard-header","title":"modify a standard header","text":"<p>If you want to modify a standard header, you might try specifying the folder where you put it in Additional Include Directories, so the compiler will always find your version first [C/C++ --&gt; All Options].</p> <p>config properties -&gt; c/c++ -&gt; language -&gt; c++ language standard</p>"},{"location":"Coding/C%2B%2B/err/#cannot-open-source-file","title":"cannot open source file","text":"<p>additional include directory: $(SolutionDir)</p>"},{"location":"Coding/C%2B%2B/err/#link-fatal-error-lnk1104-cannot-open-file","title":"LINK : fatal error LNK1104: cannot open file","text":"<p>the debug run is not stopped, so the target exe file is locked. Or the exe is still open but invisible, use taskkill to close it</p>"},{"location":"Coding/C%2B%2B/err/#error-lnk2019-unresolved-external-symbol","title":"error LNK2019: unresolved external symbol","text":"<p>circular reference is used in two classes, consider changing one class to be independent on the other</p>"},{"location":"Coding/C%2B%2B/err/#undeclared-identifier-in-method","title":"undeclared identifier in method","text":"<p>forgot to add the class to the front of the method so the class's variable is not identified</p>"},{"location":"Coding/C%2B%2B/err/#string_view","title":"string_view","text":"<p>string_view is only available with C++17 or later, should change to language C++17  </p> <p>'string_view': is not a member of 'std'. force rebuilding some failed projects separately works.</p>"},{"location":"Coding/C%2B%2B/err/#why-class-member-cannot-be-constexpr","title":"why class member cannot be constexpr","text":"<p>class member will be initialized. but we need some const values for that class only. put them in anonymous namespace should be OK. </p>"},{"location":"Coding/C%2B%2B/err/#do-not-return-vectordata","title":"do not return vector.data","text":"<p>create a vector in a function and return its data will end to no where. reason: vector in stack destroyed after return</p>"},{"location":"Coding/C%2B%2B/err/#app-terminated-silently","title":"app terminated silently","text":"<p>if we use an old dll (such as lp solver) and a new api function does not exist, the app will terminate silently.</p> <p>In events viewer we can fins something like: <pre><code>Faulting application name: Simulate.exe, version: 0.0.0.0, time stamp: 0x5e3100d2\nFaulting module name: KERNELBASE.dll, version: 10.0.17134.1184, time stamp: 0x8569b554\nException code: 0xc06d007f\nFault offset: 0x000000000003a388\nFaulting process id: 0x392c\nFaulting application start time: 0x01d5d65760043adb\nFaulting application path: C:\\Users\\dll\\Simulate.exe\nFaulting module path: C:\\WINDOWS\\System32\\KERNELBASE.dll\n</code></pre></p>"},{"location":"Coding/C%2B%2B/err/#app-stopped-working","title":"app stopped working","text":"<p>0xC000001D STATUS_ILLEGAL_INSTRUCTION\\ built using the wrong release config (Release, but should be Release NoAVX). Old machines do not have a processor supporting AVX</p> <p>In events viewer we can fins something like: <pre><code>  Problem Event Name:   APPCRASH\n  Exception Code:   c000001d\n  Exception Offset: 000000000001a915\n  OS Version:   6.3.9600.2.0.0.16.7\n</code></pre></p>"},{"location":"Coding/C%2B%2B/err/#pbd-file-not-loaded","title":"PBD File not loaded","text":"<p>These messages appear because you don't have the PDB files available in your computer. You can download them by selecting the \"Microsoft Symbol Servers\" checkbox at Tools &gt; Options &gt; Debugging &gt; Symbol.</p> <p>dll file was missing</p>"},{"location":"Coding/C%2B%2B/err/#pointer-to-incomplete-class-type-is-not-allowed","title":"Pointer to incomplete class type is not allowed","text":"<p>An \"incomplete class\" is one declared but not defined. Seems forgot to include x.h in y.cpp.</p>"},{"location":"Coding/C%2B%2B/feature/","title":"feature","text":""},{"location":"Coding/C%2B%2B/feature/#decltype","title":"decltype","text":"<pre><code>//get type of variable x\nstd::is_same&lt;decltype(x), int&gt;::value\n\n//decltype(auto) is mainly used to derive the\n//  return type of a forwarding function or package\ndecltype(auto) look_up_a_string_1() { return lookup1(); }\n</code></pre>"},{"location":"Coding/C%2B%2B/feature/#delete","title":"delete","text":"<p>default and delete allow explicit declarations to take or reject functions that come with the compiler. <pre><code>class Magic {\npublic:\n    Magic() = default; // explicit let compiler use default constructor\n    Magic&amp; operator=(const Magic&amp;) = delete; // explicit declare refuse constructor\n    Magic(int magic_number);\n}\n</code></pre></p>"},{"location":"Coding/C%2B%2B/feature/#override","title":"override","text":"<p>When overriding a virtual function, introducing the override keyword will explicitly tell the compiler to overload, and the compiler will check if the base function has such a virtual function, otherwise it will not compile. <pre><code>struct Base {\n    virtual void foo(int);\n};\nstruct SubClass: Base {\n    virtual void foo(int) override; // legal\n    virtual void foo(float) override; // illegal, no virtual function in super class\n};\n</code></pre></p> <p>when use override in derived class it is not necessary to use virtual for a function <pre><code>vector&lt;double&gt; const GetRow() const override; //good\nvirtual vector&lt;double&gt; const GetRow() const override; //not good\n</code></pre></p>"},{"location":"Coding/C%2B%2B/feature/#final","title":"final","text":"<p>final is to prevent the class from being continued to inherit and to terminate the virtual function to continue to be overloaded. <pre><code>struct Base {\n    virtual void foo() final;\n};\nstruct SubClass1 final: Base {\n}; // legal\n\nstruct SubClass2 : SubClass1 {\n}; // illegal, SubClass1 has final\n\nstruct SubClass3: Base {\n    void foo(); // illegal, foo has final\n};\n</code></pre></p>"},{"location":"Coding/C%2B%2B/feature/#noexcept","title":"noexcept","text":"<p>If a function modified with noexceptis thrown, the compiler will use std::terminate()to immediately terminate the program.</p> <p>Specifies whether a function could throw exceptions. Code that uses move_if_nothrow (or whatchamacallit) will see a performance improvement if there's a noexcept move ctor. Strictly, move_if_noexcept won't return a copy, it will return a const lvalue-reference rather than an rvalue-reference. In general that will cause the caller to make a copy instead of a move, but move_if_noexcept isn't doing the copy. <pre><code>template &lt;class T&gt;\nvoid foo() noexcept(noexcept(T())) {}\nvoid bar() noexcept(true) {}\n</code></pre></p>"},{"location":"Coding/C%2B%2B/incl/","title":"include","text":"include method \\ std:cout \\ std::replace"},{"location":"Coding/C%2B%2B/io/","title":"io","text":""},{"location":"Coding/C%2B%2B/io/#format","title":"format","text":"<pre><code>std::cout &lt;&lt; std::fixed &lt;&lt; std::setw(8);\nstd::cout &lt;&lt; std::setprecision(2) &lt;&lt; val &lt;&lt; std::endl;\n</code></pre>"},{"location":"Coding/C%2B%2B/io/#stdendl-vs-n","title":"std::endl vs /n","text":"<p>std::endl flushes the output stream. Flushing the output stream is not frequently required</p>"},{"location":"Coding/C%2B%2B/io/#output-to-file","title":"output to file","text":"<pre><code>string filepath = R\"(C:\\test.csv)\";\nofstream file(filepath);\nif (!file.good()) { return; }\nfile &lt;&lt; \"index,key,value\" &lt;&lt; \"\\n\";\nfile &lt;&lt; ind[0] &lt;&lt; \",\" &lt;&lt; key[0] &lt;&lt; \",\" &lt;&lt; val[0] &lt;&lt; \"\\n\";\nfile.close();\n</code></pre>"},{"location":"Coding/C%2B%2B/io/#insertion-operators","title":"insertion operators","text":"<p>The default value for floating-point precision is six. For example, the number 3466.9768 prints as 3466.98. To change the way this value prints, use the setprecision manipulator. The manipulator has two flags: fixed and scientific. If fixed is set, the number prints as 3466.976800. If scientific is set, it prints as 3.4669773+003. <pre><code>double values[] = { 1.23, 35.36, 653.7, 4358.24 };\nchar *names[] = { \"Zoot\", \"Jimmy\", \"Al\", \"Stan\" };\nfor (int i = 0; i &lt; 4; i++)\n    cout &lt;&lt; setiosflags(ios::left) &lt;&lt; setw(6) &lt;&lt; names[i]\n         &lt;&lt; resetiosflags(ios::left) &lt;&lt; setw(10) &lt;&lt; setprecision(1) &lt;&lt; values[i]\n         &lt;&lt; endl;\n</code></pre></p>"},{"location":"Coding/C%2B%2B/io/#a-faster-way-to-use-stringstream","title":"A faster way to use stringstream","text":"<p>A faster way to use stringstream</p> <p>use a static variable of type stringstream and before you perform a string conversion, reset that varible to be empty</p> <pre><code>#include &lt;sstream&gt;            \n#include &lt;iomanip&gt;            \n\nusing namespace std;                \nstatic stringstream buffer; // moved up here                                                    \n\n// first use                        \nint i = 20;                         \nbuffer &lt;&lt; setw(4) &lt;&lt; i &lt;&lt; '\\0'; \n\nstring v = buffer.str().c_str();                                   \n// v now contains \"  20\"                                            \n\n// second use                       \nstring tom(\"tom\");                  \nbuffer.seekp(ios::beg); \nbuffer &lt;&lt; setw(10) &lt;&lt; tom; \n\nv = buffer.str().c_str();                                            \n// v now contains \"       tom\"      \n</code></pre> <p>use the .c_str() method on the string returned by buffer.str() is that the seekp function does not reduce the total size of the string kept by the stringstream.</p>"},{"location":"Coding/C%2B%2B/io/#write-sol","title":"write sol","text":"<pre><code>#include &lt;fstream&gt;\n#include &lt;iomanip&gt;\n\nauto pri = getColPrimals();\nauto rct = getColReducedCost();\nauto obj = getColObjectiveCoeffs();\nstd::ofstream fc(\"C:/tmp/lp_col_sol.csv\");\nif (!fc.good()) return false;\nfc &lt;&lt; \"col,pri,rct,obj,\\n\";\nfor (auto i = 0; i &lt; getNumCols(); ++i) {\n  fc &lt;&lt; 'C' &lt;&lt; std::setfill('0') &lt;&lt; std::setw(7) &lt;&lt; i &lt;&lt; \", \" &lt;&lt; pri[i] &lt;&lt; \", \" &lt;&lt; rct[i] &lt;&lt; \", \" &lt;&lt; obj[i] &lt;&lt; \"\\n\";\n}\nfc.close();\n\nauto act = getRowPrimals();\nauto dua = getRowDuals();\nauto lhs = getRowLowerBounds();\nauto rhs = getRowUpperBounds();\nstd::ofstream fr(\"C:/tmp/lp_row_sol.csv\");\nif (!fr.good()) return false;\nfr &lt;&lt; \"row,dua,act,lhs,rhs,\\n\";\nfor (auto i = 0; i &lt; getNumRows(); ++i) {\n  fr &lt;&lt; 'R' &lt;&lt; std::setfill('0') &lt;&lt; std::setw(7) &lt;&lt; i &lt;&lt; \",\" &lt;&lt; dua[i] &lt;&lt; \",\" &lt;&lt; act[i] &lt;&lt; \",\" &lt;&lt; lhs[i] &lt;&lt; \",\" &lt;&lt; rhs[i] &lt;&lt; \"\\n\";\n}\nfr.close();\n</code></pre>"},{"location":"Coding/C%2B%2B/mkl/","title":"mkl","text":""},{"location":"Coding/C%2B%2B/mkl/#install","title":"install","text":"<p>Intel oneAPI MKL</p> <p>https://software.intel.com/content/www/us/en/develop/articles/intel-mkl-link-line-advisor.html</p>"},{"location":"Coding/C%2B%2B/mkl/#settings","title":"settings","text":"<p>C/C++, General-&gt;Additional Include Directories   C:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\2021.1.1\\include</p> <p>Linker, General, Additional Library Directories   C:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\2021.1.1\\lib\\intel64</p> <p>Linker, Input, Additional Dependencies   mkl_intel_lp64.lib;mkl_sequential.lib;mkl_core.lib #MKL Wrapper   mkl_intel_ilp64.lib;mkl_sequential.lib;mkl_core.lib;mkl_blacs_msmpi_ilp64.lib #Solver</p>"},{"location":"Coding/C%2B%2B/mkl/#solution","title":"solution?","text":"<p>Go to Project \u00bb Properties \u00bb Libraries \u00bb Use Intel Math Kernel Library and select Parallel, Sequential, or Cluster as appropriate. </p> <p>If the switch don't work, you may solve the problem by adding mkl header file path, library and library path manually </p> <ol> <li> <p>Select Project \u00bb Properties \u00bb Linker \u00bb General \u00bb Additional Library Directories. Add the architecture specific lib folder, for example, \\lib\\ia32. </p> </li> <li> <p>Select Project \u00bb Properties \u00bb Linker \u00bb Input \u00bb Additional Dependencies. Insert   mkl_intel_c.lib mkl_sequential.lib mkl_core.lib if 32bit application.    mkl_intel_lp64.lib mkl_sequential.lib mkl_core.lib if 64bit application  </p> </li> </ol>"},{"location":"Coding/C%2B%2B/odbc/","title":"odbc","text":"<pre><code>SQLINTEGER NativeError;\nSQLRETURN SQLSTATEs;\nSQLSMALLINT i, MsgLen;\nSQLCHAR Sqlstate[5], Msg[200];\ni = 1;\nwhile ((SQLSTATEs = SQLGetDiagRec(SQL_HANDLE_STMT, stmt.handle(), i,\n    Sqlstate, &amp;NativeError, Msg, sizeof(Msg), &amp;MsgLen)) != SQL_NO_DATA) {\n    i++;\n    std::cout &lt;&lt; \"the HandleType is:\" &lt;&lt; SQL_HANDLE_STMT &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"the InputHandle is :\" &lt;&lt; stmt.handle() &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"the output state is:\" &lt;&lt; (char *)Sqlstate &lt;&lt; std::endl;   \n}\n</code></pre>"},{"location":"Coding/C%2B%2B/other/","title":"other","text":"<p>C++ Core Guidelines\\ https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#es71-prefer-a-range-for-statement-to-a-for-statement-when-there-is-a-choice</p> <p>C++ tips\\ https://abseil.io/tips/1</p> <p>When add a new project, we need to set the Output Directory and the Intermediate Directory in Configuration Properties -&gt; General</p> <pre><code>// int to str\nchar strMonth[3];\nsprintf_s(strMonth, \"%02d\", month);\n\n// str to int dbl\nint i = stoi(str);\ndouble d = stod(str);\n\n// str combination\nstd::stringstream ss;\nss.str(\"\");\nss &lt;&lt; folderin &lt;&lt; strDay &lt;&lt; \"_\" &lt;&lt; strHour &lt;&lt; \"UT.txt\";\nstring fileName = ss.str();\n\n// progress report\nstd::cout &lt;&lt; \"Finished: \" &lt;&lt; i * 100 / (double)n &lt;&lt; \"%\\r\";\nstd::cout.flush();\n\n// read file\nifstream inputfile(fileName);\nif (!inputfile) {\n  //File doesn't exist\n  cout &lt;&lt; \"File '\" &lt;&lt; fileName + \"' doesn't exist\\n\";\n}\nelse {\n  string line;\n  while (getline(inputfile, line)) {\n    stringstream lineStream(line);\n    // Read a value one at a time from the line\n    int val;\n    while (lineStream &gt;&gt; val) {\n        int x = val;\n    }\n}\ninputfile.close();\n\n// append to file\nFILE *fp = fopen(fileName.c_str(), \"ab\");\nstringstream ss;\nss.str(\"\");\nfwrite(ss.str().c_str(), 1, ss.str().length(), fp);\nfclose(fp);\n\n// get exe full path\nchar basePath[255] = \"\";\n_fullpath(basePath, argv[0], sizeof(basePath));\n\nstring path_ = string(basePath);\nint idx = path_.find_last_of(\"/\\\\\");\ncout &lt;&lt; \"exefolder: \" &lt;&lt; path_.substr(0, idx+1) &lt;&lt; endl;\ncout &lt;&lt; \"full path: \" &lt;&lt; basePath &lt;&lt; endl;\n</code></pre>"},{"location":"Coding/C%2B%2B/parallel/","title":"parallel","text":""},{"location":"Coding/C%2B%2B/parallel/#lock","title":"lock","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;thread&gt;\n#include &lt;mutex&gt;\n\nint v = 1;\n\nvoid critical_section1(int change_v)\n{\n    static std::mutex mtx;\n    std::lock_guard&lt;std::mutex&gt; lock(mtx);\n\n    // execute contention works\n    v = change_v;\n    // mtx will be released after leaving the scope\n}\n\nvoid critical_section2(int change_v)\n{\n    static std::mutex mtx;\n    std::unique_lock&lt;std::mutex&gt; lock(mtx);\n    // do contention operations\n    v = change_v;\n    std::cout &lt;&lt; v &lt;&lt; std::endl;\n\n    // release the lock\n    lock.unlock();\n    // during this period, others are allowed to acquire v\n\n    // start another group of contention operations        \n    lock.lock(); //lock again\n    v += 1;\n    std::cout &lt;&lt; v &lt;&lt; std::endl;\n}\n\nint main()\n{\n    //basic\n    std::thread t([]() {\n        std::cout &lt;&lt; \"hello world.\" &lt;&lt; std::endl;\n    });\n    t.join();\n\n    //lock\n    std::thread t1(critical_section1, 2), t2(critical_section1, 3);\n    t1.join();\n    t2.join();\n\n    std::cout &lt;&lt; v &lt;&lt; std::endl;\n    return 0;\n}\n</code></pre>"},{"location":"Coding/C%2B%2B/parallel/#future","title":"future","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;thread&gt;\n#include &lt;future&gt;\n\nint main()\n{\n    // pack a lambda expression that returns 7 into a std::packaged_task\n    std::packaged_task&lt;int()&gt; task([]() { return 7; });\n    // get the future of task\n    std::future&lt;int&gt; result = task.get_future(); // run task in a thread\n    std::thread(std::move(task)).detach();\n    std::cout &lt;&lt; \"waiting...\";\n    result.wait(); //block until future has arrived\n    // output result\n    std::cout &lt;&lt; \"done!\" &lt;&lt; std::endl &lt;&lt; \"future result: \" &lt;&lt; result.get() &lt;&lt; std::endl;\n    return 0;\n}\n</code></pre>"},{"location":"Coding/C%2B%2B/perf/","title":"perf","text":""},{"location":"Coding/C%2B%2B/perf/#string","title":"string","text":"<p>https://www.oreilly.com/library/view/optimized-c/9781491922057/ch04.html\\</p> <pre><code>//reduce Reallocation by Reserving Storage (1.2x faster)\nstd::string remove_ctrl_reserve(std::string s) {\n    std::string result;\n    result.reserve(s.length());\n    for (int i=0; i&lt;s.length(); ++i) {\n        if (s[i] &gt;= 0x20) result += s[i];\n    }\n    return result;\n}\n//eliminate Pointer Dereference Using Iterators (1.2x faster)\nstd::string remove_ctrl_ref_args_it(std::string const&amp; s) {\n    std::string result;\n    result.reserve(s.length());\n    for (auto it=s.begin(),end=s.end(); it != end; ++it) {\n        if (*it &gt;= 0x20) result += *it;\n    }\n    return result;\n}\n//use Character Arrays Instead of Strings (10x faster)\nvoid remove_ctrl_cstrings(char* destp, char const* srcp, size_t size) {\n    for (size_t i=0; i&lt;size; ++i) {\n        if (srcp[i] &gt;= 0x20) *destp++ = srcp[i];\n    }\n    *destp = 0;\n}\n</code></pre>"},{"location":"Coding/C%2B%2B/project/","title":"project","text":"<p>when create a project, set \\ $(SolutionDir)\\ from Configuration Properties -&gt; C/C++ -&gt; General</p> <p>in pch.h add, #include \"inc/standard_pch.h\"</p>"},{"location":"Coding/C%2B%2B/regex/","title":"regex","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;string&gt;\n#include &lt;regex&gt;\n\nint main() {\n    std::string fnames[] = {\"foo.txt\", \"bar.txt\", \"test\", \"a0.txt\", \"AAA.txt\"};\n    //In C++, `\\` will be used as an escape character in the string. \n    // In order for `\\.` to be passed as a regular expression, \n    // it is necessary to perform second escaping of `\\`, thus we have `\\\\.`\n    std::regex txt_regex(\"[a-z]+\\\\.txt\");\n    for (const auto &amp;fname: fnames)\n        std::cout &lt;&lt; fname &lt;&lt; \": \" &lt;&lt; std::regex_match(fname, txt_regex) &lt;&lt; std::endl;\n}\n\nstd::regex base_regex(\"([a-z]+)\\\\.txt\");\nstd::smatch base_match;\nfor(const auto &amp;fname: fnames) {\n    if (std::regex_match(fname, base_match, base_regex)) {\n    // the first element of std::smatch matches the entire string\n    // the second element of std::smatch matches the first expression with brackets\n    if (base_match.size() == 2) {\n            std::string base = base_match[1].str();\n            std::cout &lt;&lt; \"sub-match[0]: \" &lt;&lt; base_match[0].str() &lt;&lt; std::endl;\n            std::cout &lt;&lt; fname &lt;&lt; \" sub-match[1]: \" &lt;&lt; base &lt;&lt; std::endl;\n        }\n    }\n}\n</code></pre>"},{"location":"Coding/C%2B%2B/runtime/","title":"runtime","text":""},{"location":"Coding/C%2B%2B/runtime/#lambda-overload","title":"lambda overload","text":"<pre><code>#include &lt;type_traits&gt; //std::is_same_v\n\ntemplate&lt;typename T&gt;\nconstexpr auto translate = [](T idx) \n{\n    if constexpr (std::is_same_v&lt;T, int&gt;) {\n        constexpr static int table[3]{ 2,1,0 };\n        return table[idx];\n    }\n    else if constexpr (std::is_same_v&lt;T, char&gt;) {\n        std::map&lt;char, int&gt; table{ {'a', 0}, {'b', 1}, {'c', 2}};\n        return table[idx];\n    }\n};\n\n//call it like\nint r = translate&lt;int&gt;(line[0]);\nint c = translate&lt;char&gt;(line[1]);\n</code></pre>"},{"location":"Coding/C%2B%2B/runtime/#rvalue","title":"rvalue","text":"<p>The xvalue defines a behavior in which temporary values can be identified while being able to be moved. To get a xvalue, you need to use the declaration of the rvalue reference: T &amp;&amp;, where T is the type. The statement of the rvalue reference extends the lifecycle of this temporary value, and as long as the variable is alive, the xvalue will continue to survive.</p> <pre><code>int main()\n{\n    std::string lv1 = \"string,\"; // lv1 is a lvalue\n    // std::string&amp;&amp; r1 = s1; // illegal, rvalue can't ref to lvalue\n    std::string&amp;&amp; rv1 = std::move(lv1); // legal, std::move can convert lvalue to rvalue\n    std::cout &lt;&lt; rv1 &lt;&lt; std::endl; // string,\n\n    const std::string&amp; lv2 = lv1 + lv1; // legal, const lvalue reference can extend temp variable's lifecycle\n    // lv2 += \"Test\"; // illegal, const ref can't be modified\n    std::cout &lt;&lt; lv2 &lt;&lt; std::endl; // string,string\n\n    std::string&amp;&amp; rv2 = lv1 + lv2; // legal, rvalue ref extend lifecycle\n    rv2 += \"string\"; // legal, non-const reference can be modified\n    std::cout &lt;&lt; rv2 &lt;&lt; std::endl; // string,string,string,\n\n    reference(rv2); // output: lvalue\n\n    return 0;\n}\n</code></pre>"},{"location":"Coding/C%2B%2B/runtime/#move","title":"move","text":"<pre><code>std::string str = \"Hello world.\";\nstd::vector&lt;std::string&gt; v;\n\n// use push_back(const T&amp;), copy\nv.push_back(str); \nstd::cout &lt;&lt; \"str: \" &lt;&lt; str &lt;&lt; std::endl; //\"str: Hello world.\"\n\n// use push_back(const T&amp;&amp;), no copy\nv.push_back(std::move(str)); // str is empty now\nstd::cout &lt;&lt; \"str: \" &lt;&lt; str &lt;&lt; std::endl; \n</code></pre>"},{"location":"Coding/C%2B%2B/dtype/dic/","title":"dict","text":""},{"location":"Coding/C%2B%2B/dtype/dic/#stdmap","title":"std::map","text":"<pre><code>//keys are strings and values integers\nstd::map&lt;std::string, int&gt; pwdUid;\n\n//keys are integers and values are strings\nstd::map&lt;int, std::string&gt; uidNam;\n\n//insert a user\nuidNam[0] = std::string(\"John Small\");\n//retrieve a user\nstd::cout &lt;&lt; \"User #0 is named: \" &lt;&lt; uidNam[0] &lt;&lt; std::endl;\n</code></pre>"},{"location":"Coding/C%2B%2B/dtype/dic/#checking-if-a-key-exists-in-map","title":"Checking if a key exists in map","text":"<pre><code>std::map::count(key) //return the number of keys which matches the queried key\nstd::map::find(key) //return an iterator to the found object, or std::map::end() if the object is not found\n</code></pre>"},{"location":"Coding/C%2B%2B/dtype/dic/#iteration","title":"Iteration","text":"<pre><code>std::map&lt;int, std::string&gt; values;\nfor(auto&amp; x : values) {\n    std::cout &lt;&lt; x.first &lt;&lt; \",\" &lt;&lt; x.second &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"Coding/C%2B%2B/dtype/dic/#delete-one-element","title":"delete one element","text":"<pre><code>//remove a user by key\nuidNam[2971] = std::string(\"John\");\nuidNam.erase(2971); //removes the entry [2971, \"John\"]\n\n//remove a user by iterator\nauto iterator = uidNam.find(\"Ana\");\nuidNam.erase(iterator); //removes the entry [7362, \"Ana\"]\n</code></pre>"},{"location":"Coding/C%2B%2B/dtype/dic/#avoid-copying","title":"avoid copying","text":"<p>this will create a copy of the map value vector <pre><code>myMap = map&lt;int, vector&lt;double&gt;&gt;();\nmyMap[key].push_back('a');\n</code></pre></p> <p>to avoid the copying we should use <pre><code>auto &amp;vec = myMap[key];\nvec.push_back('a');\n</code></pre></p>"},{"location":"Coding/C%2B%2B/dtype/str/","title":"string","text":"<p>string is same to other basic types like int, double, so assignment will make a copy of the string [e.g., string s = str_old;].</p>"},{"location":"Coding/C%2B%2B/dtype/str/#include-when-using-stdcout","title":"include  when using std::cout <p>For all the standard library types the member function empty() means \"are you empty?\".</p> <p>The clear() member function is inherited from ios and is used to clear the error state of the stream, e.g. if a file stream has the error state set to eofbit (end-of-file), then calling clear() will set the error state back to goodbit (no error).</p> <p>if stringstream is at EOF, ss.str(\"\") only sets the content to empty and clears the buffer, we should also use ss.clear() to clear EOF.</p> <p>You could also use seekp(0), but that won't clear the buffer, it will just insert the next value at the beginning. However, it is not possible to use seekp() method on an empty stringstream.</p>","text":""},{"location":"Coding/C%2B%2B/dtype/str/#string_view","title":"string_view","text":"<p>std::string_view is just an abstraction of the (char * begin, char * end) pair. std::string_view is a good alternative to std::string const&amp; if you don't need to guarantee a null terminated string. also string_view has a better performance.</p> <pre><code>std::string str{\"foobar\"};\nstd::string_view bar{str.c_str(), str.size()};\nbar.remove_prefix(3);\nassert(bar == \"bar\");\n\n//string_view to string\nstring str(str_view); //will make a copy\n//bad, why? \n//returns a pointer to the underlying character array, \n//may return a pointer to a buffer that is not null\nstring str = str_view.data(); \n</code></pre>"},{"location":"Coding/C%2B%2B/dtype/struct/","title":"struct","text":""},{"location":"Coding/C%2B%2B/dtype/struct/#assign-vector-to-struct","title":"assign vector to struct","text":"<pre><code>struct myStruct {\n   int cnt;\n   vector&lt;double&gt; vec;\n};\n\nvector&lt;double&gt; val;\nval.reserve(10000);\n\nmyStruct stu;\nstu.cnt = 1000;\nstu.vec = std::move(val);\n</code></pre>"},{"location":"Coding/C%2B%2B/dtype/vec/","title":"vector","text":""},{"location":"Coding/C%2B%2B/dtype/vec/#stdvectoremplace","title":"std::vector::emplace","text":"<p>Construct and insert element</p>"},{"location":"Coding/C%2B%2B/dtype/vec/#stdvectoremplace_back","title":"std::vector::emplace_back","text":"<p>Construct and insert element at the end</p>"},{"location":"Coding/C%2B%2B/dtype/vec/#dynamic-array","title":"dynamic array","text":"<pre><code>double *numbers;\nnumbers = new double[size];\ndelete [] numbers;\n</code></pre>"},{"location":"Coding/Java/Java/","title":"java","text":"<p>install .jar file, in admin console: <pre><code>java -jar jbpm-installer-3.2.7.jar\n</code></pre></p>"},{"location":"Coding/Latex/CodeBlock/","title":"Code Block","text":""},{"location":"Coding/Latex/CodeBlock/#python-highlighting-using-minted","title":"Python highlighting using <code>minted</code>","text":"<ul> <li><code>pip install Pygments</code></li> <li>In Latex   <pre><code>\\usepackage{minted}\n% set code format\n\\setminted[python]{breaklines, framesep=2mm, fontsize=\\footnotesize, numbersep=5pt}\n% create a code block\n\\begin{minted}{python}\nif true:\n  print('Hello World!')\n\\end{minted}\n</code></pre></li> <li>Invoke the <code>shell-escape</code> flag to compile the document</li> </ul>"},{"location":"Coding/Latex/CodeBlock/#add-a-frame","title":"add a frame","text":"<pre><code>Add a frame for the code block (framesep is the distance between frame and content):\n\\begin{minted}[frame=single,framesep=10pt]{python}\nimport pandas as pd\nprint('Hello World!')\n\\end{minted}\n</code></pre>"},{"location":"Coding/Latex/CodeBlock/#use-newminted","title":"use <code>newminted</code>","text":"<pre><code>\\newminted{py}{\n    breaklines,\n    framesep=2mm,\n    fontsize=\\scriptsize,\n    numbersep=5pt,\n    linenos\n}\n\n% create a code block\n\\begin{pycode}\nif true:\n    print('Hello World!')\n\\end{pycode}\n</code></pre>"},{"location":"Coding/Latex/CodeBlock/#custom-the-space-before-and-after-the-code-block","title":"custom the space before and after the code block","text":"<p>after loading <code>minted</code> <pre><code>\\usepackage{etoolbox}\n\n\\BeforeBeginEnvironment{minted}{\\vspace{-8pt}}\n\\AfterEndEnvironment{minted}{\\vspace{-6pt}}\n</code></pre></p>"},{"location":"Coding/Latex/Font/","title":"Font","text":""},{"location":"Coding/Latex/Font/#some-sans-serif-font-example","title":"some <code>sans serif</code> font example","text":"<p>https://tex.stackexchange.com/questions/2095/what-is-the-simplest-way-to-typeset-an-entire-document-in-sans-serif - Fira: a little dark - Lato: looks good - notomath: good as well</p>"},{"location":"Coding/Latex/Font/#install-noto-on-ubuntu","title":"install noto on ubuntu","text":"<pre><code>sudo apt-get update\nsudo apt-get install fonts-noto\n</code></pre>"},{"location":"Coding/Latex/Font/#install-notomathsty-to-pdftex","title":"install <code>notomath.sty</code> to pdfTex","text":"<pre><code># user mode\ntlmgr install &lt;package&gt;\n# for sudo use the full path of tlmgr `which tlmgr`\n\n#all users\nwhich tlmgr\nsudo /path/to/tlmgr install &lt;package&gt;\n</code></pre>"},{"location":"Coding/Latex/Install/","title":"Install","text":""},{"location":"Coding/Latex/Install/#ubuntu-install-latest-tex-live","title":"ubuntu install latest tex live","text":"<p>https://fahim-sikder.github.io/post/installing-texlive-latest-ubuntu/</p> <pre><code>sudo apt install wget perl-tk\n\nwget http://mirror.ctan.org/systems/texlive/tlnet/install-tl-unx.tar.gz\ntar -xzf install-tl-unx.tar.gz\ncd install-tl-****\nsudo ./install-tl --scheme=medium --no-interaction --no-doc-install --no-src-install\n\nexport PATH=/usr/local/texlive/2023/bin/x86_64-linux${PATH:+:${PATH}}\n\n# as doc is not installed, these two do not exist\nexport MANPATH=/usr/local/texlive/2023/texmf-dist/doc/man${MANPATH:+:${MANPATH}}\nexport INFOPATH=/usr/local/texlive/2023/texmf-dist/doc/info${INFOPATH:+:${INFOPATH}}\n</code></pre> <p>make sure that Ubuntu thinks we have installed texlive <pre><code>sudo apt install equivs --no-install-recommends freeglut3\nwget -O debian-equivs-2022-ex.txt https://www.tug.org/texlive/files/debian-equivs-2022-ex.txt\n\nequivs-build debian-equivs-2022-ex.txt\nsudo dpkg -i texlive-local_2022.99999999-1_all.deb\nsudo apt install -f\n</code></pre></p>"},{"location":"Coding/Latex/Package/","title":"Package","text":""},{"location":"Coding/Latex/Package/#tlmgr","title":"tlmgr","text":"<pre><code># update\ntlmgr update --self\ntlmgr update --all\n# user mode\ntlmgr install &lt;package&gt;\n\n# for sudo use the full path of tlmgr `which tlmgr`\nwhich tlmgr\nsudo /path/to/tlmgr install &lt;package&gt;\n</code></pre>"},{"location":"Coding/Latex/Package/#filename","title":"filename","text":"<p>a <code>package</code> to tlmgr is a unit of files that are installed together it does not necessarily relate directly to a latex <code>package</code>.</p> <p>filename is not always the same as the package name. Here is the example to find the package name for file <code>otf.sty</code> <pre><code>tlmgr info otf.sty\n</code></pre></p>"},{"location":"Coding/Latex/Package/#find-the-package-name-that-contains-the-file","title":"find the package name that contains the file","text":"<pre><code>sudo apt update\nsudo apt install apt-file\nsudo apt-file update\n\napt-file search ptmr7t.tfm\n</code></pre>"},{"location":"Coding/Latex/Remove/","title":"remove texlive","text":"<p>https://tex.stackexchange.com/questions/95483/how-to-remove-everything-related-to-tex-live-for-fresh-install-on-ubuntu</p>"},{"location":"Coding/Linux/CPU/","title":"CPU","text":""},{"location":"Coding/Linux/CPU/#show-cpu-usage","title":"show cpu usage","text":"<pre><code>top\nhtop\n</code></pre>"},{"location":"Coding/Linux/CPU/#cpu-temperature","title":"cpu temperature","text":"<p>https://www.baeldung.com/linux/cpu-temperature</p>"},{"location":"Coding/Linux/CPU/#cpu-utilization","title":"cpu utilization","text":"<p>https://www.brendangregg.com/blog/2017-05-09/cpu-utilization-is-wrong.html</p>"},{"location":"Coding/Linux/CodeEnv/","title":"Code Env","text":""},{"location":"Coding/Linux/CodeEnv/#install-vs-code","title":"install vs code","text":"<pre><code>#Update the packages index and install the dependencies\nsudo apt update\nsudo apt install software-properties-common apt-transport-https wget\n\n#Import MS GPG key andenable vs code repository\nwget -q https://packages.microsoft.com/keys/microsoft.asc -O- | sudo apt-key add -\nsudo add-apt-repository \"deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\"\n\n#install vs code\nsudo apt install code\n\n#update\nsudo apt update\nsudo apt upgrade\n</code></pre>"},{"location":"Coding/Linux/CodeEnv/#install-python-miniforge","title":"install python miniforge","text":"<pre><code>sudo apt update\nsudo apt upgrade\n\n#get Miniforge and make it the main Python interpreter\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh -O ~/miniforge.sh\nbash ~/miniforge.sh -b -p ~/miniforge\nrm ~/miniforge.sh\n\necho \"PATH=$PATH:$HOME/miniforge/bin\" &gt;&gt; .bashrc\nsource .bashrc\n</code></pre>"},{"location":"Coding/Linux/CodeEnv/#install-powershell","title":"install powershell","text":"<pre><code>sudo apt update\nsudo apt install -y wget apt-transport-https software-properties-common                               #install pre-requisite packages\nwget -q \"https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/packages-microsoft-prod.deb\" #download Microsoft repository GPG keys\nsudo dpkg -i packages-microsoft-prod.deb                                             #register Microsoft repository GPG keys\nsudo apt update                  #update package list after adding packagesmicrosoft.com\nsudo apt-get install -y powershell        #install PowerShell\npwsh                                      #start PowerShell\n</code></pre>"},{"location":"Coding/Linux/Command/","title":"Command","text":"<p>Dig is a network tool used to look up DNS servers.</p>"},{"location":"Coding/Linux/Command/#create-temp-en-var","title":"create temp en var","text":"<pre><code>export env_var_name=env_var_value\n</code></pre>"},{"location":"Coding/Linux/CronJob/","title":"CronJob","text":""},{"location":"Coding/Linux/CronJob/#env","title":"env","text":"<p><code>cron</code> or non-interactive shells may have a different environment setup compared to the interactive shell. If a command is not found, it's better to include the command path to the <code>PATH</code>. For example, to run <code>kubectl</code> we should set <pre><code>export PATH=/usr/local/bin/kubectl:$PATH\nexport KUBECONFIG=/path/to/.config/.kube/kubeconfig\n</code></pre></p>"},{"location":"Coding/Linux/CronJob/#start-cron-service","title":"start cron service","text":"<pre><code>sudo service cron status/start\n</code></pre>"},{"location":"Coding/Linux/CronJob/#create-a-cronjob","title":"create a cronjob","text":"<p><code>erontab -e</code> and add: <pre><code>30 12 * * * /home/&lt;user-name&gt;/my.sh &gt;&gt; /mnt/c/dev/my.log 2&gt;&amp;1\n</code></pre></p>"},{"location":"Coding/Linux/CronJob/#run-every-5-minutes-but-start-from-002-not-000","title":"run every 5 minutes but start from 0:02 not 0:00","text":"<pre><code>#option 1\n2-59/5 * * * *\n#option 2 (always works)\n2,7,12,17,22,27,32,37,42,47,52,57 * * * *\n</code></pre>"},{"location":"Coding/Linux/Disk/","title":"Disk","text":""},{"location":"Coding/Linux/Disk/#show-disk-usage","title":"show disk usage","text":"<pre><code>df -h\ndu -sh ./*  #total\ndu -ahd1 . | sort -rh # `-a` include hidden files, `-d1` only prints data up to the first hierarchie level\n</code></pre>"},{"location":"Coding/Linux/Disk/#check-hard-disk-health","title":"check hard disk health","text":"<p>install <code>sudo apt install smartmontools</code> <pre><code>lsblk #list all hard drives, or use sudo fdisk -l\nlsblk -o NAME,MODEL,SIZE,TYPE #list type as well\ndf /home/user/Documents #find the disk the folder belongs to\nsudo smartctl -a /dev/sda #check disk health\n</code></pre></p>"},{"location":"Coding/Linux/Disk/#file-system-became-read-only","title":"file-system became read-only","text":"<p>A common reason for a file-system to be read-only is because it has errors, and needs <code>fsck</code> (file system check). <pre><code>mount | grep sda1\n\nsudo fsck -Af -M\nsudo fsck.ext4 -f /dev/sda1 #if the file system is ext4\nsudo reboot\n</code></pre></p>"},{"location":"Coding/Linux/Disk/#varlibdocker-device-or-resource-busy","title":"/var/lib/docker: Device or resource busy","text":"<p>When run <code>rm -rf /var/lib/docker</code> or <code>umount /dev/xxx</code> got the error. Possible reason: still in the mounted device.</p>"},{"location":"Coding/Linux/Disk/#mount-help","title":"mount help","text":"<pre><code>man mount.cifs\n</code></pre>"},{"location":"Coding/Linux/Disk/#unmount-cifs","title":"unmount cifs","text":"<pre><code>umount -a -t cifs -l    # umount all cifs\numount /mnt/cifs_share  # umount a specific cifs\n</code></pre>"},{"location":"Coding/Linux/Disk/#mount-cifs-network-drive","title":"mount cifs network drive","text":"<ul> <li>https://linuxize.com/post/how-to-mount-cifs-windows-share-on-linux/</li> <li>https://unix.stackexchange.com/questions/68079/mount-cifs-network-drive-write-permissions-and-chown</li> </ul> <p>On Linux and UNIX operating systems, a Windows share can be mounted on a particular mount point  in the local directory tree using the cifs option of the mount command.</p> <p>The Common Internet File System (CIFS) is a network file-sharing protocol. CIFS is a form of SMB. <pre><code>sudo mkdir /mnt/dat\nsudo mount -t cifs -o username=${USER},password=${PASSWORD},uid=$(id -u),gid=$(id -g) //server-address/folder /mnt/dat\nsudo mount -t cifs -o credentials=/etc/smb.credentials //server-address/folder /mnt/dat\nsudo mount -t cifs //my-drive/dev /mnt/dev -o uid=1000,gid=1000,vers=3.0,defaults,credentials=/home/user/smb.credentials --verbose\n</code></pre></p> <p>To ensure the mount persists on system reboots, this command must be added to  the <code>fstab</code> or equivalent to mount the DOCS DATA and VIEWER DATA on system startup. <pre><code># &lt;file system&gt;          &lt;dir&gt;    &lt;type&gt; &lt;options&gt;                                                   &lt;dump&gt;  &lt;pass&gt;\n//server-address/folder  /mnt/dat  cifs  file_mode=0755,dir_mode=0755,credentials=/etc/win-credentials 0       0\n\n# dat mount directory\n//xyz1wes01/dat  /mnt/dat  cifs  uid=1000,gid=1000,vers=1.0,defaults,credentials=/home/user/smb.credentials 0 0\n</code></pre> Run the following command to mount/umount the share: <pre><code>sudo mount /mnt/dat\nsudo umount /mnt/dat\n</code></pre></p>"},{"location":"Coding/Linux/Disk/#where-a-path-is-mounted-from","title":"where a path is mounted from","text":"<pre><code>findmnt --target /etc/fstab # show mounted path\nsudo cat /etc/fstab         # check the file content\nmountpoint /mnt/myfolder    # will also tell if not mounted\n</code></pre>"},{"location":"Coding/Linux/Disk/#mounted-path-is-ro-or-rw","title":"mounted path is <code>ro</code> or <code>rw</code>","text":"<pre><code>cat /proc/mounts\n//xyz1wes01/dat /mnt/dat cifs rw,relatime,vers=1.0,cache=strict,username=usr,domain=,uid=1000,forceuid,gid=1000,forcegid,\naddr=90.800.70.60,file_mode=0755,dir_mode=0755,soft,nounix,mapposix,rsize=61440,wsize=65536,echo_interval=60,actimeo=1 0 0\n</code></pre>"},{"location":"Coding/Linux/Disk/#cifs-logs","title":"cifs logs","text":"<p>https://community.microfocus.com/img/oes/w/tips/14583/cifs-writing-in-2-differents-log-file - /var/log/messages - /var/log/cifs/cifs.log - /etc/rsyslog.d/cifs-log.conf</p>"},{"location":"Coding/Linux/Disk/#cifs-troubleshooting","title":"cifs troubleshooting","text":"<p>https://wiki.samba.org/index.php/LinuxCIFS_troubleshooting - debug data: <code>/proc/fs/cifs/DebugData</code> - debug message: <code>dmesg</code></p> <p>enalbe/disable debug <pre><code>echo 7 | sudo tee /proc/fs/cifs/cifsFYI #enable debug\ndmesg                                   #check log\necho 0 | sudo tee /proc/fs/cifs/cifsFYI #disable debug\n</code></pre></p> <ul> <li>check cifs client package: <code>dpkg -l | grep cifs-utils</code></li> <li>check cifs kernel module: <code>lsmod | grep cifs</code></li> <li>list all services: <code>systemctl list-units</code> </li> </ul>"},{"location":"Coding/Linux/Disk/#check-mounted-drive","title":"check mounted drive","text":"<p>To verify that the remote Windows share is successfully mounted, use either the <code>mount</code> or <code>df -h</code> command.</p> <p>Can also check the network using <code>ping &lt;cifs_server_ip_or_hostname&gt;</code></p>"},{"location":"Coding/Linux/Disk/#cifs-issue","title":"cifs issue","text":""},{"location":"Coding/Linux/Disk/#cifs-mount-suddenly-no-longer-works","title":"cifs mount suddenly no longer works","text":"<p>https://ubuntuforums.org/showthread.php?t=2490382</p> <p><code>nodfs</code>: disable dfs on the client side - for latest new Linux kernels on the client machine <pre><code>sudo mount -t cifs //192.168.1.16/user /media/cloudbox -o username=user,password=xxx,vers=1.0,nodfs\n</code></pre></p>"},{"location":"Coding/Linux/Disk/#cifs-vfs-server-has-not-responded","title":"CIFS VFS: Server has not responded","text":"<p>https://forums.linuxmint.com/viewtopic.php?t=190961</p> <p>Solution:  - Auto Mounting Samba Shares Using AutoFS: http://forums.linuxmint.com/viewtopic.php?f=42&amp;t=144997 - AutoFS is an \"automounter\" which means it doesn't mount at boot or login it only mounts when the mount point is accessed. - It also unmounts by itself when the share is not being used.</p>"},{"location":"Coding/Linux/Disk/#access-cifs-via-smbclient","title":"access cifs via smbclient","text":"<p>https://unix.stackexchange.com/questions/706325/smbclient-works-but-mount-cifs-doesnt-nt-status-more-processing-required <pre><code>smbclient '\\\\172.25.162.14\\nda20' -N -m NT1 --option=\"client min protocol\"=NT1\nmount -t cifs -vvvv //172.25.162.14/nda20 /mnt/aa -o username=root,vers=1.0,guest\n</code></pre></p>"},{"location":"Coding/Linux/Disk/#smb-error-class-and-code","title":"smb error class and code","text":"<p>https://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-cifs/8f11e0f3-d545-46cc-97e6-f00569e3e1bc</p>"},{"location":"Coding/Linux/DualBoot/","title":"Dual boot","text":""},{"location":"Coding/Linux/DualBoot/#change-grub-menu-order","title":"Change GRUB menu order","text":"<ul> <li><code>ll /etc/grub.d/</code>: GRUB builds its menu using scripts in <code>/etc/grub.d/</code>. The numbers define the order - lower numbers appear first</li> <li>Change Script File Order so Windows appear before Linux: <code>sudo mv /etc/grub.d/30_os-prober /etc/grub.d/09_os-prober</code></li> <li>Update GRUB <code>sudo update-grub</code>: This rebuilds /boot/grub/grub.cfg with the new order</li> <li><code>sudo reboot</code></li> </ul>"},{"location":"Coding/Linux/DualBoot/#set-default-boot-os","title":"Set default boot OS","text":"<ul> <li>open <code>/etc/default/grub</code></li> <li>change <code>GRUB_DEFAULT=2</code> [order starts from zero] and save it</li> <li><code>sudo update-grub</code> and <code>reboot</code></li> </ul>"},{"location":"Coding/Linux/DualBoot/#os-selection-time","title":"OS selection time","text":"<ul> <li>open <code>/etc/default/grub</code></li> <li>change <code>GRUB_TIMEOUT=10</code> [value in second] and save it</li> <li><code>sudo update-grub</code> and <code>reboot</code></li> </ul>"},{"location":"Coding/Linux/DualBoot/#repair-grub-efi-overwritten-by-windows-update","title":"Repair grub efi overwritten by windows update","text":"<p>https://askubuntu.com/questions/88384/how-can-i-repair-grub-how-to-get-ubuntu-back-after-installing-windows</p> <p>When starting up the computer, it goes straight to Windows, without giving me the option of booting Ubuntu.</p> <p>Reason: Windows assumes it is the only operating system (OS) on the machine. During update it replaces GRUB with its own boot loader.</p> <p>Solution: Replace the Windows boot loader with GRUB. - After showing dell logo, press F12 - Boot from the live USB, in \"Try Ubuntu\" mode - Determine the partition number of your main partition <code>sudo fdisk -l</code> - Mount linux partition: <code>sudo mount /dev/nvme0n1p5 /mnt</code> - Linux filesystem - Mount efi partition: <code>sudo mount /dev/nvme0n1p1 /mnt/boot/efi</code> - EFI System - Bind mount some other stuff <code>for i in /sys /proc /run /dev; do sudo mount --rbind \"$i\" \"/mnt$i\"; done</code> - <code>sudo chroot /mnt</code> - update grub <code>update-grub</code> - reinstall grub <code>grub-install /dev/nvme0n1</code> - reboot <code>exit</code> and <code>sudo reboot</code></p>"},{"location":"Coding/Linux/DualBoot/#fsck-exited-with-status-code-4","title":"fsck exited with status code 4","text":"<p>The root filesystem on /dev/nvme0n1p5 requires a manual fsck <pre><code>fsck /dev/nvme0n1p5 -y\n</code></pre> After that run <code>exit</code></p>"},{"location":"Coding/Linux/DualBoot/#stuck-at-grub-command-line-gnu-grub-version-206-3deb11u2","title":"Stuck at GRUB command line - GNU GRUB version 2.06-3~deb11u2","text":"<p>This means the grub could not find the root system so we need to repair it.</p> <p>The second answer in the first link is same as the second link - https://askubuntu.com/questions/883992/stuck-at-grub-command-line - https://medium.com/codebrace/rescuing-a-non-booting-grub-on-linux-9409dd623743</p>"},{"location":"Coding/Linux/DualBoot/#find-disk-and-partition-info","title":"find disk and partition info","text":"<p>https://ubuntuforums.org/showthread.php?t=2455336 <pre><code>sudo fdisk -lu\n</code></pre> My disk is <code>/dev/nvme0n1</code> and the partition for linux is <code>/dev/nvme0n1p7</code></p>"},{"location":"Coding/Linux/DualBoot/#find-the-partition-with-bootgrub","title":"find the partition with <code>/boot/grub</code>","text":"<ul> <li>Type <code>ls</code> to list all partitions that GRUB sees</li> <li>Use <code>ls (hd0,gpt7)/</code> command to find the partition with <code>/boot/grub</code></li> <li>Ensure this is the linux partition: <code>grub&gt; cat (hd0,gpt7)/etc/issue</code> will return <code>Ubuntu 20.04 LTS \\n \\l</code></li> </ul>"},{"location":"Coding/Linux/DualBoot/#temporally-boot-from-grub-into-the-linux-system-only-works-once","title":"temporally boot from grub into the linux system (only works once)","text":"<p>In the second line <code>root=/dev/sdX</code> sets the location of the root filesystem. Mine is <code>/dev/nvme0n1</code>. The third line sets the initrd file, which must be the same version number as the kernel. <pre><code>grub&gt; set root=(hd0,gpt7)\ngrub&gt; linux /boot/vmlinuz-3.13.0-29-generic root=/dev/sda1\ngrub&gt; initrd /boot/initrd.img-3.13.0-29-generic\ngrub&gt; boot\n</code></pre></p>"},{"location":"Coding/Linux/DualBoot/#making-permanent-repairs","title":"making permanent repairs","text":"<p>Open a terminal in the linux <pre><code>sudo update-grub\nsudo grub-install /dev/nvme0n1p7\n</code></pre> Install the grub to the boot sector of your hard drive and not to a partition.</p>"},{"location":"Coding/Linux/DualBoot/#check-security-boot-enabled-or-not","title":"check security boot enabled or not","text":"<p><code>sudo mokutil --sb-state</code>: - SecureBoot enabled - SecureBoot disabled - SecureBoot disabled\\nPlatform is in Setup Mode</p>"},{"location":"Coding/Linux/DualBoot/#disable-fast-startup-in-windows","title":"disable fast startup in windows","text":"<p>Issue: - wifi disappeared after boot from windows - reboot ubuntu will bring back wifi - might due to windows fast startup</p> <p>https://www.windowscentral.com/software-apps/windows-11/how-to-enable-or-disable-fast-startup-on-windows-11 - Power Options - Choose what the power button does - Change settings that are currently unavailable - Under the \"Shutdown settings\" section, check the \"Turn off fast startup\" option - Save Changes</p>"},{"location":"Coding/Linux/DualBoot/#wifi-setting-is-missing","title":"Wifi setting is missing","text":"<ul> <li>power off</li> <li>disconnect power cable</li> <li>hold the power button for 30 seconds</li> <li>wifi should be back - but just a temporal solution (usually reboot works)</li> </ul> <p>Other solution for ubuntu 24.04: - https://gist.github.com/UbuntuEvangelist/e36f6a1a9ef7cb0a0b24e592eb925b68 - https://askubuntu.com/questions/1536788/no-wifi-adapter-found-dual-boot-ubuntu-24-04-and-windows-11 - https://community.frame.work/t/solved-bios-3-17-and-dual-boot-wifi/31424/4</p> <p>When windows fast startup is enabled, windows will still own the wifi device after hibernation. Disabling fast startup worked.</p>"},{"location":"Coding/Linux/DualBoot/#dual-boot-wrong-time","title":"Dual boot wrong time","text":"<p>https://itsfoss.com/wrong-time-dual-boot/</p> <p>A hardware clock which is also called RTC (real time clock) or CMOS/BIOS clock. By default, Linux assumes that the time stored in the hardware clock is in UTC, while Windows thinks that the time stored on the hardware clock is local time.</p>"},{"location":"Coding/Linux/DualBoot/#solution-1","title":"Solution 1","text":"<p>Setup for Linux system to use the local time for the hardware clock (RTC) <pre><code>timedatectl set-local-rtc 1 #0 for UTC time standard, 1 for localtime time standard\n</code></pre></p>"},{"location":"Coding/Linux/DualBoot/#solution-2","title":"Solution 2","text":"<p>Setup for Windows system to use the UTC for the hardware clock (RTC).</p> <p>For 64-bit Windows, open regedit then browse to HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation. Create a new QWORD entry called RealTimeIsUniversal, then set its value to 1. Reboot the system. The clock should now be in UTC time. <pre><code>Reg add HKLM\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation /v RealTimeIsUniversal /t REG_QWORD /d 1\n</code></pre></p>"},{"location":"Coding/Linux/Editor/","title":"Editor","text":""},{"location":"Coding/Linux/Editor/#check-available-editors","title":"check available editors","text":"<pre><code>update-alternatives --display editor\n</code></pre>"},{"location":"Coding/Linux/Editor/#create-a-file-without-editor","title":"create a file without editor","text":"<p>One line file: <pre><code>echo \"Hello World\" &gt; file.txt\n</code></pre></p> <p>Multiple line file: <pre><code>echo \"Line 1\" &gt;&gt; file.txt\necho \"Line 2\" &gt;&gt; file.txt\n</code></pre></p> <p>Multiple line file: <pre><code>cat &lt;&lt; EOF &gt; file.txt\nLine 1\nLine 2\nEOF\n</code></pre> Limitation: \"$only\" will became \"\".</p>"},{"location":"Coding/Linux/Editor/#replace-last-line","title":"replace last line","text":"<pre><code>{ head -n -1 filename.txt; echo \"New last line content\"; } &gt; filename.txt\n</code></pre>"},{"location":"Coding/Linux/Editor/#escape-special-characters-and-keep-leading-spaces-and-tabs","title":"escape special characters and keep leading spaces and tabs","text":"<ul> <li><code>'EOF'</code> will escape special characters</li> <li><code>&lt;&lt;-</code> will keep (or strip) leading spaces and tabs??? <pre><code>cat &lt;&lt;- 'EOF' &gt; file.txt\n  Special characters: $, \\, *, etc.\n  Including variable: \"$only\"\nEOF\n</code></pre></li> </ul>"},{"location":"Coding/Linux/ExeLib/","title":"Liabrary","text":""},{"location":"Coding/Linux/ExeLib/#dependency","title":"dependency","text":"<p>check exe depedency: <pre><code>ldd /path/to/exe-file\n</code></pre></p>"},{"location":"Coding/Linux/FileCopy/","title":"File copy","text":""},{"location":"Coding/Linux/FileCopy/#move-file-or-folder","title":"Move file or folder","text":"<pre><code>mv test/file-a /dev/file-b\nmv test/folder-a/ folder-b/\nmv /path/folder/* /anotherpath/folder/ #hidden files will be excluded\n</code></pre>"},{"location":"Coding/Linux/FileCopy/#delete-file-and-folder","title":"Delete file and folder","text":"<pre><code>rm -r my-folder/  #delete folder and files\nrm -r my-folder/* # delete all in the folder\n</code></pre>"},{"location":"Coding/Linux/FileCopy/#copy-folder","title":"Copy folder","text":"<pre><code>cp -option1 -option2 source destination\ncp -r ./source/ /dest/   #copy source folder into dest folder\ncp -a ./source/. ./dest/ #content in source to dest, reserve all file attributes\n#example\ncp -avr /home/books /usb/backup\n    -a: Preserve the specified attributes such as directory and file mode, ownership, timestamps,\n        if possible additional attributes: context, links, xattr, all.\n    -v: Verbose output.\n    -r: Copy directories recursively.\n</code></pre>"},{"location":"Coding/Linux/FileCopy/#copy-from-ssh-connected-machine-to-current-machine","title":"copy from ssh-connected machine to current machine","text":"<p>scp: <code>secure copy</code> allows us to securely transfer files between machines over SSH. <pre><code>scp username@remote_host:/path/to/remote_file /path/to/local/directory\n</code></pre></p> <p>sftp: <code>SSH File Transfer Protocol</code> allows browsing and transferring files between machines securely. <pre><code>sftp username@remote_host\nget remote_file ~/Downloads\nput local_file.txt /uploads/local_file.txt\nexit\n</code></pre></p>"},{"location":"Coding/Linux/FileEdit/","title":"File edit","text":""},{"location":"Coding/Linux/FileEdit/#merge-files-into-one","title":"merge files into one","text":"<pre><code>cat ./* &gt; merged-file\n\ncat file.zip* &gt; ./file.zip  #concatenate all file.zip.001 etc. into file.zip\nunzip file.zip              #unzip the file\n</code></pre>"},{"location":"Coding/Linux/FileEdit/#check-new-line-n-or-rn","title":"check new line (\\n or \\r\\n)","text":"<p>displays Unix line endings (<code>\\n</code> or LF) as <code>$</code> and Windows line endings (<code>\\r\\n</code> or CRLF) as <code>^M$</code> <pre><code>cat -e &lt;filename&gt;\n</code></pre></p> <p>convert from one to another <pre><code>apt-get install -y dos2unix\ndos2unix &lt;filename&gt;\n\napt-get install -y unix2dos\nunix2dos &lt;filename&gt;\n</code></pre></p>"},{"location":"Coding/Linux/FileSys/","title":"File system","text":""},{"location":"Coding/Linux/FileSys/#find-file-in-current-and-sub-directories","title":"find file in current and sub directories","text":"<pre><code>find . -name index.html\n</code></pre>"},{"location":"Coding/Linux/FileSys/#find-folder-position","title":"find folder position","text":"<pre><code>find /path/to/search  -name \"xyz-abc\" -type d\n</code></pre>"},{"location":"Coding/Linux/FileSys/#size-of-subfolders","title":"size of subfolders","text":"<pre><code>du -M --max-depth=1      #current folder\ndu -sh /var              #only the total size\ndu -shc /var/*           #folder and subfolders, c for grand total\ndu -h --max-depth=1 /var #each folder in var folder\ndu -h -d 1 | sort -h     #first subfolders only\ndu -h /var/ | sort -rh | head -5 #top 5 largest directories\n</code></pre>"},{"location":"Coding/Linux/FileSys/#create-file-and-add-content","title":"Create file and add content","text":"<pre><code>cat &gt; filename\n#input and then Ctrl+D to exit\n</code></pre>"},{"location":"Coding/Linux/FileSys/#create-directory","title":"Create directory","text":"<pre><code>mkdir\n</code></pre>"},{"location":"Coding/Linux/FileSys/#change-rwx-mode","title":"Change rwx mode","text":"<pre><code>chmod g+w filename\nchmod g-wx filename\nchmod o+w filename\nchmod o-rwx foldername\n</code></pre>"},{"location":"Coding/Linux/FileSys/#change-file-ownership","title":"Change file ownership","text":"<pre><code>chown &lt;owner-name&gt; &lt;filename&gt;\nchown -R user:user /myfolder  #change folder and content owner/group names\nsudo chown -R &lt;username&gt;:&lt;username&gt; ./conda-build\nchgrp &lt;group-name&gt; &lt;filename&gt;\n</code></pre>"},{"location":"Coding/Linux/Hardware/","title":"Hardware","text":""},{"location":"Coding/Linux/Hardware/#cpu","title":"cpu","text":"<pre><code>lscpu   #cat /proc/cpuinfo\ntop     #cpu and memory\n</code></pre>"},{"location":"Coding/Linux/Hardware/#free-and-used-memory","title":"free and used memory","text":"<pre><code>lsmem   #cat /proc/meminfo\nfree -m # -g-h\n</code></pre>"},{"location":"Coding/Linux/Hardware/#other","title":"other","text":"<pre><code>lsblk\nlsusb\nlspci\n</code></pre>"},{"location":"Coding/Linux/Latex/","title":"Latex","text":""},{"location":"Coding/Linux/Latex/#manualy-install-package","title":"manualy install package","text":"<ul> <li>download the package from CTAN (e.g., foo.zip)</li> <li>extract the files and place them in an appropriate directory</li> <li>generate the .sty file by running <code>latex foo.ins</code> (.sty) and <code>latex foo.dtx</code> (manual)</li> <li>copy .sty file into the LaTeX Texlive installation in Ubuntu: <code>sudo cp foo.sty  /usr/share/texlive/texmf-dist/tex/latex/base</code></li> <li>update the ls-R file in this source tree: <code>sudo mktexlsr</code></li> </ul>"},{"location":"Coding/Linux/Latex/#create-a-makefile","title":"create a makefile","text":"<pre><code>touch makefile\nchmod u+x makefile\ngedit makefile\n</code></pre>"},{"location":"Coding/Linux/Latex/#makefile","title":"makefile","text":"<ul> <li><code>$@</code> for the makefile target</li> <li><code>$&lt;</code> for the first prerequisite</li> <li><code>$^</code> for names of all the prerequisites</li> <li><code>$*</code> only for the <code>%</code> part <pre><code>#create pdf: make simple.pdf from my_simple.tex\n%.pdf : my_%.tex\n    rm -f my_pdf_name.pdf\n    pdflatex $&lt; -o my_$* 2&gt;&amp;1 | tee errors.err\n    bibtex $(basename $&lt;) 2&gt;&amp;1 | tee errors.err\n    pdflatex $&lt; -o my_$* 2&gt;&amp;1 | tee errors.err\n    pdflatex $&lt; -o my_$* 2&gt;&amp;1 | tee errors.err\n    cp my_$*.pdf my_pdf_name.pdf\n#clean\n    rm -f *.o errors.err\n    rm -f *.aux *.bbl *.blg *.log *.out\n    rm -f *.synctex.gz\n    rm -f BUILD\n    @echo\n</code></pre></li> </ul>"},{"location":"Coding/Linux/Linux/","title":"Linux","text":"<p>Ctrl + Alt + T: open terminal</p> <p>WinSCP to access file on a linux machine, but connection is not stable</p> <p>show all users <pre><code>cut -d: -f1 /etc/passwd\n</code></pre></p>"},{"location":"Coding/Linux/Linux/#create-a-folder","title":"create a folder","text":"<pre><code>mkdir /tmp/new_folder\n</code></pre>"},{"location":"Coding/Linux/Linux/#delete-things-in-folder","title":"delete things in folder","text":"<pre><code>rm -rf /tmp/*\n</code></pre>"},{"location":"Coding/Linux/Linux/#run-sh-in-linux-from-win","title":"run sh in linux from win","text":"<p>http://the.earth.li/~sgtatham/putty/0.58/htmldoc/Chapter7.html https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html <pre><code>plink -ssh hostname\n/ls list by disabling color.\nREM otherwise win console will not be able to\nREM interpret the color and show strange characters\n./dts.sh 2&gt;&amp;1 | tee backup.log\n</code></pre></p> <p>login with root if sudo is required <pre><code>plink.exe -ssh -pw xvcvvx  -t root@xxx-xxx.xx.xx\n</code></pre></p>"},{"location":"Coding/Linux/Linux/#terminal-to-console-and-file","title":"terminal to console and file","text":"<pre><code>make 2&gt;&amp;1 | tee file.txt\n</code></pre>"},{"location":"Coding/Linux/Linux/#list-scheduled-cron-jobs","title":"list scheduled cron jobs","text":"<pre><code>#root\nless /etc/crontab\ncrontab -l\n\n#current user\ncrontab -l\n#specific user\ncrontab -l -u juser\n\n#root user\n/etc/crontab\n#all other users\n/var/spool/cron/crontabs\n\n#enabled system timers\nsystemctl list-timers\n</code></pre>"},{"location":"Coding/Linux/Linux/#list-dirs-in-current-folder","title":"list dirs in current folder","text":"<pre><code>ls -d */\n</code></pre>"},{"location":"Coding/Linux/Linux/#where-is-tmp-mounted","title":"where is /tmp mounted","text":"<pre><code>$ df -h /tmp\nFilesystem     1K-blocks     Used Available Use% Mounted on\n/dev/root      480589544 42607368 413546516  10% /\n\n$ cat /proc/cmdline | grep root\nBOOT_IMAGE=/boot/vmlinuz-3.19.0-32-generic root=UUID=0cde5cf9-b15d-4369-b3b1-4405204fd9ff ro\n\n$ sudo blkid\n/dev/sda1: UUID=\"0cde5cf9-b15d-4369-b3b1-4405204fd9ff\" TYPE=\"ext4\"\n/dev/sda5: UUID=\"37bc6a9c-a27f-43dc-a485-5fb1830e1e42\" TYPE=\"swap\"\n/dev/sdb1: UUID=\"177c3cec-5612-44a7-9716-4dcba27c69f9\" TYPE=\"ext4\"\n</code></pre>"},{"location":"Coding/Linux/Linux/#check-disks","title":"check disks","text":"<pre><code>#set password\nsudo passwd\n\n#check disk info\ndf -h\ndf -h |grep ^/dev #does not work for multiline cases\nsudo fdisk -l\nsudo sfdisk -l -uM\nsudo parted -l\n</code></pre>"},{"location":"Coding/Linux/Linux/#check-mysql","title":"check mysql","text":"<pre><code>service --status-all\nservice mysqld status\n\ntop grep mysql\nsystemctrl status mysqld.service\nmysqladmin -u root -p status\nmysqladmin -u root -p version\n</code></pre>"},{"location":"Coding/Linux/Linux/#chkconfig-for-services","title":"chkconfig for services","text":"<p>https://www.thegeekdiary.com/how-to-enable-or-disable-service-on-boot-with-chkconfig/</p> <pre><code>chkconfig --list\nchkconfig --list mysqld\nchkconfig --level 345 mysqld on\nchkconfig --add [servicename]\nchkconfig [servicename] reset\nchkconfig [servicename] off\n</code></pre>"},{"location":"Coding/Linux/Linux/#bash","title":"bash","text":"<pre><code>#!/bin/bash\n\n#set executable permission\nchmod +x my.sh\n\n#show permission\nls -l my.sh\n</code></pre>"},{"location":"Coding/Linux/Linux/#backup-databases","title":"backup databases","text":"<pre><code>#!/bin/bash\n\nfdb=dump_dbs.txt\ndir=\"/media/data/test\"\n\ncreate_list=$(( $#==0 ? 1 : 0 ))\n\ntimestamp() {\n    date +\"%Y-%m-%d %H:%M:%S\"\n}\n\ni=0\necho $(timestamp) dump dbs\nwhile read db; do\n    ((++i))\n    ftb=$dir/$db\"_tbs.txt\"\n    if [ $create_list -eq 1 ]; then\n        qry='SELECT table_name FROM information_schema.tables WHERE table_schema='\\'${db}\\'' order by table_name;'\n        mysql --defaults-extra-file=$dir/svr_fr.cnf --skip-column-names -e \"${qry}\" &gt;$ftb\n    fi\n    echo $(timestamp) \"  \" db: $db\n    j=0\n    while read tb; do\n        ((++j))\n        echo $(timestamp) \"    \"$j table: $tb\n        mysqldump --defaults-extra-file=$dir/svr_fr.cnf $db $tb | mysql --defaults-extra-file=$dir/svr_to.cnf $db\n    done &lt;$ftb\ndone &lt;$fdb\n\necho $(timestamp) all done!\n</code></pre>"},{"location":"Coding/Linux/Linux/#backup-hdd","title":"backup hdd","text":"<p>DISCALIMER: this will erase the destination/new hdd.   - Get a Linux live disk. Linux Mint works great and is not a huge download.   - Boot off the linux live disk.   - Make sure you are connected to the internet. Open the web browser to make sure.   - Open a command line. On most linux systems this can be done by pressing Ctrl+Alt+T. If that doesn't work, search through the menu for a program called Terminal or something like that.   - Type \"sudo bash\". If prompted for a password, just press enter(assuming you are using linux mint. If you are using another distro, you will have to find out what the pasword for that is.)   - Type \"apt-get update\"   - Type \"apt-get -y install pv\"   - Type \"apt-get -y install gparted\"   - Type \"gparted\"   - This will open a disk utility program. Using the drop-down on the top-right of the program, figure out which hard drive is your old hdd and which is your new hdd. You can figure it out by looking which hdd has a windows partition. The new hdd should be blank. Write down which hdd is which. DO NOT GET THEM MIXED UP! Or you will erase your entire hdd. It should be something like /dev/sda and /dev/sdb but make sure.   - Type \"dd if= | pv | dd of=\" replacing  with the device id of your old hdd and  with your new hdd. For example, if your old hdd was /dev/sda and your new hdd was /dev/sdb, then you would type \"dd if=/dev/sda | pv | dd of=/dev/sdb\".   - Your drive is now being cloned. It will tell you the speed of the clone and how much it has transferred so far. It will take a LOONG time. Your new HDD will be EXACTLY the same as your old hdd. All your files will be there, and you will be able to boot off it just like always. This is the most reliable method I know, although it is slow and cumbersome. Alternatively, you could try WinDD which is a windows tool to do the same thing, but I have never used it before and cannot guarantee that it will work. I personally would just use the linux method."},{"location":"Coding/Linux/Linux/#check-hardware","title":"check hardware","text":"<pre><code>lspci\ncat /proc/cpuinfo - Displays CPU info\ncat /proc/meminfo - Displays memory info\nhdparm -I /dev/sda - Disk\nsosreport - most info in CentOS\n</code></pre>"},{"location":"Coding/Linux/LinuxLite/","title":"Linux Lite","text":""},{"location":"Coding/Linux/LinuxLite/#install-dual-boot-with-winxp","title":"Install dual boot with winxp","text":"<p>create partitions for swap, boot and home</p>"},{"location":"Coding/Linux/LinuxLite/#fix-start-with-number-lock","title":"Fix start with number lock","text":"<p>Edit: /etc/lightdm/lightdm.conf</p> <p>Comment out: greeter-setup-script=/usr/bin/numlockx on</p>"},{"location":"Coding/Linux/LinuxLite/#shutdown-buttons-missing","title":"Shutdown buttons missing","text":"<p>Open Lite Tweaks --&gt; Login &amp; Logout Options... select \"Show all Options\" and Apply.</p> <p>Then logout and login again... Shutdown and Restart buttons should be enabled once again.</p>"},{"location":"Coding/Linux/LinuxLite/#fix-hibernation-with-swap-partition","title":"Fix hibernation with swap partition","text":"<pre><code>#1. copy swap UUID number\n$ cat /etc/fstab #UUID=XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\n\n#2. edit file /etc/default/grub\nGRUB_CMDLINE_LINUX=\"\"  --&gt;\nGRUB_CMDLINE_LINUX=\"resume=UUID=XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n\n3. update grub\n$ sudo update-grub\n\n4. install initramfs-tools\n$ sudo apt-get update -y\n$ sudo apt-get install -y initramfs-tools\n\n5. update initramfs\n$ sudo update-initramfs -u\n\n6.- restart\n</code></pre>"},{"location":"Coding/Linux/LinuxLite/#slow-boot","title":"slow boot","text":"<p>https://www.reddit.com/r/Ubuntu/comments/clu0lj/short_guide_to_improve_slow_boot_on_ubuntu_1804/ <pre><code>systemd-analyze\nsystemd-analyze blame\nsystemd-analyze critical-chain\n</code></pre></p>"},{"location":"Coding/Linux/LinuxLite/#delay-service-autostart","title":"delay service autostart","text":"<p>option 1: add an ExecStarPre command after the [service] <pre><code>[Service]\nExecStartPre=/bin/sleep 5\nEnvironmentFile=/etc/default/myservice\n</code></pre></p> <p>option 2: create a .timer systemd unit file to control the execution of the .service unit file.</p> <p>Wait for 1 minute after boot-up before starting the foo.service. <pre><code>[Timer]\nOnBootSec=1min\n</code></pre> It is important that the service is disabled, and the timer enabled <pre><code>systemctl disable foo.service\nsystemctl enable foo.timer\n</code></pre></p>"},{"location":"Coding/Linux/LinuxLite/#prevent-docker-autostart","title":"prevent docker autostart","text":"<pre><code>#stop docker\nsudo systemctl stop docker.service\nsudo systemctl stop docker.socket\n#disable docker\nsudo systemctl disable docker.service\nsudo systemctl disable docker.socket\n#check status\nsystemctl status docker\n#check boot status\nsystemctl list-unit-files | grep -i docker\nsystemctl list-unit-files | grep -i 'state\\|docker'\n</code></pre>"},{"location":"Coding/Linux/LinuxLite/#colored-terminal","title":"Colored terminal","text":"<ul> <li>Add this after the last 'fi' in .bashrc <pre><code># set a fancy prompt (non-color, unless we know we \"want\" color)\ncase \"$TERM\" in\n    xterm-color) color_prompt=yes;;\nesac\n\n# uncomment for a colored prompt, if the terminal has the capability; turned\n# off by default to not distract the user: the focus in a terminal window\n# should be on the output of commands, not on the prompt\nforce_color_prompt=yes\n\nif [ -n \"$force_color_prompt\" ]; then\n    if [ -x /usr/bin/tput ] &amp;&amp; tput setaf 1 &gt;&amp;/dev/null; then\n    # We have color support; assume it's compliant with Ecma-48\n    # (ISO/IEC-6429). (Lack of such support is extremely rare, and such\n    # a case would tend to support setf rather than setaf.)\n    color_prompt=yes\n    else\n    color_prompt=\n    fi\nfi\n\nif [ \"$color_prompt\" = yes ]; then\n    PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ '\nelse\n    PS1='${debian_chroot:+($debian_chroot)}\\u@\\h:\\w\\$ '\nfi\nunset color_prompt force_color_prompt\n</code></pre></li> <li>In terminal input <pre><code>source ~/.bashrc\n</code></pre></li> </ul>"},{"location":"Coding/Linux/LinuxLite/#install-vs-code","title":"install vs code","text":"<pre><code>sudo apt update\n#install dependencies\nsudo apt install software-properties-common apt-transport-https\n#download repository and import Microsoft\u2019s GPG key\nwget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; packages.microsoft.gpg\nsudo install -o root -g root -m 644 packages.microsoft.gpg /etc/apt/trusted.gpg.d/\nsudo sh -c 'echo \"deb [arch=amd64 signed-by=/etc/apt/trusted.gpg.d/packages.microsoft.gpg] https://packages.microsoft.com/repos/vscode stable main\" &gt; /etc/apt/sources.list.d/vscode.list'\n#install vs code\nsudo apt update\nsudo apt install code\n</code></pre>"},{"location":"Coding/Linux/LinuxLite/#install-php","title":"Install PHP","text":"<pre><code>sudo apt install php php-cli php-fpm php-json php-common php-mysql php-zip php-gd php-mbstring php-curl php-xml php-pear php-bcmath\nphp --version #check version\nphp -m #listing all loaded PHP modules\n</code></pre>"},{"location":"Coding/Linux/LinuxLite/#install-python3-custom-version","title":"install Python3 custom version","text":"<pre><code>cd ~\nmkdir tmp\ncd tmp\nwget https://www.python.org/ftp/python/3.9.7/Python-3.9.7.tgz\ntar zxvf Python-3.9.7.tgz\ncd Python-3.9.7\n./configure --prefix=$HOME/opt/python-3.9.7\nmake\nmake install\nexport PATH=$HOME/opt/python-3.9.7/bin:$PATH#add to .bash_profile\n. ~/.bash_profile #update to active loacal python env\n</code></pre>"},{"location":"Coding/Linux/Logs/","title":"Logs","text":""},{"location":"Coding/Linux/Logs/#find-out-which-log-file-the-syslogd-process-is-using","title":"Find out which log file the syslogd process is using","text":"<pre><code>sudo lsof -p $(pgrep syslog) | grep log$\n</code></pre>"},{"location":"Coding/Linux/Logs/#show-most-recent-output-as-it-goes-into-the-syslog","title":"Show most recent output as it goes into the syslog","text":"<pre><code>tail -f /var/log/syslog\n</code></pre>"},{"location":"Coding/Linux/Memory/","title":"Memory","text":""},{"location":"Coding/Linux/Memory/#show-memory-usage","title":"show memory usage","text":"<pre><code>free -h\n</code></pre>"},{"location":"Coding/Linux/Monitor/","title":"Monitor","text":""},{"location":"Coding/Linux/Monitor/#top","title":"top","text":"<p>Press \"1\" in the <code>top</code> command to show individual CPU statistics.</p>"},{"location":"Coding/Linux/Monitor/#vmstat","title":"vmstat","text":"<p>https://phoenixnap.com/kb/vmstat-command#:~:text=The%20vmstat%20command%20(short%20for,by%20specifying%20a%20sampling%20period. <pre><code>vmstat [options][delay [count]]\nvmstat 1        #output every one second\nvmstat 1 5      #output every one second till 5 records\nvmstat 1 5 -S M #MBi\nvmstat -a       #active and inactive system memory\nvmstat -s -S M  #memory and scheduling statistics\n</code></pre></p>"},{"location":"Coding/Linux/Monitor/#strace","title":"strace","text":"<p>checks filesystem attributes <pre><code>strace -c -- /bin/sh -c 'time  (let SUM=0; for i in $(seq 1 1000) ; do SUM+=$i ; stat / &gt; /dev/null; done)'\n</code></pre></p>"},{"location":"Coding/Linux/Network/","title":"Network","text":""},{"location":"Coding/Linux/Network/#iproute2","title":"iproute2","text":"<p><code>iproute2</code> is the Linux networking toolkit that replaced net-tools (ifconfig, route, arp etc.).</p> <p><code>iproute2</code> is a collection of command-line utilities for managing and monitoring various aspects of networking in the Linux kernel. Common iproute2 utilities: - ip: The primary command for most networking tasks like configuring interfaces, assigning IP addresses, and managing routes. - iw: Manages wireless network interfaces. - ss: Similar to netstat but offers more detailed information on network connections. - tc: Used for traffic control and shaping network traffic. - bridge: Manages bridge networking devices.</p> <p>Note that <code>iproute2</code> is more focused on network configuration and management. For DNS-specific tasks, tools like <code>dig</code>, <code>nslookup</code>, or <code>host</code> are more appropriate.</p>"},{"location":"Coding/Linux/Network/#find-a-free-tcp-port","title":"find a free TCP port","text":"<p><pre><code>ss -ltn\n</code></pre> -l: listening ports only\\ -t: TCP ports only\\ -n: See port numbers, instead of port names like \"http\" or \"epmap\"</p>"},{"location":"Coding/Linux/Network/#check-what-uses-a-port","title":"check what uses a port","text":"<pre><code>sudo lsof -i tcp:8080\nsudo kill -9 PID #kill the process based on PID\n</code></pre>"},{"location":"Coding/Linux/Network/#nslookup","title":"nslookup","text":"<p>NSLOOKUP stands for \"Name Server Lookup,\" used to query the Domain Name System (DNS) to obtain information about domain names, IP addresses, and other DNS records.</p> <p><code>nslookup www.example.com</code> will return the domain name and ip address.</p>"},{"location":"Coding/Linux/Network/#dig","title":"dig","text":"<p>used to find the domain name of provided ip address <pre><code>dig -x 10.20.30.40\n</code></pre></p>"},{"location":"Coding/Linux/Network/#check-network-connection-in-docker-pod-without-curl-ping-etc","title":"check network connection in docker pod without <code>curl</code>, <code>ping</code> etc","text":"<p>can only work under bash <pre><code>host=1.2.3.4\nport=10\necho &gt;/dev/tcp/${host}/${port}\n(echo &gt;/dev/tcp/${host}/${port}) &amp;&gt;/dev/null &amp;&amp; echo \"open\" || echo \"closed\"\n</code></pre></p>"},{"location":"Coding/Linux/Network/#connection-refused","title":"connection refused","text":"<p>https://stackoverflow.com/questions/2333400/what-can-be-the-reasons-of-connection-refused-errors</p> <p>reasons: - The port is not open on the destination machine. - The port is open on the destination machine, but its backlog of pending connections is full. - A firewall between the client and server is blocking access (also check local firewalls).</p> <p>After checking for firewalls and that the port is open, use telnet to connect to the ip/port to test connectivity. This removes any potential issues from your application.</p>"},{"location":"Coding/Linux/Network/#list-firewall-rules","title":"list firewall rules","text":"<pre><code>sudo iptables -L\n</code></pre>"},{"location":"Coding/Linux/Perf/","title":"Perf","text":"<p>https://www.brendangregg.com/perf.html</p>"},{"location":"Coding/Linux/Perf/#process-stalls","title":"process stalls","text":"<p>https://stackoverflow.com/questions/223352/determining-the-reason-for-a-stalled-process-on-linux</p>"},{"location":"Coding/Linux/PowerShell/","title":"PowerShell","text":""},{"location":"Coding/Linux/PowerShell/#show-env","title":"Show Env","text":"<pre><code>Get-ChildItem Env:\n</code></pre>"},{"location":"Coding/Linux/PowerShell/#profile-location","title":"Profile location","text":"<p>Shows all profile file locations, whether or not the individual profile files exist. <pre><code>$profile | select *  #short for: $PROFILE | Select-Object -Property *\n\nAllUsersAllHosts       : /opt/microsoft/powershell/7/profile.ps1\nAllUsersCurrentHost    : /opt/microsoft/powershell/7/Microsoft.PowerShell_profile.ps1\nCurrentUserAllHosts    : /home/usr/.config/powershell/profile.ps1\nCurrentUserCurrentHost : /home/usr/.config/powershell/Microsoft.PowerShell_profile.ps1\nLength                 : 61\n</code></pre></p>"},{"location":"Coding/Linux/PowerShell/#creating-a-powershell-profile","title":"Creating a PowerShell profile","text":"<p>caveat: the <code>PATH</code> must be all upper case. <pre><code>pwsh\ncode $profile\n#add sqlcmd/bcp path and restart terminal\n$env:PATH += \":/opt/mssql-tools/bin\"\n\n#for bash\necho 'export PATH=\"$PATH:/opt/mssql-tools/bin\"' &gt;&gt; ~/.bash_profile #login sessions\necho 'export PATH=\"$PATH:/opt/mssql-tools/bin\"' &gt;&gt; ~/.bashrc       #interactive sessions\n\n#check\nsqlcmd -?\n</code></pre></p>"},{"location":"Coding/Linux/Process/","title":"Process","text":"<p>https://netflixtechblog.com/debugging-a-fuse-deadlock-in-the-linux-kernel-c75cd7989b6d</p>"},{"location":"Coding/Linux/Process/#ps","title":"ps","text":"<p><code>ps</code> stands for <code>process status</code> and is a command used to view information about currently running processes on a Unix-based operating system.  It allows you to see a list of processes along with their associated details, such as process IDs (PIDs), CPU and memory usage, execution status, parent process IDs (PPIDs), and more.</p> <pre><code>ps\nps aux    #user friendly format\nps awwfux #show the process tree\n</code></pre>"},{"location":"Coding/Linux/Process/#show-process-stack","title":"show process stack","text":"<pre><code>cat /proc/&lt;pid&gt;/stack\n</code></pre>"},{"location":"Coding/Linux/Process/#list-process-threads-pids","title":"list process threads (pids)","text":"<pre><code>ls /proc/&lt;pid&gt;/task\n</code></pre>"},{"location":"Coding/Linux/System/","title":"sys command","text":""},{"location":"Coding/Linux/System/#update-time","title":"update time","text":"<pre><code>sudo ntpd ntp.ubuntu.com\nsudo ntpdate ntp.ubuntu.com\n</code></pre>"},{"location":"Coding/Linux/System/#ram","title":"RAM","text":"<pre><code>free\nfree -m[-g]\ncat /proc/meminfo\ntop #check memory and CPU usage per process, kill it by Ctrl + c\ntop -b -n 1 &gt; top.txt\nhtop #similar to top, more control with color, kill it by Ctrl + c\n</code></pre>"},{"location":"Coding/Linux/System/#disk-space","title":"disk space","text":"<pre><code>df -h\ndf -h /dev/sda\n</code></pre>"},{"location":"Coding/Linux/System/#w","title":"w","text":"<p>show who is logged on and what they are doing <pre><code>w [options] user [...]\n-h no header\n-x\n</code></pre></p>"},{"location":"Coding/Linux/System/#services-started-up-in-runlevel-3","title":"services started up in runlevel 3","text":"<pre><code>ls /etc/rc3.d/S*\n</code></pre>"},{"location":"Coding/Linux/System/#folder-permission","title":"folder permission","text":"<pre><code>chmod 700 yourDirectory/\nls -a  #show all\nls -1  #one line per file\nls -t  #sort latest changes to top\nls -rt #sort latest to bottom\n</code></pre>"},{"location":"Coding/Linux/Ubuntu/","title":"Ubuntu","text":""},{"location":"Coding/Linux/Ubuntu/#install-libssl11-latest-in-ubuntu-2204","title":"install libssl1.1 latest in ubuntu 22.04","text":"<ul> <li>https://github.com/microsoft/azure-pipelines-agent/blob/master/src/Misc/layoutbin/installdependencies.sh</li> <li>https://stackoverflow.com/questions/72133316/libssl-so-1-1-cannot-open-shared-object-file-no-such-file-or-directory</li> </ul> <p>use wget <pre><code>package=$(wget -qO- http://security.ubuntu.com/ubuntu/pool/main/o/openssl/ | grep -oP '(libssl1.1_1.1.1f.*?_amd64.deb)' | head -1)\nwget \"http://security.ubuntu.com/ubuntu/pool/main/o/openssl/${package}\" &amp;&amp; dpkg -i $package\n</code></pre></p> <p>use curl <pre><code>package=$(curl -sL http://security.ubuntu.com/ubuntu/pool/main/o/openssl/ | grep -oP '(libssl1.1_1.1.1f.*?_amd64.deb)' | head -1)\ncurl -LO \"http://security.ubuntu.com/ubuntu/pool/main/o/openssl/${package}\" &amp;&amp; dpkg -i $package\n</code></pre></p>"},{"location":"Coding/Linux/Ubuntu/#change-mirror-site-in-sourceslist","title":"Change mirror site in sources.list","text":"<pre><code>sudo vi /etc/apt/sources.list\n#replace http://archive.ubuntu.com/ubuntu with another mirror\n\n#http://au.archive.ubuntu.com/ubuntu\n\n:%s/search_string/replacement_string/g\n:%s#search_string#replacement_string#g\n</code></pre>"},{"location":"Coding/Linux/Ubuntu/#gnome-top-bar-to-left","title":"gnome top bar to left","text":"<p>https://askubuntu.com/questions/1162884/can-i-move-the-gnome-top-bar-and-status-icons-to-the-side</p>"},{"location":"Coding/Linux/Ubuntu/#disable-shutdown-confirmation","title":"disable shutdown confirmation","text":"<pre><code>gsettings set org.gnome.SessionManager logout-prompt false\n</code></pre>"},{"location":"Coding/Linux/Ubuntu/#change-font-size","title":"change font size","text":"<pre><code>gsettings set org.gnome.desktop.interface text-scaling-factor 1.05\n</code></pre>"},{"location":"Coding/Linux/Ubuntu/#enable-hibernate","title":"Enable hibernate","text":"<p><pre><code>systemctl hibernate\nFailed to hibernate system via logind: Sleep verb not supported\n</code></pre> Perhaps the error should be: Failed to hibernate system via logind: Please use BIOS to <code>disable secure boot</code>.</p> <p>Disable swapfile and delete it <pre><code>sudo swapoff /swapfile\nsudo rm /swapfile\n</code></pre> https://ubuntuhandbook.org/index.php/2021/08/enable-hibernate-ubuntu-21-10/</p>"},{"location":"Coding/Linux/Ubuntu/#auto-hibernate-after-timeout","title":"Auto hibernate after timeout","text":"<p>https://unix.stackexchange.com/questions/645535/shutdown-system-by-timeout-during-suspend</p> <p>https://ubuntuhandbook.org/index.php/2021/06/automatic-shutdown-hibernate-on-idle-ubuntu/</p> <p>Shortcut key does not work but works in terminal with pwd <pre><code>dbus-send --system --print-reply --dest=org.freedesktop.login1 /org/freedesktop/login1 org.freedesktop.login1.Manager.Hibernate boolean:true\ndbus-send --system --print-reply --type=method_call --dest=org.freedesktop.login1 /org/freedesktop/login1 org.freedesktop.login1.Manager.Hibernate boolean:true\n</code></pre></p> <p>Set the timeout value: - https://wiki.archlinux.org/title/Power_management#Suspend_and_hibernate - https://man.archlinux.org/man/sleep.conf.d.5 <pre><code>/etc/systemd/sleep.conf\n</code></pre></p>"},{"location":"Coding/Linux/Ubuntu/#2404-hibernation-issues","title":"24.04 hibernation issues","text":"<p>After hibernation, the laptop will immediately wake up to the login screen.</p> <p>Solution: - after hibernation failed, quickly check log: <pre><code>journalctl --since \"3 minutes ago\" | grep -i \"hiber\"\njournalctl -p err -b | grep -i \"power|hiber|swap\"\njournalctl -p crit -b | grep -i \"power|hiber|swap\"\n</code></pre> - found the message <code>Wakeup event detected during hibernation, rolling back</code> - based on the message search online and found: https://bugs.launchpad.net/ubuntu/+source/systemd/+bug/2057687 - there are many worked solutions. #18 solved my issue as well. - #18: add the line <code>blacklist intel_hid</code> to <code>/etc/modprobe.d/blacklist.conf</code> and then run <pre><code>sudo update-grub\nsudo update-initramfs -c -k all\n</code></pre> - for similar discussion see also: https://bugzilla.kernel.org/show_bug.cgi?id=218634</p>"},{"location":"Coding/Linux/Upgrade/","title":"Upgrade","text":""},{"location":"Coding/Linux/Upgrade/#upgrade-failed","title":"Upgrade failed","text":"<p>https://koen.vervloesem.eu/blog/fixing-a-failed-upgrade-to-ubuntu-2204-lts-in-recovery-mode/ <pre><code>sudo apt update\nsudo apt upgrade\nsudo do-release-upgrade\n</code></pre></p>"},{"location":"Coding/Linux/Upgrade/#issues-after-upgrade","title":"issues after upgrade","text":"<p>Ctl + Alt + F3 to open terminal, then <pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade -y\n# if any errors follow instructions to fix it and run previous command again\nsudo reboot now # reboot system\n</code></pre></p>"},{"location":"Coding/Logging/Seq/","title":"Seq","text":""},{"location":"Coding/Logging/Seq/#signal","title":"signal","text":"<p>https://docs.datalust.co/docs/signals</p>"},{"location":"Coding/Logging/Splunk/","title":"Splunk","text":"<p>More powerful than seq but also has a steeper learning curve</p>"},{"location":"Coding/Markdown/Markdown/","title":"Markdown","text":""},{"location":"Coding/Markdown/Markdown/#markdown-preview-github-styling","title":"Markdown Preview Github Styling","text":"<p>https://github.com/mjbvz/vscode-github-markdown-preview-style?tab=readme-ov-file - switch between views: <code>Ctrl+Shift+V</code> - view the preview side-by-side: <code>Ctrl+K V</code></p>"},{"location":"Coding/Matlab/Matlab/","title":"Matlab","text":""},{"location":"Coding/Matlab/Matlab/#call-matlab-func-from-c-only-with-matlab-runtime","title":"Call Matlab func from C# only with Matlab Runtime","text":"<ul> <li>Make .NET Assembly</li> <li>Include MCR into package</li> <li>Silently install MCR</li> <li>Install C# Application</li> <li>Run Application</li> </ul>"},{"location":"Coding/Matlab/Matlab/#issues","title":"Issues","text":"<p>In C# when adding 'mlapp.tlb', get the error: A reference to 'C:\\Program Files\\MATLAB\\R2008a\\bin\\win32\\mlapp.tlb' could not be added. Please make sure that the file is accessible, and that it is a valid assembly or COM component.</p>"},{"location":"Coding/Matlab/Matlab/#solution","title":"Solution","text":"<p>The reference error is occuring because MATLAB is not registered as a COM server. To work around this issue, register MATLAB as a COM server. - Open a Command Prompt window, - using cd, change to the bin\\win32/64 subdirectory of the MATLAB installation directory - and run the following command at the DOS prompt: matlab /regserver</p>"},{"location":"Coding/Network/Network/","title":"Network","text":""},{"location":"Coding/Network/Network/#check-internet","title":"check internet","text":"<p>try https://xxx.xx.x.x and see if they get a webpage? if you do it's almost certainly an internet issue</p>"},{"location":"Coding/Network/Network/#check-connections","title":"check connections","text":"<pre><code>netstat -a # to show current connections\n</code></pre>"},{"location":"Coding/Network/Network/#when-not-use-ping","title":"when not use <code>ping</code>","text":"<p>The <code>ping</code> uses ICMP (Internet Control Message Protocol) packets, which may be blocked by network firewalls or restricted by network policies.</p> <p>For example, an service deployed in aks might only posed the http port. In this case, we should try one of the following: - <code>curl http://&lt;service-ip&gt;:&lt;port&gt;</code> - <code>telnet &lt;service-ip&gt; &lt;port&gt;</code> - <code>nc -vz &lt;service-ip&gt; &lt;port&gt;</code> All failed, why?</p>"},{"location":"Coding/PHP/PHP/","title":"php","text":"<p>the NON Thread safe (nts) php does not support apache and you have to install thread safe versions</p>"},{"location":"Coding/PHP/PHP/#install","title":"install","text":"<p>php.ini-development to php.ini and change:   display_errors = on   log_errors = on   error_log = c:\\log\\php_errors.log   SMTP = mail.svr.com   extension=curl   extension=gd2   extension=mbstring   extension=pdo_mysql</p> <p>print_r() displays information about a variable in a way that's readable by humans.</p> <p>var_dump() displays structured information about one or more expressions that includes its type and value.</p> <pre><code>//to error_log\nerror_log( print_r( $object, true ) );\n\nfunction error_log_var_dump( $object=null ){\n    ob_start();                    // start buffer capture\n    var_dump( $object );           // dump the values\n    $contents = ob_get_contents(); // put the buffer into a variable\n    ob_end_clean();                // end capture\n    error_log( $contents );        // log contents of the result of var_dump( $object )\n}\n\nerror_log_var_dump( $object );\n</code></pre>"},{"location":"Coding/PHP/PHP/#install-package","title":"install package","text":"<p>npm i --save puppeteer</p> <p>install to folder: \"C:\\Program Files\\nodejs\\node_modules\" npm install --prefix \"C:\\Program Files\\nodejs\" puppeteer --save</p> <p>get data from web js in php:\\ https://developers.google.com/web/tools/puppeteer/articles/ssr</p> <p>puppeteer-and-chrome-headless:\\ https://medium.com/@e_mad_ehsan/getting-started-with-puppeteer-and-chrome-headless-for-web-scrapping-6bf5979dee3e</p> <p>https://dev.to/alanmbarr/scraping-html-with-php-node-and-puppeteer-10m2</p>"},{"location":"Coding/PHP/PHP/#debug-in-vs-code","title":"debug in VS code","text":"<ul> <li>download xdebug based on php version: https://xdebug.org/download</li> <li>copy the dll to: C:\\php\\ext</li> <li>update php.ini, add: <pre><code>[XDebug]\nzend_extension=\"C:\\php\\ext\\php_xdebug-3.0.4-7.4-vc15-x86_64.dll\"\n;xdebug.remote_enable = 1      depreciated\n;xdebug.remote_autostart = 1   depreciated\nxdebug.mode=debug\nxdebug.start_with_request=yes\n</code></pre></li> <li>in VS code, install extension PHP Debug</li> <li>in VS code, debug -&gt; add config file</li> <li>run in debug mode</li> </ul>"},{"location":"Coding/PHP/PHP/#get-previous-day","title":"get previous day","text":"<p>$dt = date(\"Y-m-d H:i:s\", strtotime(\"-1 day\")); //$dt is a string</p>"},{"location":"Coding/PHP/PHP/#set-local-log-date","title":"set local log date","text":"<p>date_default_timezone_set('Australia/Brisbane');</p>"},{"location":"Coding/PHP/PHP/#log4phpxml","title":"log4php.xml","text":"<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;log4php:configuration xmlns:log4php=\"http://logging.apache.org/log4php/\"&gt;\n    &lt;appender name=\"myConsoleAppender\" class=\"LoggerAppenderConsole\"&gt;\n        &lt;layout class=\"LoggerLayoutPattern\"&gt;\n            &lt;param name=\"conversionPattern\" value=\"%-5level - %message%n\" /&gt;\n        &lt;/layout&gt;\n    &lt;/appender&gt;\n\n    &lt;appender name=\"myFileAppender\" class=\"LoggerAppenderFile\" threshold=\"WARN\"&gt;\n        &lt;layout class=\"LoggerLayoutPattern\"&gt;\n            &lt;param name=\"conversionPattern\" value=\"%date{Y-m-d H:i:s} %-5level - Line: %-5line File: %file%n  %-25logger %message%n\" /&gt;\n        &lt;/layout&gt;\n        &lt;param name=\"file\" value=\"myphp.log\" /&gt;\n    &lt;/appender&gt;\n\n    &lt;appender name=\"myDBAppender\" class=\"LoggerAppenderPDO\" threshold=\"WARN\"&gt;\n        &lt;layout class=\"LoggerLayoutPattern\"&gt;\n            &lt;param name=\"conversionPattern\" value=\"%date{Y-m-d H:i:s.u},%logger,%level,%message,%pid,%file,%line\" /&gt;\n        &lt;/layout&gt;\n        &lt;param name=\"dsn\" value=\"mysql:host=localhost;dbname=mydb;port=3306\"/&gt;\n        &lt;param name=\"user\" value=\"usr\" /&gt;\n        &lt;param name=\"password\" value=\"pwd\" /&gt;\n        &lt;param name=\"table\" value=\"log_warn\" /&gt;\n        &lt;param name=\"insertPattern\" value=\"%date{Y-m-d H:i:s.u},%logger,%level,%message,%pid,%file,%line\" /&gt;\n    &lt;/appender&gt;\n\n    &lt;root&gt;\n        &lt;level value=\"DEBUG\" /&gt;\n        &lt;appender_ref ref=\"myFileAppender\" /&gt;\n        &lt;appender_ref ref=\"myConsoleAppender\" /&gt;\n        &lt;appender_ref ref=\"myDBAppender\" /&gt;\n    &lt;/root&gt;\n&lt;/log4php:configuration&gt;\n</code></pre>"},{"location":"Coding/PHP/PHP/#zip","title":"zip","text":"<pre><code>function OpenCsvInZip($zipfilename) {\n    $zip = new ZipArchive;\n    if ($zip-&gt;open($zipfilename)) {\n        if ($zip-&gt;numFiles &gt; 0) {\n            $filename = $zip-&gt;getNameIndex(0);\n            $fp = $zip-&gt;getStream($filename);\n            if ($fp) {\n                return $fp;\n            }\n        }\n    }\n    return null;\n}\n</code></pre>"},{"location":"Coding/PHP/basic/","title":"basic","text":"<pre><code>//join string array\n$str = implode(',', $arr);\n\n//split str to arr\n$arr = explode(',', $row);\n</code></pre>"},{"location":"Coding/PHP/datetime/","title":"DateTime","text":""},{"location":"Coding/PHP/datetime/#shift-date-i","title":"shift date (I)","text":"<pre><code>$today = date(\"Y-m-d H:i:s\", strtotime(\"now\"));    //$today is a string\n$yestd = date(\"Y-m-d H:i:s\", strtotime(\"-1 day\")); //$yestd is a string\n$tomrw = date(\"Y-m-d H:i:s\", strtotime(\"1 day\"));  //$tomrw is a string\n</code></pre>"},{"location":"Coding/PHP/datetime/#shift-date-ii","title":"shift date (II)","text":"<pre><code>$tday = new DateTime('now');\necho \"today is: {$tday-&gt;format('Y-m-d H:i:s')}\\n\";\n\n$tday-&gt;sub(new DateInterval('P1D'));\necho \"yestd is: {$tday-&gt;format('Y-m-d H:i:s')}\\n\";\n\n$tday-&gt;add(new DateInterval('P2D'));\necho \"tomrw is: {$tday-&gt;format('Y-m-d H:i:s')}\\n\";\n</code></pre>"},{"location":"Coding/PHP/io/","title":"IO","text":""},{"location":"Coding/PHP/io/#read-csv-in-zip-file","title":"read csv in zip file","text":"<p>Disadvantages: if the zip file is huge, the memory uage is also huge.</p> <pre><code>function OpenCsvInZip($zipfilename) {\n    $zip = new ZipArchive;\n    if ($zip-&gt;open($zipfilename)) {\n        if ($zip-&gt;numFiles &gt; 0) {\n            $filename = $zip-&gt;getNameIndex(0);\n            $fp = $zip-&gt;getStream($filename);\n            if ($fp) {\n                return $fp;\n            }\n        }\n    }\n    return null;\n}\n</code></pre>"},{"location":"Coding/PHP/mysql/","title":"MySQL","text":"<p>In MySQL query returned results in PHP, every field will be a string OR NULL - be careful about the integer and float.</p>"},{"location":"Coding/PHP/mysql/#run-query","title":"run query","text":"<pre><code>$log = Logger::getLogger('MyLogger');\n$db = SqlCnnFactory::getFactory()-&gt;getConnection();\n\n$result = $db-&gt;query($qry);\nif (!$result) {\n    $log-&gt;fatal(\"SQL query error: \" . mysqli_error($db));\n    $log-&gt;info(\"SQL query error. Query: \" . substr($qry, 0, min(1000, strlen($qry))));\n}\n\nclass SqlCnnFactory {\n    private static $factory;\n    public static function getFactory() {\n        if (!self::$factory)\n            self::$factory = new SqlCnnFactory();\n        return self::$factory;\n    }\n\n    private static $db;\n    public function getConnection(?string $dbname = null) {\n        if (!self::$db) {\n            self::$db = mysqli_connect(DB_HOSTNAME, DB_USERNAME, DB_PASSWORD, $dbname, DB_PORT);\n            if (mysqli_connect_errno()) {\n                $log-&gt;fatal(\"Connect failed: \" . mysqli_connect_error());\n                exit;\n            }\n        }\n        return self::$db;\n    }\n}\n</code></pre>"},{"location":"Coding/PHP/regex/","title":"Regular Expression","text":"<pre><code>    $myMatch = array();\n    $myPattern = \"/_[0-9]+/\";\n    preg_match($myPattern, $filename, $myMatch);\n    if (count($myMatch) &gt; 0) {\n        $year = substr($myMatch[0], 1, 4);\n    echo \"Year: \" . $year;\n    }\n</code></pre>"},{"location":"Coding/PHP/setup/","title":"setup","text":"<p>the NON Thread safe (nts) php does not support apache and you have to install thread safe versions</p>"},{"location":"Coding/PHP/setup/#install","title":"install","text":"<p>php.ini-development to php.ini and change:   display_errors = on   log_errors = on   error_log = c:\\log\\php_errors.log   SMTP = mail.svr.com   extension=curl   extension=gd2   extension=mbstring   extension=pdo_mysql</p> <p>print_r() displays information about a variable in a way that's readable by humans. </p> <p>var_dump() displays structured information about one or more expressions that includes its type and value.</p> <pre><code>//to error_log\nerror_log( print_r( $object, true ) );\n\nfunction error_log_var_dump( $object=null ){\n    ob_start();                    // start buffer capture\n    var_dump( $object );           // dump the values\n    $contents = ob_get_contents(); // put the buffer into a variable\n    ob_end_clean();                // end capture\n    error_log( $contents );        // log contents of the result of var_dump( $object )\n}\n\nerror_log_var_dump( $object );\n</code></pre>"},{"location":"Coding/PHP/setup/#install-package","title":"install package","text":"<p>npm i --save puppeteer</p> <p>install to folder: \"C:\\Program Files\\nodejs\\node_modules\" npm install --prefix \"C:\\Program Files\\nodejs\" puppeteer --save</p> <p>get data from web js in php:\\ https://developers.google.com/web/tools/puppeteer/articles/ssr</p> <p>puppeteer-and-chrome-headless:\\ https://medium.com/@e_mad_ehsan/getting-started-with-puppeteer-and-chrome-headless-for-web-scrapping-6bf5979dee3e</p> <p>https://dev.to/alanmbarr/scraping-html-with-php-node-and-puppeteer-10m2</p>"},{"location":"Coding/PHP/setup/#debug-in-vs-code","title":"debug in VS code","text":"<ul> <li>download xdebug based on php version: https://xdebug.org/download</li> <li>copy the dll to: C:\\php\\ext</li> <li>update php.ini, add:  <pre><code>[XDebug]\nzend_extension=\"C:\\php\\ext\\php_xdebug-3.0.4-7.4-vc15-x86_64.dll\"\n;xdebug.remote_enable = 1      depreciated\n;xdebug.remote_autostart = 1   depreciated\nxdebug.mode=debug\nxdebug.start_with_request=yes \n</code></pre></li> <li>in VS code, install extension PHP Debug</li> <li>in VS code, debug -&gt; add config file</li> <li>run in debug mode</li> </ul>"},{"location":"Coding/PHP/setup/#set-local-log-date","title":"set local log date","text":"<p>date_default_timezone_set('Australia/Brisbane');</p>"},{"location":"Coding/PHP/setup/#set-memory-limit","title":"set memory limit","text":"<p>ini_set('memory_limit', '-1');</p>"},{"location":"Coding/PHP/setup/#log4phpxml","title":"log4php.xml","text":"<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;log4php:configuration xmlns:log4php=\"http://logging.apache.org/log4php/\"&gt;\n    &lt;appender name=\"myConsoleAppender\" class=\"LoggerAppenderConsole\"&gt;\n        &lt;layout class=\"LoggerLayoutPattern\"&gt;\n            &lt;param name=\"conversionPattern\" value=\"%-5level - %message%n\" /&gt;\n        &lt;/layout&gt;              \n    &lt;/appender&gt;    \n\n    &lt;appender name=\"myFileAppender\" class=\"LoggerAppenderFile\" threshold=\"WARN\"&gt;      \n        &lt;layout class=\"LoggerLayoutPattern\"&gt;\n            &lt;param name=\"conversionPattern\" value=\"%date{Y-m-d H:i:s} %-5level - Line: %-5line File: %file%n  %-25logger %message%n\" /&gt;\n        &lt;/layout&gt;\n        &lt;param name=\"file\" value=\"myphp.log\" /&gt;                 \n    &lt;/appender&gt;\n\n    &lt;appender name=\"myDBAppender\" class=\"LoggerAppenderPDO\" threshold=\"WARN\"&gt;\n        &lt;layout class=\"LoggerLayoutPattern\"&gt;\n            &lt;param name=\"conversionPattern\" value=\"%date{Y-m-d H:i:s.u},%logger,%level,%message,%pid,%file,%line\" /&gt;\n        &lt;/layout&gt;\n        &lt;param name=\"dsn\" value=\"mysql:host=localhost;dbname=mydb;port=3306\"/&gt;\n        &lt;param name=\"user\" value=\"usr\" /&gt;\n        &lt;param name=\"password\" value=\"pwd\" /&gt;\n        &lt;param name=\"table\" value=\"log_warn\" /&gt;\n        &lt;param name=\"insertPattern\" value=\"%date{Y-m-d H:i:s.u},%logger,%level,%message,%pid,%file,%line\" /&gt;\n    &lt;/appender&gt;\n\n    &lt;root&gt;\n        &lt;level value=\"DEBUG\" /&gt;                                \n        &lt;appender_ref ref=\"myFileAppender\" /&gt;                        \n        &lt;appender_ref ref=\"myConsoleAppender\" /&gt;  \n        &lt;appender_ref ref=\"myDBAppender\" /&gt;  \n    &lt;/root&gt;\n&lt;/log4php:configuration&gt;\n</code></pre>"},{"location":"Coding/PassManager/BitWarden/","title":"BitWarden","text":"<p>https://help.passageway.id/article/cli/</p>"},{"location":"Coding/PassManager/BitWarden/#cli-login","title":"cli login","text":"<pre><code>bw login &lt;email&gt;                       #will unlock automatically\nbw logout                              #logout\nexport BW_SESSION=\"$(bw unlock --raw)\" #generate and save session key to env\nbw lock                                #destroy the saved session key\n</code></pre>"},{"location":"Coding/PassManager/BitWarden/#create-folder","title":"create folder","text":"<pre><code>bw get template folder | jq --arg name \"$1\" '.name=$name' | bw encode | bw create folder;\n</code></pre>"},{"location":"Coding/PassManager/BitWarden/#create-item","title":"create item","text":"<p>create item with password from file and name/username from inputs <pre><code>read -r val &lt; \"$HOME/bw.txt\" &amp;&amp; bw get template item | jq --arg name \"$1\" --arg username \"$2\" --arg password \"$val\" '.name = $name | .login.username = $username | .login.password = $password' | bw encode | bw create item &gt; /dev/null\n</code></pre></p>"},{"location":"Coding/PassManager/BitWarden/#list-items-with-selected-columns","title":"list items with selected columns","text":"<pre><code>bw list items #json output\nbw list items | jq -r '.[] | [.id, .name, .login.username, .login.password] | @tsv' | column -t -s $'\\t' #convert json to table, last ensures col vals being aligned\n</code></pre>"},{"location":"Coding/PassManager/BitWarden/#list-username","title":"list username","text":"<pre><code>bw get item \"$1\" | jq -r '.login.username' &gt; bw.txt\n</code></pre>"},{"location":"Coding/PassManager/BitWarden/#get-folder-id","title":"get folder id","text":"<pre><code>fid=$(bw list folders | jq --arg name \"$1\" -r '.[] | select(.name==$name) | .id') &amp;&amp; echo $fid;\n</code></pre>"},{"location":"Coding/PassManager/BitWarden/#get-item-id","title":"get item id","text":"<pre><code>iid=$(fid=$(bwfid \"$1\") &amp;&amp; bw list items --folderid \"$fid\" | jq --arg name \"$2\" -r '.[] | select(.name==$name) | .id') &amp;&amp; echo $iid;\n</code></pre>"},{"location":"Coding/PassManager/BitWarden/#create-item_1","title":"create item","text":"<pre><code>read -r val &lt; \"bw.txt\" &amp;&amp; fid=$(get_folder_id \"$1\") &amp;&amp; bw get template item | jq --arg folderid \"$fid\" --arg name \"$2\" --arg username \"$3\" --arg password \"$val\" '.folderId = $folderid | .name = $name | .login.username = $username | .login.password = $password' | bw encode | bw create item &gt; /dev/null;\n</code></pre>"},{"location":"Coding/PassManager/BitWarden/#list-folder","title":"list folder","text":"<pre><code>bw list folders | jq -r '.[] | [.id, .name] | @tsv' | column -t -s $'\\t';\n</code></pre>"},{"location":"Coding/PassManager/BitWarden/#list-item","title":"list item","text":"<pre><code>fid=$(get_folder_id \"$1\") &amp;&amp; bw list items --folderid \"$fid\" | jq -r '.[] | [.id, .name, .creationDate] | @tsv' | column -t -s $'\\t';\n</code></pre>"},{"location":"Coding/PassManager/BitWarden/#get-item-username","title":"get item username","text":"<pre><code>bw get item \"$1\" | jq -r '.login.username' &gt; ~/bw.txt;\n</code></pre>"},{"location":"Coding/PassManager/BitWarden/#edit-item","title":"edit item","text":"<pre><code>read -r val &lt; \"bw.txt\" &amp;&amp; bw get item \"$1\" | jq --arg password \"$val\" '.login.password=$password' | bw encode | bw edit item \"$1\";\n</code></pre>"},{"location":"Coding/PowerShell/Azure/","title":"Azure","text":""},{"location":"Coding/PowerShell/Azure/#active-directory-i-am-in","title":"Active directory I am in","text":"<pre><code>$user = \"my.name@example.com\"\nConnect-AzureAD\nGet-AzureADUserMembership -ObjectId $user | Select DisplayName, ObjectId\n</code></pre>"},{"location":"Coding/PowerShell/Basic/","title":"Basic","text":""},{"location":"Coding/PowerShell/Basic/#run","title":"run","text":"<p><pre><code>./my.ps1\n</code></pre> F7 will display the command history</p> <p>Tab key to complete the PowerShell cmdlet, function, and parameter/variable names</p> <p>Install ISE on server: <code>Add-WindowsFeature -Name PowerShell-ISE</code></p>"},{"location":"Coding/PowerShell/Basic/#help","title":"help","text":"<p><code>help</code> is a function and <code>man</code> is an alias to help. <code>help</code> and <code>man</code> run <code>Get-Help</code> under the hood, but they pipe its output to more.</p> <p><code>Update-Help</code> to download and install help files.</p> <p>Help support wildcard: <code>help *service*</code> <pre><code>help about*\nhelp *send*mail* -Full #displays the full help\nhelp *send*mail* -Examples #displays usage examples only\nhelp *send*mail* -Detailed #displays details on each command parameter\nhelp *send*mail* -ShowWindow #opens full help in a pop-up window\n</code></pre></p>"},{"location":"Coding/PowerShell/Drive/","title":"Drive","text":""},{"location":"Coding/PowerShell/Drive/#mapping-azure-file-stoarge-to-local-drive","title":"mapping azure file stoarge to local drive","text":"<pre><code>$connectTestResult = Test-NetConnection -ComputerName my.file.core.windows.net -Port 445\nif ($connectTestResult.TcpTestSucceeded) {\n    # Save the password so the drive will persist on reboot\n    cmd.exe /C \"cmdkey /add:`\"my.file.core.windows.net`\" /user:`\"localhost\\my`\" /pass:`\"pass==`\"\"\n    # Mount the drive\n    New-PSDrive -Name Z -PSProvider FileSystem -Root \\\\my.file.core.windows.net\\dropzone -Persist\n} else {\n    Write-Error -Message \"Unable to reach the Azure storage account via port 445. Check to make sure your organization or ISP is not blocking port 445, or use Azure P2S VPN, Azure S2S VPN, or Express Route to tunnel SMB traffic over a different port.\"\n}\n</code></pre> <p>Delete the mapped drive <code>net use Z: /delete</code>.  Note that <code>Remove-PSDrive -Name \"Z\"</code> will not work as <code>-Persist</code> means the mapping is managed by the Windows OS so it cannot be removed by PowerShell.</p>"},{"location":"Coding/PowerShell/Env/","title":"Env","text":""},{"location":"Coding/PowerShell/Env/#print-env-path-line-by-line","title":"Print env path line by line","text":"<p><pre><code>echo $env:path.split(';')\necho ($env:path).split(';') | Sort-Object\n</code></pre> After activate the conda env, then use vscode to open the code, the env path will be included in the powershell env:path.</p>"},{"location":"Coding/PowerShell/Env/#print-env-variables","title":"Print env variables","text":"<pre><code>Get-ChildItem Env:* | Select-Object -Property Name,Value\n</code></pre>"},{"location":"Coding/PowerShell/FileSys/","title":"FileSys","text":""},{"location":"Coding/PowerShell/FileSys/#get-filename-from-path","title":"Get filename from path","text":"<pre><code>$filename = Split-Path $filepath -leaf\n</code></pre>"},{"location":"Coding/PowerShell/FileSys/#join-path-and-filename","title":"Join path and filename","text":"<pre><code>$filepath = Join-Path -Path \"$dir\" -ChildPath \"$filename\"\n</code></pre>"},{"location":"Coding/PowerShell/FileSys/#file-without-extension","title":"File without extension","text":"<pre><code>$filename_without_ext = $file.Substring(0, $file.LastIndexOf('.'))\n</code></pre>"},{"location":"Coding/PowerShell/FileSys/#check-file-exists-or-not","title":"Check file exists or not","text":"<pre><code>if (Test-Path -Path \"$filepath\" -PathType \"Leaf\") {}\n</code></pre>"},{"location":"Coding/PowerShell/FileSys/#list-subfolder-size","title":"List subfolder size","text":"<pre><code>Get-ChildItem -force 'C:\\Users\\sma' -ErrorAction SilentlyContinue | ? { $_ -is [io.directoryinfo] } | % {\n    $len = 0\n    Get-ChildItem -recurse -force $_.fullname -ErrorAction SilentlyContinue | % { $len += $_.length }\n     '{0,10:N3} MB  {1}' -f ($len / 1Mb), $_.fullname\n}\n</code></pre>"},{"location":"Coding/PowerShell/Function/","title":"Function","text":""},{"location":"Coding/PowerShell/Function/#parameter","title":"Parameter","text":"<p>Input parameter has value <pre><code>param (\n    # Input filename\n    [Alias('f')][string]$file\n)\n...\n$PSBoundParameters.ContainsKey('file')\n</code></pre></p>"},{"location":"Coding/PowerShell/Module/","title":"SSnapins and modules","text":"<p>List installed PSSnapins: <code>Get-PSSnapin \u2013Registered</code></p> <p>List all modules: <code>Get-Module -ListAvailable</code></p> <p>Show env variable value: <code>$env:psmodulepath</code></p>"},{"location":"Coding/PowerShell/Module/#check-modules-on-a-remote-machine","title":"check modules on a remote machine","text":"<pre><code>$session = New-PSSession -ComputerName Win8 #create a remote session\nGet-Module -PSSession $session -ListAvailable\n</code></pre>"},{"location":"Coding/PowerShell/Module/#entension","title":"entension","text":"<pre><code>Get-Module #display a list of all loaded modules\nRemove-Module #unload a module\nImport-Module storage #load entension\nGet-Command -Module storage #show entension commands\nActiveDirectory\\Get-ADUser #use extension to avoid conflict\nImport-Module MyModule \u2013Prefix DJR #add a prefix to all command to avoid conflict\n$PSModuleAutoLoadingPreference #manage module auto loading\n</code></pre>"},{"location":"Coding/PowerShell/Object/","title":"Object","text":""},{"location":"Coding/PowerShell/Object/#show-object-members","title":"show object members","text":"<pre><code>Get-Service | Get-Member #alias is gm\n</code></pre>"},{"location":"Coding/PowerShell/Object/#sorting","title":"sorting","text":"<pre><code>get-process | sort-object -Property vm -Descending #get-process | sort vm -desc\n</code></pre>"},{"location":"Coding/PowerShell/Object/#selecting","title":"selecting","text":""},{"location":"Coding/PowerShell/Object/#choosing-properties","title":"choosing properties","text":"<pre><code>Get-Process | Select-Object -Property Name,ID,VM,PM #get-porcess | select name, id, vm, pm\n</code></pre>"},{"location":"Coding/PowerShell/Object/#choosing-a-subset-objects","title":"choosing a subset objects","text":"<pre><code>Get-Process | sort VM -Descending | select -first 5 #select five processes using most virtual memory\nGet-Process | sort PM -Descending | select -last 5  #select five processes using least amount of paged memory\nGet-Process | sort PM -Descending | select -skip 3 -first 5 #skip first 3 first\nmeasure-command {1..5000 | select -first 5 -wait} #will process all\n</code></pre>"},{"location":"Coding/PowerShell/Object/#making-custom-properties","title":"making custom properties","text":"<pre><code>#shorter version: get-process | select name, id, @{n=\"TotMem\";e={$_.PM + $_.VM}}\nGet-Process | Select \u2013Property Name,ID,@{name=\"TotalMemory\";expression={$_.PM + $_.VM}}\nGet-Process | Select -Property Name,ID,@{name=\"TotalMemory(MB)\";expression={($_.PM + $_.VM) / 1MB -as [int]}}\nGet-Process | Select -Property Name,ID,@{name=\"VirtMem\";expression={$psitem.vm}}, @{name=\"PhysMem\";expression={$psitem.pm}}\n</code></pre>"},{"location":"Coding/PowerShell/Object/#extracting-and-expanding-properties","title":"extracting and expanding properties","text":""},{"location":"Coding/PowerShell/Operator/","title":"Operator","text":""},{"location":"Coding/PowerShell/Operator/#logical-and-comparison-operators","title":"Logical and comparison operators","text":"<pre><code>-and, -or, -not, -xor\n-band, -bor, -bxor #bitwise operator\n-eq, -ne, -gt, -lt, -ge, -le, -like, -notlike\n#all string comparisons are case insensitive by default\n-ceq, -cne, -clike #add a \u201cc\u201d after the dash in the operator to enable case-sensitive\n-ieq, -ine, -ilike #add an \u201ci\u201d after the dash in the operator to enable case-insensitive\n\u2013contains, \u2013notcontains, -in,-notin\n</code></pre>"},{"location":"Coding/PowerShell/Operator/#arithmetic-operators","title":"Arithmetic operators","text":"<pre><code>+, -, /, *, %, ++, --, +=, -=, /+, *=\n\n#js like converting\n$s = \"5\"\n$n = 5\n$s + $n #\"55\"\n$n + $s #10\n\"xy \" * 2 #\"xy xy \"\n</code></pre>"},{"location":"Coding/PowerShell/Operator/#string-and-array-manipulation-operators","title":"String and array manipulation operators","text":"<pre><code>\u2013replace #\"SERVER-DC2\" -replace \"DC\",\"FILE\", output:SERVER-FILE2\n\u2013split #\"one,two,three\" -split \",\", 2, output: one\\ntwo,three\n\u2013join\n</code></pre>"},{"location":"Coding/PowerShell/Operator/#object-type-operators","title":"Object type operators","text":"<pre><code>-is, -isnot #\"world\" -is [string], output:True\n\u2013as #attempt to convert an object of one type into another type, \"55.2\" -as [int]\n250MB+1GB #output: 1335885824 bytes\n</code></pre>"},{"location":"Coding/PowerShell/Operator/#format-operator","title":"Format operator","text":"<pre><code>\"Name is {0}; Pi is {1:N}; Today is {2:yyyy-MM-dd}\" -f (Get-Content Env:\\USERNAME), [math]::pi, (Get-Date)\n#output: Name is Admin; Pi is 3.14; Todat is 2021-10-05\n</code></pre>"},{"location":"Coding/PowerShell/Operator/#call-and-subexpression-operators","title":"Call and subexpression operators","text":"<pre><code>$cmd = \"dir\"\n&amp;cmd #run dir command\n$service = Get-Service | select -first 1\n\"Service name is $($service.name)\"\n</code></pre>"},{"location":"Coding/PowerShell/Util/","title":"Util","text":""},{"location":"Coding/PowerShell/Util/#start-exe","title":"start exe","text":"<pre><code>call :StartExe \"%exepath%\", \"%exefile%\", \"%outfile%\", \"%errfile%\"\nexit /b\n\n:StartExe\n    powershell -command \"Start-Process -WorkingDirectory '%~1' -FilePath '%~2' -ArgumentList 'p1 p2' -Wait -RedirectStandardOutput '%~3' -RedirectStandardError '%~4'\"\n    exit /b\n</code></pre>"},{"location":"Coding/PowerShell/Util/#send-email","title":"send email","text":"<pre><code>set subject=Err No1\nset mail_body=Error: CSV file ^(attached^) does not exist.^&lt;br /^&gt;Please check the \\\"issue\\\".\ncall :SendEmail \"!subject!\", \"!mail_body!\", \"%csvfile%\"\n\n:SendEmail\n    set mail_from=err@email.com\n    set mail_to=john@email.com, anna@email.com\n    powershell -command \"Send-MailMessage -BodyAsHtml -From %mail_from% -To %mail_to% -SmtpServer 'mail.svr.com' -Subject '%~1' -Body '%~2' -Attachments '%~3'\"\n    exit /b\n</code></pre>"},{"location":"Coding/R/Debug/","title":"Debug","text":""},{"location":"Coding/R/Debug/#create-break-point","title":"Create break point","text":"<ul> <li><code>Shift + F9</code> or </li> <li>Click the <code>line number</code></li> </ul>"},{"location":"Coding/R/Debug/#debug-run","title":"Debug run","text":"<ul> <li>Click <code>source</code> or</li> <li>Press <code>Ctrl + Shift + Enter</code> or</li> <li>Activate <code>source on save</code> and save</li> </ul>"},{"location":"Coding/R/Debug/#debug-options","title":"Debug options","text":"<ul> <li><code>F10</code>        next line</li> <li><code>Shift + F4</code> step into</li> <li><code>Shift + F5</code> continue</li> <li><code>Shift + F7</code> step over</li> <li><code>Shift + F8</code> finish</li> </ul>"},{"location":"Coding/R/Debug/#check-var-value","title":"Check var value","text":"<p>In terminal <code>View(var_name)</code></p>"},{"location":"Coding/R/MySql/","title":"MySQL","text":"<pre><code>require(RMySQL)\n\ncon = dbConnect(MySQL(), host=svr, port=3307, \n                user=usr, pass=pwd, dbname=dbname)\n\ntbs = dbListTables(con)\npath = \"C:/dat/\"\n\nfor (i in 1 : length(tbs)) {\n  tbl = tbs[i]\n  fls = dbListFields(con, tbl)\n  hasDt = (\"LASTUPDATE\" %in% fls)\n  dtName = ifelse(hasDt, \"LASTUPDATE\", \"SettleTime\")\n\n  qryTxt = paste(\"SELECT * FROM\", tbl,\n                 \"WHERE\", dtName, \"&gt;= '2013-01-01'\", \n                 \"ORDER BY\", dtName, \"ASC\")\n  qry = dbSendQuery(con, qryTxt)\n  dat = fetch(qry, n=-1)\n  fnm = paste0(path, tbl, \".csv\")\n  write.table(dat, file=fnm, sep=',', row.names=FALSE, quote=FALSE, na='')\n}\n\ndbDisconnect(con)\nprint('all done!')\n</code></pre>"},{"location":"Coding/Rust/Feature/","title":"Feature","text":""},{"location":"Coding/Rust/Feature/#feature-example","title":"Feature example","text":"<p>https://users.rust-lang.org/t/wrap-rust-library-for-python-without-changing-rust-library/69285/4 <pre><code>#[cfg_attr(feature= \"pyo3\", pyo3::pyclass)]\npub struct Vector3 {\n    pub x: i32,\n    pub y: i32,\n    pub z: i32\n}\n\n// Methods that are only available when building the extension module.\n#[cfg_attr(feature= \"pyo3\", pyo3::pymethods)]\nimpl Vector3 {\n    #[new]\n    pub fn new(x: i32, y: i32, z: i32) -&gt; Vector3 {\n        Vector3 { x, y, z }\n   }\n\n   // ... getters and setters for Vector3's fields here\n\n}\n\n// Methods accessible to both Rust and Python\nimpl Vector3 {\n    pub fn length(&amp;self) -&gt; f64 {\n        ((self.x*self.x + self.y*self.y + self.z*self.z) as f64).sqrt()\n    }\n}\n#[#[cfg_attr(feature= \"pyo3\", pyo3::pymodule)]]\nfn rust(_py: Python, m: &amp;PyModule) -&gt; PyResult&lt;()&gt; {\n    m.add_class::&lt;Vector3&gt;()?;\n    Ok(())\n}\n</code></pre></p>"},{"location":"Coding/Rust/Formatter/","title":"Formatter","text":""},{"location":"Coding/Rust/Formatter/#rustfmt","title":"<code>rustfmt</code>","text":"<pre><code>rustup component add rustfmt #install\ncargo fmt             #format all files from root folder\ncargo fmt --check     #check only\ncargo fmt src/main.rs #format a file\ncargo fmt src/util/   #format files in a folder\n</code></pre>"},{"location":"Coding/Rust/Formatter/#rustfmttoml","title":"rustfmt.toml","text":"<p>configuration <pre><code># Use a maximum line width of 100 characters (default is 100)\nmax_width = 100\n\n# Use 4 spaces for indentation (default is 4)\nindent_style = \"Spaces\"\ntab_spaces = 4\n\n# Put each struct field on a separate line if the struct has more than one field\nstruct_field_align_threshold = 1\n\n# Always put trailing commas in multiline expressions\ntrailing_comma = \"Always\"\n\n# Use single line where possible\nuse_small_heuristics = \"Max\"\n</code></pre></p>"},{"location":"Coding/Rust/Formatter/#vscode-integration","title":"vscode integration","text":""},{"location":"Coding/Rust/Install/","title":"Install","text":""},{"location":"Coding/Rust/Install/#windows","title":"Windows","text":"<ul> <li>Build Tools for Visual Studio 2022:</li> <li>requires a valid Visual Studio license, unless building open-source dependencies for your project</li> <li>https://visualstudio.microsoft.com/downloads</li> <li>Desktop development with C++</li> <li>Rust: https://rustup.rs</li> </ul>"},{"location":"Coding/Rust/Install/#linux","title":"Linux","text":"<ul> <li><code>rustup</code>: <code>curl --proto '=https' --tlsv1.2 https://sh.rustup.rs -sSf | sh</code></li> <li><code>linker</code> (C compiler includes a linker): GCC or Clang. For Ubuntu, install <code>build-essential</code> package</li> </ul>"},{"location":"Coding/Rust/Install/#update","title":"Update","text":"<pre><code>rustc --version\nrustup update         # update\nrustup self uninstall # uninstall\n</code></pre>"},{"location":"Coding/Rust/Install/#install-rust-analyzer-extension-in-vs-code","title":"Install <code>rust-analyzer</code> extension in vs code","text":"<p>https://code.visualstudio.com/docs/languages/rust</p> <p>Disable inlay hints: <pre><code>// Enables the inlay hints in the editor.\n//  - on: Inlay hints are enabled\n//  - onUnlessPressed: Inlay hints are showing by default and hide when holding Ctrl+Alt\n//  - offUnlessPressed: Inlay hints are hidden by default and show when holding Ctrl+Alt\n//  - off: Inlay hints are disabled\n\"editor.inlayHints.enabled\": \"on\",\n</code></pre></p> <p>Turn off underline from mut variable name: <pre><code>\"editor.semanticTokenColorCustomizations\": {\n    \"enabled\": true,\n    \"rules\": {\n        \"*.mutable\": {\n            \"underline\": false,\n        }\n    }\n}\n</code></pre></p>"},{"location":"Coding/Rust/Install/#install-debugging-support","title":"Install debugging support","text":"<ul> <li>Windows <code>Microsoft C++</code> (ms-vscode.cpptools): https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools</li> <li>Linux/MacOS <code>CodeLLDB</code> (vadimcn.vscode-lldb): https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb run in vscode (Ctrl + P): <code>ext install vadimcn.vscode-lldb</code></li> </ul>"},{"location":"Coding/Rust/Layout/","title":"Layout","text":"<p>https://medium.com/@kudryavtsev_ia/how-i-design-and-develop-real-world-python-extensions-in-rust-2abfe2377182</p> <ul> <li>with separate crates bundled into a single workspace</li> <li>separate rust from python</li> <li>with namespaces and feature flags, placing Python-related code behind the features</li> </ul>"},{"location":"Coding/Rust/Layout/#mixed","title":"mixed","text":"<p>https://stackoverflow.com/questions/69057820/how-to-structure-a-mixed-python-rust-package-with-pyo3</p> <p><code>Note</code>: - Only explicitely imported rust and python objects will be available in the python module - We should avoid namespace conflict in rust and python - can't share the same namespace</p>"},{"location":"Coding/Rust/Layout/#workspace","title":"workspace","text":"<p>https://doc.rust-lang.org/book/ch14-03-cargo-workspaces.html</p>"},{"location":"Coding/Rust/Learn/","title":"Learn","text":""},{"location":"Coding/Rust/Learn/#book-to-start","title":"Book to start","text":"<p>The Rust Programming Language: https://doc.rust-lang.org/stable/book</p>"},{"location":"Coding/Rust/Learn/#performance-book","title":"Performance book","text":"<p>The Rust Performance Book: https://nnethercote.github.io/perf-book/profiling.html</p>"},{"location":"Coding/Rust/Learn/#layout-python-packages-etc","title":"layout / python packages etc","text":"<p>https://medium.com/@kudryavtsev_ia/how-i-design-and-develop-real-world-python-extensions-in-rust-2abfe2377182</p>"},{"location":"Coding/Rust/Marurin/","title":"Maturin","text":""},{"location":"Coding/Rust/Marurin/#create-a-project","title":"create a project","text":"<p>Create a folder and add layout files: <pre><code>maturin new ./proj --bindings pyo3\ncd proj\nmaturin develop\n</code></pre></p> <p>From an existing folder/project and add layout files: <pre><code>mkdir my-proj &amp;&amp; cd \"$_\"\npip install maturin\nmaturin init --bindings pyo3\nmaturin develop\n</code></pre></p>"},{"location":"Coding/Rust/Marurin/#build-options","title":"build options","text":"<p>https://www.maturin.rs/metadata.html#add-maturin-build-options</p>"},{"location":"Coding/Rust/Marurin/#dynamic-version","title":"dynamic version","text":"<p>To use the Rust crate version from <code>Cargo.toml</code> as the Python package version: <pre><code>[project]\nname = \"my-project\"\ndynamic = [\"version\"]\n</code></pre></p>"},{"location":"Coding/Rust/Marurin/#mixed-rustpython-projects","title":"Mixed rust/python projects","text":"<p>https://github.com/PyO3/maturin</p>"},{"location":"Coding/Rust/Module/","title":"Module","text":"<p>https://www.reddit.com/r/rust/comments/y3p5hx/how_can_i_have_multi_level_module_nesting/ <pre><code>src/\n    main.rs\n    foo.rs\n    foo/\n        bar.rs\n        bar/\n            baz.rs\n</code></pre> - <code>main.rs</code> must contain a <code>mod foo;</code> line - <code>foo.rs</code> or <code>foo/mod.rs</code> must contain a <code>mod bar;</code> line - <code>foo/bar.rs</code> or <code>foo/bar/mod.rs</code> must contain a <code>mod baz;</code> line</p> <p>When rust encounters <code>mod name;</code>, it goes and finds <code>name.rs</code> or <code>name/mod.rs</code> and uses its contents as the module at that site.</p>"},{"location":"Coding/Rust/Python/","title":"Python","text":""},{"location":"Coding/Rust/Python/#python-package-maturin","title":"python package <code>maturin</code>","text":"<p><code>mamba install maturin</code>. build and package python packages that contain rust code using pyo3</p>"},{"location":"Coding/Rust/Python/#rust-package-pyo3","title":"rust package <code>pyo3</code>","text":"<p>a library with built-in types helps pass object from python to rust and the other way around. <pre><code>[dependencies]\npyo3 = { version = \"0.21.2\", features = [\"extension-module\"] } # Use the latest stable pyo3 version\n</code></pre></p>"},{"location":"Coding/Rust/Python/#create-a-lib-project","title":"create a lib project","text":"<pre><code>cargo new --lib my_rust_module # new folder\ncargo init --lib               # run inside existing folder\n</code></pre>"},{"location":"Coding/Rust/Python/#build-extension","title":"build extension","text":"<p>Use <code>maturin</code> to build the Python extension module</p> <p><code>maturin develop</code>: - compile Rust code and place the generated <code>.so</code> (Linux/macOS) or <code>.pyd</code> (Windows) file in a location where Python can find it - also install a simple <code>*.pth</code> file that points to the build directory, making it easy to test during development</p> <p><code>maturin build --release</code>: - Use <code>--release</code> for optimized builds - Compile Rust code in <code>release</code> mode (optimized for performance) - Package the compiled shared library and other python files into a .whl file - The .whl file will be placed in a target/wheels directory</p> <p><code>maturin publish --release</code>: - Run <code>maturin build --release</code> - Upload the generated source distribution (.tar.gz) and wheel (.whl) files to PyPI</p>"},{"location":"Coding/Search/Google/","title":"Google","text":""},{"location":"Coding/Search/Google/#search-options","title":"search options","text":"<ul> <li><code>+x</code> must include x</li> <li><code>-x</code> exclude x</li> <li><code>or</code> one or another</li> <li><code>site:x</code> search that site</li> <li><code>-site:x</code> exclude that site</li> <li><code>after:x</code> published after that date</li> <li><code>before:x</code> published before that date</li> <li><code>after:x before:y</code> published between the date range</li> </ul>"},{"location":"Coding/Security/Password/","title":"Password","text":""},{"location":"Coding/Security/Password/#password-best-practices","title":"password best practices","text":"<p>https://learn.microsoft.com/en-us/aspnet/identity/overview/features-api/best-practices-for-deploying-passwords-and-other-sensitive-data-to-aspnet-and-azure - you should never store passwords or other sensitive data in source code - you shouldn't use production secrets in development and test mode - usie a \"secrets manager\" service, such as Azure Key Vault, AWS Secrets Manager</p>"},{"location":"Coding/Security/Password/#azure-key-vault","title":"azure key-vault","text":"<pre><code>var keyVaultUrl = \"https://&lt;your-key-vault-name&gt;.vault.azure.net/\";\nvar credential =  new DefaultAzureCredential();    \nvar client = new SecretClient(vaultUri: new Uri(keyVaultUrl), credential);    \nKeyVaultSecret secret = client.GetSecret(\"&lt;your-secret-name&gt;\");    \nConsole.WriteLine($\"{secret.Name}: {secret.Value}\");\n</code></pre>"},{"location":"Coding/Security/Password/#aws-secret-manager","title":"aws secret manager","text":"<pre><code>var client = new AmazonSecretsManagerClient(\n    accessKeyId, secretAccessKey, \n    RegionEndpoint.APSoutheast2\n);\nvar request = new GetSecretValueRequest {\n    SecretId = secretName\n};\nGetSecretValueResponse response = null;\nresponse = client.GetSecretValueAsync(request).Result;\n</code></pre>"},{"location":"Coding/VBA/API/","title":"webapi","text":"<pre><code>Public Function GetApiRes(urlBase as string, uri as string) As Object\n    Dim req As Object: Set req = CreateObject(\"WinHttp.WinHttpRequest.5.1\")    \n    req.SetTimeouts 0, 30000, 30000, 300000 'ResolveTimeout, ConnectTimeout, SendTimeout, and ReceiveTimeout in milliseconds\n\n    req.Open \"GET\", urlBase &amp; uri\n    req.Send\n\n    If req.Status &lt;&gt; 200 Then Err.Raise Number:=Err_API_FAILED, Description:=\"WebAPI call failed: http code \" &amp; req.Status\n\n    Set GetApiRes = JsonConverter.ParseJson(req.ResponseText)\nEnd Function\n</code></pre>"},{"location":"Coding/VBA/Addin/","title":"Addin","text":""},{"location":"Coding/VBA/Addin/#create","title":"create","text":"<p>Excel <code>xlam</code> file is a regular workbook, marked as add-in. Just save an Excel <code>xlsm</code> to <code>xlam</code>. It's better also to change <code>VBAProject</code> to your AddIn name.</p>"},{"location":"Coding/VBA/Addin/#install","title":"install","text":"<p>File -&gt; Options \u2013&gt; Add-ins.</p> <p>The path is usually at: <code>C:\\Users\\&lt;user-name&gt;\\AppData\\Roaming\\Microsoft\\AddIns</code></p>"},{"location":"Coding/VBA/Addin/#scope","title":"scope","text":"<p>Functions in local Excel workbook will be used instead of the one in the addin. To use the addin version,  add the addin name like <code>VBAProjectName.FunctionName</code>, or <code>ModuleName.FunctionName</code> or <code>VBAProjectName.ModuleName.FunctionName</code></p>"},{"location":"Coding/VBA/Addin/#convert-back-to-xlsm","title":"convert back to <code>xlsm</code>","text":"<p>Open VBA -&gt; AddIn -&gt; ThisWorkbook -&gt; IsAddIn -&gt; False</p>"},{"location":"Coding/VBA/Array/","title":"array","text":"<p>You can only redim the last dimension of a multi dimension array</p>"},{"location":"Coding/VBA/Array/#check-array-empty","title":"check array empty","text":"<p>IsEmpty(arr) 'only works for Variant</p>"},{"location":"Coding/VBA/Array/#check-variant-array-element","title":"check variant array element","text":"<p>(Not arr(i, j)) = -1 'not empty, arr(i,j) can be an array</p> <p>when get the value of a range, we do not know it is a value or an array, we should use <pre><code>dim val as variant\ndim val_tmp as variant\nval_tmp = worksheets(\"x\").range(\"a3\").value\nval_tmp = worksheets(\"x\").range(\"a3:a4\").value\nif vardim(val) &gt; 0 then\n  val = val_tmp\nelse\n  redim val (1 to 1, 1 to 1)\n  val(1,1) = val_tmp\nend if \n</code></pre></p> <p>then we need to check the dimension of the val, using  <pre><code>Function VarDim(var As Variant) As Long\nOn Error GoTo Err\n    Dim i As Long\n    Dim tmp As Long\n    i = 0\n    Do While True\n        i = i + 1\n        tmp = UBound(var, i)\n    Loop\nErr:\n    VarDim = i - 1\nEnd Function\n</code></pre></p>"},{"location":"Coding/VBA/Basic/","title":"basic","text":"<p>Option Compare Text 'replace case insensitive</p> <p>VBA default is passing variable ByRef</p> <p>Use long not integer as integer will be converted to long and the performance will be affected</p> <pre><code>'check empty array\nLen(Join(NewSnapshots, \"\")) = 0\n\n'get address of named range 'data'\n=ADDRESS(ROW(data),COLUMN(data),4) &amp; \":\" &amp; ADDRESS(ROW(data)+ROWS(data)-1,COLUMN(data)+COLUMNS(data)-1,4)\n\n'get comment\nFunction GetComment(xCell As Range) As String\n   On Error Resume Next\n   GetComment = xCell.Comment.Text\nEnd Function\n\n'get defined variable value\nval = Worksheets(wsName).Range(\"varName\").Value2\n</code></pre>"},{"location":"Coding/VBA/CSV/","title":"CSV","text":""},{"location":"Coding/VBA/CSV/#wbsaveas-issues","title":"WB.SaveAS issues","text":"<ul> <li>will drop first empty line</li> <li>will save only Range.Text (not value/value2)</li> </ul>"},{"location":"Coding/VBA/CSV/#excel-to-csv-numerical-data-precision","title":"Excel to CSV: numerical data precision","text":"<p>When saving excel to csv, to keep numerical data precision, we need to use text format!</p>"},{"location":"Coding/VBA/CSV/#csv-to-sheet","title":"csv to sheet","text":"<pre><code>Sub CSVToSheet()\n    strrngfr = \"A1\"\n    filename = \"my.csv\"\n\n    Dim csvRows As Variant: csvRows = ReadCsvRows(filename)\n\n    Dim i0 As Long: i0 = LBound(csvRows)\n    Dim nrow As Long: nrow = UBound(csvRows) - i0 ' last row is empty, separated by crlf, also include header\n    Dim ncol As Long: ncol = ArrLen(Split(csvRows(LBound(csvRows)), \",\"))\n\n    Dim i As Long\n    Dim j As Long\n    Dim vals As Variant\n    ReDim arr(0 To nrow - 1, 0 To ncol - 1) As Variant\n    For i = 0 To nrow - 1\n        vals = Split(csvRows(i - i0), \",\") '0-based index\n        For j = 0 To ncol - 1\n            arr(i, j) = vals(j)\n        Next\n    Next\n\n    Dim rg As Range\n    Set rg = Range(strrngfr).Offset(RowOffset:=0).Resize(RowSize:=nrow, ColumnSize:=ncol)\n    rg.Value = arr\nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/CSV/#sheet-to-csv","title":"sheet to csv","text":"<pre><code>Sub SheetsToCSV()\n    csv_dir = \"C:\\csv\"\n    xl_file = \"C:/tmp/my_excel.xlsx\"\n    sheet_names = \"sheet1:sheet2:sheet3\"\n\n    Set xl = CreateObject(\"Excel.Application\")\n    Set wb = xl.Workbooks.Open(xl_file)\n    Set fs = CreateObject(\"Scripting.FileSystemObject\")\n    csv_file = csv_dir &amp; \"/\" &amp; fs.GetFileName(xl_file)\n    For Each sheet_name In Split(sheet_names, \":\")\n        wb.Sheets(sheet_name).Activate\n        csv_filepath = csv_file &amp; sheet_name &amp; \".csv\"\n        Debug.Print \"Saving csv file: \" &amp; csv_filepath\n        wb.SaveAs csv_filepath, 6 'csv_format = 6\n    Next\n    wb.Close False\n    xl.Quit\nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/CSV/#to_csv-advanced","title":"to_csv advanced","text":"<pre><code>Sub SheetsToCSV()\n    dir = ThisWorkbook.Path\n    xl_file = \"my.xlsx\"\n    sheet_names = \"a:b:c\"\n\n    Set xl = CreateObject(\"Excel.Application\")    \n    Debug.Print \"Opening Excel file: \" &amp; xl_file\n    Set wb = xl.Workbooks.Open(dir &amp; \"\\\" &amp; xl_file)\n\n    Dim rng As Range\n    csv_file = dir &amp; \"\\\" &amp; xl_file\n    For Each sheet_name In Split(sheet_names, \":\")\n        With wb.Sheets(sheet_name).UsedRange\n            nrow = .Rows.Count + .Row - 1\n            ncol = .Columns.Count + .Column - 1\n            Set rng = wb.Sheets(sheet_name).Range(\"A1\").Resize(RowSize:=nrow, ColumnSize:=ncol)\n            arr = rng.Value\n            arr2 = rng.Value2\n        End With        \n        csv_filepath = csv_file &amp; sheet_name &amp; \".csv\"       \n        ArrValToCSV arr, arr2, csv_filepath\n        Debug.Print \"Saved csv file: \" &amp; csv_filepath\n    Next\n    wb.Close SaveChanges:=False    \n    xl.Quit\n    Debug.Print \"Done\"\nEnd Sub\n\nSub ArrValToCSV(arr As Variant, arr2 As Variant, ByVal filename As String)\n    Set fs = CreateObject(\"Scripting.FileSystemObject\")\n    If fs.FileExists(fielname) Then\n        Kill filename\n    End If\n    Open filename For Output As #1\n\n    Dim Line As String\n    s = ArrShape(arr)\n    If s(0) = 0 Then\n        If VarType(arr) = vbDate Then\n            Line = CStr(arr)\n        ElseIf VarType(arr) = vbString Then\n            Line = \"\"\"\" &amp; CStr(arr2) &amp; \"\"\"\"\n        Else\n            Line = CStr(arr2)\n        End If\n        Print #1, Line\n    ElseIf s(0) = 1 Then\n        Line = \"\"\n        For i = s(1) To s(2)\n            If VarType(arr(i)) = vbDate Then\n                Line = Line &amp; CStr(arr(i))\n            ElseIf VarType(arr(i)) = vbString Then\n                Line = Line &amp; \"\"\"\" &amp; CStr(arr2(i)) &amp; \"\"\"\"\n            Else\n                Line = Line &amp; CStr(arr2(i))\n            End If\n            If i &lt; s(2) Then\n                Line = Line &amp; \",\"\n            End If\n        Next\n        Print #1, Line\n    Else\n        For i = s(1) To s(2)\n            Line = \"\"\n            For j = s(3) To s(4)\n                If VarType(arr(i, j)) = vbDate Then\n                    Line = Line &amp; CStr(arr(i, j))\n                ElseIf VarType(arr(i, j)) = vbString Then\n                    Line = Line &amp; \"\"\"\" &amp; CStr(arr2(i, j)) &amp; \"\"\"\"\n                Else\n                    Line = Line &amp; CStr(arr2(i, j))\n                End If\n                If j &lt; s(4) Then\n                    Line = Line &amp; \",\"\n                End If\n            Next\n            Print #1, Line\n        Next\n    End If\n\n    Close #1\nEnd Sub\n\nFunction ArrShape(arr As Variant) As Variant()\n    d = ArrDim(arr)\n    If d = 0 Then\n        ArrShape = Array(0, 1, 1, 1, 1)\n    ElseIf d = 1 Then\n        ArrShape = Array(1, LBound(arr), UBound(arr), 1, 1)\n    Else\n        ArrShape = Array(2, LBound(arr), UBound(arr), LBound(arr, 2), UBound(arr, 2))\n    End If\nEnd Function\n\nFunction ArrDim(arr As Variant) As Long\nOn Error GoTo Err\n    Dim i As Long\n    Dim tmp As Long\n    i = 0\n    Do While True\n        i = i + 1\n        tmp = UBound(arr, i)\n    Loop\nErr:\n    ArrDim = i - 1\nEnd Function\n</code></pre>"},{"location":"Coding/VBA/CSV/#to_csv-filtered-only","title":"to_csv filtered only","text":"<pre><code>Sub ToCSV(rng as Range, csvPath as String, optional visibleOnly as Boolean = True, optional wb As Workbook = Nothing)\n    Application.DisplayAlerts = False\n    Application.ScreenUpdating = False\n\n    If visibleOnly Then\n        Set rng = rng.SpecialCells(xlCellTypeVisible)\n    End If\n    rng.Copy\n\n    Dim wsName as String\n    If wb = Nothing Then\n        wsName = Nothing\n        Set wb = Application.Workbooks.Add        \n    Else\n        wsName = \"tmp.ws-\" &amp; TimestampID\n        wb.Sheets.Add.Name = wsName\n    End If\n    wb.ActiveSheet.Paste\n    wb.SaveAs Filename:=csvPath FileFormat:=xlCSV, CreateBackup:=False\n    If wsName = Nothing\n        wb.Close SaveChanges:=False\n    Else\n        wb.Sheets(wsName).Delete\n    End If    \n\n    Application.DisplayAlerts = True\n    Application.ScreenUpdating = True\nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/DateTime/","title":"datetime","text":"<pre><code>date = DateSerial(year, month, day)\nDateAdd(\"m\", 12, FirstDate)\n</code></pre>"},{"location":"Coding/VBA/DateTime/#str2dt","title":"str2dt","text":"<pre><code>'string to datetime\ndt = DateValue(\"2019-12-19\") + TimeValue(\"12:47:34\")\n</code></pre>"},{"location":"Coding/VBA/DateTime/#dt2str","title":"dt2str","text":"<pre><code>'date to string Format does not work\n'wrong with rounding error: 30/06/2016 12:00:00 AM\nApplication.Text(DateSerial(2016,7,1) - 1e-7,\"d/MM/yyyy h:mm:ss AM/PM\")\n'depend on local settings: 1-07-2016 12:00:00 AM\nFormat(DateSerial(2016,7,1) - 1e-7,\"d/MM/yyyy h:mm:ss AM/PM\")\n</code></pre>"},{"location":"Coding/VBA/DateTime/#timestamp","title":"timestamp","text":"<pre><code>Public Function TimestampID() As String\n    dt = Strings.Format(Now, \"yyyymmdd.hhnnss\")\n    dtms = dt &amp; Strings.Right(Strings.Format(Timer, \"#0.00\"), 3) 'timer error is +/- 15 ms\n    TimestampID = dtms &amp; Strings.Right(Strings.Format(Rnd, \"#0.0000000\"), 7) 'add random number\nEnd Function\n</code></pre>"},{"location":"Coding/VBA/Dict/","title":"dictionary","text":"<p>https://excelmacromastery.com/vba-dictionary/</p> <p>If you attempt to return the Item of a nonexistent key, or assign a new key to a nonexistent key, the nonexistent key is added to the dictionary, along with a blank item.</p>"},{"location":"Coding/VBA/Error/","title":"error","text":""},{"location":"Coding/VBA/Error/#catch-error","title":"catch error","text":"<pre><code>Sub GotoExample()\n    On Error GoTo Err\n\n    Dim x As Long, y As Long\n    x = 4\n    y = 4 / 0\n    x = 8\n\nDone:\n    Exit Sub\nErr:\n    Select Case MsgBox(\"Error: \" &amp; Err.Description, vbAbortIgnore+vbDefaultButton1)\n    Case vbAbort\n        End\n    Case vbIgnore\n        Ignore\n    End Select    \nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/Error/#check-error-value","title":"check error value","text":"<p>https://stackoverflow.com/questions/66054898/excel-error-testing-cells-without-error-gives-type-mismatch-cverrxlerrna</p> <pre><code>TypeName(val) = \"Error\"\nIsError(val)\n</code></pre> <p><code>Variant/Error</code> cannot be compared to anything other than a <code>Variant/Error</code>, otherwise will throw a <code>type mismatch</code> error. So if we need to check sepecif errors, we need first to ensure it's a <code>Variant/Error</code> value <pre><code>If IsError(val) Then\n    ' Variant/Error comparisons are safe here\n    If val = CVErr(xlErrNA) Then\n        ' we have the #N/A error\n    Else\n        ' cell contains an error, but it's not #N/A\n    End If\nEnd If\n</code></pre></p>"},{"location":"Coding/VBA/IO/","title":"io","text":""},{"location":"Coding/VBA/IO/#read-csv-file","title":"read csv file","text":"<pre><code>'read csv including header as array of strings\nFunction ReadCsv(filename As String) As String()\n    Dim csvText As String\n    csvText = ReadUtf8(filename)\n    ReadCsv = Split(csvText, vbCrLf)\nEnd Function\n</code></pre>"},{"location":"Coding/VBA/IO/#vb-original-method","title":"vb original method","text":"<pre><code>Function ReadUtf8(fileName As String) As String\n    On Error Resume Next\n    Dim fnum As Integer\n    Dim flen As Long\n    fnum = FreeFile 'get unused file\n    Open fileName For Binary Access Read Shared As #fnum\n    If Err.Number &lt;&gt; 0 Then\n        ErrMsgBox \"File is not found: \" &amp; fileName, \"File Reading Error\"\n    End If\n    flen = LOF(fnum)\n    ReadUtf8_vba = Space$(flen)  'Space$(LOF(fnum)) creates a string the size of the file\n    Get #fnum, , ReadUtf8_vba\n    Close #fnum\n\n    If flen = 0 Then\n        ErrMsgBox \"File is empty: \" &amp; fileName, \"File Reading Error\"\n    End If\nEnd Function\n</code></pre>"},{"location":"Coding/VBA/IO/#adodb-stream-method","title":"ADODB stream method","text":"<pre><code>'need reference to ADODB\n'Tools &gt; References &gt; Check the checkbox in front of \"Microsoft ActiveX Data Objects 6.1 Library\"\nFunction ReadUtf8_ado(fileName As String) As String\n    Dim stream As ADODB.stream\n    Set stream = New ADODB.stream\n    stream.Charset = \"utf-8\"\n    stream.Open\n    On Error GoTo Nofile\n\n    stream.LoadFromFile fileName\n    ReadUtf8_ado = stream.ReadText() &amp; \"\"\n    stream.Close\n    Set stream = Nothing\n    Exit Function\n\nNofile:\n    stream.Close\n    Set stream = Nothing\n    ErrMsgBox \"Error Number: \" &amp; Err.Number &amp; vbCrLf &amp; _\n        \"Error Description: \" &amp; Err.Description &amp; vbCrLf &amp; \"File: \" &amp; fileName, \"File Reading Error\"\nEnd Function\n</code></pre>"},{"location":"Coding/VBA/IO/#export-named-ranges","title":"Export named ranges","text":"<pre><code>Sub CopyNamedRanges()\n    For Each x In ActiveWorkbook.Names\n        Workbooks(\"Book2.xls\").Names.Add Name:=x.Name, RefersTo:=x.Value\n    Next\nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/Index/","title":"Index","text":""},{"location":"Coding/VBA/Index/#one-based","title":"one-based","text":"<ul> <li>Retrieving values into an variant array from the worksheet's cells always returns a <code>1-based 2-D array</code></li> <li><code>Option Base 0</code> (default) cannot change this behavior <pre><code>arr = rng.Value\n</code></pre></li> </ul>"},{"location":"Coding/VBA/Index/#zero-based","title":"zero-based","text":"<ul> <li><code>Split(\"x,y,z\", \",\")</code> '0-based index</li> <li><code>Application.Transpose</code> will return a 1-D <code>zero-based</code> array of a single column or single row.</li> </ul>"},{"location":"Coding/VBA/Parallel/","title":"parallel","text":"<p>https://analystcave.com/excel-multithreading-vba-vs-vbscript-vs-c-net/</p> <p>http://www.excelhero.com/blog/2010/05/multi-threaded-vba-update.html</p>"},{"location":"Coding/VBA/Perf/","title":"Perf","text":""},{"location":"Coding/VBA/Perf/#perf","title":"perf","text":""},{"location":"Coding/VBA/Perf/#turn-off-screen-updates-and-auto-calculations","title":"Turn off screen updates and auto calculations","text":"<pre><code>Public Sub DisableXlUpdate()\n    If Application.Calculation = xlCalculationManual Then\n        WaitAutoCalculationToFinish\n    End If\n\n    Application.StatusBar = \"\"\n    Application.ScreenUpdating = False\n    Application.EnableAnimations = False\n    Application.Calculation = xlCalculationManual\nEnd Sub\n\nPublic Sub EnableXlUpdate()\n    WaitAutoCalculationToFinish\n    Application.StatusBar = False\n    Application.ScreenUpdating = True\n    Application.EnableAnimations = True    \nEnd Sub\n\nPublic Sub WaitAutoCalculationToFinish()\n    Application.Calculation = xlCalculationAutomatic\n    Do Until Application.CalculationState = xlDone\n        DoEvents\n    Loop\nEnd Sub   \n</code></pre>"},{"location":"Coding/VBA/Perf/#using-ranges-and-arrays","title":"Using ranges and arrays","text":"<pre><code>'read all the values at once from the Excel cells, put into an array\nval = Range(\"A1:C10000\").Value2\n'change the values in the array, not the cells \nFor rw = LBound(val, 1) To UBound(val, 1)\n  For cl = LBound(val, 2) To UBound(val, 2)\n    v = val (rw, cl)\n    If v &gt; 0 Then \n      val(rw, cl) = v * v\n    End If\n  Next\nNext\n'write all the results back to the range at once\nRange(\"A1:C10000\").Value2 = val\n</code></pre>"},{"location":"Coding/VBA/Perf/#bypass-the-clipboard-copy-and-paste","title":"Bypass the clipboard (copy and paste)","text":"<pre><code>'copy everything (formulas, values and formatting)\nRange(\"A1\").Copy Destination:=Range(\"A2\")\n\n'copy values only\nRange(\"A2\").Value2 = Range(\"A1\").Value2\n\n'copy formulas only\nRange(\"A2\").Formula = Range(\"A1\").Formula     \n</code></pre>"},{"location":"Coding/VBA/Pivot/","title":"pivot table","text":"<pre><code>'change Pivot Table Data Source Range Address\npivot_sht.PivotTables(PivotName).ChangePivotCache \n    ThisWorkbook.PivotCaches.Create(SourceType:=xlDatabase, SourceData:=NewRange)\n\n'ensure Pivot Table is Refreshed\npivot_sht.PivotTables(PivotName).RefreshTable\n\n'get number of rows\nnrw = pivot_sht.PivotTables(PivotName).TableRange1.Rows().Count\n</code></pre>"},{"location":"Coding/VBA/Python/","title":"python","text":""},{"location":"Coding/VBA/Python/#call-python","title":"call python","text":"<pre><code>Sub RunPython()\n    Dim wsh As Object: Set wsh = VBA.CreateObject(\"Wscript.Shell\")\n    Dim pyExe: pyExe = \"\"\"python.exe\"\"\"\n    Dim pyScript As String: pyScript = \"\"\"\" &amp; ThisWorkbook.Path &amp; \"/my_python_script.py\"\"\"\n    'wsh.Run pyExe &amp; \" \" &amp; pyScript\n\n    Dim windowStyle As Integer: windowStyle = 0 '0-hide\n    Dim waitOnReturn As Boolean: waitOnReturn = True\n    wsh.Run \"cmd.exe /S /C python \" &amp; pyScript, windowStyle, waitOnReturn\nEnd Sub \n</code></pre>"},{"location":"Coding/VBA/Range/","title":"range","text":""},{"location":"Coding/VBA/Range/#range-from-cell-value","title":"range from cell value","text":"<pre><code>INDIRECT(CONCATENATE(\"A\",B2))\n</code></pre>"},{"location":"Coding/VBA/Range/#get-range","title":"Get Range","text":"<pre><code>Function Rng(ws as Sheet, r0 as long, c0 as long, r1 as long, c1 as long)\n  return ws.Range(Cells(r0,c0),Cells(r1,c1))\nEnd Function\n</code></pre>"},{"location":"Coding/VBA/Range/#get-subrange","title":"get subrange","text":"<pre><code>'remove first row\nset subrng=rng.offset(1).resize(rng.rows.count-1)\n'remove first col\nset subrng=rng.offset(0,1).resize(rng.rows.count, rng.columns.count-1)\n</code></pre>"},{"location":"Coding/VBA/Range/#generic-getrng","title":"generic getrng","text":"<pre><code>Function RngFirst(rng As Range) As Range\n    'first cell in rng\n    Set RngFirstCell = iif(rng.Cells.count = 1, rng, rng.Cells(1))\nEnd Function\n\nFunction RngLastCUsedXlDown(rng0 As Range, Optional rowOffset As Long = 0) As Range\n    'find last consecutively used cell at or below rng\n    Dim rng As Range: Set rng = RngFirst(rng0).offset(rowOffset)\n    Set RngLastCUsedXlDown = iif(IsEmpty(rng.offset(1, 0).Value), rng, rng.End(xlDown)) 'next cell is blank\nEnd Function\n\nFunction RngLastCUsedXlRight(rng0 As Range, Optional colOffset As Long = 0) As Range\n    'find last consecutively used cell at or after rng\n    Dim rng As Range: Set rng = RngFirst(start0).offset(0, colOffset)\n    Set RngLastCUsedXlRight = iif(IsEmpty(rng.offset(0, 1).Value), rng, rng.End(xlToRight)) 'next cell is blank\nEnd Function\n\nFunction GetRng(rng0 As Range,\n    Optional nrow As Long = -1, Optional ncol As Long = -1, _\n        Optional rowOffset As Long = 0, Optional colOffset As Long = 0, _\n        Optional xlDown As Boolean = True, Optional xlRight As Boolean = False, _\n        Optional rowShift As Long = 0, Optional colShift As Long = 0) As Range\n    Dim rng As Range: Set rng = RngFirst(rng0).offset(rowOffset, colOffset)\n    If nrow = -1 Then nrow = iif(xlDown, RngLastCUsedXlDown(rng, rowOffset:=rowShift).row - rng.row + 1, rng0.rows.count - rowOffset)\n    If ncol = -1 Then ncol = iif(xlRight, RngLastCUsedXlRight(rng.offset(rowShift, colShift)).Column - rng.Column + 1, rng0.Columns.count - colOffset)\n    Set GetRng = rng.Parent.Range(rng, rng.offset(nrow - 1, ncol - 1))\nEnd Function\n</code></pre>"},{"location":"Coding/VBA/SQL/","title":"sql","text":""},{"location":"Coding/VBA/SQL/#create-table","title":"create table","text":"<pre><code>Public Sub CreateMDBTable(cnn As ADODB.Connection, table As String, cols As Variant, ctys As Variant)\nOn Error GoTo Errhandler\n    DeleteMDBTable cnn, table\n\n    Dim i As Long\n    Dim qry As String: qry = \"create table \" &amp; table &amp; \" (\"    \n    For i = LBound(cols) To UBound(cols)\n        qry = qry &amp; cols(i) &amp; \" \" &amp; ctys(i) &amp; IIf(i &lt; UBound(cols), \",\", \");\")\n    Next\n    cnn.Execute qry\n    Exit Sub\n\nErrhandler:\n    MsgBox \"Please close your MDB\", vbCritical, \"Error in creating MDB table\"\nEnd Sub\n\nPublic Sub DeleteMDBTable(cnn As ADODB.Connection, table As String)\n    On Error Resume Next\n    cnn.Execute \"drop table \" &amp; table &amp; \";\"\n    On Error GoTo 0\nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/SQL/#query-to-array","title":"query to array","text":"<pre><code>Public Function QueryToArray(cnn As ADODB.Connection, qry As String) As Variant\n    Dim rs As ADODB.Recordset: Set rs = CreateObject(ADODB.Recordset)\n    rs.Open qry, cnn, adOpenStatic, adLockOptimistic\n\n    Dim records As Variant\n\n    rs.MoveLast\n    Dim cnt As Long: cnt = rs.RecordCount\n    Dim j As Long\n    Dim i As Long: i = 0\n    rs.MoveFirst\n    While Not rs.EOF\n        If IsEmpty(records) Then\n            ReDim records(0 To cnt - 1, 0 To rs.fields.Count - 1)\n        End If\n        For j = 0 To rs.fields.Count - 1\n            records(i, j) = rs(j)\n        Next\n        i = i + 1\n        rs.MoveNext\n    Wend\n    rs.Close\n\n    QueryToArray = records\nEnd Function\n</code></pre>"},{"location":"Coding/VBA/SQL/#array-to-table","title":"array to table","text":"<pre><code>Public Sub WriteArrayToTable(cnn As ADODB.Connection, table As String, records As Variant)\n    Dim i, j As Integer\n    Dim rs As ADODB.Recordset: Set rs = New ADODB.Recordset\n\n    rs.Open table, cnn, adOpenKeyset, adLockOptimistic, adCmdTable\n    With rs\n        For i = LBound(records, 1) To UBound(records, 1)\n            .AddNew\n            For j = LBound(records, 2) To UBound(records, 2)\n                .fields(j) = records(i, j)\n            Next\n            .Update\n        Next\n    End With\n    rs.Close\n    Set rs = Nothing\nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/SQL/#get-record-count","title":"get record count","text":""},{"location":"Coding/VBA/SQL/#client-side-cursor-type","title":"client-side cursor type","text":"<pre><code>Dim qry As String: qry = \"select id, name from my_tbl\"\nDim rs As ADODB.Recordset: Set rs = New ADODB.Recordset\n\n'client-side cursor\nrs.CursorLocation = adUseClient\nrst.Open qry, CurrentProject.Connection\n\n'returns actual recordcount\nDebug.Print \"Client-side rs record count: \" &amp; rs.RecordCount\nrs.Close\n</code></pre>"},{"location":"Coding/VBA/SQL/#server-side-cursor-types","title":"server-side cursor types","text":"<pre><code>Dim qry As String: qry = \"select id, name from my_tbl\"\nDim rs As ADODB.Recordset: Set rs = New ADODB.Recordset\n\n'return -1\nrs.Open qry, CurrentProject.Connection, adOpenForwardOnly\nrs.Close\n\n'return either -1 or actual count, depends\nrs.Open qry, CurrentProject.Connection, adOpenDynamic\nrs.Close    \n\n'return valid count; if -1 specifying lock type as adLockOptimistic may fix the issue\nrs.Open qry, CurrentProject.Connection, adOpenKeyset\nrs.Close   \n\n'return valid count; if -1 specifying lock type as adLockOptimistic may fix the issue\nrs.Open qry, CurrentProject.Connection, adOpenStatic\nrs.Close     \n</code></pre>"},{"location":"Coding/VBA/SQL/#get-odbc","title":"get odbc","text":"<pre><code>Public Function GetMySQLODBCDriverStr() As String\n    GetMySQLODBCDriverStr = \"DRIVER={\" &amp; GetODBCStr(\"MySQL\") &amp; \"};\"\nEnd Function\n\nPublic Function GetODBCStr(typ As String) As String\n    Dim entryNames As Variant\n    entryNames = GetRegEntryNames(\"SOFTWARE\\ODBC\\ODBCINST.INI\\ODBC Drivers\")\n\n    Dim odbc As String: odbc = \"\"\n    If Not IsEmpty(entryNames) Then\n        Dim eName As Variant\n        For Each eName In entryNames\n            If (InStr(eName, typ)) Then\n                odbc = eName\n                Exit For\n            End If\n        Next\n    End If\n\n    If odbc = \"\" Then\n#If Win64 Then\n            MsgBox \"You need to install 64 bit \" &amp; typ &amp; \" ODBC Driver!\"\n#Else\n            MsgBox \"You need to install 32 bit \" &amp; typ &amp; \" ODBC Driver!\"\n#End If\n        End\n    End If\n\n    GetODBCStr = odbc\nEnd Function\n\nPublic Function GetRegEntryNames(keyPath) As Variant\n    Const HKEY_LOCAL_MACHINE = &amp;H80000002\n    Dim entryNames As Variant\n    Dim valueTypes As Variant\n    Set reg = GetObject(\"winmgmts:{impersonationLevel=impersonate}!\\\\.\\root\\default:StdRegProv\")\n    reg.EnumValues HKEY_LOCAL_MACHINE, keyPath, entryNames, valueTypes\n    GetRegEntryNames = entryNames\nEnd Function\n</code></pre>"},{"location":"Coding/VBA/Sort/","title":"sort","text":"<pre><code>Range(\"A1:E5\").Sort Key1:=Range(\"A1\"), Order1:=xlAscending, _\n                    Key2:=Range(\"B1\"), Order2:=xlAscending, _\n                    Header:=xlGuess, OrderCustom:=1, MatchCase:=False, _\n                    Orientation:=xlTopToBottom\n\nHeader:=xlYes                   \n</code></pre>"},{"location":"Coding/VBA/Trick/","title":"trick","text":""},{"location":"Coding/VBA/Trick/#isempty-returns-true-for-cell","title":"IsEmpty returns true for \"\" cell","text":"<pre><code>If val = \"\" Or IsEmpty(val) Then\n  v = \"Blank\"\nEnd If\n</code></pre>"},{"location":"Coding/VBA/Trick/#isnumeric-returns-true-for-empty-cell","title":"IsNumeric returns true for empty cell","text":"<pre><code>Function ValIsNumber(ByVal val As Variant) As Boolean\n    If IsError(val) Then\n        ValIsNumber = False\n    Else\n        ValIsNumber = (Len(val) &gt; 0 And IsNumeric(val))\n    End If\nEnd Function\n</code></pre>"},{"location":"Coding/VBA/VBA/","title":"VBA Notes","text":"<p>https://docs.microsoft.com/en-us/office/vba/api</p>"},{"location":"Coding/VBA/VBA/#programmatic-access-to-visual-basic-project-is-not-trusted","title":"Programmatic Access to Visual Basic Project is not trusted","text":"<p>File -&gt; Options -&gt; Trust Center -&gt; Trust Center Settings -&gt; Macro Settings -&gt; Trust access to the VBA project object model</p>"},{"location":"Coding/VBA/VBE/","title":"VBE","text":""},{"location":"Coding/VBA/VBE/#other","title":"other","text":""},{"location":"Coding/VBA/VBE/#visual-basic-editor-vbe","title":"Visual Basic Editor (VBE)","text":"<p>The VBE is Excel\u2019s code area.   Clicking Developer tab, then Visual Basic in the Code group.   A shortcut for accessing the VBE is Alt-F11.</p>"},{"location":"Coding/VBA/VBE/#jump-to-procedurefunction","title":"Jump to Procedure/Function","text":"<p>Select Some Procedure then hit Shift-F2   And Ctrl-Shift-F2 takes you back</p>"},{"location":"Coding/VBA/VBE/#turn-off-annoying-error-alerts-in-vbe","title":"Turn Off Annoying Error Alerts in VBE","text":"<p>In VBE (Alt-F11) Tools &gt; Options -&gt; Auto Syntax Check</p>"},{"location":"Coding/VBA/VBE/#only-popup-msgbox-for-unhandled-error","title":"Only popup MsgBox for unhandled error","text":"<p>In VBE (Alt-F11) Tools &gt; Options -&gt; General -&gt; Error Trapping -&gt; Break on unhandled errors</p>"},{"location":"Coding/VBA/VBE/#method-and-function","title":"Method and Function","text":"<p>VBA method should use Call with (), or use ',' to separate parameters   Call Meth(a,b,c) or Meth a,b,c</p>"},{"location":"Coding/VBA/VBE/#show-message-in-progress-bar","title":"Show message in progress bar","text":"<p>Application.StatusBar = \"Macro running\"</p>"},{"location":"Coding/VBA/VBE/#excel-disappears-when-run-macro","title":"Excel disappears when run macro","text":"<p>This is because vba will not recompile the vba when \"CompileOnDemand\" = False</p> <p>Reg.exe add \"HKCU\\Software\\Microsoft\\Office\\16.0\\Excel\\Options\" /v \"ForceVBALoadFromSource\" /t REG_DWORD /d \"1\" /f Reg.exe add \"HKCU\\Software\\Microsoft\\VBA\\7.1\\Common\" /v \"CompileOnDemand\" /t REG_DWORD /d \"0\" /f </p>"},{"location":"Coding/VBA/Blog/FixCorruptedExcelFile/","title":"How to fix a corrupted Excel file with high CPU usage","text":"<p>If you've ever experienced Excel freezing or using excessive CPU, you know how frustrating it can be. Here we discuss a commonly encountered problem with high CPU usage and provide practical solutions to optimize your Excel worksheets.</p>"},{"location":"Coding/VBA/Blog/FixCorruptedExcelFile/#issues","title":"Issues","text":"<p>At some point, when you open your Excel file for editing, you are frustrated that the Excel file does not allow you do anything.</p> <p>Note that sometimes you can open the Excel file. But after editing some contents and trying to save the file the Excel becomes not responding again. It does not help even by turning off the calculation with the calculation option being set to <code>Manual</code>.</p> <p>Commonly encountered issues include: - The Excel becomes blank and not responding - The Excel bottom status is showing <code>Calculating (4 Threads): 0%</code> for a long time without any progress - The Excel CPU usage keeps high for a long time and the Excel uses all the remaining CPU - Your Excel file size is large but there is not much data in the worksheets</p> <p>These issues can be caused by many factors. If this is only related to a specific file and the same issue can be observed on different machines, it's most likely that this file was corrupted.</p>"},{"location":"Coding/VBA/Blog/FixCorruptedExcelFile/#solutions","title":"Solutions","text":"<p>Here we discuss the steps to optimize your Excel file to avoid the mentioned issues.</p>"},{"location":"Coding/VBA/Blog/FixCorruptedExcelFile/#deleting-excessive-empty-rows-with-formulas","title":"Deleting excessive empty rows with formulas","text":"<p>A common problem for Excel files is that there are too many formulas copied to the far bottom with many empty rows. This is created to allow more rows in the future. But these empty rows will significantly increase the file size and increase the time to calculate the formulas.</p> <p>So we need to delete such empty rows. If the number of rows is large you will find it's not convenient to delete all the empty rows from the Excel. In this case, you can delete the empty rows with the VBA scripts provided here (please backup your file before doing any fix, and also change the values of the three variables <code>xlFile</code>, <code>sheetNames</code>, and <code>checkColumn</code> accordingly): <pre><code>Sub DeleteEmptyRowsAtSheetEnd()\n    xlFile = \"C:/Test.xlsx\"        ' Excel filepath\n    sheetNames = \"Sales:Marketing\" ' The name of the sheets\n    Const checkColumn As Long = 1  ' Based on which column to check empty rows\n\n    Set xl = CreateObject(\"Excel.Application\")\n    Set wb = xl.Workbooks.Open(xlFile)\n\n    For Each sheetName In Split(sheetNames, \":\")\n        Set ws = wb.Sheets(sheetName)\n        ' Find the last non-empty row in the specified column\n        lastRow = ws.Cells(ws.Rows.Count, checkColumn).End(xlUp).Row\n        Debug.Print sheetName &amp; \" last non-empty row: \" &amp; lastRow\n        ' Delete all rows below the last non-empty row\n        ws.Rows((lastRow + 1) &amp; \":\" &amp; ws.Rows.Count).Delete\n    Next\n    Debug.Print \"Now save workbook\"\n    wb.Close True\n    Debug.Print \"Done\"\n    xl.Quit\nEnd Sub\n</code></pre></p>"},{"location":"Coding/VBA/Blog/FixCorruptedExcelFile/#replacing-low-performance-excel-functions-by-more-efficient-functions","title":"Replacing low performance Excel functions by more efficient functions","text":"<p>Excel includes a variety of functions, and some of them are known to be slower than others, especially when dealing with large datasets.</p> <p>Here is a list of some slower Excel functions along with alternatives that generally offer better performance: - VLOOKUP/HLOOKUP: Alternative INDEX-MATCH - SUMPRODUCT: Alternative SUMIFS or SUMPRODUCT with * operator - COUNTIF: Alternative COUNTIFS - OFFSET: Alternative INDEX - ARRAY FORMULAS: Alternative: Use SUMIFS, INDEX-MATCH, or other non-array alternatives - INDIRECT: INDIRECT can be slow and volatile. Avoid using INDIRECT if possible.</p> <p>Remember to use <code>ChatGPT</code> or <code>Bard</code> to help you to convert the formulas to the more efficient alternatives.</p>"},{"location":"Coding/VBA/Blog/FixCorruptedExcelFile/#replacing-excel-formulas-by-vba-code","title":"Replacing Excel formulas by VBA code","text":"<p>If your Excel will be used doing complicated work and you find the Excel calculation is still very slow after the modification based on the previous two steps, please consider using VBA scripts instead of Excel formulas to do some of the calculations.</p> <p>For example, in VBA we can create a dictionary instead of using the <code>INDEX-MATCH</code> functions to lookup values. By using VBA scripts, we only need to trigger the calculation when everything is ready. Thus we can significantly reduce the amount of calculation required.</p>"},{"location":"Coding/VBA/Blog/VersionControl/","title":"Git version control for Excel files","text":""},{"location":"Coding/VBA/Blog/VersionControl/#extract-vba-code","title":"Extract VBA Code","text":"<p>OleTools: This Python library can extract VBA code from Excel files into separate text files (e.g., <code>.bas</code>, <code>.cls</code>). - Install OleTools: <code>pip install oletools</code> - Extract VBA: <code>olevba your_workbook.xlsm</code></p>"},{"location":"Coding/VBA/Blog/VersionControl/#version-control-the-extracted-code","title":"Version Control the Extracted Code","text":"<ul> <li>Initialize a Git repository: <code>git init</code></li> <li>Add the extracted VBA files: <code>git add *.bas *.cls</code></li> <li>Commit the initial version: <code>git commit -m \"Initial commit of VBA code\"</code></li> <li>*Push to a remote repository: <code>git push -u origin main</code></li> </ul>"},{"location":"Coding/VBA/Blog/VersionControl/#git-hooks","title":"Git Hooks","text":"<p>Create a Git pre-commit hook script to automate the VBA extraction process before each commit.</p> <p>pre-commit.py: <pre><code>#!/usr/bin/env python3\n\nimport subprocess\n\ndef main():\n    subprocess.run([\"olevba\", \"your_workbook.xlsm\"]) \n    subprocess.run([\"git\", \"add\", \"*.bas\", \"*.cls\"]) \n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p>"},{"location":"Coding/VBA/Blog/VersionControl/#important-notes","title":"Important Notes","text":"<ul> <li>We still need to include the Excel file itself in our repository for a complete record.</li> <li>Tools like <code>VBA Editor Pro</code> or <code>Rubberduck</code> can enhance your VBA development experience and potentially offer better integration with Git.</li> </ul>"},{"location":"Coding/VBA/Blog/VersionControl/#use-vba-to-export-files","title":"Use VBA to export files","text":"<pre><code>Sub ExportVBACode()\n    Dim vbComp As Object\n    Dim fileExt As String\n    Dim folderPath As String\n    Dim wb As Workbook\n\n    ' Define the folder path where the VBA code will be saved (Change to your preferred folder)\n    folderPath = \"C:\\path\\to\\your\\export\\folder\\\"  ' Adjust path as needed\n\n    ' Ensure folder exists\n    If Dir(folderPath, vbDirectory) = \"\" Then\n        MkDir folderPath\n    End If\n\n    ' Loop through each component in the workbook's VBA project\n    For Each vbComp In ThisWorkbook.VBProject.VBComponents\n        ' Check if the module is a standard module, class module, or user form\n        Select Case vbComp.Type\n            Case vbext_ct_StdModule\n                ' Export standard module as .bas\n                fileExt = \".bas\"\n            Case vbext_ct_ClassModule\n                ' Export class module as .cls\n                fileExt = \".cls\"\n            Case vbext_ct_MSForm\n                ' Export user form as .frm\n                fileExt = \".frm\"\n            Case Else\n                ' Skip unsupported types\n                fileExt = \"\"\n                Debug.Print \"Skipping: \" &amp; vbComp.Name\n        End Select\n        If fileExt &lt;&gt; \"\" Then\n            vbComp.Export folderPath &amp; vbComp.Name &amp; fileExt\n         End If\n    Next vbComp\n\n    ' Notify user that the export is complete\n    MsgBox \"VBA code exported successfully to: \" &amp; folderPath, vbInformation\nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/Blog/VersionControl/#use-vba-to-import-files","title":"Use VBA to import files","text":"<pre><code>Sub ImportVBACode()\n    Dim vbComp As Object\n    Dim folderPath As String\n    Dim fileName As String\n    Dim filePath As String\n    Dim file As String\n\n    ' Set the folder path where the .cls and .bas files are located\n    folderPath = \"C:\\path\\to\\your\\files\\\"  ' Adjust this path to your folder with .cls and .bas files\n\n    ' Loop through all .bas and .cls files in the specified folder\n    file = Dir(folderPath &amp; \"*.bas\") ' Start by looking for .bas files\n    Do While file &lt;&gt; \"\"\n        ' Import .bas file as a Standard Module\n        filePath = folderPath &amp; file\n        ThisWorkbook.VBProject.VBComponents.Import filePath\n        file = Dir ' Move to next file\n    Loop\n\n    file = Dir(folderPath &amp; \"*.cls\") ' Now look for .cls files\n    Do While file &lt;&gt; \"\"\n        ' Import .cls file as a Class Module\n        filePath = folderPath &amp; file\n        ThisWorkbook.VBProject.VBComponents.Import filePath\n        file = Dir ' Move to next file\n    Loop\n\n    MsgBox \"VBACode imported successfully!\", vbInformation\nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/Excel/Button/","title":"Button","text":""},{"location":"Coding/VBA/Excel/Button/#spin-button","title":"spin button","text":"<p>https://support.microsoft.com/en-au/office/add-a-scroll-bar-or-spin-button-to-a-worksheet-f8443be3-ff00-4cad-bb2f-bf0f88ebf5bb - Developer &gt; Insert &gt; Form Controls &gt; Scroll bar - Developer &gt; Controls ? Properties &gt; Min/max value - Copy column <code>C:I</code> based on Row number in <code>A4</code>: <code>=INDEX(mysheet!$C$1:$I$1000, A4, COLUMN(C1:I1)-2)</code></p>"},{"location":"Coding/VBA/Excel/Datetime/","title":"Datetime","text":""},{"location":"Coding/VBA/Excel/Datetime/#avoid-comparing-general-string-date-with-custom-date","title":"avoid comparing general string date with custom date","text":"<p><pre><code>A1: \"2023-01-01\" format: General\nA2: \"2023-07-01\" format: Custom \"mmm-yy\"\n=A1&gt;A2 will return True\n</code></pre> Solution: Change \"General\" string date to \"Date\" format</p>"},{"location":"Coding/VBA/Excel/Edit/","title":"Edit","text":""},{"location":"Coding/VBA/Excel/Edit/#newline-in-cell-text","title":"newline in cell text","text":"<pre><code>\"First Line\" &amp; Chr(10) &amp; \"Second Line\"\n</code></pre>"},{"location":"Coding/VBA/Excel/Edit/#non-breaking-space","title":"non-breaking space","text":"<p>Can be used to replace special chars such non-breaking space <pre><code>substitue(A1, char(160), \"\")\n\n# code() char to ascii code\n# char() ascii code to char\n</code></pre></p>"},{"location":"Coding/VBA/Excel/Edit/#insertdelete-rowscolumns","title":"insert/delete rows/columns","text":"<pre><code>Range(\"A4:A5\").EntireRow.Insert/Delete\nRange(\"A4:B4\").EntireColumn.Insert/Delete\n\n'format from above row\nRows(5).Insert , xlFormatFromLeftOrAbove\n'format from below row\nRows(5).Insert , xlFormatFromRightOrBelow\n</code></pre>"},{"location":"Coding/VBA/Excel/Excel/","title":"excel","text":""},{"location":"Coding/VBA/Excel/Excel/#hide-ribbon","title":"hide ribbon","text":"<p>Ctrl + F1</p>"},{"location":"Coding/VBA/Excel/Excel/#find-named-ranges","title":"Find named ranges","text":"<p>Ctrl + G</p>"},{"location":"Coding/VBA/Excel/Excel/#shortcut-key","title":"shortcut key","text":"<p>add newline: Ctrl + j</p> <p>refresh or recalculate cell values: F9</p> <p>move between sheets: Ctrl + PgUp/Dn</p> <p>add filter: Shift + Ctrl + l</p> <p>create chart: Ctrl + F1</p> <p>goto cell: Ctrl + g</p> <p>recalc all cells: CTRL + ALT + SHIFT + F9</p> <p>show formulas: Ctrl + `</p>"},{"location":"Coding/VBA/Excel/Excel/#col-letter-to-number","title":"col letter to number","text":"<p>=COLUMN(INDIRECT(\"D1\"))</p>"},{"location":"Coding/VBA/Excel/Excel/#interpolation","title":"interpolation","text":"<p>forecast only works for perfectly linear spaced data for interpolation\\ =FORECAST(f2,INDEX($c$2:$c$37,MATCH(f2,$b$2:$b$37,1)):INDEX($c$2:$c$37,MATCH(f2,$b$2:$b$37,1)+1),INDEX($b$2:$b$37,MATCH(f2,$b$2:$b$37,1)):INDEX($b$2:$b$37,MATCH(f2,$b$2:$b$37,1)+1))  f2:x_new, c:y, b:x</p> <ul> <li> <p>copy cols to col (x is the matrix name, G1 is the first cell in destination)   =OFFSET(x, MOD(ROW()-ROW($G$1),ROWS(x)), TRUNC((ROW()-ROW($G$1))/ROWS(x)), 1, 1)</p> </li> <li> <p>copy rows to col (x is the matrix name, G1 is the first cell in destination)   =OFFSET(x, TRUNC((ROW()-ROW($G$1))/COLUMNS(x)), MOD(ROW()-ROW($G$1),COLUMNS(x)), 1, 1)</p> </li> <li> <p>text to date   =value(\"2019-02-02 12:55:00\")</p> </li> </ul> <pre><code>MAX(INDIRECT(\"A\"&amp;B1&amp;\":A\"&amp;B2))\n</code></pre> <ul> <li>get dir of current workbook</li> </ul> <pre><code>=LEFT(CELL(\"filename\",A1),FIND(\"[\",CELL(\"filename\",A1))-1)\n</code></pre>"},{"location":"Coding/VBA/Excel/Excel/#add-in-error","title":"Add-In error","text":"<p>Error: Object variable or With block variable not set</p> <p>Since the add-in was registered with excel, the following line succeeded:\\ Set addin = Application.COMAddIns(\"My AddIn\")\\ But since it was disabled, the object was not created and\\ Set automationObject = addin.Object resulted in Nothing</p> <p>Solution: File -&gt; Options -&gt; Add-Ins -&gt; Select the type of that Add-In and click 'Go' -&gt; Enabled it</p>"},{"location":"Coding/VBA/Excel/Excel/#unhide-cols","title":"Unhide cols","text":"<p>When hide and unhide do not work, it is possible that the panel is freezed.</p>"},{"location":"Coding/VBA/Excel/Excel/#change-module-names","title":"change module names","text":"<p>select the module and press F4</p>"},{"location":"Coding/VBA/Excel/Excel/#get-value-based-on-row-number","title":"get value based on row number","text":"<p>=INDEX(A1:E100,row,col)</p> <p>=AVERAGE(OFFSET($C$1,E2-1,0,E3-E2,1))</p>"},{"location":"Coding/VBA/Excel/Excel/#force-vba-rebuid","title":"force vba rebuid","text":"<p>Reg.exe add \"HKCU\\Software\\Microsoft\\Office\\16.0\\Excel\\Options\" /v \"ForceVBALoadFromSource\" /t REG_DWORD /d \"1\" /f Reg.exe add \"HKCU\\Software\\Microsoft\\VBA\\7.1\\Common\" /v \"CompileOnDemand\" /t REG_DWORD /d \"0\" /f</p>"},{"location":"Coding/VBA/Excel/Excel/#excel-locked-for-editing","title":"excel locked for editing","text":"<p>first delete the hidden ownership file, in the same folder, using a dos command 'dir' and 'del'</p> <p>then perhaps need to check which user locks the file using cmd mmc (Microsoft Management Console) by adding opened files (need server admin permission).</p>"},{"location":"Coding/VBA/Excel/Excel/#check-if-a-cell-is-referenced-or-not","title":"check if a cell is referenced or not","text":"<p>Formulas &gt; Trace Dependents   Formulas &gt; Remove Arrows &gt; Remove Dependent Arrows</p>"},{"location":"Coding/VBA/Excel/Excel/#find-links-external-references-in-a-workbook","title":"Find links (external references) in a workbook","text":"<p>'does not work for formula in conditional format,   Find and Replace -&gt; Options    -&gt; [Find what]=.xl, [Within box]=Workbook,[Look in]=Formulas    -&gt; Find All   'use findlink.xlam instead: https://www.manville.org.uk/software/findlink.htm   -&gt; Data -&gt; BMA Group -&gt; Find Links</p>"},{"location":"Coding/VBA/Excel/Excel/#arrayformula-does-not-work-when-including-indirect-function","title":"ArrayFormula does not work when including INDIRECT function","text":"<p>why? do not need to input the brackets but use Shift + Ctrl + Enter</p>"},{"location":"Coding/VBA/Excel/Excel/#index-match-skip-empty-value","title":"index match skip empty value","text":"<p>{=INDEX(A:A,MATCH(1,(B:B=C1)*(A:A&lt;&gt;\"\"),0))}</p>"},{"location":"Coding/VBA/Excel/Formatting/","title":"Formatting","text":"<p>https://docs.microsoft.com/en-us/office/vba/api/excel.databar.showvalue</p>"},{"location":"Coding/VBA/Excel/Formatting/#change-mix-of-text-and-number-to-text","title":"change mix of text and number to text","text":"<ul> <li>simply format cells as text will not work after copying text</li> <li>create a new line using <code>=text(a1, \"@\")</code></li> <li>copy value and format to new column</li> </ul>"},{"location":"Coding/VBA/Excel/Formatting/#cell-chars-font-size","title":"cell chars font size","text":"<pre><code>With Sheets(\"Test\").Range(\"A1\")\n    .Value = \"Abc\" &amp; Chr(10) &amp; \"123\" #new line\n    .Characters(Start:=3, Length:=4).Font.Size = 6\nEnd With\n</code></pre>"},{"location":"Coding/VBA/Excel/Formatting/#cell-formatting-color","title":"cell formatting color","text":"<p>Get cell conditional formatting background color: <pre><code>Function BgHexColor(rng As Range)\n    i = rng.DisplayFormat.Interior.Color 'ColorIndex\n    r = (i Mod 256)\n    g = (i \\ 256) Mod 256\n    b = (i \\ 256 \\ 256) Mod 256\n\n    BgHexColor = \"#\" &amp; Format(Hex(r), \"00\") &amp; Format(Hex(g), \"00\") &amp; Format(Hex(b), \"00\")\nEnd Function\n</code></pre></p>"},{"location":"Coding/VBA/Excel/Formatting/#color-scale-2","title":"color scale 2","text":"<pre><code>Sub CondFmt_ColorScales2()\n    Dim rg As Range: Set rg = Range(\"F5:I5\")\n    rg.FormatConditions.Delete\n\n    'color scale will have 2 colors\n    Dim cs As ColorScale: Set cs = rg.FormatConditions.AddColorScale(ColorScaleType:=2)\n    With cs\n        'the first color is white\n        With .ColorScaleCriteria(1)\n            .FormatColor.Color = RGB(255, 255, 255)\n            .Type = xlConditionValueLowestValue\n        End With\n\n        'the third color is red\n        With .ColorScaleCriteria(2)\n            .FormatColor.Color = RGB(255, 0, 0)\n            .Type = xlConditionValueHighestValue\n        End With\n    End With\nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/Excel/Formatting/#color-scale-3","title":"color scale 3","text":"<pre><code>Sub CondFmt_ColorScales3()\n    Dim rg As Range: Set rg = Range(\"F5:I5\")\n    rg.FormatConditions.Delete\n\n    'color scale will have 3 colors\n    Dim cs As ColorScale: Set cs = rg.FormatConditions.AddColorScale(ColorScaleType:=3)\n    With cs\n        'color 1 is blue\n        With .ColorScaleCriteria(1)\n            .FormatColor.Color = RGB(102, 153, 255)\n            .Type = xlConditionValueLowestValue\n        End With\n\n        'color 2 is yellow set at value 50\n        With .ColorScaleCriteria(2)\n            .FormatColor.Color = RGB(255, 230, 153)\n            .Type = xlConditionValueNumber\n            .Value = 50\n        End With\n\n        'color 3 is red\n        With .ColorScaleCriteria(3)\n            .FormatColor.Color = RGB(255, 0, 0)\n            .Type = xlConditionValueHighestValue\n        End With\n    End With\nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/Excel/Formatting/#data-bar","title":"data bar","text":"<pre><code>Sub CondFmt_DataBar()\n    Dim rg As Range: Set rg = Range(\"E30:E44\")\n    rg.FormatConditions.Delete\n\n    Dim db As Databar: Set db = rg.FormatConditions.AddDatabar\n    With db\n        .ShowValue = False\n\n        'positive bar with green gradient &amp; green border\n        .BarColor.Color = RGB(0, 176, 80)\n        .BarFillType = xlDataBarFillGradient\n        .BarBorder.Type = xlDataBarBorderSolid\n        .BarBorder.Color.Color = RGB(0, 176, 80)\n\n        'the axis positioned automatically and coloured black\n        .AxisPosition = xlDataBarAxisAutomatic\n        .AxisColor.Color = vbBalck\n\n        'negative bar with a red gradient &amp; red border\n        With .NegativeBarFormat\n            .ColorType = xlDataBarColor\n            .Color.Color = RGB(255, 0, 0)\n            .BorderColorType = xlDataBarColor\n            .BorderColor.Color = vbRed 'RGB(255, 0, 0)\n        End With\n    End With\n\nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/Excel/HiddenRowCol/","title":"Hidden rows/cols","text":""},{"location":"Coding/VBA/Excel/HiddenRowCol/#get-hidden-rowscols","title":"get hidden rows/cols","text":"<pre><code>' Determine the hidden rows\nFor i = 1 To 1048576\n   If Rows(i).Hidden Then\n\n   End If\nNext i\n' Determine the hidden columns\nFor i = 1 To 16384\n   If Columns(i).Hidden Then\n\n   End If\nNext i\n</code></pre>"},{"location":"Coding/VBA/Excel/InsertPicture/","title":"Insert Fig","text":""},{"location":"Coding/VBA/Excel/InsertPicture/#insert","title":"insert","text":"<pre><code>Sub InsertPicture(rng As Range, picpath As String)\n    Dim pic As Shape ' -1 means original size for width and height\n    Set pic = rng.Parent.Shapes.AddPicture(filename:=imgpath, LinkToFile:=msoFalse, SaveWithDocument:=msoTrue, Left:=rng.Left, Top:=rng.Top, Width:=-1, Height:=-1)\nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/Excel/InsertPicture/#delete","title":"delete","text":"<pre><code>Sub DeletePictures()\n    Dim pic As Picture\n    For Each pic In ThisWorkbook.ActiveSheet.Pictures\n       pic.Delete\n    Next\nEnd Sub\n</code></pre>"},{"location":"Coding/VBA/Excel/Match/","title":"Match","text":""},{"location":"Coding/VBA/Excel/Match/#excel-cant-match-or-vlookup-on-column-with-mixed-text-and-numbers","title":"excel can't MATCH or VLOOKUP on column with mixed text and numbers","text":"<p>https://stackoverflow.com/questions/71755390/excel-cant-match-or-vlookup-on-column-with-mixed-text-and-numbers</p> <p>A column with both integer and text values, the <code>match</code> does not work.</p> <p>Solution <pre><code>=INDEX(Details!E:E, MATCH(B2&amp;\"\", Details!D:D&amp;\"\", 0))\n</code></pre></p> <p>This is better than vlookup especially for large data sets <pre><code>=VLOOKUP(B2&amp;\"\", Details!D:E&amp;\"\", 2, FALSE)\n</code></pre></p>"},{"location":"Coding/VBA/Excel/Pivot/","title":"Pivot","text":""},{"location":"Coding/VBA/Excel/Pivot/#expand-index-levels","title":"expand index levels","text":"<p>move hierarchical data from rows to columns - Design -&gt; Report Layout -&gt; Show in Tabular Form</p>"},{"location":"Coding/VBA/Excel/Protect/","title":"Protect","text":""},{"location":"Coding/VBA/Excel/Protect/#unprotect-sheets","title":"unprotect sheets","text":"<ul> <li>Review &gt; Protect sheet, or</li> <li>File &gt; Info &gt; Protect workbook</li> </ul>"},{"location":"Coding/VBA/Excel/Protect/#show-sheet-tabs","title":"show sheet tabs","text":"<p>First ensure that the Show sheet tabs is enabled. To do this, for all other Excel versions, - click File &gt; Options &gt; Advanced - in under <code>Display options for this workbook</code> - and then ensure that there is a check in the <code>Show sheet tabs</code> box.</p>"},{"location":"Coding/VBA/Excel/ShortCutKeys/","title":"shortcut keys","text":""},{"location":"Coding/VBA/Excel/ShortCutKeys/#format","title":"format","text":"<ul> <li><code>Ctrl + B</code> Apply or remove bold format</li> <li><code>Ctrl + I</code> Apply or remove italic format</li> <li><code>Ctrl + U</code> Apply or remove underline format</li> <li><code>Ctrl + 5</code> Apply or remove strike format</li> <li><code>Ctrl + 1</code> Open the Format Cells dialog box</li> </ul>"},{"location":"Coding/VBA/Excel/ShortCutKeys/#format-values","title":"format values","text":"<ul> <li><code>Ctrl + Shift + ~</code> Apply general format</li> <li><code>Ctrl + Shift + !</code> Apply number format</li> <li><code>Ctrl + Shift + @</code> Apply time format</li> <li><code>Ctrl + Shift + #</code> Apply date format</li> <li><code>Ctrl + Shift + $</code> Apply currency format</li> <li><code>Ctrl + Shift + %</code> Apply percent format</li> <li><code>Ctrl + Shift + ^</code> Apply scientific format</li> </ul>"},{"location":"Coding/VBA/Excel/ShortCutKeys/#format-table","title":"format table","text":"<ul> <li><code>Ctrl + Shift + L</code> table row</li> <li><code>Alt + H, O, A</code> auto fit rows</li> <li><code>Alt + H, O, I</code> auto fit cols</li> </ul>"},{"location":"Coding/VBA/Excel/ShortCutKeys/#create-chart","title":"create chart","text":"<p>select data and then - <code>Alt + F1</code></p>"},{"location":"Coding/VBA/Excel/Type/","title":"Type","text":""},{"location":"Coding/VBA/Excel/Type/#integer-to-text","title":"integer to text","text":"<pre><code>=TEXT(A1, \"0\")\n</code></pre>"},{"location":"Coding/VS/NuGet/","title":"NuGet","text":"<p>https://learn.microsoft.com/en-us/nuget/consume-packages/install-use-packages-powershell</p> <pre><code>Tools &gt; NuGet Package Manager &gt; Package Manager Console\n</code></pre>"},{"location":"Coding/VS/NuGet/#solution-package-source","title":"solution package source","text":"<pre><code>Solution/.nuget/NuGet.Config\n</code></pre>"},{"location":"Coding/VS/NuGet/#project-package-config","title":"project package config","text":"<p>The <code>packages.config</code> file in a C# project is used to manage NuGet package dependencies in older project formats.  <pre><code>Project/packages.config\n</code></pre></p> <p>A newer approach called <code>PackageReference</code> typically uses <code>.csproj</code> files to manage dependencies directly.</p> <p>Migrate to the new approach: https://learn.microsoft.com/en-us/nuget/consume-packages/migrate-packages-config-to-package-reference</p>"},{"location":"Coding/VS/NuGet/#find-all-versions-of-a-package","title":"find all versions of a package","text":"<pre><code>Find-Package &lt;PackageName&gt; -AllVersions -ExactMatch\nFind-Package &lt;PackageName&gt; -AllVersions -ExactMatch | Format-Table -Property Id,Versions,Description\n</code></pre>"},{"location":"Coding/VS/NuGet/#install-a-specific-version-of-a-package","title":"install a specific version of a package","text":"<pre><code>Install-Package &lt;PackageName&gt; -Version 4.4.1            # install to default project\nInstall-Package &lt;PackageName&gt; -ProjectName UtilitiesLib # install to a specific project\n</code></pre>"},{"location":"Coding/VS/NuGet/#update-a-package-in-all-projects","title":"update a package in all projects","text":"<pre><code>Update-Package &lt;PackageName&gt; -Version &lt;newVersion&gt; -ProjectName &lt;ProjectName&gt;\nGet-Project -All | Update-Package &lt;PackageName&gt; -Version newVersion\n</code></pre>"},{"location":"Coding/VS/NuGet/#nuget-cli","title":"nuget cli","text":"<p>https://learn.microsoft.com/en-us/nuget/reference/nuget-exe-cli-reference?tabs=windows</p>"},{"location":"Coding/VS/NuGet/#add-a-package-source","title":"add a package source","text":"<pre><code>nuget help\nnuget source add -name \"name\" -source http://nuget.example.com/nuget/\nnuget source # list all package sources\n</code></pre>"},{"location":"Coding/VS/NuGet/#list-versions-of-a-package","title":"list versions of a package","text":"<pre><code>nuget list &lt;package-id&gt; -verbosity detailed                    # latest version: normal, quiet, detailed\nnuget list &lt;package-id&gt; -source &lt;source&gt; -verbose -allversions # all versions\n</code></pre>"},{"location":"Coding/VS/NuGet/#but-the-package-does-not-contain-any-assembly-references-or-content-files-that-are-compatible-with-that-framework","title":"but the package does not contain any assembly references or content files that are compatible with that framework","text":"<p><pre><code>Update-Package : Could not install package '&lt;package-name&gt;'.\nYou are trying to install this package into a project that targets '.NETFramework,Version=v4.6.1',\nbut the package does not contain any assembly references or content files that are compatible with that framework.\nFor more information, contact the package author.\n</code></pre> Solution: https://stackoverflow.com/questions/34991703/nuget-package-installation-failure - goto <code>Tools-&gt;Options-&gt;Nuget-&gt;General</code> and clear the cache</p>"},{"location":"Coding/VS/Shortcuts/","title":"Shortcut keys","text":""},{"location":"Coding/VS/Shortcuts/#use-vscode-shortcut-keys-in-vs","title":"use vscode shortcut keys in vs","text":"<p>https://stackoverflow.com/questions/62050877/make-visual-studio-use-vs-code-shortcut-keys-key-bindings <pre><code>Tools -&gt; Options -&gt; Environment -&gt; Keyboard -&gt; vs code\n</code></pre></p>"},{"location":"Coding/VS/settings/","title":"settings","text":""},{"location":"Coding/VS/settings/#font-and-text-size","title":"font and text size","text":"<pre><code>Tools &gt; Options &gt; Environment &gt; Fonts and Colors\n</code></pre>"},{"location":"Coding/VS/settings/#line-space","title":"line space","text":"<pre><code>Tools &gt; Options &gt; Text Edotor &gt; General\n</code></pre>"},{"location":"Coding/VSCode/Config/","title":"Config","text":""},{"location":"Coding/VSCode/Config/#settings","title":"Settings","text":"<p>Customizing IntelliSense: https://code.visualstudio.com/docs/editor/intellisense</p>"},{"location":"Coding/VSCode/Config/#user-vs-workspace-settings","title":"User vs workspace settings","text":"<p>Note: the workspace settings will overwrite the user settings</p>"},{"location":"Coding/VSCode/Config/#disable-vs-code-breadcrumbs-and-sticky-scroll","title":"Disable vs code breadcrumbs and sticky scroll","text":"<pre><code>    \"breadcrumbs.enabled\": false,                    // disable breadcrumbs\n    \"editor.stickyScroll.enabled\": false,            // disable sticky scroll\n    \"editor.inlayHints.enabled\": \"offUnlessPressed\", // disable rust inlay hints\n    \"editor.semanticTokenColorCustomizations\": {     // disable rust mut var underline\n        \"enabled\": true,\n        \"rules\": {\n            \"*.mutable\": {\n                \"underline\": false,\n            }\n        }\n    },\n    \"window.zoomLevel\": -0.1,                        // zoom in/out size of windows, -1:20% smaller, 1:20% larger\n    \"window.titleBarStyle\": \"custom\",                // reduce the height of the title bar\n    \"window.density.editorTabHeight\": \"compact\",     // reduce the height of the files tab bar\n</code></pre>"},{"location":"Coding/VSCode/Config/#editor-font-size-and-line-space","title":"Editor font size and line space","text":"<p>settings &gt; editor &gt; line height &gt; 15 <pre><code>\"editor.fontSize\": 13,\n\"editor.lineHeight\": 13,\n</code></pre></p>"},{"location":"Coding/VSCode/Config/#debug-console-font-size-and-line-space","title":"Debug console font size and line space","text":"<pre><code>\"debug.console.fontSize\": 13,\n\"debug.console.lineHeight\": 10,\n</code></pre>"},{"location":"Coding/VSCode/Config/#terminal-font-size","title":"Terminal font size","text":"<p>Line height can't be scaled smaller than 1 <pre><code>\"terminal.integrated.fontSize\": 12,\n</code></pre></p>"},{"location":"Coding/VSCode/Config/#terminal-line-number","title":"terminal line number","text":"<p>Default is 1000 <pre><code>\"terminal.integrated.scrollback\": 5000,\n</code></pre></p>"},{"location":"Coding/VSCode/Config/#new-line-at-eof","title":"New line at EOF","text":"<p>File &gt; Preference &gt; Settings (^,) &gt; 'insert final newline' <pre><code>\"files.insertFinalNewline\": true,\n\"files.trimFinalNewlines\": true,\n</code></pre></p>"},{"location":"Coding/VSCode/Config/#trim-trailing-whitespaces","title":"Trim trailing whitespaces","text":"<p>user settings &gt; Trim Trailing Whitespace <pre><code>\"files.trimTrailingWhitespace\": true,\n</code></pre></p>"},{"location":"Coding/VSCode/Config/#wrap-line","title":"Wrap line","text":"<p>workspace &gt; settings.json <pre><code>\"editor.rulers\": [80, 88, 120],\n\"editor.wordWrap\": \"wordWrapColumn\",\n\"editor.wordWrapColumn\": 120,\n\"[markdown]\": {\n    \"editor.wordWrap\": \"wordWrapColumn\",\n    \"editor.wordWrapColumn\": 120\n}\n</code></pre></p>"},{"location":"Coding/VSCode/Config/#vertical-rulers","title":"Vertical rulers","text":"<p><code>^,</code> user &gt; settings.json &gt; search for <code>ruler</code> <pre><code>\"editor.minimap.enabled\": false,\n\"editor.rulers\": [80,88],\n\"workbench.colorCustomizations\": {\n    \"editorRuler.foreground\": \"#333333\",\n    \"tab.activeBorderTop\": \"#066794\",\n    \"tab.unfocusedActiveBorder\": \"#000000\"\n},\n</code></pre></p>"},{"location":"Coding/VSCode/Config/#active-tab-settings","title":"Active tab settings","text":"<p>Search \"workbench.action.openSettingsJson\" &gt; Open user settings.json <pre><code>\"editor.rulers\": [80,88],\n\"workbench.colorCustomizations\": {\n    \"editorRuler.foreground\": \"#312f30\",\n    \"tab.inactiveForeground\":\"#ECECEC\",\n    \"tab.activeBackground\": \"#8f6155\",\n    \"tab.activeBorderTop\": \"#007acc\",\n    \"tab.activeBorder\": \"#ff0000\",\n    \"tab.unfocusedActiveBorder\": \"#000000\"\n},\n</code></pre></p>"},{"location":"Coding/VSCode/Config/#copy-relative-filepath-separator-setting","title":"Copy relative filepath separator setting","text":"<p>Search <code>Explorer: Copy Relative Path Separator</code> in settings</p>"},{"location":"Coding/VSCode/Config/#auto-closing-quotesbrackets","title":"Auto closing quotes/brackets","text":"<pre><code>\"editor.autoClosingQuotes\": \"never\",\n\"editor.autoClosingBrackets\": \"never\",\n</code></pre>"},{"location":"Coding/VSCode/Config/#disable-spell-check-in-problems","title":"disable spell check in problems","text":"<p>in <code>settings.json</code> add: <pre><code>\"cSpell.diagnosticLevel\": \"Hint\",\n</code></pre> For version &gt;= 4.0, a better solution: <pre><code>\"cSpell.useCustomDecorations\": true,\n\"cSpell.textDecoration\": \"underline wavy #bbbbbb66\", //\"underline wavy #fc9867 auto\", //\n</code></pre></p>"},{"location":"Coding/VSCode/Config/#disable-inline-values-in-debugger","title":"disable inline values in debugger","text":"<p>It's annoying that Python debugger in VS Code is showing inline variable values (faded text next to your code while debugging) <pre><code>\"debug.inlineValues\": \"off\"\n</code></pre></p>"},{"location":"Coding/VSCode/Config/#dockerfile","title":"dockerfile","text":"<p>mark files with some extension as docker file <pre><code>{\n  \"files.associations\": {\n        \"*.docker\": \"dockerfile\"\n    }\n}\n</code></pre></p>"},{"location":"Coding/VSCode/Regex/","title":"Regex","text":"<p>https://learn.microsoft.com/en-us/visualstudio/ide/using-regular-expressions-in-visual-studio?view=vs-2022</p>"},{"location":"Coding/VSCode/Regex/#patterns","title":"patterns","text":"<ul> <li><code>\\d</code> matches decimal digit character</li> <li><code>\\s+</code> catches at least one space</li> <li><code>\\b</code> words with a boundary</li> <li><code>\\w+</code> gives us a word</li> <li><code>\\r?\\n</code> line break</li> </ul>"},{"location":"Coding/VSCode/Regex/#examples","title":"examples","text":"<p><code>^\\d+$\\n</code> matches number in a separate line</p>"},{"location":"Coding/VSCode/Search/","title":"Search","text":""},{"location":"Coding/VSCode/Search/#advanced","title":"advanced","text":"<p>https://code.visualstudio.com/docs/editor/codebasics#_advanced-search-options</p>"},{"location":"Coding/VSCode/Shortcuts/","title":"Shortcuts","text":""},{"location":"Coding/VSCode/Shortcuts/#open-command-palette","title":"open command palette","text":"<p><pre><code>Ctrl + Shift + P\n</code></pre> Then search for what you need</p>"},{"location":"Coding/VSCode/Shortcuts/#open-hide-sidebarsecondary-panel","title":"open hide sidebar/secondary panel","text":"<pre><code>Ctrl + B         #open/hide sidebar\nCtrl + Alt + B   #open/hide secondary panel\n</code></pre>"},{"location":"Coding/VSCode/Shortcuts/#open-keybindingsjson","title":"open <code>keybindings.json</code>","text":"<p>This file contains your custom keybindings. I - Open the Command Palette by pressing <code>Ctrl + Shift + P</code> - Type <code>Open Keyboard Shortcuts (JSON)</code> and select the corresponding command</p>"},{"location":"Coding/VSCode/Shortcuts/#search-text","title":"search text","text":"<pre><code>Ctrl + F         # search in the current file\nCtrl + Shift + F # search in all files\n</code></pre>"},{"location":"Coding/VSCode/Shortcuts/#select-current-word","title":"select current word","text":"<pre><code>Ctrl + D\nCtrl + Shift + Arrow # select more\n</code></pre>"},{"location":"Coding/VSCode/Shortcuts/#select-current-line","title":"select current line","text":"<p><pre><code>Ctrl + L\n</code></pre> When using <code>Ctrl + C</code> without any selection the line where the cursor is will be copied.</p>"},{"location":"Coding/VSCode/Shortcuts/#copy-line-without-newline","title":"copy line without newline","text":"<p>option 1: - Got to Settings - Search for \"Editor: Copy with Syntax Highlighter\" and uncheck it</p> <p>option 2: using <code>macros</code> - install extensions <code>macros</code> - create a macro by adding it to <code>settings.json</code> <pre><code>\"macros.list\": {\n    \"copyWithoutNewLine\": [\n        \"cursorHome\",\n        \"cursorEndSelect\",\n        \"editor.action.clipboardCopyAction\",\n        \"cancelSelection\",\n        \"cursorUndo\",\n        \"cursorUndo\",\n        \"cursorUndo\"\n    ]\n}\n</code></pre> - add the macro to <code>keybindings.json</code> <pre><code>{\n    \"key\": \"ctrl+alt+c\",\n    \"command\": \"macros.copyWithoutNewLine\",\n    \"when\": \"editorTextFocus &amp;&amp; !editorHasSelection\"\n}\n</code></pre></p>"},{"location":"Coding/VSCode/Shortcuts/#cut-line","title":"cut line","text":"<pre><code>Ctrl + X\n</code></pre>"},{"location":"Coding/VSCode/Shortcuts/#delete-line","title":"delete line","text":"<pre><code>Ctrl + Shift + K\n</code></pre>"},{"location":"Coding/VSCode/Shortcuts/#insert-line","title":"insert line","text":"<pre><code>Ctrl + Enter          #insert below\nCtrl + Shift + Enter  #insert above\n</code></pre>"},{"location":"Coding/VSCode/Shortcuts/#movecopy","title":"move/copy","text":"<pre><code>Alt + Up/Down         # Move line up/down\nShift + Alt + Up/Down # Copy line up/down\n</code></pre>"},{"location":"Coding/VSCode/Shortcuts/#commentuncomment-code","title":"comment/uncomment code","text":"<pre><code>Ctrl + /\n</code></pre>"},{"location":"Coding/VSCode/Shortcuts/#column-selection","title":"column selection","text":"<pre><code>Ctrl + Shift + Alt + Down/Up/left/Right/PageUp/PageDown\n</code></pre>"},{"location":"Coding/VSCode/Shortcuts/#go-to-line","title":"go to line","text":"<pre><code>Home     #beginning of line\nEnd      #end of line\nCtrl + G #go to line\n</code></pre>"},{"location":"Coding/VSCode/Shortcuts/#go-to-error","title":"go to error","text":"<pre><code>F8         #next error/warning\nShift + F8 #previous error/warning\n</code></pre>"},{"location":"Coding/VSCode/Shortcuts/#new-terminal-window","title":"new terminal window","text":"<pre><code>Ctrl + J\n</code></pre>"},{"location":"Coding/VSCode/Shortcuts/#folding","title":"folding","text":"<p>mouse over the class or function, the folding arrows will be shown. - <code>Ctrl + Shift + [</code>: Fold the innermost uncollapsed region at the cursor - <code>Ctrl + Shift + ]</code>: unfolds the collapsed region at the cursor - <code>Ctrl + K, Ctrl + 0</code>: folds all regions in the editor. <code>Ctrl+K, Ctrl+[n]</code> where n is the level number upto which to fold - <code>Ctrl + K, Ctrl + J</code>: unfolds all regions in the editor.</p>"},{"location":"Coding/VSCode/Terminal/","title":"Terminal","text":""},{"location":"Coding/VSCode/Terminal/#troubleshoot-terminal-launch-failures","title":"troubleshoot terminal launch failures","text":"<p>https://code.visualstudio.com/docs/supporting/troubleshoot-terminal-launch</p>"},{"location":"Coding/VSCode/Terminal/#shell-integration","title":"shell integration","text":"<p>https://code.visualstudio.com/docs/terminal/shell-integration</p> <p>Manual installation (add to .bashrc): <pre><code>[[ \"$TERM_PROGRAM\" == \"vscode\" ]] &amp;&amp; . \"$(code --locate-shell-integration-path bash)\"\n</code></pre></p> <p>Better to copy the <code>bash</code> output to a shell script file to enhance loading performance. <pre><code># Output the executable's path first:\ncode --locate-shell-integration-path bash\n\n# Add the result of the above to the source statement:\n[[ \"$TERM_PROGRAM\" == \"vscode\" ]] &amp;&amp; . \"/path/to/shell/integration/script.sh\"\n</code></pre></p>"},{"location":"Coding/VSCode/Terminal/#show-branch-in-color-in-ubuntu-bash","title":"show branch in color in ubuntu bash","text":"<p>https://askubuntu.com/questions/730754/how-do-i-show-the-git-branch-with-colours-in-bash-prompt - Add these lines in your <code>~/.bashrc</code> file - Reload the <code>.bashrc</code> file with this command: <code>source ~/.bashrc</code> <pre><code># Show git branch name\nforce_color_prompt=yes\ncolor_prompt=yes\nparse_git_branch() {\n   git branch 2&gt; /dev/null | sed -e '/^[^*]/d' -e 's/* \\(.*\\)/(\\1)/'\n}\nif [ \"$color_prompt\" = yes ]; then\n   PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[01;31m\\]$(parse_git_branch)\\[\\033[00m\\]\\$ '\nelse\n   PS1='${debian_chroot:+($debian_chroot)}\\u@\\h:\\w$(parse_git_branch)\\$ '\nfi\nunset color_prompt force_color_prompt\n</code></pre></p>"},{"location":"Coding/VSCode/Terminal/#git-bash-terminal-auto-activate-conda-env","title":"git bash terminal auto activate conda env","text":"<p>https://github.com/microsoft/vscode-python/discussions/20800 - where the command is from? not answered - the path for the <code>activate</code> command is in windows path format, not correct - but can disable conda env auto activation (not good, env is base): <code>\"python.terminal.activateEnvironment\": false</code></p>"},{"location":"Coding/VSCode/VSCode/","title":"VSCode","text":"<p>There are 4 levels of settings in VS Code, which in ascending order of priority are: Default, User, Workspace, and Workspace Folder.</p> <p>Ctrl+Shift+P -&gt; python: select interpreter</p> <p>VS Code Conda issue fix:   - Disable activating terminal automatically, \"python.terminal.activateEnvironment\": false, and exit VSCode   - Open command prompt or power shell outside of VSCode.   - Navigate to your project or workspace directory.   - Activate conda there.   - Launch VSCode from the activated conda environment using code . or code project.code-workspace</p> <p>in launch.json of folder .vscode, add [this will ensure the parent path is added to the path search list]   \"env\": {\"PYTHONPATH\": \"${workspaceRoot}\"},</p>"},{"location":"Coding/Vim/Change/","title":"Change","text":""},{"location":"Coding/Vim/Change/#vi","title":"vi","text":"<pre><code>esc  -&gt; back to command mode\no    -&gt; create a line below and insert\nO    -&gt; create a line above and insert\ni    -&gt; insert characters before cursor\nI    -&gt; insert characters at line front\na    -&gt; append characters after cursor\nA    -&gt; append characters at line end\nr    -&gt; replace character\nR    -&gt; replace more than one character from the cursor\nx    -&gt; delete character\n:q!  -&gt; force quit without change\n:wq! -&gt; force save and quit\n</code></pre>"},{"location":"Coding/Vim/Change/#cut-and-paste-line","title":"cut and paste line","text":"<pre><code>dd  -&gt; cut current line\np   -&gt; paste after the current line\n</code></pre>"},{"location":"Coding/Vim/Change/#indentation","title":"indentation","text":"<ul> <li>Press <code>V</code> to start selecting in block mode</li> <li>Move cursor to select more lines</li> <li><code>&lt;</code> or <code>&gt;</code> to de-indent or indent the selected rows</li> </ul>"},{"location":"Coding/Vim/Change/#set-tabwidth-in-vimrc","title":"set tabwidth in <code>.vimrc</code>","text":"<pre><code>set expandtab      \" Use spaces instead of tabs\nset tabstop=2      \" Set the width of a tab to 2 spaces\nset shiftwidth=2   \" Amount to indent when using `&gt;` or `&lt;` commands\n</code></pre>"},{"location":"Coding/Vim/Change/#undo","title":"undo","text":"<pre><code>u    -&gt; undo the last change\n2u   -&gt; undo the two last changes\nU    -&gt; undo a whole line\n-r   -&gt; redo the last undo\n^R   -&gt; redo (undo the undo's)\n</code></pre>"},{"location":"Coding/Vim/Copy/","title":"Copy and Paste","text":""},{"location":"Coding/Vim/Copy/#paste-from-clipboad","title":"paste from clipboad","text":"<p>Keep source format <pre><code>:set paste   #before\n:set nopaste #after\n</code></pre></p>"},{"location":"Coding/Vim/Copy/#copy-and-paste_1","title":"copy and paste","text":"<pre><code>v  -&gt; highlight one character at a time\nV  -&gt; highlight one line at a time\nShift+V -&gt; highlight current line\n^v -&gt; highlight by columns\ny  -&gt; yank text into the copy buffer (copy)\nyw -&gt; copy one word without v\nyy -&gt; copy current line without v\np  -&gt; paste text after the current line\nP  -&gt; paste text on the current line\n</code></pre>"},{"location":"Coding/Vim/Delete/","title":"Delete","text":""},{"location":"Coding/Vim/Delete/#delete-word","title":"delete word","text":"<pre><code>dw  -&gt; delete until the start of next word\nde  -&gt; delete after the end of current word\ndb  -&gt; delete to beginning of current word\nd2w -&gt; delete until start of 2ed word\n</code></pre>"},{"location":"Coding/Vim/Delete/#delete-line","title":"delete line","text":"<pre><code>d0  -&gt; delete to line front\nd$  -&gt; delete to line end\ndd  -&gt; delete current line\n2dd -&gt; delete 2 lines\n</code></pre>"},{"location":"Coding/Vim/Delete/#delete-file","title":"delete file","text":"<pre><code>:%d -&gt; delete the whole file\ndgg -&gt; delete to the beginning of the file\ndG  -&gt; delete to the end of the file\n</code></pre>"},{"location":"Coding/Vim/Delete/#change","title":"change","text":"<pre><code>ce  -&gt; delete to word end and insert\nc2w -&gt; delete to 3rd word front and insert\nc3e\nc0\nc$\n</code></pre>"},{"location":"Coding/Vim/File/","title":"file","text":""},{"location":"Coding/Vim/File/#save-and-quit","title":"save and quit","text":"<pre><code>esc  -&gt; back to command mode\n:w   -&gt; save to file\n:q!  -&gt; quit\n:wq! -&gt; save and quit\nZZ   -&gt; save and quit\n</code></pre>"},{"location":"Coding/Vim/File/#52-write-file","title":"5.2 write file","text":"<pre><code>:w filename   -&gt; write current file to filename\n:w            -&gt; :'&lt;,'&gt;w filename, write selection to file\n:!rm filename -&gt; delete file filename\n</code></pre>"},{"location":"Coding/Vim/File/#54-merge-files","title":"5.4 merge files","text":"<pre><code>:r filename   -&gt; read file to line below\n:r !ls        -&gt; read output of ls and put it below\n</code></pre>"},{"location":"Coding/Vim/Move/","title":"Move","text":"<p>Here\u2019s a handy tip: prefacing a movement command with a number will execute that movement multiple times. So, if you want to move up 6 lines, enter 6k and Vim will move the cursor up 6 lines. If you want to move over 5 words, enter 5w. To move 10 words back, use 10b.</p>"},{"location":"Coding/Vim/Move/#char","title":"char","text":"<pre><code>h -&gt; left\nl -&gt; right\nk -&gt; up\nj -&gt; down\n</code></pre>"},{"location":"Coding/Vim/Move/#word","title":"word","text":"<pre><code>w  -&gt; move forward one word\nb  -&gt; move backward one word\nde -&gt; move to end of current word\n2w -&gt; move to start of 2ed word\n3e -&gt; move to end of 3ed word\n</code></pre>"},{"location":"Coding/Vim/Move/#line","title":"line","text":"<pre><code>0  -&gt; moves to the beginning of the line\n^  -&gt; first non empty of the line\n$  -&gt; moves to the end of the line\n</code></pre>"},{"location":"Coding/Vim/Move/#file","title":"file","text":"<pre><code>gg -&gt; move to the beginning of the file\nG  -&gt; move to the end of the file\n#G -&gt; move to line#\n`. -&gt; move to the last edit\n^G -&gt;cursor location and file status\n</code></pre>"},{"location":"Coding/Vim/Search/","title":"Search and replace","text":""},{"location":"Coding/Vim/Search/#42-search","title":"4.2 search","text":"<pre><code>/text  -&gt; search for text in the document, going forward\n?text  -&gt; search for text in the document, going backwards\nn      -&gt; move the cursor to the next instance\nN      -&gt; move the cursor to the previous instance\nCTRL-O -&gt; go back\nCTRL-I -&gt; go forward\n</code></pre>"},{"location":"Coding/Vim/Search/#43-match","title":"4.3 match","text":"<pre><code>% -&gt;find a matching ), ], or }\n</code></pre>"},{"location":"Coding/Vim/Search/#44-substitute","title":"4.4 substitute","text":"<pre><code>:s/old/new      -&gt; substitute first 'new' for 'old' in the line\n:s/old/new/g    -&gt; substitute all 'new' for 'old' in the line\n:#,#s/old/new/g -&gt; substitute between line# and line#\n:8,9s/old/new/g -&gt;replace all in line 8 to line 9\n:%s/old/new/g   -&gt; substitute all in the file\n:%s/old/new/gc  -&gt; substitute all in the file with confirmation\nreplace with foo (y/n/a/q/l/^E/^Y)? #q-quit, l-stop, ^e-scroll up, ^y down\n</code></pre>"},{"location":"Coding/Vim/Vim/","title":"vim","text":""},{"location":"Coding/Vim/Vim/#51-execute-command","title":"5.1 execute command","text":"<pre><code>:!cmd -&gt;execute cmd\n:!ls  -&gt;execute ls\n</code></pre>"},{"location":"Coding/Vim/Vim/#65-set-option","title":"6.5 set option","text":"<pre><code>:set paste\n:set nopaste\n:set ic     -&gt;ignore case\n:set noic   -&gt;disable ignoring case\n:set hls is -&gt;highlight search and incremental search\n:nohlsearch -&gt;remove highligting of searches\n/ignore\\c   -&gt;ignore case only in current search\n:set number -&gt;line number\n:set nonumber\n</code></pre>"},{"location":"Coding/Vim/Vim/#syntax-highlight","title":"syntax highlight","text":"<p>put it in .vimrc <pre><code>:sy enable\n:syn enable\n:syntax enable\n</code></pre></p>"},{"location":"Coding/Vim/Vim/#71-online-help","title":"7.1 online help","text":"<pre><code>&lt;F1&gt; key\n:help\nCTRL-W CTRL-W   -&gt;to jump from one window to another\n:q              -&gt;close help window\n:help w\n:help c_CTRL-D\n:help insert-index\n:help user-manual\n</code></pre>"},{"location":"Coding/Vim/Vim/#book","title":"book","text":"<ul> <li>Vim - Vi Improved - by Steve Oualline</li> <li>Learning the Vi Editor - by Linda Lamb</li> </ul>"},{"location":"Coding/Web/Apache/","title":"apache","text":""},{"location":"Coding/Web/Apache/#install","title":"install","text":"<pre><code>cd C:\\Apache\\Apache2.2\\bin\nhttpd.exe -k install -n \"Apache server\"\n</code></pre>"},{"location":"Coding/Web/Apache/#uninstall","title":"uninstall","text":"<pre><code>cd C:\\Apache\\Apache2.2\\bin\nhttpd.exe -k uninstall -n \"Apache server\"\n</code></pre>"},{"location":"Coding/Web/Apache/#restart","title":"restart","text":"<pre><code>cd C:\\Apache\\Apache2.2\\bin\nhttpd.exe -k restart\n</code></pre>"},{"location":"Coding/Web/Apache/#change-port","title":"change port","text":"<pre><code>Listen 8080\n</code></pre>"},{"location":"Coding/Web/Apache/#apache-does-not-run-php-in-httpdconf","title":"apache does not run .php, in httpd.conf","text":"<pre><code>AddType application/x-httpd-php .php\nLoadModule php7_module \"C:\\php\\php7apache2_4.dll\"\n</code></pre>"},{"location":"Coding/Web/UI/","title":"UI","text":""},{"location":"Coding/Web/UI/#kendo-ui-for-jquery-grid-overview","title":"Kendo UI for jQuery Grid Overview","text":"<p>https://demos.telerik.com/kendo-ui/grid/index</p> <p>The Kendo UI for jQuery Grid component renders data in a <code>tabular format</code> and supports  a vast range of powerful data management and customization features such as filtering, grouping, sorting, editing, and many more.</p>"},{"location":"Coding/Windows/Cert/","title":"Cert","text":""},{"location":"Coding/Windows/Cert/#show-all-certs","title":"show all certs","text":"<pre><code>Get-ChildItem -Path cert:\\LocalMachine\\Root\n</code></pre>"},{"location":"Coding/Windows/Cert/#add-cert-to-windows-cert-store","title":"add cert to windows cert store","text":"<pre><code>Import-Certificate -CertStoreLocation cert:\\LocalMachine\\Root -FilePath C:\\Path\\To\\Your\\Certificate.crt \n</code></pre>"},{"location":"Coding/Windows/CredentialManager/","title":"Credential Manager","text":""},{"location":"Coding/Windows/CredentialManager/#add-git-credential-to-win-credentail-manager","title":"Add git credential to Win Credentail Manager","text":"<p>https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/cmdkey <pre><code>#can only change the pwd temporally, delete it\nrundll32.exe keymgr.dll, KRShowKeyMgr\n\n#not work\ncmdkey /list #show all credentials\ncmdkey /generic:LegacyGeneric:target=git:https://github.com /user:username /pass:mypassword #update pwd\n</code></pre></p>"},{"location":"Coding/Windows/EnvVar/","title":"Env Var","text":""},{"location":"Coding/Windows/EnvVar/#list-all-env-vars","title":"list all env vars","text":"<pre><code>set\n</code></pre> <pre><code>$env:Path -split ';'\n</code></pre>"},{"location":"Coding/Windows/EnvVar/#env-vars-set-from-command","title":"env vars set from command","text":"<p>These env will not be shown in the UI <code>Computer\\HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor &gt; AutoRun</code></p>"},{"location":"Coding/Windows/FileSys/","title":"FileSys","text":""},{"location":"Coding/Windows/FileSys/#find-file-or-folder-locked-by-which-process","title":"Find file or folder locked by which process","text":"<pre><code>resmon.exe \n&gt; CPU \n&gt; Use the search field in the Associated Handles section \n&gt; End the process\n</code></pre>"},{"location":"Coding/Windows/Firewall/","title":"Firewall","text":""},{"location":"Coding/Windows/Firewall/#open-firewall","title":"Open firewall","text":"<pre><code>control firewall.cpl\n</code></pre>"},{"location":"Coding/Windows/Firewall/#open-windows-firewall-with-advanced-security","title":"Open Windows firewall with Advanced Security","text":"<p>Firewall and Network Protecttion</p>"},{"location":"Coding/Windows/Network/","title":"Network","text":"<pre><code># show ip info\nipconfig /all\n# ping xxx.example.com\nping xxx.example.com\n# check domain name resolution\nnslookup xxx.example.com\n</code></pre>"},{"location":"Coding/Windows/RDP/","title":"RDP","text":""},{"location":"Coding/Windows/RDP/#open-terminal-in-remote-desktop","title":"open terminal in remote desktop","text":"<pre><code>remoted into the machine use Ctrl + Alt + End instead of Ctrl + Alt + Del\npress Ctrl + Shift + Esc simultaneously to bring up the Task Manager - Then, File -&gt; Run New Task -&gt; cmd/powershell\nlogoff\n</code></pre>"},{"location":"Coding/Windows/SchdTask/","title":"Scheduled Task","text":""},{"location":"Coding/Windows/SchdTask/#create-a-shceduled-task","title":"create a shceduled task","text":"<ul> <li>taskschd.msc</li> <li>actions panel -&gt; Create Task -&gt; General</li> <li>Triggers</li> </ul> <p>When set the scheduled task to run regardless of the user logged in, the optional 'start in' in action must be set. Otherwise it shows that the task run successfully but nothing happened. The \"start in\" is mainly to make sure that if you have relative paths in the task to run it understands which directory to run the script in.</p> <p>Scheduled task cannot access network drive mapping, so mapped address must be changed to a network pass, e.g. '%%//server/share/%%' is mapped to 'I:', to access the file in 'I:' we should use '%%//server/share/myfile.csv%%'.</p>"},{"location":"Coding/Windows/SchdTask/#priority","title":"priority","text":"<p>how to change the priority of scheduled task: - Create the task, Right click on the task and \"export\" it - Edit the task.xml file that you just exported, - You will find a line similar to 7, - Change the value to a normal priority (between 4-6) - A value of 4 will have the same I/O and memory priority as an interactive process. - 0-real time; 1-high; 2,3-above normal - Values of 5 and 6 will have lower memory priority - In the task scheduler, delete the task you initially created - In the task scheduler, in the actions area, import the task from the XML file</p>"},{"location":"Coding/Windows/VPN/","title":"VPN","text":"<p>VPN connection can be blocked by company firewall.</p>"},{"location":"Coding/Windows/VPN/#openconnect","title":"OpenConnect","text":"<p>The group name should be put in the authgroup. </p> <p>Decrease the value of base-mtu if get error: Failed to write to SSL socket: The transmitted packet is too large.</p> <p>Limitation: It seems OpenConnect CLI does not provide the method to get the VPN connection status in Windows. <pre><code>openconnect.exe my.svr.com --authgroup my-grp --protocol=anyconnect -u usr --base-mtu=1000 -q --passwd-on-stdin\n</code></pre></p>"},{"location":"Coding/Windows/VPN/#cisco-mobile-vpn-client","title":"Cisco Mobile VPN Client","text":"<pre><code>vpncli connect my.svr.com grp usr pwd y\nvpncli disconnect\nvpncli status\n</code></pre>"},{"location":"Coding/Windows/WSL/","title":"WSL","text":""},{"location":"Coding/Windows/WSL/#task-manager","title":"task manager","text":"<p>https://stackoverflow.com/questions/51364707/how-does-windows-10-task-manager-detect-a-virtual-machine</p> <p>task manager - performance will show your machine is running as <code>virtualization</code> or <code>virtual machine</code>.  To install wsl2, it should be virtualization.</p>"},{"location":"Coding/Windows/WSL/#check-if-intel-virtualization-is-enabled","title":"check if Intel Virtualization is enabled","text":"<p><pre><code>Get-ComputerInfo -property \"HyperV*\"\n</code></pre> To check if virtualization is enabled in the BIOS (Basic Input/Output System) of your computer, you'll need to follow these general steps. Keep in mind that the exact steps might vary slightly depending on your computer's manufacturer and model. Here's a basic guide:</p> <ol> <li>Access the BIOS/UEFI:</li> <li>Reboot your computer.</li> <li> <p>During the boot process, press the key to access the BIOS or UEFI. Common keys include F2, F10, F12, ESC, or DEL. The correct key is usually displayed on the screen during the boot process.</p> </li> <li> <p>Navigate to the Virtualization Setting:</p> </li> <li>Once in the BIOS/UEFI, use the arrow keys to navigate through the menus.</li> <li> <p>Look for a section related to \"Advanced,\" \"CPU Configuration,\" \"Advanced CPU Configuration,\" or something similar.</p> </li> <li> <p>Find Virtualization Technology:</p> </li> <li> <p>Within the advanced settings, look for an option called \"Virtualization Technology,\" \"Intel VT,\" \"Intel Virtualization Technology,\" \"AMD-V,\" or something similar. The exact wording might differ based on your CPU manufacturer (Intel or AMD).</p> </li> <li> <p>Enable Virtualization:</p> </li> <li> <p>If you find the virtualization option, make sure it is set to \"Enabled.\" Use the arrow keys to navigate to the option, and change the setting if necessary.</p> </li> <li> <p>Save and Exit:</p> </li> <li> <p>After enabling virtualization, navigate to the option to save your changes and exit the BIOS/UEFI. This is typically done by selecting \"Save &amp; Exit\" or a similar option. Confirm that you want to save the changes.</p> </li> <li> <p>Reboot:</p> </li> <li> <p>Allow the computer to reboot for the changes to take effect.</p> </li> <li> <p>Verify Virtualization:</p> </li> <li>Once your computer has restarted, you can verify if virtualization is enabled by using a system information tool or checking within the operating system.</li> </ol> <p>Keep in mind that not all computers support virtualization, and the option might not be available in the BIOS/UEFI on some systems. If you're unable to find the virtualization option or are unsure, consult your computer's documentation or the manufacturer's website for specific instructions.</p>"},{"location":"Coding/Windows/WSL/#enable-windows-features","title":"enable windows features","text":"<ul> <li>Virtual Machine Platform</li> <li>Windows Subsystem for Linux</li> </ul>"},{"location":"Coding/Windows/WSL/#install","title":"install","text":"<pre><code>wsl -l -o #show available Linux distributions\nwsl --install -d &lt;Distribution Name&gt; #install distribution\nwsl -l -v #show installed Linux distributions\n</code></pre>"},{"location":"Coding/Windows/WSL/#config-to-limit-memory","title":"config to limit memory","text":"<p>https://learn.microsoft.com/en-us/windows/wsl/wsl-config#configuration-setting-for-wslconfig - <code>vi editor \"$(wslpath \"C:\\Users\\&lt;usr&gt;\\.wslconfig\")\"</code> add to <code>.wslconfig</code> file (unix newline):  <pre><code>[wsl2]\nmemory=4GB\n</code></pre> - <code>wsl --shutdown</code> #restart - <code>free -h --giga</code> #check total memory</p>"},{"location":"Coding/Windows/WSL/#release-disk-space-back","title":"release disk space back","text":"<p>https://github.com/microsoft/WSL/issues/4699</p> <p>WSL2 will not automatically release used disk space - have to do it manually</p>"},{"location":"Coding/Windows/WSL/#compact-option-in-diskpart","title":"compact option in diskpart","text":"<pre><code>wsl --shutdown\ndiskpart\n# open window Diskpart\nselect vdisk file=\"C:\\Users\\&lt;user&gt;\\AppData\\Local\\Packages\\CanonicalGroupLimited.Ubuntu20.04onWindows_79rhkp1fndgsc\\LocalState\\ext4.vhdx\"\nattach vdisk readonly\ncompact vdisk\ndetach vdisk\nexit\n</code></pre>"},{"location":"Coding/Windows/WSL/#optimize-vhd","title":"optimize-vhd","text":"<p>only available in Windows 10 Pro with Hyper-v feature installed <pre><code>cd C:\\Users\\&lt;usr&gt;\\AppData\\Local\\Packages\\CanonicalGroupLimited.Ubuntu20.04onWindows_79rhkp1fndgsc\\LocalState\nwsl --shutdown\noptimize-vhd -Path .\\ext4.vhdx -Mode full\n</code></pre></p>"},{"location":"Coding/Windows/WSL/#install-docker","title":"install docker","text":"<p>https://dev.to/bowmanjd/install-docker-on-windows-wsl-without-docker-desktop-34m9</p>"},{"location":"Coding/Windows/WSL/#system-has-not-been-booted-with-systemd-as-init-system-pid-1-cant-operate","title":"System has not been booted with systemd as init system (PID 1). Can't operate.","text":"<pre><code>#do not use\nsudo systemctl status docker\n#instead\nsudo service docker status\nsudo service docker start\n#get docker-server-name\nservice --status-all\n</code></pre>"},{"location":"Coding/Windows/WSL/#auto-start-cron-in-wsl","title":"Auto start cron in wsl","text":"<p>https://www.howtogeek.com/746532/how-to-launch-cron-automatically-in-wsl-on-windows-10-and-11/</p>"},{"location":"Coding/Windows/WSL/#change-wsl-settings-in-wsl2-terminal","title":"Change wsl settings in wsl2 terminal","text":"<p>Disable password requirement for starting cron service - run: <code>sudo visudo</code> - add: <code>%sudo&lt;tab not space&gt;ALL=NOPASSWD: usr/sbin/service cron start</code> - add an empty at the end</p>"},{"location":"Coding/Windows/WSL/#create-a-scheduled-task-in-windows","title":"Create a scheduled task in windows","text":"<ul> <li>taskschd.msc</li> <li>Actions &gt; Create Basic Task</li> <li>Triggers: When the computer starts</li> <li>Action: Start a program: <code>C:\\Windows\\System32\\wsl.exe</code>, args: <code>sudo /usr/sbin/service cron start</code></li> <li>Finish: Run whether the user logged on or not</li> </ul>"},{"location":"Coding/Windows/WSL/#clock-is-out-of-sync-with-windows","title":"Clock is out of sync with Windows","text":"<p>temporal solution (need to run it after each restart) <pre><code>sudo hwclock -s # not work\nsudo ntpdate pool.ntp.org\n</code></pre></p>"},{"location":"Coding/Windows/Win/","title":"windows","text":""},{"location":"Coding/Windows/Win/#exclude-a-folder-from-indexing","title":"exclude a folder from indexing","text":"<p>indexing options -&gt; modify</p>"},{"location":"Coding/Windows/Win/#exclude-a-exe-from-anti-malware","title":"exclude a exe from anti-malware","text":"<p>win + i -&gt; update &amp; security -&gt; windows security -&gt; virus &amp; threat protection -&gt; exclusions -&gt; MsMpEng.exe</p>"},{"location":"Coding/Windows/Win/#change-pwd-in-remote-machine","title":"Change PWD in remote machine","text":"<p>when remote to that machine using CRTL+ALT+END to change your password</p>"},{"location":"Coding/Windows/Win/#enable-remote-access-for-an-account","title":"enable remote access for an account","text":"<p>Add Remote Desktop Users in Local Users and Groups: lusrmgr.msc\\ groups: remote desktop users, add</p>"},{"location":"Coding/Windows/Win/#explorerexe-eats-memory","title":"explorer.exe eats memory","text":"<p>3rd party Explorer shell extensions are the normal cause. There are utilities like ShellEXView to track these down.</p> <p>On a File Explorer Window if you click: File -&gt; \"Change Folder and Search Options\", there's a checkbox in \"Adbanced settings\" called \"Launch Folder Windows in a separate process\" - that may limit any memory leaks to the Video folder windows.</p>"},{"location":"Coding/Windows/Win/#check-for-administrator-rights-using-command-prompt","title":"Check for administrator rights using Command Prompt","text":"<p>net user [username]</p> <p>Computer Management -&gt; user and groups</p>"},{"location":"Coding/Windows/Win/#ring-fence-network-folder","title":"ring-fence network folder","text":"<p>note we do this through group membership though and windows only refreshes that on login (ie you may need to log out and in to get access back).</p>"},{"location":"Coding/Windows/Win/#remote-desk-setting","title":"remote desk setting","text":"<p>run mstsc. Click Show Options and select Advanced. Click Settings and select Use these RD Gateway server settings and enter exchange.rxxcxx.com.au as the server name.  Other settings can be left as default, but you may want to check \u201cUse my RD Gateway credentials for the remote computer\u201d on this dialog as well to avoid another prompt.</p>"},{"location":"Coding/Windows/Win/#change-remote-machine-password","title":"Change remote machine password","text":"<p>Press CRTL+ALT+END</p>"},{"location":"Coding/Windows/Win/#check-cpu-cores","title":"check cpu cores","text":"<p>msinfo32</p>"},{"location":"Coding/Windows/WindowsServer/","title":"Windows Server","text":""},{"location":"Coding/Windows/WindowsServer/#remote-to-windows-server-core-serve","title":"Remote to Windows Server core serve","text":""},{"location":"Coding/Windows/WindowsServer/#open-cmd","title":"Open cmd","text":"<pre><code>remoted into the machine use Ctrl + Alt + End instead of Ctrl + Alt + Del\npress Ctrl + Shift + Esc simultaneously to bring up the Task Manager -  Then, File -&gt; Run New Task -&gt; cmd/powershell\n</code></pre>"},{"location":"Coding/Windows/splitlargefile/","title":"Split large file","text":""},{"location":"Coding/Windows/splitlargefile/#git-bash","title":"Git Bash","text":"<pre><code>cd /c//tmp\nsplit file.csv -b 500m\nsplit file.csv file -b 500m -a 3 -d\nsplit file.csv file -l 1000 -a 3 -d\n</code></pre>"},{"location":"Cybersecurity/Cybersecurity/","title":"Cybersecurity","text":""},{"location":"Cybersecurity/Cybersecurity/#cybersecurity","title":"cybersecurity","text":"<ul> <li>https://www.cisecurity.org/controls/application-software-security</li> <li>https://guidehouse.com/-/media/www/site/insights/energy/2017/best-practices-for-utility-cybersecurity--final.pdf</li> </ul> <p>Layers of Defense: 1. Asset controls: Measures including server and desktop hardening, antivirus, and whitelisting to improve the resiliency of systems if attacked   - Antivirus   - Whitelisting   - Hardening 2. Information controls: Protections for information at rest and in transit from unauthorized access through data or communications encryption   - Encryption   - PKI   - SFTP 4. Network controls: Measures to manage and protect data transmission across networks, including managing the ability of different users to gain access to sensitive systems and data compliance with cybersecurity standards, and address any potential cybersecurity threats   - Firewall   - NIDS   - NBAD 3. Cybersecurity management controls: Tools and processes to monitor systems and networks, ensure continuous compliance with cybersecurity standards, and address any potential cybersecurity threats   - SEIM   - Patch Management   - Log Management</p>"},{"location":"Data/Data/","title":"Data","text":""},{"location":"Data/Data/#data-process","title":"data process","text":"<ul> <li>ingest</li> <li>store</li> <li>analyze</li> <li>visualize</li> </ul>"},{"location":"Data/Data/#learn","title":"learn","text":"<p>Data Quality Fundamentals</p>"},{"location":"Data/Data/#dataset","title":"dataset","text":"<p>https://www.datacamp.com/workspace/datasets</p> <p>online retail: https://www.datacamp.com/workspace/datasets/dataset-r-e-commerce</p>"},{"location":"Data/Learn/","title":"Learn","text":""},{"location":"Data/Learn/#data-science-for-beginners","title":"data science for beginners","text":"<p>https://microsoft.github.io/Data-Science-For-Beginners/#/</p>"},{"location":"Data/Learn/#machine-learning-for-beginners","title":"machine learning for beginners","text":"<p>https://microsoft.github.io/AI-For-Beginners/</p>"},{"location":"Data/Learn/#azure","title":"azure","text":"<p>https://azure.microsoft.com/mediahandler/files/resourcefiles/data-engineer-learning-journey/Azure%20Data%20Engineer%20Learning%20Path.pdf</p>"},{"location":"Data/Learn/#aws","title":"AWS","text":"<p>Redshift, Athena, EMR (Elastic MapReduce)</p>"},{"location":"Data/Tool/","title":"Tool","text":"<p>Databricks, DBT, and Kafka, SQL, Spark and Python</p>"},{"location":"Data/Tool/#databricks","title":"Databricks","text":"<p>Pyspark, Spark SQL, Cluster Management, query optimizations, performance tuning</p>"},{"location":"Data/Tool/#database-source","title":"[Database] Source","text":"<ul> <li>MySQL</li> <li>PostgreSQL</li> <li>NoSQL: MongoBD</li> </ul>"},{"location":"Data/Tool/#extraction-batch-and-streaming","title":"[Extraction] Batch and Streaming","text":"<ul> <li><code>Kafka</code> (streaming storage)</li> <li><code>Flink</code> (streaming computation)</li> </ul>"},{"location":"Data/Tool/#scheduling-and-orchestration","title":"Scheduling and Orchestration","text":"<ul> <li>Apache Airflow</li> <li><code>Argo</code> Kubernetes native</li> </ul>"},{"location":"Data/Tool/#loadingtransformation-processing-and-transformation","title":"[Loading/Transformation] Processing and Transformation","text":"<ul> <li> <p><code>Apache Hive</code> Hive is a distributed database which operates on Hadoop Distributed File System.</p> </li> <li> <p><code>Apache Spark</code> Spark is a distributed data framework which helps extract and process large volumes of data in RDD (Resilient Distributed Data) format for analytical purposes.</p> </li> <li> <p><code>dbt</code> dbt is an open-source data modeling and transformation tool, designed for analytics engineers and data analysts to transform data in their data warehouse (usually in SQL-based databases like Snowflake, BigQuery, or Redshift).</p> </li> </ul>"},{"location":"Data/Tool/#storage-data-warehouse","title":"[Storage] Data Warehouse","text":"<ul> <li> <p><code>Redshift</code> Redshift is a fully managed cloud warehouse by Amazon.</p> </li> <li> <p><code>BigQuery</code> BigQuery is a fully managed cloud warehouse by Google.</p> </li> <li> <p>Azure Warehouse</p> </li> <li>Delta Lake</li> <li>Snowflake</li> </ul>"},{"location":"Data/Tool/#application-data-analysis","title":"[Application] Data Analysis","text":"<ul> <li>PowerBI</li> <li>Looker (should avoid?) Looker is BI software that helps users visualize data.</li> </ul>"},{"location":"Data/Airflow/Airflow/","title":"Airflow","text":""},{"location":"Data/Airflow/Airflow/#a-complete-apache-airflow-tutorial-building-data-pipelines-with-python","title":"A complete Apache Airflow tutorial: building data pipelines with Python","text":"<p>https://theaisummer.com/apache-airflow-tutorial/</p>"},{"location":"Data/Airflow/Airflow/#airflow-vs-prefect-vs-argo","title":"Airflow vs. Prefect vs. Argo","text":"<ul> <li>https://huyenchip.com/2021/09/13/data-science-infrastructure.html</li> <li>https://neptune.ai/blog/argo-vs-airflow-vs-prefect-differences</li> <li>https://medium.com/arthur-engineering/picking-a-kubernetes-orchestrator-airflow-argo-and-prefect-83539ecc69b</li> </ul> <p>airflow - DAGs are not parameterized - you can't pass parameters into your workflows - DAGs are static - it can't automatically create new steps at runtime as needed - data workflows are defined using Python instead of YAML - package the entire workflow into one container</p> <p>prefect -  workflows are defined in Python, parameterized and dynamic -  can run each step in a container, but has container problem</p> <p>argo worflow - workflows are defined in YAML   - allow you to define each step and its requirements in the same file   - workflow definitions can become messy and hard to debug - every step in an workflow is run in its own container - can only run on Kubernetes clusters</p>"},{"location":"Data/Airflow/Airflow/#prefect-vs-temporal","title":"Prefect vs Temporal","text":"<ul> <li>https://linen.prefect.io/t/22927872/ulva73b9p-how-does-prefect-compare-to-temporal</li> <li>https://news.ycombinator.com/item?id=39210757</li> <li>https://community.temporal.io/t/what-are-the-pros-and-cons-of-temporal-with-respect-to-prefect/5671/3?u=loren</li> </ul> <p>Orchestration vs. Workflow Platform: - <code>Prefect</code> focuses primarily on data workflow orchestration. It's designed to run and monitor data pipelines, with a strong emphasis on ease of use and deployment, especially for complex DAGs (Direct Acyclic Graphs). - <code>Temporal</code> is a more general-purpose workflow as a code platform. It's designed for a wide variety of use cases, not just data workflows, and provides robust features for state management, retries, and long-running processes.</p> <p>State Management: - Prefect: Uses state management abstractions that allow for easy retries and failure handling within data workflows. - Temporal: Provides a more robust system for state management across complex workflows, supporting long-running activities and reliable execution across failures.</p> <p>Ecosystem and Integrations: - Prefect has built-in integrations with popular data engineering tools and platforms, such as Dask, DBT, and various cloud services. - Temporal has broader integrations suitable for various applications, extending beyond data engineering.</p> <p>Ease of Use: - Prefect is often praised for its user-friendly API, making it easier to define and orchestrate complex workflows using Python. - Temporal requires more investment in learning its model, but it offers strong guarantees and flexibility once mastered.</p>"},{"location":"Data/Airflow/Temporal/","title":"Temporal","text":""},{"location":"Data/Airflow/Temporal/#workflows-integrated-with-code","title":"workflows integrated with code","text":"<p>Temporal workflows are written as code, so they are integrated within your application's codebase.</p>"},{"location":"Data/Argo/API/","title":"API","text":"<p>Argo Server provides the API to programmatically auto tasks. - url: http://localhost:2746/api/v1 - info: http://localhost:2746/api/v1/info</p>"},{"location":"Data/Argo/API/#access-token","title":"access token","text":"<p>To allow the api to automate the tasks, an <code>access token</code>, which is a Kubernetes service account token, should be created: - a <code>service account</code> for the automation user:   <pre><code>kubectl create serviceaccount jenkins\n</code></pre> - a <code>role</code> with the right permission:   <pre><code>kubectl create role jenkins --verb=create,get,list \\\n--resource=workflows.argoproj.io --resource=workfloweventbindings --resource=workflowtemplates\n</code></pre> - a <code>role binding</code> to bind the role to the service account   <pre><code>kubectl create rolebinding jenkins --role=jenkins --serviceaccount=argo:jenkins\n</code></pre> - a <code>service account token</code> for the service account   <pre><code>ARGO_TOKEN=\"Bearer $(kubectl create token jenkins)\"\ncurl http://localhost:2746/api/v1/info -H \"Authorization: $ARGO_TOKEN\"\n</code></pre></p> <p>## use api   API endpoints use json as plyload   <pre><code>curl http://localhost:2746/api/v1/workflows/argo \\\n-H \"Authorization: $ARGO_TOKEN\" \\\n-H 'Content-Type: application/json' \\\n-d '{\n  \"workflow\": {\n    \"metadata\": {\n      \"generateName\": \"my-hello-wf\"\n    },\n    \"spec\": {\n      \"templates\": [\n        {\n          \"name\": \"main\",\n          \"container\": {\n            \"image\": \"docker/whalesay\",\n            \"command\": [\n              \"cowsay\"\n            ],\n            \"args\": [\n              \"hello\"\n            ]\n          }\n        }\n      ],\n      \"entrypoint\": \"main\"\n    }\n  }\n}'\n</code></pre></p>"},{"location":"Data/Argo/API/#webhook","title":"webhook","text":"<p>The endpoint that is specifically designed to create workflows via an api is <code>api/v1/events</code>. - only allowed to create workflows from a <code>WorkflowTemplate</code>, thus is more secure - only allowed to parse the HTTP payload and use it as parameters - only allowed to integrate with other systems without having to change those systems - also supports GitHub and GitLab, so can trigger workflow from git actions</p> <p>To use the webhook, a <code>WorkflowTemplate</code> and a <code>WorkflowEventBinding</code> should be created <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowEventBinding\nmetadata:\n  name: hello\nspec:\n  event:\n    selector: payload.message != \"\"\n  submit:\n    workflowTemplateRef:\n      name: hello\n    arguments:\n      parameters:\n        - name: message\n          valueFrom:\n            event: payload.message\n</code></pre></p>"},{"location":"Data/Argo/Argo/","title":"Argo","text":"<p>You have a running Kubernetes Cluster: use Argo.</p> <p>Argo has all configuration written in YAML and all tasks containerized running on Kubernetes.  Writing DAGs is completly independent of any programming language. </p>"},{"location":"Data/Argo/Argo/#argo-server","title":"Argo Server","text":"<p>And the Argo Server provides a user interface and API <pre><code>kubectl -n argo get deploy argo-server\n</code></pre></p>"},{"location":"Data/Argo/Argo/#workflow-controller","title":"Workflow Controller","text":"<p>The Workflow Controller is responsible for running workflows <pre><code>kubectl -n argo get deploy argo-workflow-controller\n</code></pre></p>"},{"location":"Data/Argo/Artifact/","title":"Artifact","text":"<ul> <li>https://argoproj.github.io/argo-workflows/walk-through/artifacts/</li> <li>https://argoproj.github.io/argo-workflows/configure-artifact-repository/</li> <li>https://argoproj.github.io/argo-workflows/key-only-artifacts/</li> <li>https://argoproj.github.io/argo-workflows/artifact-repository-ref/</li> </ul> <p>Artifact is a file that is compressed and stored in object storage. - An <code>input artifact</code> is a file downloaded from storage and mounted as a volume within the container. - An <code>output artifact</code> is a file created in the container that is uploaded to storage.</p>"},{"location":"Data/Argo/CLI/","title":"CLI","text":""},{"location":"Data/Argo/CLI/#install-cli","title":"install cli","text":"<p><pre><code>curl -sLO https://github.com/argoproj/argo-workflows/releases/download/v3.4.9/argo-linux-amd64.gz\ngunzip argo-linux-amd64.gz\nchmod +x argo-linux-amd64\nmv ./argo-linux-amd64 /usr/local/bin/argo\n</code></pre> check if installed correctly <code>argo version</code></p>"},{"location":"Data/Argo/CLI/#get-help","title":"get help","text":"<pre><code>argo --help\n</code></pre>"},{"location":"Data/Argo/CLI/#run-a-workflow","title":"run a workflow","text":"<pre><code>argo submit -n argo --watch https://raw.githubusercontent.com/argoproj/argo-workflows/master/examples/hello-world.yaml\n</code></pre>"},{"location":"Data/Argo/CLI/#list-workflows","title":"list workflows","text":"<pre><code>argo list -n argo\nargo get -n argo @latest  #get details about the latest one\nargo logs -n argo @latest #view logs\n</code></pre>"},{"location":"Data/Argo/Event/","title":"Event","text":""},{"location":"Data/Argo/Event/#install-argo-events","title":"Install Argo Events","text":"<p>Argo Events is normally installed into a namespace called <code>argo-events</code> <pre><code># create namespace\nkubectl create ns argo-events\n# create argo events\nkubectl apply -n argo-events -f https://github.com/argoproj/argo-events/releases/download/v1.8.0/install.yaml\n# create event bus where events will be sent to\nkubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/eventbus/native.yaml\n</code></pre></p>"},{"location":"Data/Argo/Event/#trigger-a-workflow-based-on-an-event","title":"Trigger a workflow based on an event","text":"<p>create a sensor to trigger the event <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Sensor\nmetadata:\n  name: my-sensor\nspec:\n  dependencies:\n    - name: example-dep\n      eventSourceName: my-sensor\n      eventName: example\n  triggers:\n    - template:\n</code></pre></p>"},{"location":"Data/Argo/Event/#troubleshoot-kubernetes-rbac-issues-related-to-argo-events-and-workflows","title":"Troubleshoot Kubernetes RBAC issues related to Argo Events and Workflows","text":"<p>rbac: https://kubernetes.io/docs/reference/access-authn-authz/rbac/</p> <p>The default <code>Service Account</code> in the <code>argo-events</code> namespace does not have permission to create workflows in the <code>argo</code> namespace.</p> <p>We will grant the default Service Account permission to create workflows using a <code>ClusterRole</code>,  and a <code>ClusterRoleBinding</code> to bind the role to the Service Account. <pre><code>kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: argo-events-create-wf-role\n  namespace: argo-events\nrules:\n- apiGroups: [\"argoproj.io\"]\n  resources: [\"workflows\"]\n  verbs: [\"create\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: argo-events-create-wf-role-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: argo-events-create-wf\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: argo-events\n</code></pre></p>"},{"location":"Data/Argo/Helm/","title":"Helm","text":"<p>https://github.com/argoproj/argo-helm/blob/main/charts/argo-workflows/values.yaml</p>"},{"location":"Data/Argo/Helm/#install-argo-using-helm","title":"install Argo using Helm","text":"<pre><code>helm repo add argo https://argoproj.github.io/argo-helm\nhelm repo update\nhelm install argo argo/argo-workflows --namespace argo --create-namespace\n</code></pre>"},{"location":"Data/Argo/Input/","title":"Input","text":"<p>https://argoproj.github.io/argo-workflows/workflow-inputs/</p>"},{"location":"Data/Argo/Learn/","title":"Learn","text":"<p>https://argoproj.github.io/argo-workflows/training/</p>"},{"location":"Data/Argo/Learn/#hands-on","title":"hands on","text":"<p>https://killercoda.com/pipekit/course/argo-workflows/</p>"},{"location":"Data/Argo/Parameter/","title":"Parameter","text":"<p>https://argoproj.github.io/argo-workflows/walk-through/parameters/</p>"},{"location":"Data/Argo/Parameter/#input-parameter","title":"Input parameter","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: my-input-parameters\nspec:\n  entrypoint: main\n  arguments:\n    parameters:\n      - name: message\n        value: hello\n  templates:\n    - name: main\n      inputs:\n        parameters:\n          - name: message\n      container:\n        image: docker/whalesay\n        command: [ cowsay ]\n        args: [ \"{{inputs.parameters.message}}\" ]\n</code></pre>"},{"location":"Data/Argo/Parameter/#output-parameter","title":"output parameter","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: my-parameters\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      dag:\n        tasks:\n          - name: generate-parameter\n            template: whalesay\n          - name: consume-parameter\n            template: print-message\n            dependencies:\n              - generate-parameter\n            arguments:\n              parameters:\n                - name: message\n                  value: \"{{tasks.generate-parameter.outputs.parameters.hello-param}}\"\n\n    - name: whalesay\n      container:\n        image: docker/whalesay\n        command: [ sh, -c ]\n        args: [ \"echo -n hello world &gt; /tmp/hello_world.txt\" ]\n      outputs:\n        parameters:\n          - name: hello-param\n            valueFrom:\n              path: /tmp/hello_world.txt\n\n    - name: print-message\n      inputs:\n        parameters:\n          - name: message\n      container:\n        image: docker/whalesay\n        command: [ cowsay ]\n</code></pre>"},{"location":"Data/Argo/Template/","title":"Template","text":"<p>https://killercoda.com/pipekit/course/argo-workflows/templates</p>"},{"location":"Data/Argo/Template/#list-workflow-pods","title":"list workflow pods","text":"<pre><code>kubectl get pods -l workflows.argoproj.io/workflow\n</code></pre>"},{"location":"Data/Argo/Template/#work-template","title":"Work template","text":"<ul> <li>Container: run a container</li> <li>Container Set: run multiple containers in a single pod</li> <li>Data: get data from storage </li> <li>Resource: create a Kubernetes resource and wait for it to meet a condition</li> <li>Script: run a script in a container</li> </ul>"},{"location":"Data/Argo/Template/#container-template","title":"Container template","text":"<p><pre><code>#run\nargo submit --watch container-workflow.yaml\n#port forward\nkubectl -n argo port-forward --address 0.0.0.0 svc/argo-server 2746:2746 &gt; /dev/null &amp;\n</code></pre> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow                 \nmetadata:\n  generateName: my-container-name \nspec:\n  entrypoint: main         \n  templates:\n  - name: main             \n    container:\n      image: docker/whalesay\n      command: [cowsay]         \n      args: [\"hello\"]\n</code></pre></p>"},{"location":"Data/Argo/Template/#work-orchestration-template","title":"Work orchestration template","text":"<ul> <li>DAG</li> <li>Steps</li> <li>Suspend</li> </ul>"},{"location":"Data/Argo/Template/#dag-template","title":"DAG template","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: dag-\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      dag:\n        tasks:\n          - name: a\n            template: whalesay\n          - name: b\n            template: whalesay\n            dependencies:\n              - a\n    - name: whalesay\n      container:\n        image: docker/whalesay\n        command: [ cowsay ]\n        args: [ \"hello world\" ]\n</code></pre>"},{"location":"Data/Argo/Template/#loops","title":"Loops","text":"<p>loop over a number of items in parallel using <code>withItems</code> <pre><code>dag:\n  tasks:\n    - name: print-message\n      template: whalesay\n      arguments:\n        parameters:\n          - name: message\n            value: \"{{item}}\"\n      withItems:\n        - \"hello\"\n        - \"goodbye\"\n</code></pre></p> <p>loop over a sequence of numbers using <code>withSequence</code> <pre><code>dag:\n  tasks:\n    - name: print-message\n      template: whalesay\n      arguments:\n        parameters:\n          - name: message\n            value: \"{{item}}\"\n      withSequence:\n        count: 5\n</code></pre></p>"},{"location":"Data/Argo/Template/#exit-handler","title":"Exit Handler","text":"<p>perform a task after something has finished using <code>Exit Handler</code> via <code>OnExit</code> <pre><code>dag:\n  tasks:\n    - name: a\n      template: whalesay\n      onExit: tidy-up\n</code></pre></p>"},{"location":"Data/Argo/Workflow/","title":"Workflow","text":"<p>https://github.com/argoproj/argo-workflows</p>"},{"location":"Data/Argo/Workflow/#workflow_1","title":"workflow","text":"<p>A workflow is a Kubernetes resource, consisting of one or more <code>templates</code>, with one defined as the entrypoint.  Each template can be one of several types, such as container.</p>"},{"location":"Data/Argo/Workflow/#argo-server-auth-mode","title":"argo server auth mode","text":"<p>https://argoproj.github.io/argo-workflows/argo-server-auth-mode/</p>"},{"location":"Data/Argo/Workflow/#install-workflow","title":"install workflow","text":"<p><pre><code>kubectl create ns argo\nkubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/download/v3.4.9/install.yaml\n</code></pre> installed - <code>Workflow Controller</code> is responsible for running workflows   <pre><code>kubectl -n argo get deploy workflow-controller\n</code></pre> - <code>Argo Server</code> provides a user interface and API   <pre><code>kubectl -n argo get deploy argo-server\nkubectl -n argo port-forward --address 0.0.0.0 svc/argo-server 2746:2746 &gt; /dev/null &amp;\n</code></pre> wait for all to be ready <pre><code>kubectl -n argo wait deploy --all --for condition=Available --timeout 5m\n</code></pre></p>"},{"location":"Data/Argo/WorkflowTemplate/","title":"Workflow template","text":""},{"location":"Data/Argo/WorkflowTemplate/#workflow-template_1","title":"workflow template","text":"<p>A workflow template is a library of code that can be reused. <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: hello\nspec:\n  entrypoint: main\n  templates:\n    - name: main\n      container:\n        image: docker/whalesay\n        command: [ cowsay ]\n</code></pre></p>"},{"location":"Data/Argo/WorkflowTemplate/#cron-workflow","title":"cron workflow","text":"<p>A cron workflow runs on a schedule. <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: CronWorkflow\nmetadata:\n  name: cron\nspec:\n  schedule: \"* * * * *\"\n  workflowSpec:\n    entrypoint: main\n    templates:\n      - name: main\n        container:\n          image: docker/whalesay\n</code></pre></p>"},{"location":"Data/Athena/Athena/","title":"Athena","text":"<p>AWS Athena is a serverless, interactive query service provided by Amazon Web Services (AWS). It enables you to analyze data stored in Amazon S3 (Simple Storage Service) using standard SQL queries. With Athena, you can easily run ad-hoc queries on large datasets without the need for any infrastructure management or setup.</p> <p>Athena is designed to work with structured, semi-structured, and unstructured data. It uses Presto, an open-source distributed SQL query engine, under the hood. Athena automatically scales and executes your queries in parallel across multiple nodes to provide fast query performance, even with large datasets.</p> <p>Some key features of AWS Athena include:</p> <ul> <li> <p>Serverless Architecture: Athena is serverless, which means there is no need to provision or manage any underlying infrastructure. You only pay for the queries you run and the amount of data scanned.</p> </li> <li> <p>Standard SQL Queries: You can use standard SQL (Structured Query Language) to query your data in Amazon S3. Athena supports various data formats, including CSV, JSON, Parquet, Avro, and more.</p> </li> <li> <p>Scalability: Athena is highly scalable. It automatically parallelizes and distributes queries across a cluster of nodes, allowing you to process large datasets efficiently.</p> </li> <li> <p>Data Catalog Integration: Athena integrates with AWS Glue Data Catalog, which provides a centralized metadata repository for your data. This integration simplifies query execution and allows you to define schemas, partitions, and table metadata.</p> </li> <li> <p>Query Result Output: You can save the query results to Amazon S3 or use Athena's built-in query result storage. Additionally, you can integrate Athena with other AWS services like AWS Lambda, AWS Glue, and Amazon QuickSight for further data processing and visualization.</p> </li> <li> <p>Security and Encryption: Athena supports AWS Identity and Access Management (IAM) for access control, allowing you to manage user permissions and authentication. It also provides encryption options for data at rest and in transit.</p> </li> </ul> <p>Overall, AWS Athena is a powerful tool for ad-hoc querying and analysis of data stored in Amazon S3, without the need for complex setup or infrastructure management. It is particularly useful for analyzing large-scale datasets, log files, and performing data exploration tasks.</p>"},{"location":"Data/Blog/OrchestrationTool/","title":"Which orchestration tool is better: Airflow, Prefect, Argo Workflows, or Temporal?","text":"<p>Nowadays there are many tools for task orchestration. Some popular ones include Airflow, Prefect, Argo Workflows, and Temporal. Now the question is which tool should I use in my team?</p> <p>Here I will briefly list the features of the four task orchestration tools. Hopefully this will help you decide which tool is best for your task scheduling.</p>"},{"location":"Data/Blog/OrchestrationTool/#airflow","title":"Airflow","text":"<p>Airflow is a popular task scheduling tool. The data workflows in Airflow are defined using Python.</p> <p>It has a large user base but also has some limitations as it was created earlier than other orchestration tools: - DAGs are not parameterized - you can't pass parameters into your workflows - DAGs are static - it can't automatically create new steps at runtime as needed - We have to package the entire workflow into one container</p>"},{"location":"Data/Blog/OrchestrationTool/#prefect","title":"Prefect","text":"<p>Prefect was created to overcome some of the limitations in Airflow, with a strong emphasis on ease of use and deployment, especially for complex DAGs.</p> <p>Some features of Prefect: - Workflows are defined in Python, parameterized and dynamic - Can run each step in a container, but need to register docker with workflows in Prefect - Uses state management abstractions that allow for easy retries and failure handling within data workflows - Has built-in integrations with popular data engineering tools and platforms, such as Dask, DBT, and various cloud services</p>"},{"location":"Data/Blog/OrchestrationTool/#argo-workflows","title":"Argo Workflows","text":"<p>Argo Workflows is a container-native workflow engine for orchestrating jobs on Kubernetes. It naturally addresses the deployment issue in Airflow and Prefect. - Workflows are defined in YAML - Every step in an workflow is run in its own container - Relies on Kubernetes for state management - Can only run on Kubernetes clusters</p>"},{"location":"Data/Blog/OrchestrationTool/#temporal","title":"Temporal","text":"<p>While Airflow, Prefect and Argo Workflows focus primarily on data workflow orchestration, Temporal is a more general-purpose workflow tool. - Workflows are defined using the language for the tasks - Provides robust features for state management, retries, and long-running processes - Requires more investment in learning - has a steep learning curve</p>"},{"location":"Data/Blog/OrchestrationTool/#summary","title":"Summary","text":"<p>As we can see, each tool has its own use cases. We need to select the tool that is most suitable for our work. - If our tasks are about data processing, we perhaps should consider Airflow, Prefect or Argo Workflows. - If we already have our tasks running on Kubernetes cluster, for easy of deployment we might choose Argo Workflows. - If our tasks are more general and require high robustness and reliability, we had better to use Temporal. - If our workflows are complex, a tool using a programming language instead of yaml files might be more suitable. - If our workflows are simple and we do not want to invest too much time in learning, a tool with ease of use might be the best choice.</p> <p>Want to know more tips about coding and other things, please visit: https://github.com/seanslma/maki</p>"},{"location":"Data/Caching/Caching/","title":"Caching","text":""},{"location":"Data/Caching/Caching/#caching-types","title":"Caching types","text":"<p>https://hackernoon.com/5-caching-mechanisms-to-speed-up-your-application</p>"},{"location":"Data/Caching/Caching/#cache-eviction","title":"Cache eviction","text":"<p>Cache eviction is the process of removing older or less frequently used data. - Least-recently used (LRU) eviction: evict data that hasn't been accessed for the longest period of time - Least-frequently used (LFU) eviction: evict data that has been accessed the fewest number of times - Time-to-live (TTL) eviction: evict data after a given period of time - First-in-first-out (FIFO) eviction: not common</p>"},{"location":"Data/Caching/Redis/Command/","title":"Command","text":"<p>https://redis.io/commands/</p>"},{"location":"Data/Caching/Redis/Command/#sort","title":"sort","text":"<p>can be used to - sort numbers character by character (via the alpha keyword argument), - sort items based on external data, and - fetch external data to return</p>"},{"location":"Data/Caching/Redis/Command/#transaction-commands","title":"transaction commands","text":"<ul> <li>WATCH,</li> <li>MULTI,</li> <li>EXEC,</li> <li>UNWATCH, and</li> <li>DISCARD</li> </ul> <p><code>MULTI</code> and <code>EXEC</code> can be used for a client to execute multiple commands without being interrupted by other clients.</p>"},{"location":"Data/Caching/Redis/Command/#expiration-commands","title":"expiration commands","text":"<ul> <li><code>persist key</code>: remove expiration from key</li> <li><code>ttl key</code>: return time remaining in seconds before key expires</li> <li><code>expire key seconds</code>: set key to expire in seconds</li> <li><code>expireat key timestamp</code>: set expiration time of unix timestamp in seconds</li> <li><code>pttl key</code>: return time remaining in illiseconds before key expires</li> <li><code>pexpire key milliseconds</code>: set key to expire in ms</li> <li><code>pexpireat key timestamp-milliseconds</code>: set expiration time of unix timestamp in ms</li> </ul>"},{"location":"Data/Caching/Redis/Command/#info-command","title":"info command","text":"<p><code>INFO</code> command can offer a wide range of information about the current status of a Redis server: - cpu, memory used, - server/clients, such has number of connected clients, - stats, number of keys in each database, number of commands executed since the last snapshot, and so on - persistence, such as <code>aof_pending_bio_fsync</code>: is 0 if all data that the server knows about has been written to disk</p>"},{"location":"Data/Caching/Redis/Command/#perf-command","title":"perf command","text":"<p><code>redis-benchmark</code> will show the time for some commands</p>"},{"location":"Data/Caching/Redis/Config/","title":"Config","text":""},{"location":"Data/Caching/Redis/Config/#persistence","title":"Persistence","text":"<ul> <li>snapshot: write a snapshot to disk</li> <li>append-only file: copy new write to disk</li> </ul> <pre><code>#for both\ndir ./\n\n#snapshot\nsave 60 1000\nstop-writes-on-bgsave-error no\nrdbcompression yes\ndbfilename dump.rdb\n\n#append-only file\nappendonly no\nappendfsync everysec\nno-appendfsync-on-rewrite no\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\n</code></pre>"},{"location":"Data/Caching/Redis/Failure/","title":"Failure","text":""},{"location":"Data/Caching/Redis/Failure/#verify-snapshots-and-append-only-files","title":"Verify snapshots and append-only files","text":"<p><code>redis-check-aof</code> and <code>redis-check-dump</code> commands can be used for testing the status of a snapshot and an append-only file.</p>"},{"location":"Data/Caching/Redis/Failure/#replace-a-failed-master","title":"Replace a failed master","text":"<p><code>Redis Sentinel</code> will monitor the Redis masters and slaves and automatically handles failover if the master goes down.</p> <p>Mannually replace a failed master: <pre><code># connect to slave\nssh root@redis.slave.vpn\n# run redis cli\nredis-cli\n# produce a fresh snapshot\nSAVE and QUIT\n# copy snapshot to machine X (the new master)\nscp /var/local/redis/dump.rdb redis.x.vpn:/var/local/redis/dump.rdb\n# connect to machine X\nssh redis.x.vpn\n# start redis server\nsudo /etc/init.d/redis-server start\n# set slave to use machine x as master in slave cli\nSLAVEOF redis.x.vpn 6379\n</code></pre></p>"},{"location":"Data/Caching/Redis/Hash/","title":"Hash","text":"<p>save one level json (a Python dict) but can operate on each key-value pair separately.</p> <p>commands - <code>hmget key k [k ...]</code>: get values based on sub-keys <code>deprecated</code>? - <code>hmset key k v [k v ...]</code>: set values with key-value pairs - <code>hdel key k [k ...]</code>: delete key-value pairs, return number of pairs found and deleted - <code>hlen key</code>: return number of key-value pairs</p> <p>other commands - <code>hexists key k</code>: check key existence - <code>hkeys key</code>: get keys - <code>hvals key</code>: get values - <code>hgetall key</code>: get all key-value pairs - <code>hincrby key k val_incr</code>: increase value at k - <code>hincrbyfloat key k val_incr</code>: increase value at k by float</p>"},{"location":"Data/Caching/Redis/Install/","title":"Install","text":""},{"location":"Data/Caching/Redis/Install/#install-redis-python-client","title":"Install redis python client","text":"<pre><code>pip install redis\npip install redis[hiredis] #faster performance\nmamba install redis-py hiredis\n</code></pre>"},{"location":"Data/Caching/Redis/Install/#start-redis-via-a-docker-file","title":"Start redis via a docker file","text":"<pre><code># running a redis container for local development\ndocker run --rm -p 6379:6379 redis:7.0-alpine\n</code></pre>"},{"location":"Data/Caching/Redis/Install/#install-redis-server-on-linux","title":"Install redis server on Linux","text":"<p>https://redis.io/docs/latest/operate/oss_and_stack/install/archive/install-redis/install-redis-on-linux/</p> <p>install latest stable version: <pre><code>curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n\nsudo apt-get update\nsudo apt-get install redis\n</code></pre></p> <p>start and connect <pre><code>sudo service redis-server start\nredis-cli\n127.0.0.1:6379&gt; ping\n</code></pre></p>"},{"location":"Data/Caching/Redis/Install/#linux-with-build","title":"Linux with build","text":"<p>install <code>build</code> tools <pre><code>sudo apt-get update\nsudo apt-get install make gcc python-dev\n</code></pre></p> <p>download Redis source code from: http://download.redis.io/releases/ <pre><code>wget -q http://download.redis.io/releases/redis-7.0.11.tar.gz\ntar -xzf redis-7.0.11.tar.gz\ncd 7.0.11\n</code></pre></p> <p>compile and install <pre><code>make                       #compile redis\nsudo make install          #install redis\nredis-server redis.conf    #start redis server\n</code></pre></p>"},{"location":"Data/Caching/Redis/Install/#windows","title":"Windows","text":"<p>Redis is not officially supported on Windows.</p> <p>Had to install it on WSL2. See Linux section.</p> <p>After start the redis-server from WSL2, we can use the server from windows (Need to install redis-py in windows).</p>"},{"location":"Data/Caching/Redis/Kubernetes/","title":"Kubernetes","text":""},{"location":"Data/Caching/Redis/Kubernetes/#run-spark-on-kubernetes","title":"run spark on kubernetes","text":"<p>https://spark.apache.org/docs/latest/running-on-kubernetes.html</p>"},{"location":"Data/Caching/Redis/List/","title":"list","text":"<p>commonly used comnmands - <code>l/rpush key val [val ...]</code>: pushe  value(s) onto the left/right end of the list - <code>l/rpop key</code>: remove and return left/right most item from the list - <code>lindex key offset</code>: return item at the given offset - <code>lrange key start end-incl</code>: return items from start to end inclusive - <code>ltrim key start end-incl</code>: trim the list to only include items between start and end inclusive - <code>lrem key n element</code>: remove first n occurrences of elements: n&gt;0:head-&gt;tail, n&lt;0:tail-&gt;head, n=0:all</p> <p>block pop and item move - <code>bl/rpop key- [key ...] timeout</code>: pop left/right most item from first non-empty LIST, or wait timeout in seconds for an item - <code>rpoplpush src-key dest-key</code>: pop right most item from source and lpush to dest, also return the item to the user - <code>rpoplpush src-key dest-key timeout</code>: pop right most item from source and lpush to dest, also return to the user, and wait up to timeout if source is empty</p>"},{"location":"Data/Caching/Redis/Lock/","title":"Lock","text":"<p><code>SETNX</code> can be used to acquire a lock, which will only set a value if the key doesn't already exist.</p>"},{"location":"Data/Caching/Redis/Lua/","title":"Lua","text":"<p>Redis Lua is a scripting feature in Redis that allows you to execute Lua scripts on the server side.  It provides a way to perform complex operations and manipulations on Redis data using the Lua programming language.</p> <p>Lua scripting in Redis offers several advantages, including  - atomicity, - reduced network round-trips, and - the ability to execute multiple commands as a single operation - Lua scripts is as atomic as single commands or MULTI/EXEC</p>"},{"location":"Data/Caching/Redis/Lua/#how-lua-scripting-works-in-redis","title":"how Lua scripting works in Redis","text":"<ul> <li>Writing Lua Scripts: use Lua programming language. Redis also provides a set of Lua functions called \"Redis Lua Scripting API\"</li> <li>Loading Lua Scripts: Lua script must be loaded into Redis before use via SCRIPT LOAD command (return script hash)</li> <li>Executing Lua Scripts: EVAL command or EVALSHA with the previously obtained script hash</li> <li>Script Result: Lua script can return a result, which can be a data structure or a simple value</li> </ul>"},{"location":"Data/Caching/Redis/Lua/#returned-value","title":"returned value","text":"<p>returned values from Lua will be altered (Lua -&gt; Python) - true/fale: 1/None - nil: stop returning anything - float/large float: integer part/min integer (better to return string) - integer/string: unchanged</p>"},{"location":"Data/Caching/Redis/Lua/#kill-script-run","title":"kill script run","text":"<ul> <li>readonly: SCRIPT KILL if executing for longer than <code>configured lua-time-limit</code></li> <li>has write: SHUTDOWN NOSAVE to kill Redis (not good) - should always test Lua before prod</li> </ul>"},{"location":"Data/Caching/Redis/Memory/","title":"Memory","text":""},{"location":"Data/Caching/Redis/Memory/#methods-to-reduce-memory","title":"methods to reduce memory","text":"<ul> <li>short data structures</li> <li>sharding to make larger structures small</li> <li>packing fixed-length data into STRINGs</li> </ul>"},{"location":"Data/Caching/Redis/Memory/#ziplist","title":"ziplist","text":"<p>ziplist is more compact then general list, hash and zset. After a ziplist is converted to a regular structure, it doesn't get re-encoded as a ziplist if the structure later meets the criteria.</p> <p>check the object info: <code>r.debug_object('key')</code> <pre><code>list-max-ziplist-entries 512\nlist-max-ziplist-value 64\n\nhash-max-ziplist-entries 512\nhash-max-ziplist-value 64\n\nzset-max-ziplist-entries 128\nzset-max-ziplist-value 64\n</code></pre></p>"},{"location":"Data/Caching/Redis/Memory/#intset","title":"intset","text":"<p>will change intset to hashtable after the limit. <pre><code>set-max-intset-entries 512\n</code></pre></p>"},{"location":"Data/Caching/Redis/Memory/#sharding","title":"sharding","text":"<p>not good for zset.</p>"},{"location":"Data/Caching/Redis/Memory/#packing-bits-and-bytes","title":"packing bits and bytes","text":"<p>commands: <code>getrange</code>, <code>setrange</code>, <code>getbit</code>, and <code>setbit</code></p>"},{"location":"Data/Caching/Redis/Mode/","title":"Mode","text":"<p><code>multi-master replication</code> is not supported.</p>"},{"location":"Data/Caching/Redis/Mode/#master-slave-chain","title":"master-slave chain","text":"<p>when there are too many slaves, the master might be too busy to write to or be connected by slaves.  In this case, we can set some intermediate slaves with each connects to a few slaves further down in a tree structure.</p>"},{"location":"Data/Caching/Redis/Persistence/","title":"redis persistence","text":"<ul> <li>Snapshots (e.g., 5min) and</li> <li>AOF (Append Only File)</li> </ul>"},{"location":"Data/Caching/Redis/Pipeline/","title":"Pipeline","text":""},{"location":"Data/Caching/Redis/Pipeline/#transactional-pipeline","title":"transactional pipeline","text":"<p>command in the pipeline will be executed once but will block other commands. <pre><code>example\n```py\ndef trans():\n    pipe = r.pipeline(True)\n    pipe.incr('trans:', 2)\n    time.sleep(.1)\n    pipe.incr('trans:', -1)\n    ret = pipe.execute()\n    print(ret[0])\nfor i in xrange(5):\n    threading.Thread(target=trans).start()\n    time.sleep(.5)\n</code></pre></p>"},{"location":"Data/Caching/Redis/Pipeline/#non-transactional-pipeline","title":"non-transactional pipeline","text":"<p>still execute the commands in the pipeline without blocking other commands, for the benefit of increading performance. <pre><code>pipe = r.pipeline(False)\npipe.xyz()\npipe.execute()\n</code></pre></p>"},{"location":"Data/Caching/Redis/Pubsub/","title":"Publish/subscribe","text":"<p>pub/sub events mode. can cause high memory usage in redis and sub can lost a little data.</p> <p>commands - <code>publish channel message</code>: publishe a message to channel - <code>subscribe channel [channel ...]</code>: subscribe to channels - <code>unsubscribe [channel [channel ...]]</code>: unsubscribe from channels - <code>psubscribe pattern [pattern ...]</code>: subscribe to messages match given pattern - <code>punsubscribe [pattern [pattern ...]]</code>: unsubscribe from provided patterns</p>"},{"location":"Data/Caching/Redis/Redis/","title":"Redis","text":"<p>Redis (Remote Dictionary Server) can be used for caching, logging, statistics, cookies, and so on. Each can be on a separate Redis server on a single machine on different ports.</p>"},{"location":"Data/Caching/Redis/Redis/#data-structure","title":"data structure","text":"<ul> <li>STRING: Strings, integers, or floating point values   command: set, get, del</li> <li>LIST: Linked list of strings   command: lpush/rpush, lpop/rpop, lindex, lrange</li> <li>SET: Unordered collection of unique strings   command: sadd, srem, sismember, smembers, sinter, sunion, sdiff</li> <li>HASH: Unordered hash table of keys to values   command: hset, hget, hgetall, hdel</li> <li>ZSET: Ordered mapping of string members to floating-point scores, ordered by score   command: zadd, zrem, zrange, zrangebyscore</li> </ul>"},{"location":"Data/Caching/Redis/RedisPy/","title":"Redis-py","text":""},{"location":"Data/Caching/Redis/RedisPy/#install-redis-py","title":"install redis-py","text":"<p><code>redis-py</code> client for Redis <pre><code>pip install redis[hiredis]\nmamba install redis-py hiredis\n</code></pre></p>"},{"location":"Data/Caching/Redis/RedisPy/#methods","title":"methods","text":"<p><code>time</code> can be represented by int or a Python timedelta/datetime object - <code>r.setx(key, time, value)</code>   set value of key that expires in time seconds - <code>r.expire(key, time)</code>   set expire flag for time seconds - <code>r.psetex(key, time_ms, value)</code>   set value of key that expires in time_ms milliseconds - <code>r.expireat(key, when)</code>   set an expire flag - <code>r.persist(key)</code>   remove expiration - <code>r.ttl(key)</code>   return the number of seconds until the key expires - <code>r.pttl(key)</code>   return the number of milliseconds until the key expires</p>"},{"location":"Data/Caching/Redis/RedisPy/#create-server","title":"create server","text":"<pre><code>import redis\nr = redis.Redis(host='localhost', port=6379, db=0)\nprint(r.ping())\nprint(r.keys(\"*\"))\n</code></pre>"},{"location":"Data/Caching/Redis/RedisPy/#connetect-to-localcluster","title":"connetect to local/cluster","text":"<pre><code>from redis.cluster import RedisCluster\n\n# `decode_responses=True` convert return value from bytes to str.\n#r = redis.Redis(host='localhost', port=6379, decode_responses=True)\nrc = RedisCluster(host='localhost', port=16379)\n\nprint(rc.get_nodes())\n# [[host=127.0.0.1,port=16379,name=127.0.0.1:16379,server_type=primary,\n#  redis_connection=Redis&lt;ConnectionPool&lt;Connection&lt;host=127.0.0.1,port=16379,db=0&gt;&gt;&gt;], ...\n</code></pre>"},{"location":"Data/Caching/Redis/RedisPy/#connect-with-tls","title":"connect with tls","text":"<pre><code>import redis\n\nr = redis.Redis(\n    host=\"my-redis.cloud.redislabs.com\",\n    port=6379,\n    username=\"default\", # use your Redis user. More info https://redis.io/docs/management/security/acl/\n    password=\"secret\", # use your Redis password\n    ssl=True,\n    ssl_certfile=\"./redis_user.crt\",\n    ssl_keyfile=\"./redis_user_private.key\",\n    ssl_ca_certs=\"./redis_ca.pem\",\n)\n</code></pre>"},{"location":"Data/Caching/Redis/RedisPy/#examples","title":"examples","text":"<pre><code>r.set('key', 'val')\nr.get('key')\n\ndv = {\n    'a': 'x',\n    'b': 'y'\n}\nr.mset(dv)\nr.get('b')\n\nlv = ['a', 'b']\nr.sadd('lv', *lv)\nr.smembers('lv')\n\nlv = ['x', 'y']\nr.lpush('lp', *lv)\nr.lrange('lp', 0, -1)\n\nimport json\njv = {\n    'name': 'John',\n    'age': 20,\n    'address':{\n        'number': 19,\n        'street': 'Goldey',\n        'suburb': 'Cuddly'\n    },\n    'skills': ['art', 'math']\n}\nr.set('jv', json.dumps(jv))\njson.loads(r.get('jv'))\n</code></pre>"},{"location":"Data/Caching/Redis/RedisPy/#difference-between-redisredis-and-redisstrictredis","title":"difference between redis.Redis and redis.StrictRedis","text":"<p>In the <code>redis-py</code> library, both <code>redis.Redis</code> and <code>redis.StrictRedis</code> are classes that provide a client interface for interacting with a Redis server. However, there is a subtle historical difference between them.</p> <ol> <li><code>redis.Redis</code> (Deprecated):</li> <li><code>redis.Redis</code> was the original class in <code>redis-py</code> for creating a Redis client.</li> <li>Over time, the developers realized that the name <code>StrictRedis</code> better reflects the fact that the library adheres strictly to the official Redis command syntax.</li> <li> <p>To avoid confusion, <code>StrictRedis</code> was introduced, and it was recommended to use <code>StrictRedis</code> instead of <code>Redis</code>. As a result, <code>Redis</code> is now deprecated.</p> </li> <li> <p><code>redis.StrictRedis</code> (Recommended):</p> </li> <li><code>redis.StrictRedis</code> is the replacement for <code>redis.Redis</code> and is the recommended class to use in modern code.</li> <li>It provides a clean and consistent API, strictly following the official Redis command syntax.</li> <li><code>StrictRedis</code> is more feature-complete, and its usage is considered best practice.</li> </ol> <p>Usage: <pre><code># Recommended: Use StrictRedis\nimport redis\nredis_client = redis.StrictRedis(host='localhost', port=6379, db=0, decode_responses=True)\n\n# Deprecated: Avoid using Redis\n# import redis\n# redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)\n</code></pre></p> <p>In summary, while both <code>redis.Redis</code> and <code>redis.StrictRedis</code> can be used, it's strongly recommended to use <code>redis.StrictRedis</code> in modern code to take advantage of the latest features, improvements, and best practices. If you encounter existing code that uses <code>redis.Redis</code>, consider updating it to use <code>redis.StrictRedis</code>.</p>"},{"location":"Data/Caching/Redis/Serialization/","title":"Serialization","text":"<p>https://stackoverflow.com/questions/13060632/getting-values-with-the-right-type-in-redis</p>"},{"location":"Data/Caching/Redis/Serialization/#pickle","title":"pickle","text":"<ul> <li>high memory usage</li> <li>not portable between different python versions</li> </ul>"},{"location":"Data/Caching/Redis/Serialization/#struct","title":"struct","text":"<ul> <li>low memory usage</li> <li>not portable between different platforms</li> <li>not easy to use for pandas df???</li> </ul>"},{"location":"Data/Caching/Redis/Set/","title":"Set","text":"<p>commonly used commands - <code>sadd key val [val ...]</code>: add values and return the number of values added that weren't already present - <code>srem key val [val ...]</code>: remove values and return the number of values that were removed - <code>sismember key val</code>: return whether the value is in the set - <code>scard key</code>: return number of values in the set - <code>smembers key</code>: rturns all values as a Python set - <code>srandmember key [count]</code>: return one or more random values. positive count: distinct randomly chosen values; negative count:  randomly chosen values may not be distinct - <code>spop key</code>: remove and return a random value - <code>smove srs-key dest-key val</code>: if value exists, remove it from source and add it to destination, return True if the value was moved</p> <p>sets manipulation commands - <code>sdiff key [key ...]</code>: return values in first SET but not in any of the other SETs - <code>sdiffstore dest-key key [key ...]</code>: store at dest-key the values in first SET but not in any of the other SETs - <code>sinter key [key ...]</code>: return values in all of the SETs - <code>sinterstore dest-key key [key ...]</code>: store at dest-key the values in all of the SETs - <code>sunion key [key ...]</code>: return values in at least one of the SETs - <code>sunionstore dest-key key [key ...]</code>: stores at dest-key the values in at least one of the SETs</p>"},{"location":"Data/Caching/Redis/String/","title":"string","text":"<p>string can be use to cache three types of values. - Byte string values - Integer values - Floating-point values - The maximum size limit for a string value is 512MB</p>"},{"location":"Data/Caching/Redis/String/#incrdecr-commands","title":"incr/decr commands","text":"<p>assume non-existent or empty string having a value of zero: - <code>incr key</code>: increase the value of key by 1 - <code>desc key</code>: decrease the value of key by 1 - <code>incrby key intval</code>: increase the value of key by intval - <code>descby key intval</code>: decrease the value of key by intval - <code>incrbyfloat key val</code>: increase the value of key by val</p>"},{"location":"Data/Caching/Redis/String/#substring-commands","title":"substring commands","text":"<ul> <li><code>append key val</code>: concatenate value to string stored at the given key</li> <li><code>getrange key start end-incl</code>: get substring from start to end inclusive</li> <li><code>setrange key offset val</code>: append value to stored string ended before offset</li> <li><code>getbit key idx</code>: get bit value at idx</li> <li><code>setbit key idx val</code>: set bit value at idx</li> <li><code>bitcount key [start, end]</code>: count the bits in the sub/string</li> <li><code>bitop op des-key key-name [key-name2 ...]</code>: bitwise operations, <code>AND</code>, <code>OR</code>, <code>XOR</code>, or <code>NOT</code>, on provided strings, storing result in destination key</li> </ul>"},{"location":"Data/Caching/Redis/Transaction/","title":"Transaction","text":""},{"location":"Data/Caching/Redis/Transaction/#commands","title":"commands","text":"<p>These commands will protect us from data corruption. They ensure that the data doesn't change while we're doing something important. - MULTI/EXEC: commands after multi will run together on exec, like a pipeline - WATCH: whe a key(s) is watched, if at any time the key is replaced, updated, or deleted before the EXEC operation, exec will fail with an error message (so we can retry or abort the operation) - UNWATCH: reset connection if sent after WATCH but before MULTI - DISCARD: similar to unwatch</p>"},{"location":"Data/Caching/Redis/Transaction/#example-of-watch","title":"example of watch","text":"<p>WATCH/MULTI/EXEC transactions sometimes don't scale at load. <pre><code>pipe = conn.pipeline()\nwhile time.time() &lt; end:\n    try:\n        pipe.watch('key-a', 'key-b')\n        if not pipe.sismember('key-a', 'itemid'):\n            pipe.unwatch()\n            return None # condotion not satisfied\n        # do the transaction\n        pipe.multi()\n        pipe.zadd('key-x', 'item', 100)\n        pipe.srem('key-a', 'itemid')\n        pipe.execute()\n        return True\n    except redis.exceptions.WatchError:\n        pass # values in key-a or key-b were changed\n</code></pre></p>"},{"location":"Data/Caching/Redis/Zset/","title":"ZSET","text":"<p>similar to hash or dict, but will sort ans manipulate the values (numeric).</p> <p>By setting all scores to 0 in a ZSET, Redis will sort by member name.</p> <p>commands - <code>zadd key score member [score member ...]</code>: add members with scores - <code>zrem key member [member ...]</code>: remove members, return number of members removed - <code>zcard key</code>: return number of members - <code>zincrby key incr member</code>: increase member score - <code>zcount key min max</code>: return number of members with scores between min and max - <code>z[rev]rank key member</code>: return member position - <code>zscore key member</code>: return member score  - <code>z[rev]range key start stop [withscores]</code>: return members and optionally the scores with ranks between start and stop</p> <p>other commands - <code>z[rev]rangebyscore key min max [withscores] [limit offset count]</code>: get members with scores between min and max - <code>zremrangebyrank key start stop</code>: remove members with ranks between start and stop - <code>zremrangebyscore key min max</code>: remove members with scores between min and max - <code>zinterstore dest-key nkey key [key ...] [weights weight [weight ...]] [aggregate sum|min|max]</code>: save intersection of zsets to new zset, default sum scores - <code>zunionstore dest-key nkey key [key ...] [weights weight [weight ...]] [aggregate sum|min|max]</code>: save union of zsets to new zset, default sum scores</p> <p>zinterstore/zunionstore can also work together with sets by assuming the scores are 1. </p>"},{"location":"Data/Cleaning/Outlier/","title":"Outlier","text":"<p>check outlier by hisgram and scatter plot.</p> <p>https://www.analyticsvidhya.com/blog/2021/05/feature-engineering-how-to-detect-and-remove-outliers-with-python-code/</p> <p>https://stats.stackexchange.com/questions/69874/how-to-correct-outliers-once-detected-for-time-series-data-forecasting</p>"},{"location":"Data/Cleaning/Outlier/#remove-outliers-by-stastical-method","title":"remove outliers by stastical method","text":"<ul> <li>Q1: first quartile, median of first part</li> <li>Q3: third quartile, median of second part</li> </ul> <p>Minor outlier - Lower bound: Q1 - (1.5 * (Q3-Q1)) - Upper bound: Q3 + (1.5 * (Q3-Q1))</p> <p>Major outlier - Lower bound: Q1 - (3 * (Q3-Q1)) - Upper bound: Q3 + (3 * (Q3-Q1))</p> <p>Python code to remove major outlier <pre><code>p25 = d1['val'].quantile(0.25) #percentile25\np75 = d1['val'].quantile(0.75) #percentile75\nlower_limit = p25 - 3 * (p75 - p25)\nupper_limit = p75 + 3 * (p75 - p25)\ndf = d1.copy()\ndf['val'] = np.where(df['val'] &lt; lower_limit, np.nan, df['val'])\ndf['val'] = np.where(df['val'] &gt; upper_limit, np.nan, df['val'])\n</code></pre></p>"},{"location":"Data/Cleaning/Outlier/#reduce-effects-of-outliers-by-using-a-weighting-mechanism","title":"reduce effects of outliers by using a weighting mechanism","text":"<p>Weights of observations are adjusted so as to append less weight to extreme values. - Huber regression - Bisquare regression <pre><code>huber_regressor = sklearn.linear_model.HuberRegressor(\n    epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05\n)\n</code></pre></p>"},{"location":"Data/Cleaning/Outlier/#time-series-shocks","title":"time series shocks","text":"<p>Use Kalman Filter to adjust for time series shocks.</p>"},{"location":"Data/Cleaning/SanityCheck/","title":"Sanity Check","text":"<ul> <li>number of rows and cols</li> <li>unique of uids</li> <li>data type consistency</li> <li>missing values</li> </ul>"},{"location":"Data/Flink/API/","title":"API","text":"<p>Table API: - Write SQL-like queries for data processing. - Ideal for those familiar with SQL and tabular data. - Operates on structured data.</p> <p>DataStream API: - Lower-level control over Flink's core building blocks (state, time). - Suitable for complex stream processing scenarios. - Handles both structured and unstructured data.</p>"},{"location":"Data/Flink/API/#pyflink","title":"PyFlink","text":"<p>A Python API for Flink: https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/overview/</p>"},{"location":"Data/Flink/Flink/","title":"Flink","text":""},{"location":"Data/Flink/Flink/#flink-vs-spark","title":"Flink vs Spark","text":"<p>Flink:  - Stream-first with batch processing capabilities as an extension. - Designed for real-time processing of continuous data streams with low latency. - Used in Fraud detection in real-time transactions, IoT data stream processing, real-time anomaly detection, event-driven applications.</p> <p>Spark:  - Batch-first with micro-batching for streaming. - Optimized for large-scale batch processing but can handle near real-time streaming through micro-batching intervals. - Used in Large-scale data analysis, machine learning model training, graph processing, interactive data exploration, batch ETL pipelines.</p>"},{"location":"Data/Hadoop/MapReduce/","title":"MapReduce","text":"<p>Hadoop MapReduce is a framework to perform computation tasks in parallel across many nodes in a computer cluster.</p>"},{"location":"Data/Hadoop/MapReduce/#shortcomings","title":"shortcomings","text":"<p>That make Hadoop relatively ill-suited for use cases of an iterative or low-latency nature - high overheads to launch each job - reliance on storing intermediate data and results of the computation to disk</p>"},{"location":"Data/Kafka/Kafka/","title":"Kafka","text":"<p>https://medium.com/webstep/apache-kafka-hands-on-demo-385002dbb8c1</p> <p>Kafka focuses on reliable data <code>ingestion</code> and <code>distribution</code>, ensuring data is always available for downstream processing. - distributed, horizontally-scalable, highly available, fault-tolerant commit log - publish-subscribe messaging system - streaming data platform - can store data as long as necessary</p>"},{"location":"Data/Kafka/Schema/","title":"Schema","text":"<p>Apache Avro, Google\u2019s Protocol Buffers and Apache Thrift</p>"},{"location":"Data/Kafka/Schema/#avro","title":"Avro","text":"<p>Apache Avro is binary data serialization system which provides rich data structures.</p>"},{"location":"Data/Notification/LongPolling/","title":"Long Polling","text":"<p>Long polling is used to achieve real-time updates or push notifications from a web server to a client.  The server intentionally delays its response to the client's request. Instead of immediately responding with the data,  the server holds the request open until new information is available or a specified timeout period is reached.</p>"},{"location":"Data/Notification/WebSocket/","title":"WebSocket","text":"<p>WebSocket provides a persistent connection between the client and the server. Communication is bi-directional.</p>"},{"location":"Data/Spark/Firewall/","title":"Firewall","text":"<p>https://www.ibm.com/docs/en/zpfas/1.1.0?topic=spark-configuring-networking-apache</p> <p>https://medium.com/@madtopcoder/enable-firewall-for-spark-cluster-86e6edfe229f</p>"},{"location":"Data/Spark/Kubernetes/","title":"Kubernetes","text":""},{"location":"Data/Spark/Kubernetes/#spark-helm-chart","title":"spark helm chart","text":"<p>https://bitnami.com/stack/spark/helm</p>"},{"location":"Data/Spark/Kubernetes/#spark-on-aks","title":"spark on aks","text":"<p>https://tsmatz.wordpress.com/2020/12/08/apache-spark-on-azure-kubernetes-service-aks/</p> <p>https://www.oak-tree.tech/blog/spark-kubernetes-primer</p>"},{"location":"Data/Spark/Learn/","title":"Learn","text":""},{"location":"Data/Spark/Learn/#learning-spark-lightning-fast-data-analysis","title":"Learning-Spark-Lightning-Fast-Data-Analysis","text":"<p>https://github.com/databricks/LearningSparkV2</p>"},{"location":"Data/Spark/Learn/#mastering-spark-with-r","title":"Mastering Spark with R","text":"<p>https://therinspark.com/connections.html</p>"},{"location":"Data/Spark/Spark/","title":"Spark","text":"<p>Apache Spark is a framework for distributed computing. - store intermediate data and results in memory - provide an API to write applications - is fully compatible with the Hadoop ecosystem</p>"},{"location":"Data/Spark/Spark/#driver","title":"driver","text":"<ul> <li>communicate with cluster manager</li> <li>request resources (CPU, memory, etc.) from cluster manager for Spark's executors (JVMs)</li> <li>transform all Spark operations into DAG computations</li> <li>schedule and distribute computations as tasks across the Spark executors</li> <li>communicate directly with executors after resource allocation</li> </ul>"},{"location":"Data/Spark/Spark/#cluster-manager","title":"cluster manager","text":"<p>Cluster manager manages and allocates resources for cluster nodes. - built-in standalone cluster manager - Apache Hadoop YARN - Apache Mesos - Kubernetes</p>"},{"location":"Data/Spark/Spark/#executor","title":"executor","text":"<p>An executor runs on each worker node in the cluster. The executors communicate with the driver program.</p>"},{"location":"Data/Spark/SparkUI/","title":"SparkUI","text":"<p>https://medium.com/analytics-vidhya/spark-tuning-and-debugging-fe32fded8454</p> <ul> <li>Spark UI is on port 4040 (default) for running apps</li> <li>After app execution it will be available on history server(port 18080 by default)</li> </ul>"},{"location":"Data/dbt/Learn/","title":"Learn","text":"<ul> <li>https://docs.getdbt.com/</li> <li>https://www.getdbt.com/community</li> <li>https://getdbt.slack.com/archives/C01DU491K1A/p1670356944621679</li> </ul>"},{"location":"Data/dbt/dbt/","title":"dbt","text":"<ul> <li>Data transformation and testing tool for building and managing <code>data models</code>, emphasizing data quality, documentation, and maintainability.</li> <li>Primarily <code>batch-oriented</code>, typically running transformations periodically on static datasets within the data warehouse.</li> <li>Primarily works with <code>structured data</code> stored in tables within <code>data warehouses</code>.</li> <li>Uses <code>SQL</code> for data transformations, making it accessible to those familiar with SQL.</li> </ul>"},{"location":"DevOps/DevOps/","title":"DevOps","text":""},{"location":"DevOps/DevOps/#infrastructure","title":"Infrastructure","text":"<p>Terraform</p>"},{"location":"DevOps/DevOps/#deployment","title":"Deployment","text":"<p>ArgoCD, Helm</p>"},{"location":"DevOps/DevOps/#container","title":"Container","text":"<p>Docker</p>"},{"location":"DevOps/DevOps/#orchestrator","title":"Orchestrator","text":"<p>Kubernetes</p>"},{"location":"DevOps/DevOps/#security","title":"Security","text":""},{"location":"DevOps/Resource/","title":"Resource","text":"<p>DevOps books: https://github.com/jidibinlin/Free-DevOps-Books-1</p> <p>Template cheatsheet: https://lzone.de/cheat-sheet/Helm%20Templates</p> <p>github.com/bregman-arie/devops-exercises</p>"},{"location":"DevOps/%20Prometheus/Prometheus/","title":"Prometheus","text":"<p>https://prometheus.io/docs/introduction/overview/</p> <p>Prometheus is a monitoring system that is designed to be scalable, flexible, and easy to use. It can collect metrics from all parts of the Kubernetes cluster, and it provides a unified view of the health of the system.</p>"},{"location":"DevOps/%20Prometheus/Prometheus/#how-to-run","title":"how to run","text":"<p>download the binary file into a folder and setup the config using yaml.  then run <code>prometheus --config.file=prometheus.yml</code></p>"},{"location":"DevOps/%20Prometheus/Prometheus/#metrics","title":"metrics","text":"<ul> <li>counter: can only increase or reset i.e the value cannot reduce</li> <li>gauge: a number which can either go up or down</li> <li>histogram: for calculated values which are counted based on bucket values</li> <li>summary: an alternative to histogram - cheaper, but lose more data. calculated on app level hence aggregation of metrics from multiple instances of the same process is not possible. use histogram whenever possible.</li> </ul>"},{"location":"DevOps/ArgoCD/ACR/","title":"ARC","text":""},{"location":"DevOps/ArgoCD/ACR/#connection-argocd-to-acr","title":"connection ArgoCD to ACR","text":"<p>https://stackoverflow.com/questions/74928026/how-to-use-aks-managed-identity-in-argocd-to-connect-to-acr</p>"},{"location":"DevOps/ArgoCD/Application/","title":"Application","text":"<p>https://argo-cd.readthedocs.io/en/stable/operator-manual/application.yaml</p> <p>Config file for setting the helm chart parameters and other paremeters.</p>"},{"location":"DevOps/ArgoCD/Application/#delete-app","title":"delete app","text":"<p>https://argo-cd.readthedocs.io/en/stable/user-guide/app_deletion/</p> <p>cascade deletion <pre><code>kubectl patch app APPNAME  -p '{\"metadata\": {\"finalizers\": [\"resources-finalizer.argocd.argoproj.io\"]}}' --type merge\nkubectl delete app APPNAME\n</code></pre></p> <p>non-cascade deletion <pre><code>kubectl patch app APPNAME  -p '{\"metadata\": {\"finalizers\": null}}' --type merge\nkubectl delete app APPNAME\n</code></pre></p>"},{"location":"DevOps/ArgoCD/ArgoCD/","title":"ArgoCD","text":"<p>https://argoproj.github.io/</p> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/</p> <p>https://www.digitalocean.com/community/tutorials/how-to-deploy-to-kubernetes-using-argo-cd-and-gitops</p> <p>https://levelup.gitconnected.com/getting-started-with-argocd-on-your-kubernetes-cluster-552ca5d8cf41</p> <p>Argo CD provides Continuous Delivery tooling that automatically synchronizes and deploys applications whenever a change is made in GitHub repository.</p>"},{"location":"DevOps/ArgoCD/ArgoCD/#install-argocd-to-cluster","title":"install ArgoCD to cluster","text":"<p>https://medium.com/@talhaaziz37/argo-cd-installation-in-kubernetes-kubeadm-ubuntu-lts-22-04-b429df6d4655 <pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\nwatch kubectl get pods -n argocd\n</code></pre></p>"},{"location":"DevOps/ArgoCD/ArgoCD/#log-to-argocd-ui","title":"log to argocd ui","text":"<ul> <li>use port-forward to expose a port to the service, and forward it to localhost</li> <li>get the login password (default user name is <code>admin</code>) <pre><code>kubectl port-forward svc/argocd-server -n argocd 8080:443 # https://localhost:8080\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre></li> </ul> <p>Also can deploy a NodePort service for ArgoCD so we do not need <code>port-forward</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: argocd-server-nodeport\n  namespace: argocd\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/part-of: argocd\nspec:\n  type: NodePort\n  selector:\n    app.kubernetes.io/name: argocd-server\n  ports:\n    - name: http\n      port: 80\n      protocol: TCP\n      targetPort: 8080\n      nodePort: 30007\n    - name: https\n      port: 443\n      protocol: TCP\n      targetPort: 8080\n      nodePort: 30008\n  sessionAffinity: None\n</code></pre></p>"},{"location":"DevOps/ArgoCD/ArgoCD/#project-and-application","title":"project and application","text":"<p>https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/</p>"},{"location":"DevOps/ArgoCD/ArgoCD/#terraform","title":"terraform","text":"<p>https://registry.terraform.io/providers/hashicorp/helm/latest/docs/resources/release</p> <p>https://github.com/argoproj/argo-helm/tree/master/charts/argo-cd</p> <p>https://www.bootiq.io/en/deploy-helm-charts-using-terraform-module</p>"},{"location":"DevOps/ArgoCD/CLI/","title":"CLI","text":""},{"location":"DevOps/ArgoCD/CLI/#argocd-cli","title":"argocd cli","text":"<pre><code>#install argocd: https://argo-cd.readthedocs.io/en/stable/cli_installation\ncurl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\nrm argocd-linux-amd64\n\n#extract admin password\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo\nargocd login localhost:8080\nargocd account update-password\n\n#handling multiple clusters\nkubectl config get-contexts -o name\nargocd cluster add target-k8s\n</code></pre>"},{"location":"DevOps/ArgoCD/CLI/#add-repo","title":"add repo","text":"<pre><code>argocd repo add acrxxxxxx.azurecr.io/helm --type helm --name acrxxxx --enable-oci \\\n--username argocd-creds --password \"oJwFNBGsC1qnL2y+calxp/TmuhkhDmFD0Jhnxxxxxxxxxx\" --upsert\n</code></pre>"},{"location":"DevOps/ArgoCD/CLI/#deploy-app","title":"deploy app","text":"<pre><code>argocd app create helm-guestbook --repo https://github.com/argoproj/argocd-example-apps.git \\\n--path helm-guestbook --dest-server https://kubernetes.default.svc --dest-namespace default\nargocd app get helm-guestbook\nargocd app sync helm-guestbook\n</code></pre>"},{"location":"DevOps/ArgoCD/Cert/","title":"Cert","text":"<p>https://argo-cd.readthedocs.io/en/stable/user-guide/private-repositories/</p> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#repositories</p> <p>ArgoCD uses certs (public and private keys) to connect to private repos.</p>"},{"location":"DevOps/ArgoCD/Cert/#self-signed-cert","title":"self-signed cert","text":"<p>https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#repositories-using-self-signed-tls-certificates-or-are-signed-by-custom-ca</p> <ul> <li>TLS certificates is used to verify the authenticity of the repository servers in a ConfigMap object named <code>argocd-tls-certs-cm</code>.</li> <li>The data section should contain a map, with the <code>repository server's hostname</code> part (not the complete URL) as <code>key</code>, and the <code>certificate(s) in PEM format</code> as <code>data</code>.</li> <li>If the repository URL is https://server.example.com/repos/my-repo, we should use <code>server.example.com</code> as key.</li> <li>The certificate data should be either the <code>server's certificate</code> (in case of self-signed certificate) or the certificate of the CA that was used to sign the server's certificate.</li> <li>You can configure multiple certificates for each server, e.g. if you are having a certificate roll-over planned.</li> </ul>"},{"location":"DevOps/ArgoCD/Cert/#add-cert-into-argocd-tls-cert-cm","title":"add cert into <code>argocd-tls-cert-cm</code>","text":"<p>https://github.com/argoproj/argo-cd/issues/6048</p> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#repositories-using-self-signed-tls-certificates-or-are-signed-by-custom-ca</p> <p>The <code>argocd-tls-certs-cm</code> ConfigMap will be mounted as a volume at the mount path <code>/app/config/tls</code> in the pods of <code>argocd-server</code> and <code>argocd-repo-server</code>.</p> <p>The certs should be added in the values.yaml file <pre><code>configs:\n  repositories:\n    my-repo:\n      url: https://my.example.com/repo\n  tlsCerts:\n    data:\n      my.example.com: |\n        -----BEGIN CERTIFICATE-----\n        xx...yy\n        -----END CERTIFICATE-----\n</code></pre> The certificates are propagated through a volume mount of the <code>argocd-tls-certs-cm</code> ConfigMap. If the cert is added, at <code>/app/config/tls</code> in the <code>argocd-repo-server</code> pod there should be a file named like the repo server.</p> <p>Argo CD uses a kind of certificate pinning - that means, each certificate is pinned to the name of the repository server, and must be configured accordingly.</p>"},{"location":"DevOps/ArgoCD/Cert/#connect-to-private-repo","title":"Connect to private repo","text":"<p>Error: <code>Unable to deploy revision: rpc error: code = Unknown desc = authentication required</code> Solution: - settings &gt; repositories &gt; connecting repo using https &gt; url and pat</p> <p>Error: <code>x509: certificate signed by unknown authority</code> Solution: - add the server's certificate, or the certificate of the CA used to sign the server's certificate to <code>argocd-tls-cert-cm</code>. - test it <code>helm repo add jh https://example.com/jupyterhub --ca-file /app/config/tls/my.crt</code></p>"},{"location":"DevOps/ArgoCD/Cert/#issue-for-private-helm-chart-repo-with-self-signed-cert","title":"issue for private helm chart repo with self-signed cert","text":"<p>https://github.com/argoproj/argo-cd/issues/9607 - create new argocd image: add cert to <code>/usr/local/share/ca-certificates/</code> and update <code>update-ca-certificates</code> - mount the tls certificates from argocd's config-map into the default ssl certificate folder of golang's http client   <pre><code>applicationSet:\n  extraVolumeMounts:\n  - name: certificates\n    mountPath: /etc/ssl/certs/cert1.crt\n    subPath: cert1.crt\n</code></pre>  - use insecure option   <pre><code>name: private-repo\ntype: helm\ninsecure: true\nurl: https://private-repo.com\n</code></pre></p>"},{"location":"DevOps/ArgoCD/Cert/#x509-certificate-signed-by-unknown-authority","title":"x509: certificate signed by unknown authority","text":"<p>https://github.com/argoproj/argo-cd/issues/6048</p> <p>The <code>ssh-known-hosts</code> and <code>tls-certs</code> ConfigMaps need to be mounted to the argocd-repo-server pods. <pre><code>kubectl -n argocd describe deployments.apps argocd-repo-server\n</code></pre></p> <p>Reason: the website cert has been updated. So the cert in argocd should be updated as well.</p>"},{"location":"DevOps/ArgoCD/Error/","title":"Error","text":""},{"location":"DevOps/ArgoCD/Error/#rpc-error-code-unknown-desc-authentication-required","title":"rpc error: code = Unknown desc = authentication required","text":"<ul> <li>add the user/pass for the helm chart repo to argocd</li> <li>or the user/pass(token) is not correct</li> </ul>"},{"location":"DevOps/ArgoCD/Error/#argocd-repositories-connection-status-failed","title":"argocd repositories connection status failed","text":"<p>https://levelup.gitconnected.com/connect-argocd-to-your-private-github-repository-493b1483c01e</p>"},{"location":"DevOps/ArgoCD/Error/#could-not-read-username-for-httpsgithubcom-no-such-device-or-address","title":"could not read Username for 'https://github.com': No such device or address","text":"<p>There are two ways to check out code from Github: - with user name/password, - authenticate with an SSH key</p> <p>In argocd, the credentials template still use <code>https</code> not <code>git@</code> but the credentials are provided via a pair of ssh keys.</p> <p>when change the url to <code>git@</code> the error became <code>authentication required</code>. Maybe the ssh key is not correct??? The private key should not be base64 encoded!!!</p> <p>If the repositories are connected seccussfully but the applications still show the error, the <code>AppProject sourceRepos</code> must also use the <code>git@</code> format when the authentication is via <code>ssh</code>.</p>"},{"location":"DevOps/ArgoCD/Error/#the-server-could-not-find-the-requested-resource","title":"the server could not find the requested resource","text":"<p>The <code>apiVersion</code> for the app such Cronjob might be old and should be updated.</p>"},{"location":"DevOps/ArgoCD/Error/#argo-will-not-report-helm-chart-rendering-error","title":"Argo will not report helm chart rendering error","text":"<p>So the pods are simply not deployed.</p>"},{"location":"DevOps/ArgoCD/Error/#argo-renders-templates-first-then-contacts-the-apiserver","title":"Argo renders templates first, then contacts the apiserver","text":"<p>In a helm chart, if the apiVersion depends on the k8s version. This could be go wrong <pre><code>{{- if .Capabilities.APIVersions.Has \"policy/v1\" }}\napiVersion: policy/v1\n{{- else }}\napiVersion: policy/v1beta1\n{{- end }}\nkind: PodDisruptionBudget\n</code></pre></p>"},{"location":"DevOps/ArgoCD/Error/#argocd-outofsync-pending-deletion","title":"argocd outofsync <code>Pending deletion</code>","text":"<p>one pod is waiting for another pod but the other one was dead.</p>"},{"location":"DevOps/ArgoCD/Error/#current-sync-status-unknown","title":"Current Sync Status \"Unknown\"","text":"<p>https://github.com/argoproj/argo-cd/issues/15489</p>"},{"location":"DevOps/ArgoCD/Error/#connection-reset-by-peer","title":"connection reset by peer","text":"<p>firewall blokced the website? <pre><code>rpc error: code = Unknown desc = Manifest generation error (cached):\n`helm dependency build` failed exit status 1:\nError: could not download https://traefik.github.io/charts/traefik/traefik-10.19.5.tgz:\nGet \"https://traefik.github.io/charts/traefik/traefik-10.19.5.tgz\":\nread tcp 12.345.678.910:42622-&gt;188.199.100.155:443: read: connection reset by peer\n</code></pre></p>"},{"location":"DevOps/ArgoCD/Error/#auto-sync-will-wipe-out-all-resources","title":"auto-sync will wipe out all resources","text":"<p>if disable a deployment, argo-cd will show:  <pre><code>Skipping sync attempt to xyz: auto-sync will wipe out all resources\n</code></pre></p> <p>Solutions: - manually sync with prune first - change config: https://github.com/argoproj/argo-cd/issues/10987   <pre><code>syncPolicy:\n  automated: \n    prune: true \n    allowEmpty: true\n    selfHeal: true \n</code></pre></p>"},{"location":"DevOps/ArgoCD/Learn/","title":"Learn","text":"<p>https://www.arthurkoziel.com/setting-up-argocd-with-helm/</p>"},{"location":"DevOps/ArgoCD/Repo/","title":"Private repo","text":"<p>https://argo-cd.readthedocs.io/en/stable/user-guide/private-repositories/</p> <p>There are two options to set the key used to connect to the private helm repo - HTTPS using username/password, or github user/token (PAT) - SSH private-public key pair with github deployment key (recommended more secure but one only for one repo) Get the user/pass/key from secret store such as Azure KeyVault using managed identity</p> <p>https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/</p> <p>Each repository must have a <code>url</code> field and, depending on whether you connect using HTTPS, SSH, or GitHub App - <code>username</code> and <code>password</code> (for <code>HTTPS</code>), - <code>sshPrivateKey</code> (for <code>SSH</code>), or - <code>githubAppPrivateKey</code> (for <code>GitHub App</code>)</p>"},{"location":"DevOps/ArgoCD/Repo/#https","title":"https","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: https://github.com/argoproj/my-private-repo\n  password: my-password\n  username: my-username\n</code></pre>"},{"location":"DevOps/ArgoCD/Repo/#ssh","title":"ssh","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: git@github.com:argoproj/my-private-repo\n  sshPrivateKey: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    ...\n    -----END OPENSSH PRIVATE KEY-----\n</code></pre>"},{"location":"DevOps/ArgoCD/Repo/#repository-credentials-template","title":"Repository Credentials template","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-repo-1\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: https://github.com/argoproj/private-repo-1\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: my-repo-2\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: https://github.com/argoproj/private-repo-2\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo-creds\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repo-creds\nstringData:\n  type: git\n  url: https://github.com/argoproj\n  password: my-password\n  username: my-username\n</code></pre>"},{"location":"DevOps/ArgoCD/Repo/#github-user-and-token","title":"github user and token","text":""},{"location":"DevOps/ArgoCD/Repo/#store-pat-as-a-kubernetes-secret-in-the-same-namespace-as-your-argocd-installation","title":"Store PAT as a Kubernetes secret in the same namespace as your ArgoCD installation","text":"<pre><code>kubectl create secret generic github-creds \\\n  --namespace &lt;argocd-namespace&gt; \\\n  --from-literal=username=&lt;github-username&gt; \\\n  --from-literal=password=&lt;github-personal-access-token&gt;\n</code></pre>"},{"location":"DevOps/ArgoCD/Repo/#modify-the-argocd-repository-configuration-to-use-the-personal-access-token","title":"Modify the ArgoCD repository configuration to use the personal access token","text":"<p>We can do this by adding the following to the argocd-cm ConfigMap: <pre><code>data:\n  repositories: |\n    - url: https://github.com/&lt;github-username&gt;/&lt;repository-name&gt;.git\n      type: helm\n      name: &lt;repo-name&gt;\n      usernameSecret:\n        name: github-creds\n        key: username\n      passwordSecret:\n        name: github-creds\n        key: password\n</code></pre></p>"},{"location":"DevOps/ArgoCD/Repo/#get-userpass-from-azure-key-vault","title":"get user/pass from azure key vault","text":"<p>in values.yaml <pre><code>configs:\n  repositories:\n    private-helm-repo:\n      url: https://my-private-chart-repo.internal\n      #   name: private-repo\n      #   type: helm\n      #   password: my-password\n      #   username: my-username\n</code></pre></p> <p>in resource <code>helm_release</code> <pre><code>  set {\n    name  = \"configs.repositories.dev.username\"\n    value = \"${jsondecode(data.azurerm_key_vault_secret.dev_creds.value)[\"username\"]}\"\n  }\n  set {\n    name  = \"configs.repositories.dev.password\"\n    value = \"${jsondecode(data.azurerm_key_vault_secret.dev_creds.value)[\"password\"]}\"\n  }\n</code></pre></p> <p>Here are some best practices to consider when using Azure Key Vault to retrieve secrets for your Helm charts: - Limit access to the Azure Key Vault. Only grant access to the Azure Key Vault to the entities (users, groups, or services) that need it. Ensure that you have an access policy in place that restricts access to only the secrets that are required. - Use Managed Identity. Use Managed Identity to authenticate to Azure Key Vault instead of using a client ID and secret, if possible. Managed Identity provides a secure way to access Azure resources without requiring any explicit credentials. - Store the Azure Key Vault secrets in encrypted form. Ensure that the secrets stored in the Azure Key Vault are encrypted at rest and in transit. This will prevent unauthorized access to the secrets. - Use Azure Key Vault with TLS. Configure your Azure Key Vault to use TLS, which will encrypt data in transit between the client and the vault.</p>"},{"location":"DevOps/ArgoCD/Repo/#ssh-key-pair","title":"ssh key pair","text":"<p>https://levelup.gitconnected.com/connect-argocd-to-your-private-github-repository-493b1483c01e</p> <p>https://rderik.com/blog/setting-up-access-to-a-private-repository-in-argocd-with-ssm-parameter-store-and-external-secrets-operator/</p> <p>Setup ArgoCD and Github so argocd can have access to private github repos.</p>"},{"location":"DevOps/ArgoCD/Repo/#generate-ssh-key-pair","title":"Generate ssh key-pair","text":"<pre><code>ssh-keygen -t rsa -b 4096 -C \"argocd key for private repo\" -f id_rsa_argocd_repo\n</code></pre>"},{"location":"DevOps/ArgoCD/Repo/#add-deploy-key-on-github-repo","title":"Add deploy key on GitHub repo","text":"<p>On the repository settings, Deploy keys -&gt; Add deploy key -&gt; add the generated public key</p>"},{"location":"DevOps/ArgoCD/Repo/#setup-ssh-authentication-for-argocd","title":"Setup ssh authentication for ArgoCD","text":"<p>Encoding private key via base64: <pre><code>cat id_rsa_argocd_repo | base64\n</code></pre></p> <p>Store the private key secret using Kubernetes <code>Secret</code> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: argocd-repo-key\n  namespace: argocd\ntype: Opaque\ndata:\n  privateKey: one line base64 private key\n</code></pre></p> <p>Terraform <pre><code>resource \"kubernetes_secret\" \"argocd\" {\n  metadata {\n    name = \"argocd-repo-key\"\n  }\n\n  data = {\n    privateKey = base64encode(\"&lt;privatekey&gt;\")\n  }\n\n  type = \"helm.sh/private\"\n}\n</code></pre></p> <p>Link the secret by <code>ConfigMap</code> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\n  namespace: argocd\n  labels:\n    app.kubernetes.io/name: argocd-cm\n    app.kubernetes.io/part-of: argocd\ndata:\n  repositories: |\n    - url: git@github.com:&lt;user&gt;/repo.git\n      sshPrivateKeySecret:\n        name: argocd-repo-key\n        key: privateKey\n</code></pre></p> <p>how to get the privateKey from Azure Key Vault??? https://github.com/argoproj/argo-cd/issues/7878</p> <p>Terraform <pre><code>resource \"kubernetes_config_map\" \"argocd\" {\n  metadata {\n    name = \"argocd-cm\"\n  }\n\n  data = {\n    \"repositories.yaml\" = &lt;&lt;-EOT\n    apiVersion: v1\n    repositories:\n    - name: my-private-repo\n      url: https://my-private-repo.com/charts\n      username: ${kubernetes_secret.private_helm_repo_creds.data[\"username\"]}\n      password: ${kubernetes_secret.private_helm_repo_creds.data[\"password\"]}\n    EOT\n  }\n}\n</code></pre></p> <p>Change <code>repoURL</code> from https to ssh and apply <pre><code>https://github.com/&lt;user&gt;/repo.git\ngit@github.com:&lt;user&gt;/repo\n</code></pre></p>"},{"location":"DevOps/ArgoCD/Values/","title":"Values","text":""},{"location":"DevOps/ArgoCD/Values/#repositories","title":"repositories","text":"<pre><code>configs:\n  repositories:\n    helm-acr-repo:      \n      type: helm\n      name: helm-acr\n      url: https://myacr.azurecr.io\n      enableOCI: true\n</code></pre>"},{"location":"DevOps/Buildx/Buildctl/","title":"Buildctl","text":"<p>https://github.com/moby/buildkit/blob/master/docs/reference/buildctl.md <pre><code>buildctl [global options] command [command options] [arguments...]\n</code></pre></p> <p>https://github.com/moby/buildkit#imageregistry</p>"},{"location":"DevOps/Buildx/Buildctl/#install-buildkit-client","title":"install buildkit client","text":"<pre><code>ARG BUILDKIT_VER=v0.17.5\nRUN url=\"https://github.com/moby/buildkit/releases/download/\" \\\n    \"${url}${BUILDKIT_VERSION}/buildkit-${BUILDKIT_VER}.linux-amd64.tar.gz\" &amp;&amp; \\\n    curl -sL $url -o buildkit.tar.gz &amp;&amp; \\\n    tar xvf buildkit.tar.gz &amp;&amp; \\\n    mv bin/buildctl /usr/local/bin/ &amp;&amp; \\\n    rm -rf buildkit.tar.gz bin\n</code></pre>"},{"location":"DevOps/Buildx/Buildctl/#transition-from-docker-to-buildkit","title":"transition from docker to buildkit","text":"<p>https://dille.name/slides/2020-05-28/110_ecosystem/buildkit/transition.final/</p> <p>example <pre><code>docker build \\\n    . -f ./docker/linux/my-app.docker \\\n    --build-arg NAME=my-app \\\n    --no-cache --force-rm \\\n    -t docker.example.com/uat/my-app:0.1.0\n\nbuildctl build \\\n    --frontend dockerfile.v0 \\\n    --local context=. \\\n    --local dockerfile=./docker/linux/ \\ # can only be a directory\n    --opt filename=my-app.docker \\       # the dockerfile name\n    --opt build-arg:NAME=my-app \\  \n    --opt no-cache=true   \n    --output type=image,name=docker.example.com/uat/my-app:0.1.0\n</code></pre></p> <p>Note that the <code>NAME</code> is a variable define in dockerfile <code>IMAGE_NAME=${NAME}</code>.</p>"},{"location":"DevOps/Buildx/Buildctl/#buildkitd-address","title":"buildkitd address","text":"<p>Global option. Default: <code>unix:///run/buildkit/buildkitd.sock</code> <pre><code>--addr tcp://buildkitd:1234\n</code></pre></p>"},{"location":"DevOps/Buildx/Buildctl/#deploy-buildkit-in-kubernetes","title":"deploy buildkit in kubernetes","text":"<p>https://kubernetes.courselabs.co/labs/buildkit/ - deploy <code>buildkitd</code> - deploy agent pod with <code>buildctl</code> - test build with a dummy dockerfile</p> <pre><code># install wget\nsudo apt update &amp;&amp; sudo apt install wget\n\n# install buildkit release\nwget https://github.com/moby/buildkit/releases/download/v0.15.2/buildkit-v0.15.2.linux-amd64.tar.gz\ntar xvf buildkit-v0.15.2.linux-amd64.tar.gz\n\n# download a dockerfile\ncd bin\nwget --no-check-certificate https://raw.githubusercontent.com/courselabs/kubernetes/main/labs/docker/simple/Dockerfile\nmkdir linux\nmv ./Dockerfile ./linux/my-app.docker\ncat ./linux/my-app.docker\n\n# build the image\n./buildctl --addr tcp://buildkitd:1234 build --frontend=dockerfile.v0 --local context=. --local dockerfile=./linux/ --opt filename=my-app.docker --output type=image,name=docker.example.com/dev/test,push=true\n</code></pre>"},{"location":"DevOps/Buildx/Buildctl/#build","title":"build","text":"<p>To build and push an image using Dockerfile: <pre><code>buildctl build \\\n    --frontend dockerfile.v0 \\\n    --local context=. \\\n    --local dockerfile=./docker/linux/ \\ # can only be a directory\n    --opt filename=my-app.docker \\       # the dockerfile name\n    --opt target=foo \\\n    --opt build-arg:name=my-app \\\n</code></pre></p>"},{"location":"DevOps/Buildx/Buildctl/#output","title":"output","text":"<pre><code># to local folder\n--output type=local,dest=path/to/output-dir\n# to registry\n--output type=image,name=docker.io/username/image,push=true\n</code></pre> <p>have multiple tags: https://stackoverflow.com/questions/58691068/buildctl-command-to-tag-multiple-images <pre><code>buildctl build \\\n  --frontend dockerfile.v0 \\\n  --local context=. \\\n  --local dockerfile=. \\\n  --output type=image,\\\"name=test/repo:tag1,test/repo:tag2\\\",push=true\n</code></pre></p>"},{"location":"DevOps/Buildx/Buildctl/#cache-azure-blob-storage","title":"cache: azure blob storage","text":"<pre><code># export cache to local directory\n--export-cache type=local\n# use azure blob storage\n--import-cache type=azblob,account_url=https://myaccount.blob.core.windows.net,name=my_image \\  \n--export-cache type=azblob,account_url=https://myaccount.blob.core.windows.net,name=my_image \\\n</code></pre>"},{"location":"DevOps/Buildx/Buildctl/#metadata-to-file","title":"metadata to file","text":"<p>will write <code>metadata.json</code> only when <code>--output</code> is set. <pre><code>--metadata-file metadata.json\n</code></pre></p> <p>cat metadata.json <pre><code>{\n  \"containerimage.config.digest\": \"sha256:bf6010de5e36c4a6d2cefd562e65d25b8713c36575\",\n  \"containerimage.descriptor\": {\n    \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n    \"digest\": \"sha256:09423f01912c6cacd4363ac24bfd90760ce52606e8c71cf4f\",\n    \"size\": 891,\n    \"platform\": {\n      \"architecture\": \"amd64\",\n      \"os\": \"linux\"\n    }\n  },\n  \"containerimage.digest\": \"sha256:09423f01912c6cacd4363ac24bfd90760ce52606e8c71cf4f\",\n  \"image.name\": \"docker.example.com/dev/test\"\n}\n</code></pre></p>"},{"location":"DevOps/Buildx/Buildctl/#buildkit_host","title":"BUILDKIT_HOST","text":"<p>We should set the env var <code>BUILDKIT_HOST</code> for a <code>buildkitd</code> service like this <pre><code>BUILDKIT_HOST=tcp://&lt;buildkitd-service-name&gt;:&lt;port&gt;\n</code></pre></p> <p>if the <code>&lt;buildkitd-service-name&gt;</code> is incorrect, we might get the following error: <pre><code>error: listing workers for Build: failed to list workers: Unavailable: connection error:\ndesc = \"transport: Error while dialing: dial tcp: lookup buildkitd on 10.255.0.9:53: server misbehaving\"\n</code></pre></p>"},{"location":"DevOps/Buildx/Buildkit/","title":"Buildkit","text":""},{"location":"DevOps/Buildx/Buildkit/#disk-usage","title":"disk usage","text":"<p>https://github.com/moby/buildkit/issues/4218</p> <p>check where disk has been used and the solution!!!</p> <p>run into the buildkitd pod: <pre><code>du -sh\n</code></pre></p>"},{"location":"DevOps/Buildx/Buildkit/#docs","title":"docs","text":"<p>https://docs.docker.com/build/buildkit/</p>"},{"location":"DevOps/Buildx/Buildkit/#how-it-works","title":"how it works","text":"<p>https://depot.dev/blog/buildkit-in-depth - buildkit coverts everything from frontends and backends to LLB (low-level build) and DAGs (directed acyclic graphs) - to take advantage of parallelization, must rewrite Dockerfile to use multi-stage builds.</p>"},{"location":"DevOps/Buildx/Buildkit/#k8s-example","title":"k8s example","text":"<p>https://kubernetes.courselabs.co/labs/buildkit/</p>"},{"location":"DevOps/Buildx/Buildkit/#deploy-example","title":"deploy example","text":"<p>https://medium.com/@t-velmachos/build-docker-images-on-k8s-faster-with-buildkit-3443e36aef2e</p>"},{"location":"DevOps/Buildx/Buildkit/#extract-info","title":"extract info","text":"<pre><code>buildctl --version      # get buildkit version\nbuildctl --frontend dockerfile.v0 --opt export=true --local . # get active builders\nbuildctl debug labels\n</code></pre>"},{"location":"DevOps/Buildx/Buildkit/#using-k8s-internal-url","title":"using k8s internal url","text":"<pre><code>./buildctl --addr tcp://buildkitd:1234 build \\\n--frontend=dockerfile.v0 \\\n--local context=. \\\n--local dockerfile=./linux/ \\\n--opt filename=my-app.docker \\\n--output type=image,name=&lt;registry-service-name&gt;.&lt;namespace&gt;.svc.cluster.local:5000/dev/test,push=true,registry.insecure=true\n</code></pre>"},{"location":"DevOps/Buildx/Buildkit/#local-registry-using-http","title":"local registry using http","text":"<p>https://stackoverflow.com/questions/75192693/how-to-use-buildctl-with-localhost-registry-with-tls</p> <p>remote error: <code>tls: unrecognized name</code> <pre><code>buildctl build \\\n--frontend=dockerfile.v0 \\\n--local context=. \\\n--local dockerfile=. \\\n--output type=image,name=192.168.0.110:8082/docker-local/test,push=true,registry.insecure=true \\\n--export-cache type=registry,ref=192.168.0.110:8082/docker-local/test,mode=max,push=true,registry.insecure=true \\\n--import-cache type=registry,ref=192.168.0.110:8082/docker-local/test,registry.insecure=true \n</code></pre></p> <p>The buildkit daemon needs to be run with a configuration file that specifies the registry is <code>http</code> instead of <code>https</code>.  See the documentation on buildkitd.toml: https://github.com/moby/buildkit/blob/master/docs/buildkitd.toml.md <pre><code>[registry.\"192.168.0.110:8082\"]\n  http = true\n  insecure = true #required?\n</code></pre> The file path is  - rootful mode: <code>/etc/buildkit/buildkitd.toml</code> - rootless mode: <code>~/.config/buildkit/buildkitd.toml</code></p>"},{"location":"DevOps/Buildx/Buildkit/#workaround-for-dockerfile-from-http-registry","title":"workaround for dockerfile from http registry","text":"<p>https://github.com/moby/buildkit/issues/2044</p> <p>Use Secret or ConfigMap to mount the config file:  <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: buildkit\n  namespace: buildkit\ndata:\n  buildkitd.toml: |\n    debug = false\n    [worker.containerd]\n      namespace = \"buildkit\"\n    [registry.\"registry:5000\"]\n      http = true\n      insecure = true\n---\n      containers:\n      - name: buildkitd\n        image: moby/buildkit:v0.9.0\n        args:\n          - --addr\n          - unix:///run/buildkit/buildkitd.sock\n          - --addr\n          - tcp://0.0.0.0:1234\n          - --config=/home/user/.config/buildkit/buildkitd.toml\n        securityContext:\n          privileged: true #cannot be false\n        ports:\n          - name: buildkitd\n            containerPort: 1234\n        volumeMounts:\n        - name: buildkitd-config\n          mountPath: /home/user/.config/buildkit\n      volumes:\n      - name: buildkitd-config\n        configMap:\n          name: buildkitd\n</code></pre></p> <p>Worked solution: If you are building on top of the job.rootless.yaml template then you can add <code>--config /etc/buildkit/buildkitd.toml</code> in https://github.com/moby/buildkit/blob/master/examples/kubernetes/job.rootless.yaml#L31-L32</p>"},{"location":"DevOps/Buildx/Buildkit/#self-signed-vertificate","title":"self-signed vertificate","text":"<p>https://github.com/moby/buildkit/issues/4149 - install the self-signed certificates on the machine - maybe also works for installing the certs in the buildkit docker image - if the registry in the same kubernetes cluster, using internal url</p> <p>https://docs.docker.com/build/buildkit/configure/#setting-registry-certificates - set self-signed registry certificate</p>"},{"location":"DevOps/Buildx/Buildkit/#failed-to-do-request-io-timeout","title":"failed to do request <code>i/o timeout</code>","text":"<p>https://github.com/docker/buildx/issues/191 - used the wrong url for the registry - did not include the port when using internal url</p>"},{"location":"DevOps/Buildx/Buildkit/#why-there-is-no-metadatajson","title":"why there is no <code>metadata.json</code>","text":"<p>when set <code>--metadata-file metadata.json</code> after build I cannot find the json file. reason: the json file will be created only if you set <code>--output</code></p>"},{"location":"DevOps/Buildx/Buildkit/#authorization","title":"authorization","text":"<p><pre><code>failed to authorize:\nfailed to fetch anonymous token:\nunexpected status from GET request to\nhttps://my_acr.azurecr.io/oauth2/token?scope=repository%3Adev%2Ftest%3Apull%2Cpush&amp;service=my_acr.azurecr.io:\n401 Unauthorized\n</code></pre> https://github.com/moby/buildkit/issues/2136 - The docker registry credentials should be on the <code>buildctl</code> client side, not on the buildkit daemon</p>"},{"location":"DevOps/Buildx/Caching/","title":"Caching","text":""},{"location":"DevOps/Buildx/Caching/#how-buildkit-caching-works","title":"How buildkit caching works","text":"<p>https://www.augmentedmind.de/2023/11/19/advanced-buildkit-caching</p>"},{"location":"DevOps/Buildx/Caching/#improve-ci-performance-using-caching","title":"improve CI performance using caching","text":"<p>https://medium.com/vouchercodes-tech/speeding-up-ci-in-kubernetes-with-docker-and-buildkit-7890bc47c21a</p>"},{"location":"DevOps/Buildx/Learn/","title":"Learn","text":""},{"location":"DevOps/Buildx/Learn/#build-linux-container-images-on-aks-using-docker-buildx-without-docker-engine","title":"Build Linux container images on AKS using <code>docker buildx</code> without Docker engine","text":"<p>https://github.com/clarenceb/docker-buildx-demo</p>"},{"location":"DevOps/Buildx/Learn/#deploy-buildkit-in-aks","title":"deploy <code>buildkit</code> in aks","text":"<p>https://medium.com/@aabeing/aks-as-azure-devops-agents-buildkit-5af8e5cd43d1</p>"},{"location":"DevOps/Buildx/Mode/","title":"Buildkit deployment mode","text":""},{"location":"DevOps/Buildx/Mode/#modes","title":"modes","text":"<p>https://github.com/moby/buildkit/tree/master/examples/kubernetes - <code>Pod</code>: good for quick-start - <code>Deployment + Service</code>: good for random load balancing with registry-side cache - <code>StateFulset</code>: good for client-side load balancing, without registry-side cache - <code>Job</code>: good if you don't want to have daemon pods</p>"},{"location":"DevOps/Buildx/Mode/#deployment-service","title":"Deployment + Service","text":"<p>I'd recommend using a combination of <code>Deployment + Service</code> for deploying BuildKit in AKS with a self-hosted TFS agent.</p> <p>Scalability and Load Balancing: - Deployment allows you to scale your BuildKit pods up or down based on demand, ensuring optimal resource utilization. - Service provides random load balancing across your BuildKit pods, distributing incoming build requests evenly.   This is crucial for handling random spikes in build workloads.</p> <p>Caching: - While Deployment + Service doesn't provide registry-side caching directly,   you can leverage Azure Container Registry (ACR) as your container image registry.   ACR offers built-in caching capabilities, which can significantly improve build performance by reusing previously downloaded layers.</p> <p>Self-Hosted TFS Agent: - You can deploy your self-hosted TFS agent as a separate Deployment with a Service.   This ensures the agent pods are always available to handle build requests triggered by TFS.</p> <p>Here's a breakdown of how each mode fits your scenario: - Pods: While good for quick-starts, Pods aren't ideal for production due to limitations in scaling and load balancing. - StatefulSet: Client-side load balancing might not be necessary in your case if you're using a container image registry like ACR with built-in caching. - Job: Jobs are suitable for one-time or infrequent tasks, not ideal for continuously running BuildKit services.</p> <p>Deployment and Service Configuration: 1. Create a Deployment for your BuildKit pods, specifying the desired number of replicas for scaling. 2. Define a Service of type <code>LoadBalancer</code> to expose your BuildKit pods externally. This allows TFS to access the BuildKit service for initiating builds. 3. Configure ACR as your container image registry to benefit from built-in caching mechanisms.</p> <p>By combining Deployment, Service, and ACR, you'll achieve a scalable, load-balanced BuildKit deployment  with caching capabilities, making it suitable for production use with your self-hosted TFS agent.</p>"},{"location":"DevOps/Buildx/Registry/","title":"Registry","text":""},{"location":"DevOps/Buildx/Registry/#push-to-acr-from-aks-pod","title":"push to acr from aks pod","text":"<p>https://github.com/Azure/acr/issues/582 <pre><code>error: failed to solve: failed to push my-acr.azurecr.io/dev/test:0.0.1: \nfailed to authorize: failed to fetch anonymous token:\nunexpected status: 401 Unauthorized\n</code></pre> bug in buildkit? - https://github.com/moby/buildkit/blob/ffe2301031c8f8bfb8d5fc5034e5e509c5624913/session/auth/authprovider/authprovider.go#L91</p>"},{"location":"DevOps/Buildx/Registry/#buildkit-private-registry","title":"buildkit + private registry","text":"<p>connect to private registry: - https://medium.com/@aabeing/aks-as-azure-devops-agents-buildkit-5af8e5cd43d1 - If credentials are required, buildctl will attempt to read Docker configuration file $DOCKER_CONFIG/config.json. - $DOCKER_CONFIG defaults to ~/.docker.   <pre><code>{\n  \"auths\": {\n      \"docker.example.com\": {\n        \"identitytoken\": \"xyz=\"\n      }\n  }\n}\n</code></pre> script to create config.json <pre><code>acr_name=my_acr\ndocker_config_file=~/.docker/config.json\n# retrieve token\ntoken_value=$(az acr login --name $acr_name --expose-token --output tsv --query accessToken)\n# create a sample file if not exists\nif [ ! -f $docker_config_file ]; then\n    echo '{}' &gt; $docker_config_file\nfi\n# save the token in required docker config file format\njq -n --arg token $token_value '{\"auths\":{\"'$acr_name'.azurecr.io\":{\"identitytoken\":$token}}}' &gt; tmp &amp;&amp; mv tmp $docker_config_file\n</code></pre></p>"},{"location":"DevOps/CICD/Bamboo/","title":"Bamboo","text":""},{"location":"DevOps/CICD/GitOps/","title":"GitOps","text":"<p>Use <code>Terraform</code> to manage Cloud Provider services (Network, DNS, Compute, etc).</p> <p>Use <code>ArgoCD</code> to manage Kubernetes deployments, typically packaged as different <code>Helm chart</code>s, with dynamic properties defined as Helm values.</p>"},{"location":"DevOps/CICD/Jenkins/","title":"Jenkins","text":""},{"location":"DevOps/CICD/Jenkins/#install-jenkins-locally","title":"Install Jenkins locally","text":"<p><pre><code>mkdir jenkins\ncd jenkins\ndocker run --name jenkins \\\n           -u root \\\n           -d \\\n           -v $(pwd):/var/jenkins_home \\\n           -v /var/run/docker.sock:/var/run/docker.sock \\\n           -p 80:8080 \\\n           -p 50000:50000 \\\n           jenkinsci/blueocean\n</code></pre> Note:   * -u root configures to run Jenkins by root user   * -d detaches the container   * -v $(pwd):/var/jenkins_home mounts the current directory to Jenkins home inside the container   * -v /var/run/docker.sock:/var/run/docker.sock mounts Docker socks   * -p 80:8080 maps port 80 in the host to port 8080 inside the container (if port 80 is used change to another port e.g., 8086)   * -p 50000:50000 maps ports 50000 which is the default port for agent registration   * jenkinsci/blueocean is the image maintained by jenkinsci.</p> <p>Then open http://127.0.0.1:80 and follow the directions</p>"},{"location":"DevOps/CICD/Jenkins/#integrate-with-github-private-repo","title":"Integrate with github private repo","text":"<p>Do other steps same as the public repo.</p>"},{"location":"DevOps/CICD/Jenkins/#setup-ssh-key","title":"setup ssh key","text":"<ul> <li><code>docker exec -it jenkins /bin/bash</code></li> <li><code>ssh-keygen -t rsa</code></li> <li><code>cat id_rsa.pub</code> copy key to github: repo settings -&gt; Deploy keys -&gt; Add deploy key</li> <li>add ssh private key inside Jenkins:<ul> <li>manage jenkins -&gt; manage credentials -&gt;</li> <li>Kind: ssh username with private key -&gt;</li> <li>Username: github-repo</li> </ul> </li> <li>configure jenkins job to use SSH keys:<ul> <li>job -&gt; source code management</li> <li>after repo url -&gt; select credential (github-repo) -&gt; apply</li> </ul> </li> </ul>"},{"location":"DevOps/CICD/Jenkins/#use-the-ssh-repo-url-not-the-https-url-on-jenkins","title":"use the SSH repo URL, not the HTTPS URL on jenkins","text":"<ul> <li>HTTPS URL like https://github.com/user/repo.git</li> <li>SSH URL like git@github.com:user/repo.git</li> </ul>"},{"location":"DevOps/CICD/Jenkins/#errors","title":"errors","text":"<p>ERROR: Couldn't find any revision to build. Verify the repository and branch configuration for this job.</p> <p>Solution: job configuration -&gt; Source Code Management -&gt; Git -&gt; Branches to build -&gt; it is default to \"/master\" -&gt; change to \"/main\" github repo project is under \"main\" branch</p>"},{"location":"DevOps/CICD/Link/","title":"Link","text":""},{"location":"DevOps/CICD/Link/#pipeline","title":"pipeline","text":"<p>https://github.com/letmaik/vsts-docs/tree/master/docs/articles</p> <p>https://github.com/MicrosoftDocs/pipelines-anaconda/blob/master/azure-pipelines.yml</p>"},{"location":"DevOps/CICD/Trigger/","title":"Trigger","text":""},{"location":"DevOps/CICD/Trigger/#cicd-pipeline","title":"CI/CD pipeline","text":"<pre><code>trigger:\n  - master\n  - refs/tags/*\npr:\n  - master\n</code></pre> <p><code>trigger: - master - refs/tags/*</code>: - The pipeline is triggered on:   - Changes pushed directly to the master branch (e.g. commits, merges).   - Any new Git tag (e.g. v1.0.0, release-2025) \u2014 this is useful for releases.</p> <p><code>pr: - master</code>: - Triggered when a pull request   - is created or updated with the master branch as the target   - i.e. PRs that want to merge into master</p>"},{"location":"DevOps/CICD/Trigger/#image-vmimagename-ubuntu-latest","title":"image <code>vmImageName: 'ubuntu-latest'</code>","text":"<ul> <li>Not good for production same for python version.</li> <li>Best to use a pinned version: <code>vmImageName: 'ubuntu-22.04'</code></li> </ul>"},{"location":"DevOps/Config/Ansible/","title":"Ansible","text":""},{"location":"DevOps/Config/Chef/","title":"Chef","text":""},{"location":"DevOps/Config/Puppet/","title":"Puppet","text":""},{"location":"DevOps/Container/Install/","title":"Install","text":""},{"location":"DevOps/Container/Install/#base-image","title":"base image","text":"<p>Use tag or digest? - A tag like ubuntu:22.04 is a human-readable, mutable pointer to an image. The image can be updated with the same tag. - A digest, such as sha256:1aa979..., is a cryptographic hash of the image's content. It is a unique, immutable identifier for that exact image. This will make the pipeline always reproducible as the image is not changed. It also has the benefit of security. - Better to include both tag and digest for readability and reproducibility <pre><code>FROM ubuntu:22.04\nFROM ubuntu@sha256:1aa979d85661c488ce030ac292876cf6ed04535d3a237e49f61542d8e5de5ae0\nFROM ubuntu:22.04@sha256:1aa979d85661c488ce030ac292876cf6ed04535d3a237e49f61542d8e5de5ae0\n</code></pre></p>"},{"location":"DevOps/Container/Install/#default-shell","title":"default shell","text":"<p>use default shell or bash? - default shell: <code>/bin/sh</code> - overwride os' default shell to run commands such as RUN, CMD, or ENTRYPOINT - allows us to use features and syntax that are specific to the Bash shell, not available in default shell <pre><code>SHELL [\"/bin/bash\", \"-c\"]\n</code></pre></p>"},{"location":"DevOps/Container/Install/#set-env-vars","title":"set env vars","text":"<pre><code>ENV DEBIAN_FRONTEND=noninteractive\nENV TZ=Australia/Sydney\n</code></pre>"},{"location":"DevOps/Container/Install/#install-packages","title":"install packages","text":"<pre><code>USER root # switch to root if required\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y vim curl dnsutils traceroute netcat telnet jq\nUSER user-name # switch back to user\n</code></pre>"},{"location":"DevOps/Container/Install/#install-ssl-certificates","title":"install SSL certificates","text":"<pre><code>COPY docker/dev1.crt docker/dev2.crt /usr/local/share/ca-certificates/extra/\nRUN update-ca-certificates\n</code></pre>"},{"location":"DevOps/Container/Install/#install-ms-odbc-driver","title":"install MS ODBC driver","text":"<pre><code>ARG MSODBC_URL=https://packages.microsoft.com\nRUN curl -fsSL ${MSODBC_URL}/keys/microsoft.asc | apt-key add - \\\n    &amp;&amp; curl -fsSL ${MSODBC_URL}/config/ubuntu/22.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list \\\n    &amp;&amp; apt-get update \\\n    &amp;&amp; ACCEPT_EULA=Y apt-get install -y gnupg2 msodbcsql18=18.4.1.1-1\n</code></pre>"},{"location":"DevOps/Container/Install/#install-oracle-client-sqlplus-and-odbc-driver","title":"install Oracle Client, SqlPlus and ODBC driver","text":"<ul> <li>from 23ai, there is a new folder <code>META-INF</code> used for verifying the zip file</li> <li>we need to delete the folder to avoid the <code>replace</code> prompt</li> <li>https://www.oracle.com/au/database/technologies/instant-client/linux-x86-64-downloads.html <pre><code>ARG ORACLE_URL=https://download.oracle.com/otn_software/linux/instantclient/2390000\nARG ORACLE_CLIENT=${ORACLE_URL}/instantclient-basiclite-linux.x64-23.9.0.25.07.zip\nARG ORACLE_SQLPLUS=${ORACLE_URL}/instantclient-sqlplus-linux.x64-23.9.0.25.07.zip\nARG ORACLE_ODBC=${ORACLE_URL}/instantclient-odbc-linux.x64-23.9.0.25.07.zip\nADD ${ORACLE_CLIENT} /opt/oracle/client.zip\nADD ${ORACLE_SQLPLUS} /opt/oracle/sqlplus.zip\nADD ${ORACLE_ODBC} /opt/oracle/odbc.zip\nRUN apt-get install -y unixodbc unzip libaio1 &amp;&amp; \\\n    &amp;&amp; unzip /opt/oracle/client.zip -d /opt/oracle/ &amp;&amp; rm -r /opt/oracle/META-INF \\\n    &amp;&amp; unzip /opt/oracle/sqlplus.zip -d /opt/oracle/ &amp;&amp; rm -r /opt/oracle/META-INF \\\n    &amp;&amp; unzip /opt/oracle/odbc.zip -d /opt/oracle/ &amp;&amp; rm -r /opt/oracle/META-INF \\\n    &amp;&amp; mv /opt/oracle/instantclient_23_9 /opt/oracle/instantclient \\\n    &amp;&amp; /opt/oracle/instantclient/odbc_update_ini.sh / /opt/oracle/instantclient \\\n    &amp;&amp; echo /opt/oracle/instantclient/ &gt; /etc/ld.so.conf.d/oracle-instantclient.conf \\\n    &amp;&amp; ldconfig &amp;&amp; \\\n    &amp;&amp; rm /opt/oracle/*.zip\nENV PATH=\"/opt/oracle/instantclient/:${PATH}\"\n</code></pre></li> </ul>"},{"location":"DevOps/Container/Install/#install-miniforge","title":"install miniforge","text":"<pre><code>ARG MINIFORGE_VERSION=25.3.1-0\nARG MINIFORGE_URL=https://github.com/conda-forge/miniforge/releases/download\nARG MINIFORGE_PATH=${MINIFORGE_URL}/${MINIFORGE_VERSION}/Miniforge3-${MINIFORGE_VERSION}-Linux-x86_64.sh\nADD --chown=user_name:group_name ${MINIFORGE_PATH} /home/user_name/miniforge.sh\n</code></pre>"},{"location":"DevOps/Container/Registry/","title":"Registry","text":""},{"location":"DevOps/Container/Registry/#create-a-private-registry-for-k8s-container-images","title":"create a private registry for k8s container images","text":""},{"location":"DevOps/Container/Registry/#deployment","title":"deployment","text":"<p>https://hub.docker.com/_/registry <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: registry\n  namespace: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: registry\n  template:\n    metadata:\n      labels:\n        app: registry\n    spec:\n      containers:\n        - name: registry\n          image: registry:2.8.2\n          env:\n            - name: REGISTRY_VALIDATION_DISABLED\n              value: \"true\"\n            - name: REGISTRY_STORAGE_DELETE_ENABLED\n              value: \"true\"\n          ports:\n            - containerPort: 5000\n          volumeMounts:\n            - name: registry-data\n              mountPath: /var/lib/registry\n      volumes:\n        - name: registry-data\n          hostPath:\n            path: /mnt/dev/container-images \n</code></pre></p>"},{"location":"DevOps/Container/Registry/#service","title":"service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: registry\n  namespace: dev\nspec:\n  ports:\n    - name: http\n      targetPort: 5000\n      port: 5000\n  selector:\n    app: registry  \n</code></pre>"},{"location":"DevOps/Container/Registry/#ingress","title":"ingress","text":"<pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: registry\n  namespace: dev\n  annotations:\n    kubernetes.io/ingress.class: \"traefik\"\nspec:\n  rules:\n    - host: docker.example.com\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: registry\n              servicePort: http\n  tls:\n    - secretName: example.com-tls-cert  \n</code></pre>"},{"location":"DevOps/Docker/Add/","title":"Add","text":"<ul> <li>Destination address must not be relative. Something like <code>~/dev</code> will not copy anything to the folder.</li> <li>Docker requires root escalation in order to execute an image, that creates some problem with files creation. The copied file is in root usr group and cannot be removed.</li> </ul>"},{"location":"DevOps/Docker/Add/#cannot-actually-delete-files-in-a-previous-layer","title":"cannot actually delete files in a previous layer","text":"<p>https://pythonspeed.com/articles/docker-build-secrets/ \\ NOTE: Deleting a file does not actually remove it from the image, because Docker uses layer caching: all previous layers are still present in the image. That means the secret ends up in one of the image's layers, even if you delete it in a later layer.</p> <p>https://www.educative.io/answers/how-to-remove-directories-and-files-in-another-layer-using-docker Solutions: - use <code>WORKDIR</code> - use <code>VOLUME</code> - use multistage build</p>"},{"location":"DevOps/Docker/Add/#multistage-build-copy-install-and-then-delete-the-copied-files","title":"multistage build: copy install and then delete the copied files","text":"<p>https://rabbithole.wwwdotorg.org/2021/03/02/1-avoiding-docker-add-copy-layers.html</p> <p>Docker <code>buildkit</code> added a <code>--mount</code> option to the <code>RUN</code> statement. must set <code>export DOCKER_BUILDKIT=1</code>. The first stage image will not be included. <pre><code># syntax=docker/dockerfile:1.2\nFROM ubuntu:20.04 AS downloader\nADD https://server.com/foo.deb /downloads\n\nFROM ubuntu:20.04\nRUN --mount=type=bind,from=downloader,source=/downloads,target=/downloads dpkg -i /downloads/*.deb\n</code></pre> In this case, the layers in the first stage will be dropped after the second stage run. So the conda package files will not be preserved in the final image.</p>"},{"location":"DevOps/Docker/Add/#add-failed-forbidden-path-outside-the-build-context","title":"ADD failed: Forbidden path outside the build context","text":"<p>This error occurs because the path you're trying to add with the ADD instruction is located outside the Docker build context. Docker restricts file operations to within the build context directory for security reasons.</p> <p>To resolve this issue, we need to copy the folder to the build contect directory.</p>"},{"location":"DevOps/Docker/Add/#copy-content-of-folder-x-into-folder-y","title":"copy content of folder x into folder y","text":"<p>https://stackoverflow.com/questions/26504846/copy-directory-to-another-directory-using-add-command <pre><code>ADD x /home/user/y/\n</code></pre></p>"},{"location":"DevOps/Docker/Add/#copy-folder-x-to-folder-y","title":"copy folder x to folder y","text":"<pre><code>ADD x /home/user/y/x\n</code></pre>"},{"location":"DevOps/Docker/Azure/","title":"Azure","text":""},{"location":"DevOps/Docker/Azure/#acr","title":"acr","text":"<p>docker push to azure acr error: <code>unauthorized: authentication required</code></p> <p>Solution: <code>az acr login -n &lt;acr-name&gt;</code></p>"},{"location":"DevOps/Docker/Build/","title":"Build","text":"<ul> <li>build context</li> <li>pass value to ARG: <code>--build-arg NAME=my-app</code></li> </ul>"},{"location":"DevOps/Docker/Build/#build-an-image","title":"build an image","text":"<p><pre><code>docker build https://github.com/dev/test.git#&lt;branch&gt;:&lt;docker-dir&gt;\ndocker build . --no-cache --force-rm -t docker.example.com/image-name:linux\ndocker build . -f ./docker/linux/my-dev.docker --platform linux/amd64 -t 1.0.1\n</code></pre> - <code>--no-cached</code>: do not use cached intermediate layers and regenerate them as well - <code>--rm</code>: remove intermediate containers after a successful build - <code>--force-rm</code>: those intermediate containers would always been removed even in case of an unsuccessful compilation - <code>-t --tag</code>: name and optionally a tag in the <code>name:tag</code> format</p> <p>Note that the <code>dot</code> is the context. When doing a docker build, - the files from your local context are sent to the daemon, and put in a temporary directory; - that temporal directory is used to build the image (the actual build is performed on the daemon side). - paths in a Dockerfile are always relative to the the context directory</p>"},{"location":"DevOps/Docker/Build/#multistage-build","title":"multistage build","text":"<p>Multi-stage build is a feature introduced in Docker 17.05. Multistage build can reduce docker image size. - https://collabnix.com/getting-started-with-docker-multi-stage-builds/ - https://www.docker.com/blog/advanced-dockerfiles-faster-builds-and-smaller-images-using-buildkit-and-multistage-builds/</p>"},{"location":"DevOps/Docker/Build/#copy-failed-stat-varlibdockertmpdocker-builderxxxx-no-such-file-or-directory","title":"COPY failed: stat /var/lib/docker/tmp/docker-builder/xxxx no such file or directory <ul> <li>the current context is not correct</li> <li>the directory not in the current context folder</li> <li>the directory was in <code>.dockerignore</code> file so it was not copied to the builder folder</li> <li>the docker daemon doesn't have sufficient rights or space to create the temporary directory and files</li> <li>there's a bug in the directive that fails to place the expected files from 'current context' to into the temporary directory</li> </ul>","text":""},{"location":"DevOps/Docker/Build/#how-to-show-the-created-date-of-the-based-image-in-dockerfile","title":"how to show the created date of the based image in dockerfile?","text":""},{"location":"DevOps/Docker/Build/#command-output-from-dockerfile","title":"command output from dockerfile <p>cat output will show in the output <pre><code>RUN openssl s_client -connect www.example.com:443 &amp;&gt; log_openssl.txt; cat log_openssl.txt\n</code></pre></p>","text":""},{"location":"DevOps/Docker/Build/#enable-command-output","title":"enable command output <p>will also force not using cache <pre><code>docker build --progress=plain --no-cache\n</code></pre> or set <pre><code>DOCKER_BUILDKIT=0 docker build ...\n</code></pre></p>","text":""},{"location":"DevOps/Docker/Command/","title":"Docker Command","text":""},{"location":"DevOps/Docker/Command/#arg-vs-env","title":"ARG vs ENV","text":"<ul> <li>https://vsupalov.com/docker-arg-env-variable-guide/</li> <li>https://docs.docker.com/engine/reference/builder/</li> <li>https://stackoverflow.com/questions/63278658/build-and-push-a-docker-image-with-build-arguments-from-devops-to-acr</li> </ul> Feature <code>ARG</code> <code>ENV</code> Scope Build-time only Runtime and build-time Default Value Can be set with <code>ARG &lt;name&gt;=&lt;val&gt;</code> Can be set with <code>ENV &lt;name&gt;=&lt;val&gt;</code> Overridable Overridden with <code>--build-arg</code> Overridden with <code>docker run -e</code> Persisted in Image \u274c Not saved in final image \u2705 Saved in image and available at runtime Use in RUN \u2705 Yes \u2705 Yes Use in CMD/ENTRYPOINT \u274c Not available \u2705 Available"},{"location":"DevOps/Docker/Command/#arg","title":"arg","text":"<p>Pass argument <pre><code>arguments: '--build-arg ubuntu_version=20.04 --build-arg image_version=1.0.0'\n</code></pre></p> <p><pre><code>ARG VERS=22.04\nFROM ubuntu:$VERS\nARG VERS\nRUN echo version is $VERS\n</code></pre> - Second <code>VERS</code> will get default value from the first <code>global</code> one - The ARG statement after a FROM is required to import the value into the build target</p>"},{"location":"DevOps/Docker/Command/#add-vs-copy","title":"ADD vs COPY","text":"<p>https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#:~:text=ADD%20or%20COPY&amp;text=COPY%20only%20supports%20the%20basic,rootfs.tar.xz%20%2F%20. - ADD: has some features (like local-only tar extraction and remote URL support) - COPY: only supports the basic copying of local files into the container, preferred as  it\u2019s more transparent</p>"},{"location":"DevOps/Docker/Command/#run","title":"RUN","text":"<p>Mainly used to build images and install applications and packages, creating a new layer.</p>"},{"location":"DevOps/Docker/Command/#workdir","title":"WORKDIR","text":"<ul> <li>Define the working directory of a Docker container at any given time</li> <li>Any RUN, CMD, ADD, COPY, or ENTRYPOINT command will be executed in the specified working directory</li> <li>If reused, the path of the new working directory must be given relative to the current working directory <pre><code>ARG USER=\"user-name\"\nARG HOME=\"/home/${USER}\"\nWORKDIR $HOME\n</code></pre></li> </ul>"},{"location":"DevOps/Docker/Command/#cmd-vs-entrypoint","title":"CMD vs ENTRYPOINT","text":"<ul> <li>CMD: Default parameters that can be overridden from the Docker CLI when a container is running.</li> <li>ENTRYPOINT: Default parameters that cannot be overridden when the Docker Container runs with CLI parameters.</li> <li>CMD provides default arguments to the ENTRYPOINT.</li> </ul> Feature <code>CMD</code> <code>ENTRYPOINT</code> Purpose Default command + args to run if no command is given during <code>docker run</code> Command that always runs when container starts Can be overridden Yes \u2014 by providing command/args in <code>docker run</code> Yes \u2014 but you must explicitly override with <code>--entrypoint</code> Syntax forms Shell form (string), Exec form (list) Exec form (list) recommended Typical usage Default command with default args Main executable, often for making the container behave like an executable <p>Example #1: <pre><code>ENTRYPOINT [\"python\", \"app.py\"]\nCMD [\"--help\"]\n</code></pre> - If you run <code>docker run my-image</code>, it runs: <code>python app.py --help</code> - If you run <code>docker run my-image --version</code>, it runs: <code>python app.py --version</code></p> <p>Example #2 <pre><code>COPY docker/entrypoint.sh /\nENTRYPOINT [\"/entrypoint.sh\"]\nCMD [\"/bin/bash\"]\n</code></pre></p>"},{"location":"DevOps/Docker/Container/","title":"Container","text":""},{"location":"DevOps/Docker/Container/#run-containers","title":"run containers","text":"<pre><code>docker run -it &lt;image-name&gt; bash         #bash\ndocker run -it &lt;image-name&gt; bash -il     #bash as login user\ndocker run -it --name=&lt;image-name&gt; ubuntu bash\ndocker exec -it &lt;image-name&gt; bash        #execute command on running container\ndocker run -rm -it &lt;image-name&gt; bash -il #delete after run\n</code></pre>"},{"location":"DevOps/Docker/Container/#delete-containers","title":"delete containers","text":"<pre><code>docker rm &lt;container-id&gt;\ndocker rm $(docker ps -aq) #delete all containers\n</code></pre>"},{"location":"DevOps/Docker/Container/#list-containers","title":"list containers","text":"<p>https://www.tutorialspoint.com/how-to-list-containers-in-docker</p> <p>Options: - <code>-a</code>: all - <code>-s</code>: size - <code>-q</code>: quiet - <code>-f</code>: filter on container ID, image, name, and status</p> <p>List running containers <pre><code>docker ps\ndocker container ls\n</code></pre></p> <p>List stopped containers <pre><code>docker container ls --filter \"status=exited\"\n</code></pre> Status options: created, restarting, running, paused, exited, dead</p>"},{"location":"DevOps/Docker/Debug/","title":"Debug","text":"<p>how to debug docker build - <code>--no-cache</code>: forece docker to rebuild all layers from scratch - <code>--progress=plain</code>: disable progress indicator and get more detailed output - <code>--verbose</code>: enable verbose output to identify the root cause of the failure - check system logs for system-level issues - run the container interactively</p> <p>https://stackoverflow.com/questions/26220957/how-can-i-inspect-the-file-system-of-a-failed-docker-build</p>"},{"location":"DevOps/Docker/Debug/#get-image-layer-id-hash","title":"get image layer id (hash)","text":"<p>Build Kit can make your build faster but doesn't support intermediate container hashes: https://github.com/moby/buildkit/issues/1053 <pre><code>DOCKER_BUILDKIT=0 docker build ...\n</code></pre> Changed to <code>DOCKER_BUILDKIT=plain</code>???</p> <p>windows terminal <code>set Var=val</code>, powershell <code>$env:Var=val</code>.</p> <p>Example (the id is the one like <code>---&gt; d62bcfd4677c</code> not the <code>Running in</code>) <pre><code>Step 1/5 : FROM busybox\n ---&gt; d62bcfd4677c\nStep 2/5 : SHELL [\"/bin/bash\", \"-c\"]\n ---&gt; Running in 8058f86dcb62\nRemoving intermediate container 8058f86dcb62\n ---&gt; c5a1a5c423e6\n...\n</code></pre></p>"},{"location":"DevOps/Docker/Debug/#run-into-last-succsessful-layer","title":"run into last succsessful layer","text":"<p>Everytime docker successfully executes a <code>RUN</code> command from a Dockerfile, a new layer in the image filesystem is committed.  We can use those layers ids as images to start a new container.</p> <p>Start a new container from layer id, then can try any commands, including the commanf failed. <pre><code>docker run --rm d62bcfd4677c cat /tmp/foo.txt   #run a command\ndocker run --rm -it d62bcfd4677c0 sh            #start a shell\ndocker run --rm -it d62bcfd4677c0 bash -il\n</code></pre> - <code>--rm</code>: This option tells Docker to automatically remove the container when it exits.   This is useful for cleaning up containers that are no longer needed, especially when running one-off tasks. - <code>-il</code>: This instructs Docker to start an interactive Bash shell inside the container and to treat it as if it   were a login shell, which means it will execute the login shell startup files to set up the environment.</p>"},{"location":"DevOps/Docker/Debug/#run-into-the-failed-layer","title":"run into the failed layer","text":"<p>find the container that failed: <pre><code>docker ps -a\n</code></pre> Commit it to an image: <pre><code>docker commit &lt;container-id&gt;\n</code></pre> And then run the image (if necessary, running bash): <pre><code>docker run -it &lt;new-image-id&gt; [bash -il]\n</code></pre></p> <p>Why need to create a new image from the container? - start the failed container again it would run the command that failed again, and you'd be back where you started. - By creating an image you can start a container with a different start command. </p>"},{"location":"DevOps/Docker/Docker/","title":"Docker","text":"<p>https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-20-04</p>"},{"location":"DevOps/Docker/Docker/#cannot-connect-to-the-docker-daemon","title":"cannot connect to the Docker daemon","text":"<pre><code>sudo service docker status\nsudo service docker start\n</code></pre>"},{"location":"DevOps/Docker/Docker/#run-container-from-another-entrypoint","title":"run container from another entrypoint","text":"<pre><code>docker run -it &lt;image-id&gt;                        #from default entrypoint\ndocker run -it --entrypoint /bin/bash &lt;image-id&gt; #from another entrypoint\n</code></pre>"},{"location":"DevOps/Docker/Docker/#ssh-into-running-container","title":"SSH into running container","text":"<ul> <li>use <code>docker ps</code> to get the name of the existing container</li> <li>run <code>docker exec -it &lt;container name&gt; /bin/bash</code> to get a bash shell in the container</li> <li>run <code>docker exec -it &lt;container name&gt; &lt;command&gt;</code> to execute command in the container</li> <li>create a rsa key pair: <code>ssh-keygen -t rsa</code></li> </ul>"},{"location":"DevOps/Docker/Docker/#login-to-on-prem-registry","title":"login to on-prem registry","text":"<pre><code>docker login -u usr -p pwd mydocker.registry.com\n</code></pre>"},{"location":"DevOps/Docker/Docker/#install-from-a-package","title":"Install from a package","text":"<pre><code>curl -LO https://download.docker.com/linux/ubuntu/dists/focal/pool/stable/amd64/containerd.io_1.4.9-1_amd64.deb\ncurl -LO https://download.docker.com/linux/ubuntu/dists/focal/pool/stable/amd64/docker-ce-cli_20.10.9~3-0~ubuntu-focal_amd64.deb\ncurl -LO https://download.docker.com/linux/ubuntu/dists/focal/pool/stable/amd64/docker-ce_20.10.9~3-0~ubuntu-focal_amd64.deb\n\nsudo dpkg -i ./containerd.io_1.4.9-1_amd64.deb\nsudo dpkg -i ./docker-ce-cli_20.10.9~3-0~ubuntu-focal_amd64.deb\nsudo dpkg -i ./docker-ce_20.10.9~3-0~ubuntu-focal_amd64.deb\n\nsudo docker run hello-world\n</code></pre>"},{"location":"DevOps/Docker/Docker/#install-from-repo","title":"install from repo","text":"<p>Install Docker from the official Docker repository to ensure we get the latest version. First to add a new package source, add the GPG key from Docker to ensure the downloads are valid, and then install the package.</p> <pre><code>sudo apt update\n#install prerequisite packages which let apt use packages over HTTPS\nsudo apt install apt-transport-https ca-certificates curl software-properties-common\n#add GPG key for the official Docker repository to the system\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n#add Docker repository to APT sources\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\"\n#update package database again with Docker packages\nsudo apt update\n#install from Docker repo instead of default Ubuntu repo\napt-cache policy docker-ce\n#install Docker\nsudo apt install docker-ce\n#check Docker status\nsudo systemctl status docker\n</code></pre>"},{"location":"DevOps/Docker/Docker/#add-usr-to-docker-group","title":"add usr to docker group","text":"<p>By default, the docker command can only be run the root user or by a user in the docker group. So it is best to add your username to the docker group. <pre><code>#add username to docker group\nsudo usermod -aG docker ${USER}\n#apply the new group membership\nsu - ${USER}\n#confirm your user is added to docker group\nid -nG\n</code></pre></p>"},{"location":"DevOps/Docker/Docker/#command","title":"command","text":"command comment docker ps List containers; -a show stopped containers as well docker run Run a command in a new container docker rm Remove one or more containers docker rmi Remove one or more images docker tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE"},{"location":"DevOps/Docker/Docker/#map-host-directory","title":"map host directory","text":"<pre><code># mount `/tmp` directory from host into `/container/dat` in the container,\n# and run `ls` command to show the contents of that directory\ndocker run -v /tmp:/container/dat my_image \\\n    ls /container/dat\n</code></pre>"},{"location":"DevOps/Docker/Docker/#link-containers","title":"link containers","text":"<p>allow comunications between different containers; all containers can use the user defined network to comunicate <pre><code>#user defined network\nsudo docker network create my_network\n#run container using user defined network: 8080 is host port, 80 docker container port\nsudo docker run --name my_image -p 8080:80 -d --network my_network my_image\n</code></pre></p>"},{"location":"DevOps/Docker/Docker/#change-docker-instance-port","title":"change docker instance port","text":"<p>open file docker-compose.yml and change the ports</p>"},{"location":"DevOps/Docker/Docker/#start-your-docker-container-with-a-different-port","title":"start your Docker container with a different port","text":"<ol> <li>Find a free TCP port: ss -ltn</li> <li>Delete existing container: docker rm my_container</li> <li>Restart with a different host port number: docker run -d -p 8083:8080 -p 55555:55555 jenkinsci/blueocean</li> </ol>"},{"location":"DevOps/Docker/Docker/#show-history-with-full-commands","title":"show history with full commands","text":"<pre><code>docker history image-id --no-trunc\n</code></pre>"},{"location":"DevOps/Docker/Docker/#push-to-docker-registry","title":"push to docker registry","text":"<pre><code>az acr login -n container-registry-name\ndocker pull docker.example.com/image-name:0.0.1-linux\ndocker tag docker.example.com/image-name:0.0.1-linux container-registry-name.azurecr.io/name-space/image-name:0.0.1\ndocker push container-registry-name.azurecr.io/name-space/image-name:0.0.1\n</code></pre>"},{"location":"DevOps/Docker/DockerFile/","title":"Dockerfile","text":""},{"location":"DevOps/Docker/DockerFile/#simple-test","title":"simple test","text":"<p><pre><code>FROM alpine\n\nARG BASE_URL=https://example.com\nARG FILE_URL=${BASE_URL}/file.txt\n\nRUN echo \"Downloading from: ${FILE_URL}\"\n</code></pre> Then run it <code>docker build -f ./test.docker -t test .</code></p>"},{"location":"DevOps/Docker/DockerFile/#example","title":"example","text":"<pre><code>ARG BASE_IMAGE=ubuntu:22.04\nFROM ${BASE_IMAGE}\nSHELL [\"/bin/bash\", \"-c\"] #SHELL [\"/bin/bash\", \"--login\", \"-c\"]\n\nENV PATH=\"/opt/my-path/:${PATH}\"\n\nARG USER=\"app\"\nARG GROUP=\"app\"\nARG UID=\"1000\"\nARG GID=\"1000\"\n\nRUN groupadd $GROUP --gid $GID \\\n    &amp;&amp; useradd $USER --uid $UID --gid $GID --shell /bin/bash\n\nUSER $USER:$GROUP\nWORKDIR /home/$USER\nCMD [\"/bin/bash\"]\n\nARG AUTHORS\nARG RELEASE\nARG TIMESTAMP\n\nLABEL \\\n    org.opencontainers.image.authors=\"${AUTHORS}\" \\\n    org.opencontainers.image.release=\"${RELEASE}\" \\\n    org.opencontainers.image.created=\"${TIMESTAMP}\"\n</code></pre>"},{"location":"DevOps/Docker/DockerFile/#multiline","title":"multiline","text":"<p>https://github.com/moby/moby/issues/1799 <pre><code>RUN echo 'All of your\\n\\\nmultiline that you ever wanted\\n\\\ninto a dockerfile\\n'\\\n&gt;&gt; /etc/example.conf\n</code></pre></p>"},{"location":"DevOps/Docker/EntryPoint/","title":"EntryPoint","text":"<p>https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#entrypoint</p>"},{"location":"DevOps/Docker/EntryPoint/#example","title":"example","text":"<p>docker-entrypoint.sh <pre><code>#!/bin/bash\nset -e\nexec \"$@\"\n</code></pre></p> <p>docker entrypoint <pre><code>COPY ./docker-entrypoint.sh /\nENTRYPOINT [\"/docker-entrypoint.sh\"]\nCMD [\"/bin/bash\"]\n</code></pre></p>"},{"location":"DevOps/Docker/EntryPoint/#change-the-docker-entrypoint-file-to-update-condarc-channel_alias","title":"change the docker-entrypoint file to update .condarc channel_alias","text":"<p>docker-entrypoint.sh <pre><code>#!/bin/bash\nset -e\n# Update channel_alias setting in .condarc\nif [ -n \"$CONDA_CHANNEL_ALIAS\" ]; then\n    conda config --set channel_alias \"$CONDA_CHANNEL_ALIAS\"\nfi\nexec \"$@\"\n</code></pre></p>"},{"location":"DevOps/Docker/EntryPoint/#cmd-vs-entrypoint","title":"cmd vs entrypoint","text":"<p>The main difference between <code>CMD</code> and <code>ENTRYPOINT</code> in a Dockerfile lies in how they are used and overridden when running a container.</p>"},{"location":"DevOps/Docker/EntryPoint/#cmd","title":"CMD","text":"<ul> <li>The <code>CMD</code> instruction in a Dockerfile specifies the default command to run when the container starts. </li> <li>If a Dockerfile has multiple <code>CMD</code> instructions, only the last one will take effect.</li> <li>The <code>CMD</code> instruction can be overridden when running the container by specifying a command at the end of the <code>docker run</code> command. If no command is specified, the <code>CMD</code> instruction from the Dockerfile is executed.</li> <li>If the Dockerfile specifies an <code>ENTRYPOINT</code>, the <code>CMD</code> values are appended to the <code>ENTRYPOINT</code> to form the command that is run.</li> </ul> <p>Example: <pre><code>CMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre> When running a container from this image, if no command is specified, it will start nginx with the specified configuration. However, you can override the <code>CMD</code> by specifying a different command when running the container.</p>"},{"location":"DevOps/Docker/EntryPoint/#entrypoint_1","title":"ENTRYPOINT","text":"<ul> <li>The <code>ENTRYPOINT</code> instruction in a Dockerfile specifies the executable that will run when the container starts.</li> <li>If a Dockerfile has multiple <code>ENTRYPOINT</code> instructions, only the last one will take effect.</li> <li>The <code>ENTRYPOINT</code> instruction cannot be overridden like <code>CMD</code>. Instead, any commands provided when running the container are passed as arguments to the <code>ENTRYPOINT</code>.</li> <li>If the Dockerfile specifies both <code>ENTRYPOINT</code> and <code>CMD</code>, the <code>CMD</code> values are passed as arguments to the <code>ENTRYPOINT</code>.</li> </ul> <p>Example: <pre><code>ENTRYPOINT [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre> In this case, <code>nginx</code> is the executable that will run when the container starts. Any additional arguments provided when running the container are appended to the <code>ENTRYPOINT</code> command.</p> <p>In summary, <code>ENTRYPOINT</code> sets the main command and <code>CMD</code> provides default arguments for that command. When both are used together, <code>CMD</code> values are passed as arguments to <code>ENTRYPOINT</code>.</p>"},{"location":"DevOps/Docker/Image/","title":"Image","text":"<p>Every statement like RUN, COPY or ADD in the dockerfile will add a layer into docker image.</p>"},{"location":"DevOps/Docker/Image/#run-image","title":"run image","text":"<pre><code>docker run -it &lt;image-id&gt;                        #from default entrypoint\ndocker run -it --entrypoint /bin/bash &lt;image-id&gt; #from another entrypoint\n</code></pre>"},{"location":"DevOps/Docker/Image/#list-images-with-sort","title":"list images with sort","text":"<pre><code>docker images | sort -k1 -h #sort by col1 - repo\n</code></pre>"},{"location":"DevOps/Docker/Image/#remove-image-with-multiple-repostags","title":"remove image with multiple repos/tags","text":"<pre><code>docker rmi -f &lt;image-id&gt;\ndocker rmi [repo-name1]:[tag1] [repo-name2]:[tag2]\n</code></pre>"},{"location":"DevOps/Docker/Image/#check-docker-image-layer-sizes","title":"check docker image layer sizes","text":"<p><pre><code>docker history &lt;image-id&gt;\ndocker history --no-trunc &lt;image-id&gt; #keep full created-by command\n</code></pre> Use <code>dive</code> to check layer content: https://github.com/wagoodman/dive</p>"},{"location":"DevOps/Docker/Image/#reduce-docker-image-size","title":"reduce docker image size","text":"<p>https://jcristharif.com/conda-docker-tips.html</p> <p>https://uwekorn.com/2021/03/01/deploying-conda-environments-in-docker-how-to-do-it-right.html</p> <ul> <li>Use an official base image whenever possible</li> <li>Use variations of Alpine images when possible</li> <li>Every <code>RUN</code> instruction in the <code>Dockerfile</code> writes a new layer in the image</li> <li>Every layer requires extra space on disk</li> <li>To keep the number layers to a minimum, commands should be combined into a single RUN instruction</li> <li>Free spaces after <code>update</code> using <code>apt clean all</code></li> <li>Use multistage builds</li> <li>Use .dockerignore to exclude files not needed</li> <li>Squash layers: https://github.com/goldmann/docker-squash</li> </ul>"},{"location":"DevOps/Docker/Image/#conda-docker-image-size","title":"Conda docker image size","text":"<p>https://pythonspeed.com/articles/conda-docker-image-size/</p> <p>Use <code>conda-pack</code> to remove conda and use multistage build together to reduce image size.</p> <p>The base Conda environment is necessary for installation of packages, but once we\u2019re running the code it really doesn\u2019t add much.</p>"},{"location":"DevOps/Docker/Image/#multiple-versions-of-package-in-the-same-image-why","title":"multiple versions of package in the same image, why???","text":""},{"location":"DevOps/Docker/Issue/","title":"Issue","text":""},{"location":"DevOps/Docker/Issue/#dockerfile-in-subdirectory","title":"dockerfile in subdirectory","text":"<p>Solution: - use <code>context: .</code> so docker knows where to start - for azure devops pipeline, use <code>buildContext: .</code> - for docker build, use <code>-f</code> pointing to the dockerfile</p> <p>https://stackoverflow.com/questions/47785706/docker-using-copy-when-the-dockerfile-is-located-in-a-subdirectory</p>"},{"location":"DevOps/Docker/Ngrok/","title":"ngrok","text":"<p>Allows you to expose a web server running on your local machine to the internet</p>"},{"location":"DevOps/Docker/Ngrok/#install","title":"install","text":"<ul> <li>download zip from: https://dashboard.ngrok.com/get-started/setup</li> <li>unzip to install: unzip /path/to/ngrok.zip</li> <li>connect your account: ./ngrok authtoken 1xwtkqswEnLLU5UPIlL4il9QggA_6Y4q2iWNVt4qY9ziHJVeF</li> <li>fire it up: ./ngrok http 8086</li> </ul>"},{"location":"DevOps/Docker/Port/","title":"Port","text":"<p>https://www.cloudbees.com/blog/docker-expose-port-what-it-means-and-what-it-doesnt-mean</p> <p>https://blog.knoldus.com/exposing-container-ports-in-docker/</p>"},{"location":"DevOps/Docker/Port/#expose-port","title":"expose port","text":"<p>expose 80/tcp</p>"},{"location":"DevOps/Docker/Port/#publish-port","title":"publish port","text":"<p>-p 80</p>"},{"location":"DevOps/Docker/Python/","title":"Python","text":""},{"location":"DevOps/Docker/Python/#build-docker-image","title":"build docker image","text":"<p>https://towardsdatascience.com/a-simple-guide-to-deploying-a-dockerized-python-app-to-azure-29753ee507eb <pre><code>FROM tiangolo/uvicorn-gunicorn-fastapi:python3.9\n\nCOPY ./app /app\n\nCOPY requirements.txt . #include package info\nRUN pip --no-cache-dir install -r requirements.txt\n</code></pre> <pre><code>docker build -t my-api\n</code></pre></p>"},{"location":"DevOps/Docker/Python/#dockerfile-using-pip","title":"Dockerfile using pip","text":"<pre><code>ARG BASE_IMAGE=continuumio/miniconda3\nFROM ${BASE_IMAGE}\nSHELL [\"/bin/bash\", \"-c\"]\n\nWORKDIR /usr/app\n\nCOPY *.whl ./\nCOPY requirements.txt ./\nRUN python -m pip install *.whl \\\n    &amp;&amp; python -m pip install --no-cache-dir -r requirements.txt\n\nCMD [ \"test-run\" ]\n</code></pre>"},{"location":"DevOps/Docker/Python/#activate-conda-env-in-dockerfile","title":"Activate conda env in Dockerfile","text":"<p>https://pythonspeed.com/articles/activate-conda-dockerfile/</p>"},{"location":"DevOps/Docker/Python/#python-docker-perf","title":"python /docker perf","text":"<p>https://pythonspeed.com</p>"},{"location":"DevOps/Docker/Python/#reduce-pipeline-time-run-unit-test-in-docker-build","title":"reduce pipeline time (run unit test in docker build)","text":"<ul> <li>build conda package</li> <li>use multi-stage docker builds</li> <li>install the package in docker build</li> <li>run unit test, pipe unit test logs to a file</li> <li>remove unit test files </li> </ul>"},{"location":"DevOps/Docker/Registry/","title":"Registry","text":""},{"location":"DevOps/Docker/Registry/#push-docker-image-to-private-registry","title":"push docker image to private registry","text":"<ul> <li>omit <code>--password</code> so it will not be stored in command history</li> <li>the <code>username</code> can be the registry address such as <code>docker push docker.example.com/uat/redis:0.1.0</code> <pre><code>docker login --username &lt;username&gt; [--password]\ndocker tag &lt;image-id&gt; &lt;username&gt;/my-repo\ndocker push &lt;username&gt;/my-repo\n</code></pre></li> </ul>"},{"location":"DevOps/Docker/Registry/#create-a-self-hosted-registry","title":"create a self-hosted registry","text":"<ul> <li>https://microsoft.github.io/PartsUnlimitedMRP/adv/adv-24-Docker-Hub.html</li> <li>https://github.com/distribution/distribution</li> </ul>"},{"location":"DevOps/Docker/Registry/#delete-docker-images-in-private-registry","title":"delete docker images in private registry","text":"<ul> <li>https://gist.github.com/jaytaylor/86d5efaddda926a25fa68c263830dac1</li> <li>https://gist.github.com/abelmferreira/c38036f9642f2adf260ad068ac08f187</li> <li>https://stackoverflow.com/questions/25436742/how-to-delete-images-from-a-private-docker-registry</li> </ul> <p>Steps: - Step 4: Delete the manifest   <pre><code>curl -sS -X DELETE &lt;domain-or-ip&gt;:5000/v2/&lt;repo&gt;/manifests/&lt;digest&gt;\n</code></pre> - Garbage collect the image. Run this command in your <code>docker registry container</code>:   <pre><code>registry garbage-collect -m /etc/docker/registry/config.yml\n</code></pre> config.yml <pre><code>version: 0.1\nlog:\n    fields:\n    service: registry\nstorage:\n    cache:\n        blobdescriptor: inmemory\n    filesystem:\n        rootdirectory: /var/lib/registry\n    delete:\n        enabled: true\nhttp:\n    addr: :5000\n    headers:\n        X-Content-Type-Options: [nosniff]\nhealth:\n    storagedriver:\n        enabled: true\n        interval: 10s\n        threshold: 3\n</code></pre> If set <code>delete: enabled=true</code> there is no need to set the env var <code>REGISTRY_STORAGE_DELETE_ENABLED</code>.</p>"},{"location":"DevOps/Docker/Registry/#garbage-collect","title":"garbage-collect","text":"<pre><code>Usage:\n  registry garbage-collect &lt;config&gt; [flags]\nFlags:\n  -m, --delete-untagged=false: delete manifests that are not currently referenced via tag\n  -d, --dry-run=false: do everything except remove the blobs\n  -h, --help=false: help for garbage-collect\n</code></pre>"},{"location":"DevOps/Docker/Run/","title":"Run","text":"<pre><code>docker images\ndocker run -it &lt;image-id&gt;                        #from default entrypoint\ndocker run -it --entrypoint /bin/bash &lt;image-id&gt; #from another entrypoint\n</code></pre>"},{"location":"DevOps/Docker/SecretProviderClass/","title":"SecretProviderClass","text":"<p>https://secrets-store-csi-driver.sigs.k8s.io/getting-started/usage.html</p> <p>Create a <code>SecretProviderClass</code> to use the Secrets Store CSI driver, and add the <code>SecretProviderClass</code> into the <code>Deployment</code> Yaml.</p> <p>On pod start and restart, the driver will communicate with the provider using gRPC to retrieve the secret content from the external Secrets Store you have specified in the SecretProviderClass custom resource. Then the volume is mounted in the pod as tmpfs and the secret contents are written to the volume.</p>"},{"location":"DevOps/Docker/SecretProviderClass/#azure-key-vault-provider","title":"azure key vault provider","text":"<p>https://docs.microsoft.com/en-us/azure/aks/csi-secrets-store-driver</p> <p>https://github.com/Azure-Samples/secrets-store-csi-with-aks-akv</p> <p>After the pod starts: <pre><code># show secrets held in secrets-store\nkubectl exec -n &lt;namespace&gt; &lt;pod-name&gt; -- ls &lt;mountPath&gt;\n\n# print a test secret 'ExampleSecret' held in secrets-store\nkubectl exec -n &lt;namespace&gt; &lt;pod-name&gt; -- cat &lt;mountPath&gt;/ExampleSecret\n</code></pre> or run into the pod: <pre><code>kubectl exec -it -n &lt;namespace&gt; &lt;pod-name&gt; -- /bin/bash\nls -l ~/&lt;mountPath&gt;\n</code></pre></p>"},{"location":"DevOps/Docker/Secrets/","title":"Secrets","text":"<p>https://docs.docker.com/engine/swarm/secrets/</p> <p>https://earthly.dev/blog/docker-secrets/</p>"},{"location":"DevOps/Docker/Security/","title":"Security","text":""},{"location":"DevOps/Docker/Security/#check-image-packagescves","title":"check image packages/CVEs","text":"<p>install <code>syft</code> and <code>grype</code> <pre><code>curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin\ncurl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /usr/local/bin\n</code></pre></p> <p>check packages <pre><code>syft ubuntu:18.04\nsyft &lt;repo&gt;:1.0.0\n</code></pre></p> <p>check CVEs <pre><code>grype ubuntu:18.04\ngrype &lt;repo&gt;:1.0.0\n</code></pre></p> <p>or use Snyk to scan the docker hub image to check vulnerabilities or security weakness. https://docs.snyk.io/scan-using-snyk/snyk-container/scan-container-images</p>"},{"location":"DevOps/Docker/Storage/","title":"Storage","text":""},{"location":"DevOps/Docker/Storage/#docker-storage-mount-types","title":"docker storage mount types","text":"<p>https://docs.docker.com/storage/</p>"},{"location":"DevOps/Docker/Tag/","title":"Tag","text":"<p>https://docs.docker.com/engine/reference/commandline/image_tag/</p>"},{"location":"DevOps/Docker/Tag/#change-tag","title":"change tag","text":"<pre><code>docker tag &lt;image-id&gt; &lt;repository&gt;:&lt;new-tag&gt; #create a new tag\ndocker rmi &lt;repository&gt;:&lt;old-tag&gt;            #delete old tag\n</code></pre>"},{"location":"DevOps/Docker/Topic/","title":"Topic","text":""},{"location":"DevOps/Docker/Topic/#build-context","title":"build context","text":""},{"location":"DevOps/Docker/Topic/#basic-build-commands","title":"basic build commands","text":""},{"location":"DevOps/Docker/Topic/#debug-build-dockerfile","title":"debug build dockerfile","text":""},{"location":"DevOps/Docker/Topic/#multistage-docker-build","title":"multistage docker build","text":"<ul> <li>reduce image size</li> <li>drop not required files after build</li> </ul>"},{"location":"DevOps/Docker/User/","title":"User","text":"<p>Run app under <code>user</code> not <code>root</code>: - http://redhatgov.io/workshops/security_containers/exercise1.2/</p>"},{"location":"DevOps/Docker/User/#add-user","title":"Add user","text":"<pre><code>RUN groupadd group_name --gid 1000 \\\n    &amp;&amp; useradd user_name --uid 1000 --gid 1000 \\\n        --no-log-init --shell /bin/bash --create-home --home /home/user_name \\\n    &amp;&amp; touch /home/user_name/.bashenv \\ # create file\n    &amp;&amp; chown user_name:group_name /home/user_name/.bashenv \\ # ensure user owns this file\n    &amp;&amp; echo \"source /home/user_name/.bashenv\" &gt;&gt; /home/user_name/.bashrc # append command to .bashrc file\nUSER user_name:group_name\n</code></pre>"},{"location":"DevOps/Docker/User/#switch-between-root-and-user","title":"Switch between root and user","text":"<pre><code>USER root # switch to root\nRUN apt-update\nUSER user_name:group_name # switch back to user_name\n</code></pre>"},{"location":"DevOps/Env/Certificate/","title":"Certificate","text":"<p>https://levelup.gitconnected.com/solve-the-dreadful-certificate-issues-in-python-requests-module-2020d922c72f</p> <p>https://github.com/Azure/azure-cli/issues/5099</p>"},{"location":"DevOps/Env/Certificate/#certificate-packages","title":"certificate packages","text":"<ul> <li><code>python-certifi-win32</code> uses Windows certificate store to check the validity of certificates.</li> <li>newer versions of python use <code>certifi</code> package.</li> </ul> <p>Show certificate path <pre><code>import certifi\ncertifi.where()\n</code></pre> Get self signed certificate from a server <pre><code>openssl s_client -showcerts -connect server.com:443 &lt;/dev/null 2&gt;/dev/null | openssl x509 -outform PEM &gt;ca.pem\n</code></pre></p>"},{"location":"DevOps/Env/Certificate/#self-signed-certificate-err","title":"self signed certificate err","text":"<p>ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1091)</p> <p>soluation:   * set env var: will probably break things   <pre><code>set REQUESTS_CA_BUNDLE=\"path-to-my.crt\"\necho %REQUESTS_CA_BUNDLE%\n</code></pre>   * pass crt to verify   <pre><code>import requests\nurl = 'https://test.web.com'\nr = requests.get(url, verify='path-to-my.crt')\n</code></pre>   * best solution: python-certifi-win32 monkey-patches certifi (used by requests and most Python HTTP libs) so that it uses the Windows Certificate Store (where your local Certificate is located)   <pre><code>install certifi\ninstall python-certifi-win32\n</code></pre></p>"},{"location":"DevOps/Env/Certificate/#unable-to-get-local-issuer-certificate","title":"Unable to get local issuer certificate","text":"<p>https://confluence.atlassian.com/bitbucketserverkb/ssl-certificate-problem-unable-to-get-local-issuer-certificate-816521128.html</p> <p>popup window after run??? <pre><code>git config --global http.sslbackend schannel\n</code></pre></p>"},{"location":"DevOps/Git/Branch/","title":"Git Branch","text":""},{"location":"DevOps/Git/Branch/#check-the-setup-tracking-branches","title":"check the setup tracking branches","text":"<pre><code>git branch -vv\n#if it shows that the tracking branch is not upstream/master\n# we need to fix it as below\n</code></pre>"},{"location":"DevOps/Git/Branch/#point-local-master-to-upstreammaster","title":"point local master to upstream/master","text":"<p>if has error: <code>fatal: ambiguous argument 'upstream/master': unknown revision or path not in the working tree</code>, run first <pre><code>git fetch upstream\n</code></pre> then run <pre><code>git checkout master\ngit reset --hard upstream/master\ngit branch --set-upstream-to upstream/master\n</code></pre></p>"},{"location":"DevOps/Git/Branch/#show-branches","title":"Show branches","text":"<pre><code>git branch             #list all local branches\ngit branch -r          #list all remote branches\ngit branch -a          #list both local and remote branches\ngit branch --merged    #list all branches that have been merged\ngit branch --no-merged #list all branches not merged\n</code></pre>"},{"location":"DevOps/Git/Branch/#rename-master-to-main","title":"rename <code>master</code> to <code>main</code>","text":"<pre><code>git branch -m master main       #local\ngit push -u origin main         #remote\ngit push origin --delete master #delete remote master\n</code></pre>"},{"location":"DevOps/Git/Branch/#create-branch","title":"create branch","text":"<pre><code>git branch &lt;new-branch&gt;\ngit checkout -b &lt;new-branch&gt;  #create and switch to\n</code></pre>"},{"location":"DevOps/Git/Branch/#create-branch-from-commit","title":"create branch from commit","text":"<pre><code>git checkout -b &lt;new-branch&gt; &lt;commit-hash or HEAD~i&gt;\n</code></pre>"},{"location":"DevOps/Git/Branch/#create-branch-from-remote-branch","title":"create branch from remote branch","text":"<p>create a new local branch and track a remote branch. For example, add commits to other's branch <pre><code>git checkout -b &lt;local-branch&gt; origin/&lt;remote-branch&gt;\n...\ngit push -u origin &lt;branch-name&gt; # local/remote branches have the same name, automatic tracking\ngit push origin &lt;local-branch&gt;:&lt;remote-branch&gt; # push from specific local to remote branch, no automatic tracking\n</code></pre></p>"},{"location":"DevOps/Git/Branch/#create-branch-from-another-branch","title":"Create branch from another branch","text":"<pre><code>#checkout branch to copy\ngit checkout dev\n\n#create new branch\n#git checkout -b dev-test [current_active_branch]\ngit checkout -b dev-test\n\n#push changes in dev-test to remote\ngit push origin dev-test\n</code></pre>"},{"location":"DevOps/Git/Branch/#restore-deleted-branch","title":"restore deleted branch","text":"<pre><code>git reflog show --all             #get sha1 of the deleted branch\ngit branch &lt;NewBranchName&gt; &lt;sha1&gt; #restore the branch\n</code></pre>"},{"location":"DevOps/Git/Branch/#delete-local-branches","title":"Delete local branches","text":"<pre><code>git branch -d &lt;branch-name&gt; #delete merged branch\ngit branch -D &lt;branch-name&gt; #force delete branch (even if not merged)\n</code></pre>"},{"location":"DevOps/Git/Branch/#delete-multiple-local-branches","title":"Delete multiple local branches","text":"<pre><code># UNIX\ngit branch --merged | egrep -v \"(^\\*|master|main|develop)\" | xargs git branch -d    #merged\ngit branch --no-merged | egrep -v \"(^\\*|master|main|develop)\" | xargs git branch -D #unmerged\n\n# PowerShell: `%` shortcut to foreach, `$_` is the piped variable\ngit branch --merged | Where-Object { !($_ | Select-String \"master|main|develop\" -quiet) } | %{git branch -d $_.Trim()}\ngit branch --no-merged | Where-Object { !($_ | Select-String \"master|main|develop\" -quiet) } | %{git branch -D $_.Trim()}\n</code></pre>"},{"location":"DevOps/Git/Branch/#delete-remote-branches","title":"Delete remote branches","text":"<pre><code>git push origin --delete &lt;branch-name&gt;\n</code></pre>"},{"location":"DevOps/Git/Branch/#delete-multiple-remote-branches","title":"Delete multiple remote branches","text":"<pre><code>git branch -r --merged | egrep -v \"(^\\*|master|main|develop)\" | sed 's/origin\\///' | xargs -n 1 git push origin --delete #UNIX\ngit branch -r | Where-Object { !($_ | Select-String \"HEAD|upstream|master|main|develop\" -quiet) } |\n  %{$_ -replace ([regex]::Escape('origin/'))} | %{git push origin --delete $_} #PowerShell\n</code></pre>"},{"location":"DevOps/Git/Branch/#branch-is-out-of-date-with-the-base-branch","title":"branch is out-of-date with the base branch","text":"<pre><code>git remote add upstream remote-upstream-repo-url\ngit checkout my-branch\ngit pull upstream master\ngit push origin my-branch\n</code></pre>"},{"location":"DevOps/Git/Branch/#your-branch-is-ahead-of-originmaster-by-x-commit","title":"Your branch is ahead of 'origin/master' by x commit","text":"<pre><code>#origin/master is the remote forked branch\n# you need to push your changes to the forked branch (origin/master)\n#add the upstream repo if not (forked from)\ngit remote add upstream https://github.com/whoever/whatever.git\n#get all change from the upstream\ngit fetch upstream\n#go to local master branch\ngit checkout master\n#rewrite master branch based on upstream\ngit rebase upstream/master\n#push local changes to forked repo (-f only needed the first time after rebased)\ngit push -f origin master\n</code></pre>"},{"location":"DevOps/Git/Branch/#ambiguous-argument-upstreammaster","title":"ambiguous argument 'upstream/master'","text":"<p>should not use <code>git pull</code>, instead use <code>git getch upstream</code></p>"},{"location":"DevOps/Git/Commit/","title":"Commit","text":""},{"location":"DevOps/Git/Commit/#multi-line-comment","title":"multi-line comment","text":"<pre><code>git commit -m \"Head line\" -m \" * First content line\" -m \" * Second content line\"\n</code></pre>"},{"location":"DevOps/Git/Commit/#drop-commits-not-pushed","title":"drop commits not pushed","text":"<ul> <li><code>HEAD~1</code> the first parent of the commit</li> <li><code>HEAD^</code>means something different, on windows cane be escaped as <code>HEAD^^</code> <pre><code>git reset --soft HEAD~1 #revert previous commit to staging area\ngit reset --hard HEAD~1 #DELETE previous commit\ngit reset --hard HEAD~3 #DELETE last three commits\n</code></pre></li> </ul>"},{"location":"DevOps/Git/Commit/#take-commits-not-pushed-to-new-branch","title":"take commits not pushed to new branch","text":"<pre><code>git checkout -b &lt;new-branch&gt;\ngit checkout &lt;master&gt;\ngit reset --hard HEAD~&lt;n&gt;    #remove last n commits\n</code></pre>"},{"location":"DevOps/Git/Commit/#revert-last-commit-pushed","title":"revert last commit pushed","text":"<p>careful, the changes will be dropped! <pre><code>git revert &lt;commit-hash&gt; #will keep history\ngit push origin &lt;your-branch&gt;\n</code></pre></p>"},{"location":"DevOps/Git/Commit/#delete-last-commit-pushed","title":"delete last commit pushed","text":"<pre><code>git reset --hard HEAD^\ngit push origin -f\n</code></pre>"},{"location":"DevOps/Git/Commit/#diff-between-two-commits","title":"diff between two commits","text":"<pre><code>git log -5 --oneline                          #show commits\ngit diff &lt;sha-new&gt; &lt;sha-old&gt; &gt; my.patch       #diff between new and old commits\ngit checkout -b &lt;new-branch&gt;                  #make a copy from the new commit\ngit apply my.patch                            #apply diff to copied branch\n</code></pre>"},{"location":"DevOps/Git/Commit/#cherry-pick","title":"cherry pick","text":"<pre><code>git checkout master       #start from an updated master\ngit pull\ngit switch -c new-branch  #create a copy of master and switch to it\ngit cherry-pick &lt;sha&gt;     #get commit from other branch using it's sha\n</code></pre>"},{"location":"DevOps/Git/Commit/#add-commit-to-johns-pr","title":"add commit to john's pr","text":"<pre><code>git clone https://github.com/my/repo.git\ngit remote add john https://github.com/john/repo.git\ngit fetch john\ngit checkout -b john-main john/main #git checkout -b john-feature john/feature\ngit commit -am \"my commit\"\ngit push john HEAD:main #git push john john-feature:feature\n</code></pre>"},{"location":"DevOps/Git/Commit/#merge-last-n-1-commits","title":"merge last n-1 commits","text":"<pre><code>git rebase -i HEAD~n\n#change \"pick\" to \"squash\" except the first one and save the changes\n#update the new commit message and save\ngit push -f\n</code></pre>"},{"location":"DevOps/Git/Commit/#merge-all-the-commits-from-a-specific-commit","title":"merge all the commits from a specific commit","text":"<p>squash all the commits from a specific commit (sha:xyz) up to the latest commit: <pre><code># Leave first commit as pick, for all the subsequent commits, change pick to squash\ngit rebase -i &lt;sha:xyz&gt;^ # merge commit from xyz (inclusive) to latest\ngit rebase -i xyz~2      # 2 commit from sha:xyz (exclusive)\n</code></pre></p>"},{"location":"DevOps/Git/Config/","title":"Git Config","text":"<p>Git will search the config settings from  <code>local</code>, <code>global</code> to <code>system</code>.</p>"},{"location":"DevOps/Git/Config/#local","title":"local","text":"<p>Local config file can be found in the repo's .git directory: <code>.git/config</code>. <pre><code>\n</code></pre></p>"},{"location":"DevOps/Git/Config/#global","title":"global","text":"<p>Global config file locates in user's home directory: - unix: <code>~/.gitconfig</code> - windows: <code>C:/Users/&lt;usr&gt;/.gitconfig</code> <pre><code>git config --global core.editor \"vim\"\ngit config --global --unset core.excludesfile #remove the setting\n</code></pre></p> <p>Avoid ssl error <pre><code>[http]\n    sslbackend = schannel\n</code></pre></p> <p>Use windows Credentials Manager, <code>git config --global credential.helper manager</code></p> <p>Note that credential-manager-core was renamed to credential-manager. <pre><code>[credential \"https://user.github.com\"]\n    provider = github\n[credential]\n    helper = manager\n</code></pre></p>"},{"location":"DevOps/Git/Config/#system","title":"system","text":"<p>System level config file lives in the system root path: - unix: <code>$(prefix)/etc/gitconfig</code> - windows: <code>C:\\ProgramData\\Git\\config</code> <pre><code>\n</code></pre></p>"},{"location":"DevOps/Git/Config/#config-git-editor","title":"config git editor","text":"<pre><code>#error: cannot spawn more: No such file or directory\ngit config --global core.pager \"\"\n</code></pre>"},{"location":"DevOps/Git/Config/#create-git-alias","title":"create git alias","text":"<p><pre><code>git config --global alias.a '! git pull &amp;&amp; git add . &amp;&amp; git commit -m \"d\" &amp;&amp; git push'\ngit config --global --unset alias.a\n</code></pre> open <code>.gitconfig</code> and add <pre><code>[alias]\n    a = ! git fetch &amp;&amp; git merge origin/master &amp;&amp; git add . &amp;&amp; git commit -m sdoc &amp;&amp; git push \n</code></pre></p> <p><pre><code>a = \"!sh -c \\\"git pull &amp;&amp; git add . &amp;&amp; if [ -z \\\\\\\"$1\\\\\\\" ]; then git commit -m \\\\\\\"update\\\\\\\"; else git commit -m \\\\\\\"$1\\\\\\\"; fi &amp;&amp; git push\\\"\"\n</code></pre> Here is an example to use it: <code>git a \"My first commit\"</code>.</p>"},{"location":"DevOps/Git/Config/#user-config-example","title":"user config example","text":"<pre><code># This is Git's per-user configuration file.\n[core]\n    editor = \\\"C:\\\\Users\\\\&lt;user&gt;\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin\\\\code\\\" --wait\n[http]\n    sslbackend = schannel \n[user]\n    name = &lt;user&gt;\n    email = my.email@example.com   \n[alias]\n    a = ! git fetch &amp;&amp; git merge origin/master &amp;&amp; git add . &amp;&amp; git commit -m sdoc &amp;&amp; git push \n        m = ! git checkout master &amp;&amp; git pull     \n[credential \"https://repo.example.com\"]\n    provider = github\n[credential]\n    helper = manager\n[credential \"https://dev.azure.com\"]\n    useHttpPath = true\n</code></pre>"},{"location":"DevOps/Git/Difference/","title":"Difference","text":""},{"location":"DevOps/Git/Difference/#show-files-changed","title":"show files changed","text":"<pre><code>git status\n</code></pre>"},{"location":"DevOps/Git/Difference/#show-changes-in-a-file","title":"show changes in a file","text":"<pre><code>git diff HEAD~1 -- &lt;path/to/file&gt;\n</code></pre>"},{"location":"DevOps/Git/Drop/","title":"Drop","text":""},{"location":"DevOps/Git/Drop/#drop-local-changes","title":"drop local changes","text":"<pre><code>git checkout -- .\n</code></pre>"},{"location":"DevOps/Git/Drop/#drop-unstaged-files","title":"drop unstaged files","text":"<pre><code>git checkout -- .\ngit checkout -- path-to-file #only one file\n</code></pre>"},{"location":"DevOps/Git/Drop/#drop-untracked-files","title":"drop untracked files","text":"<pre><code>git clean -n  # --dry-run\ngit clean -f  # delete untracked files and directories\ngit clean -fx # delete any ignored files (according to your .gitignore rules) as well\n</code></pre>"},{"location":"DevOps/Git/Drop/#drop-deleted-file-history","title":"drop deleted file history","text":"<pre><code>git filter-branch --force --index-filter \"git rm --cached --ignore-unmatch ./dev/my_file.json\" --prune-empty --tag-name-filter cat -- --all\ngit push origin --force --all\n</code></pre>"},{"location":"DevOps/Git/Drop/#drop-all-commit-history","title":"drop all commit history","text":"<pre><code>git checkout --orphan &lt;branch-copy&gt;\ngit add .\ngit commit -m \"Initial commit\"\ngit branch -D main\ngit branch -m main\ngit push -f origin main\n</code></pre>"},{"location":"DevOps/Git/Env/","title":"Git Env","text":""},{"location":"DevOps/Git/Env/#git-command-in-conda-env-not-show-anything","title":"git command in conda env not show anything","text":"<p>Run the command in terminal and it will show the error, e.g., msys-2.0.exe not found. This means the exe in the conda env is missing.</p> <p>Under PowerShell, check the linked exe to git command: <pre><code>Get-Command git #will show the source path of git.exe\n</code></pre></p>"},{"location":"DevOps/Git/Extension/","title":"Extension","text":""},{"location":"DevOps/Git/Extension/#posh-git","title":"<code>posh-git</code>","text":"<p>A PowerShell environment for Git:  https://github.com/dahlbyk/posh-git</p>"},{"location":"DevOps/Git/Git/","title":"Git","text":""},{"location":"DevOps/Git/Git/#git","title":"Git","text":"<ul> <li>Rebase will change history and best to use it in private branch.</li> <li>Merge will not change history and is preferred to rebase in most cases.</li> </ul>"},{"location":"DevOps/Git/Git/#terms","title":"terms","text":"<ul> <li><code>Untracked changes</code> are not in Git</li> <li><code>Unstaged changes</code> are in Git but not marked for commit</li> <li><code>Staged changes</code> are in Git and marked for commit</li> </ul>"},{"location":"DevOps/Git/Git/#update","title":"update","text":"<pre><code>sudo apt update\nsudo apt install git\ngit --version\n</code></pre>"},{"location":"DevOps/Git/Git/#untrack-file","title":"untrack file","text":"<pre><code>#keep file on server\ngit update-index --assume-unchanged &lt;file&gt;\n#delete file from server\ngit rm --cached &lt;file&gt;\n#delete folder from server\ngit rm -r --cached path_to_folder/\n</code></pre>"},{"location":"DevOps/Git/Git/#git-tips","title":"git tips","text":"<p>https://wikihub.berkeley.edu/display/drupal/Git+Tips#GitTips-Gitmessage:Yourbranchisaheadof'upstream/develop'byXXcommits</p>"},{"location":"DevOps/Git/Git/#github-standard-fork-pull-request-workflow","title":"GitHub Standard Fork &amp; Pull Request Workflow","text":"<p>https://gist.github.com/Chaser324/ce0505fbed06b947d962</p> <p>git example https://forum.freecodecamp.org/t/push-a-new-local-branch-to-a-remote-git-repository-and-track-it-too/13222</p>"},{"location":"DevOps/Git/Git/#commit-changes","title":"commit changes","text":"<pre><code>git add .\ngit commit\ngit push\n</code></pre>"},{"location":"DevOps/Git/Git/#work-example","title":"work example","text":"<pre><code>git checkout master\n\n#create a new branch\ngit checkout -b new-branch-name\n\n#make changes and first commit\ngit add .\ngit commit -m \"my first commit\"\ngit push --set-upstream origin new-branch-name\n\n#submit a pr\ndo it on github\n\n#make another commit\ngit add .\ngit commit -m \"my commit after pr review\"\ngit push\n</code></pre>"},{"location":"DevOps/Git/Ignore/","title":"Ignore","text":""},{"location":"DevOps/Git/Ignore/#rules","title":"rules","text":"<p>https://git-scm.com/docs/gitignore</p>"},{"location":"DevOps/Git/Ignore/#list-ignored-files","title":"list ignored files","text":"<p>only for tracked files <pre><code>git status --ignored\n</code></pre></p>"},{"location":"DevOps/Git/Ignore/#list-all-ignored-files","title":"list all ignored files","text":"<p>https://stackoverflow.com/questions/466764/git-command-to-show-which-specific-files-are-ignored-by-gitignore <pre><code>find . -not -path './.git/*' | git check-ignore --stdin    #list files only\nfind . -not -path './.git/*' | git check-ignore --stdin -v #include ignore rules\ngit check-ignore *       #does not work\ngit check-ignore **/*    #does not work\n</code></pre></p>"},{"location":"DevOps/Git/Ignore/#debug-gitignore","title":"debug gitignore","text":"<p>https://git-scm.com/docs/git-check-ignore</p> <p>check why file (not) ignored <pre><code>git check-ignore --verbose ignored.file #git check-ignore -v ignored.file\n</code></pre></p>"},{"location":"DevOps/Git/Info/","title":"Info","text":""},{"location":"DevOps/Git/Info/#git-describe-long-always-dirty","title":"<code>git describe --long --always --dirty</code>","text":"<p>Provide detailed information about the current Git commit, including its tag, commit hash, and distance from the latest tag. Indicates whether the working directory is dirty (i.e., has uncommitted changes). - <code>--long</code>: Includes the commit hash, distance from the nearest tag, and the tag name in the output. - <code>--always</code>: Ensures that a description is always provided, even if there are no tags in the repository. - <code>--dirty</code>: Appends a -dirty suffix to the output if there are uncommitted changes in the working directory.</p> <p>Output: <code>&lt;tag_name&gt;-&lt;distance_from_tag&gt;-g&lt;commit_hash&gt;</code> - example: <code>v1.2.3-3-gabcdef1234567890-dirty</code> - <code>&lt;tag_name&gt;</code>: The name of the nearest tag. - <code>&lt;distance_from_tag&gt;</code>: The number of commits between the current commit and the nearest tag. - <code>g&lt;commit_hash&gt;</code>: The commit hash of the current commit. - <code>-dirty</code>: Appended if there are uncommitted changes.</p>"},{"location":"DevOps/Git/Info/#git-rev-parse-head","title":"<code>git rev-parse HEAD</code>","text":"<p>Return the commit hash of the current branch head: <code>abcdef1234567890</code>.</p>"},{"location":"DevOps/Git/Info/#git-remote","title":"<code>git remote</code>","text":"<p>Display a list of all configured remote repositories: <code>origin</code>, <code>upstream</code>.</p>"},{"location":"DevOps/Git/Info/#git-ls-remote-get-url-origin","title":"<code>git ls-remote --get-url origin</code>","text":"<p>Retrieve the URL associated with the remote repository named <code>origin</code>.</p>"},{"location":"DevOps/Git/Issue/","title":"Git Issue","text":""},{"location":"DevOps/Git/Issue/#no-output-from-git-log-in-powershell","title":"No output from <code>git log</code> in PowerShell","text":"<p>git command in conda env not show anything</p> <p>Under PowerShell, check the linked exe to git command: <pre><code>Get-Command git #will show the source path of git.exe\n</code></pre> Solution: - Run the command in terminal and it will show the error, e.g., msys-2.0.exe not found - This means the exe in the conda env is missing or corrupted - upgrading <code>git</code> in the conda-env fixed the issue</p>"},{"location":"DevOps/Git/Issue/#procedure-entry-point-could-not-be-located-in-the-dynamic-link-library","title":"Procedure Entry Point Could Not Be Located in the Dynamic Link Library","text":"<ul> <li>dll not exists</li> <li>dll path not included</li> <li>dll corrupted</li> </ul>"},{"location":"DevOps/Git/Issue/#no-pr-in-github-after-push","title":"No PR in github after push","text":"<p>reset upstream to remote upstream/master <pre><code>git branch --set-upstream-to upstream/master\ngit push upstream HEAD\n</code></pre></p>"},{"location":"DevOps/Git/Issue/#fatal-repository-not-found","title":"fatal: repository not found","text":"<ul> <li>did not authenticate</li> <li>password has changed</li> <li>not a collaborator</li> <li>incorrect case or a word misspelled</li> <li>repo has been deleted</li> </ul>"},{"location":"DevOps/Git/Issue/#fatal-refusing-to-merge-unrelated-histories","title":"fatal: refusing to merge unrelated histories","text":"<pre><code>git reset HEAD~ #unstage last commit if not init commit, else\ngit update-ref -d HEAD &amp;&amp; git rm --cached -r .\ngit stash       #stash changes\ngit pull        #fetch and merge remote branch\ngit pop         #unstash changes\n</code></pre>"},{"location":"DevOps/Git/Issue/#fatal-ambiguous-argument-upstreammaster-unknown-revision-or-path-not-in-the-working-tree","title":"fatal: ambiguous argument 'upstream/master': unknown revision or path not in the working tree","text":"<pre><code>git fetch upstream\ngit checkout master\ngit reset --hard upstream/master\ngit push origin master --force\ngit branch --set-upstream-to upstream/master\n</code></pre>"},{"location":"DevOps/Git/MapDrive/","title":"Network Drive","text":""},{"location":"DevOps/Git/MapDrive/#tips","title":"Tips","text":"<p>Stop tracking the file but not delete it from your system use: <pre><code>git rm --cached filename\n</code></pre> Tracking the file again: <pre><code>git add filename\n</code></pre></p> <p>To untrack every file that is now in your .gitignore: - commit any outstanding code changes - removes any changed files from the index: <code>git rm -r --cached .</code> - then just run: <code>git add .</code> - commit it: <code>git commit -m \".gitignore is now working\"</code></p>"},{"location":"DevOps/Git/MapDrive/#using-git-on-a-windows-network-drive","title":"Using Git on a Windows Network Drive","text":"<ul> <li>move to the soon to be git repository <code>cd /D \"Y:\\dev\\MyProject\"</code></li> <li>create an empty git repository <code>git init --bare</code></li> <li>clone a local reporitory (this will copy to local) <code>cd /D \"C:\\dev\\MyProject &amp; git clone file:\"//Y:\\dev\"</code></li> </ul> <p>already have a local git repository <pre><code>cd \"C:\\dev\\MyProject\ngit remote add origin file:\"//Y:\\dev\\MyProject\"\ngit push origin master\n</code></pre></p> <p>Fetch master branch from Windows Network Drive - move to the soon to be git repository <code>cd \"C:\\dev\\MyProject\"</code> - reate an empty git repository <code>git init</code> - fetch master branch from origin <code>$ git pull \"Y:\\dev\\MyProject\" \"master\"</code></p>"},{"location":"DevOps/Git/Merge/","title":"Merge","text":""},{"location":"DevOps/Git/Merge/#how-to-merge","title":"how to merge","text":"<pre><code>#edit conflict file\ngit add &lt;file-path&gt;\ngit commit -m \"Updated\"\ngit push\n</code></pre>"},{"location":"DevOps/Git/Merge/#in-conflict-state-accept-theirs","title":"in conflict state accept <code>theirs</code>","text":"<pre><code>git checkout --theirs &lt;file-path&gt;\ngit add &lt;file-path&gt;\n</code></pre>"},{"location":"DevOps/Git/Merge/#resolve-conflict","title":"resolve conflict","text":"<pre><code>git switch master\ngit pull\ngit switch my-branch\ngit rebase master\n</code></pre>"},{"location":"DevOps/Git/Merge/#refusing-to-merge-unrelated-histories","title":"refusing to merge unrelated histories","text":"<p><pre><code>$ git merge azure/main\nfatal: refusing to merge unrelated histories\n</code></pre> - two repos do not have shared histories - force merge: <code>git merge azure/main --allow-unrelated-histories</code> - then solve the conflicts: <code>git add xxx; git commit yyy</code> - <code>not good</code>: will include all commits from the source repo - use <code>cherry-pick</code>?</p>"},{"location":"DevOps/Git/Rebase/","title":"Rebase","text":"<p>https://github.com/openedx/edx-platform/wiki/How-to-Rebase-a-Pull-Request</p>"},{"location":"DevOps/Git/Rebase/#fetch-latest-version-of-upstream-master","title":"fetch latest version of upstream master","text":"<pre><code>git fetch upstream\n</code></pre>"},{"location":"DevOps/Git/Rebase/#find-hash-of-commit-which-is-base-of-my-branch","title":"find hash of commit which is base of my-branch","text":"<pre><code>git merge-base my-branch upstream/master # will return a commit hash\n</code></pre>"},{"location":"DevOps/Git/Rebase/#merge-commits-by-rebasing-the-pr-branch","title":"merge commits by rebasing the pr branch","text":"<p><pre><code>git rebase -i &lt;hash&gt;\n</code></pre> Replace \"pick\" with \"squash\" except first one. Then save and close the file.</p>"},{"location":"DevOps/Git/Rebase/#perform-a-rebase","title":"perform a rebase","text":"<pre><code>git rebase upstream/master\n# resolve conflicts and\ngit rebase --continue\n# push the changes\ngit push -f\n</code></pre>"},{"location":"DevOps/Git/Remote/","title":"Remote","text":""},{"location":"DevOps/Git/Remote/#verify-remote-url","title":"verify remote url","text":"<pre><code>git remote -v\n</code></pre>"},{"location":"DevOps/Git/Remote/#change-remote-url","title":"change remote url","text":"<pre><code>git remote set-url origin https://github.com/usr/repo.git\n</code></pre>"},{"location":"DevOps/Git/Remote/#add-remote","title":"add remote","text":"<pre><code>git remote add upstream https://github.com/prj/repo.git\n</code></pre>"},{"location":"DevOps/Git/Remote/#show-remote-branches","title":"show remote branches","text":"<pre><code>gt remote -vv\n</code></pre>"},{"location":"DevOps/Git/Repo/","title":"Repo","text":""},{"location":"DevOps/Git/Repo/#create-a-repo","title":"create a repo","text":"<p>local folder to github repo <pre><code>#create a github repo first, do not add readme etc\ngit init &amp;&amp; git checkout -b main            #initialize local directory as a git repo, old git\ngit init -b main                            #new git\ngit add . &amp;&amp; git commit -m \"initial commit\" #stage and commit all files in your project\ngit remote add origin https://github.com/&lt;user&gt;/repo.git  #link to github, remove, set-url\ngit push -u origin main                     #push to github repo\n</code></pre></p>"},{"location":"DevOps/Git/Repo/#clone-a-repo","title":"clone a repo","text":"<p>github repo to local folder <pre><code>fork repo to &lt;user&gt;\ngit clone https://github.com/&lt;user&gt;/repo.git\ncd repo\ngit remote add upstream https://github.com/&lt;upstream&gt;/repo.git\ngit remote -v\npip install --no-deps --editable .\n# wrong? delete it: pip uninstall repo\ncode .\n</code></pre></p>"},{"location":"DevOps/Git/Search/","title":"Search","text":""},{"location":"DevOps/Git/Search/#search-deleted-code","title":"search deleted code","text":"<p>Search for <code>abc xyz</code> <pre><code>git log -G \"abc +xyz\" --oneline\n</code></pre></p>"},{"location":"DevOps/Git/Search/#search-deleted-filepath","title":"search deleted filepath","text":"<pre><code>git log --oneline -- **/my_file.py\n</code></pre>"},{"location":"DevOps/Git/Stage/","title":"Stage","text":""},{"location":"DevOps/Git/Stage/#unstage-all-files","title":"unstage all files","text":"<pre><code>git reset\n</code></pre>"},{"location":"DevOps/Git/Stage/#unstage-specific-file","title":"unstage specific file","text":"<pre><code>git reset &lt;commit&gt; -- &lt;filepath&gt;\n</code></pre>"},{"location":"DevOps/Git/Stash/","title":"Stash","text":"<p>Save uncommitted changes (both staged and unstaged) for later use, and then revert them from working copy.</p> <p>When run <code>git stash</code>, will stash any changes that haven't been committed, reverting the branch back to the latest commit. If switch branches and run <code>git stash pop</code>, Git will then apply the changes to the new branch.</p>"},{"location":"DevOps/Git/Stash/#include-new-files-in-stash","title":"include new files in stash","text":"<pre><code>git add .\ngit stash --staged -m \"stash-name\"\n</code></pre>"},{"location":"DevOps/Git/Stash/#save-and-pop","title":"save and pop","text":"<pre><code>git stash list                 #list all stashes\ngit stash push -m \"stash-name\" #save a stash with a name\ngit stash pop stash@{n}        #apply and remove from the stack\n</code></pre>"},{"location":"DevOps/Git/Stash/#apply-only","title":"apply only","text":"<pre><code>git stash apply stash@{n}      #apply and keep in the stack\ngit stash apply stash-name     #apply by name\ngit stash apply n              #apply stash@{n}\n</code></pre>"},{"location":"DevOps/Git/Stash/#remove-stash","title":"remove stash","text":"<pre><code>git stash drop &lt;stash-id&gt;      #remove one stash\ngit stash clear                #remove all stashes\n</code></pre>"},{"location":"DevOps/Git/Stash/#untracked-files","title":"untracked files","text":"<pre><code>git stash -u #--include-untracked stash untracked files\ngit stash -a #--all stash untracked files and ignored files\n</code></pre>"},{"location":"DevOps/Git/Stash/#error-unknown-switch-e","title":"error: unknown switch <code>e</code>","text":"<p>Curly braces have special meaning in PowerShell. <pre><code>git stash pop 'stash@{0}'\n</code></pre></p>"},{"location":"DevOps/Git/Sync/","title":"Sync","text":""},{"location":"DevOps/Git/Sync/#push","title":"push","text":"<p>push local branch to remote branch with the same name <pre><code>git push &lt;remote&gt; &lt;local-branch&gt;\n</code></pre></p>"},{"location":"DevOps/Git/Sync/#fetch","title":"fetch","text":"<p>fetch remote branch <pre><code>git fetch &lt;repo&gt; &lt;remote-branch&gt;\n</code></pre></p>"},{"location":"DevOps/Git/Sync/#merge","title":"merge","text":"<p>merge remote branch to current branch <pre><code>git merge &lt;repo&gt;/&lt;remote-branch&gt;\n</code></pre></p>"},{"location":"DevOps/Git/Sync/#pull","title":"pull","text":"<p>fetch remote branch and merge to local branch <pre><code>git pull &lt;repo&gt; &lt;remote-branch&gt;:&lt;local-branch&gt;\n</code></pre></p>"},{"location":"DevOps/Git/Sync/#remove-a-local-repo","title":"remove a local repo","text":"<pre><code>rm -fr .git  #will keep all\n</code></pre>"},{"location":"DevOps/Git/Sync/#clone-repo-to-local-folder","title":"clone repo to local folder","text":"<pre><code>git clone https://github.com/usr/repo.git repo_name\n</code></pre>"},{"location":"DevOps/Git/Sync/#clone-local-to-azure-repo","title":"clone local to azure repo","text":"<pre><code>git remote add azure https://azure.com/repo-name\ngit push -u azure --all\n</code></pre>"},{"location":"DevOps/Git/Sync/#upstream-to-master-sync-a-fork","title":"upstream to master (sync a fork)","text":"<pre><code>git checkout master       #check out fork's local default branch\ngit fetch upstream        #fetch upstream branches and commits\ngit merge upstream/master #merge upstream/master into local current branch\ngit push origin master    #push local changes to origin/master\n</code></pre>"},{"location":"DevOps/Git/Sync/#upstream-to-local-branch-rebase","title":"upstream to local branch (rebase)","text":"<pre><code>git checkout my-branch\ngit stash\ngit fetch upstream\ngit rebase upstream/master #rebase my-branch from the upstream\u2019s master branch\ngit stash\n\n# if the local branch is already in a pr\ngit push --force-with-lease origin my-branch # don't overwrite remote changes we don't have locally\n</code></pre>"},{"location":"DevOps/Git/Sync/#master-to-local-branch","title":"master to local branch","text":"<pre><code>git checkout my-branch\ngit fetch origin        #git pull origin master, OK but better use\ngit merge origin/master\n</code></pre>"},{"location":"DevOps/Git/Sync/#remote-azure-branch-to-local-master","title":"remote <code>azure</code> branch to local master","text":"<pre><code>git checkout &lt;local-branch&gt;\ngit fetch azure\ngit merge &lt;repo&gt;/&lt;remote-branch&gt; #merge to local checked-out branch\n\n# example\ngit checkout main\ngit fetch origin\ngit merge origin/main\n</code></pre>"},{"location":"DevOps/Git/Sync/#remote-azure-branch-to-local-new-branch","title":"remote <code>azure</code> branch to local new branch","text":"<pre><code>git fetch &lt;repo&gt; &lt;remote-branch&gt;\ngit checkout -b &lt;local-branch&gt; &lt;repo&gt;/&lt;remote-branch&gt;\n</code></pre>"},{"location":"DevOps/Git/Sync/#local-branch-to-remote-azure-branch","title":"local branch to remote <code>azure</code> branch","text":"<pre><code># first time add upstream\ngit push -u azure local-branch\n# git push &lt;repo&gt; &lt;local-branch&gt;:&lt;remote-branch&gt;\ngit push azure local-branch:remote-branch\n</code></pre>"},{"location":"DevOps/Git/Sync/#merge-upstream-repo-into-fork","title":"merge upstream repo into fork","text":"<pre><code>#check out branch to merge to\ngit checkout my-branch\n\n#pull the desired branch from the upstream repo\ngit pull https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git branch-name\n\n#commit the merge\n\n#push merge to GitHub repo\ngit push origin branch-name\n</code></pre>"},{"location":"DevOps/Git/Sync/#cherry-pick","title":"cherry-pick","text":"<p>cherry-pick from github repo to azure devops repo <pre><code>git checkout -b sync-1\ngit fetch azure\ngit reset --hard azure/main\n\ngit fetch github\n\n# one by one\ngit cherry-pick &lt;commit-hash-1&gt;\n..\ngit cherry-pick &lt;commit-hash-n&gt;\n\n# or in a range\ngit cherry-pick &lt;oldest-commit-hash&gt;^..&lt;newest-commit-hash&gt;\n\ngit push azure sync-1\n</code></pre></p>"},{"location":"DevOps/Git/Tag/","title":"Tag","text":""},{"location":"DevOps/Git/Tag/#delete-local-tag","title":"delete local tag","text":"<pre><code>git tag -d 1.0.0\n</code></pre>"},{"location":"DevOps/Git/Tag/#delete-remote-tag","title":"delete remote tag","text":"<pre><code>git push --delete origin 1.0.0\ngit push --delete upstream 1.0.0\n</code></pre>"},{"location":"DevOps/GitHub/Actions/","title":"Actions","text":""},{"location":"DevOps/GitHub/Actions/#cache-env","title":"Cache env","text":"<p>https://dev.to/epassaro/caching-anaconda-environments-in-github-actions-5hde</p>"},{"location":"DevOps/GitHub/Auth/","title":"Auth","text":""},{"location":"DevOps/GitHub/Auth/#update-pat","title":"update pat","text":"<pre><code>gh auth login #then input the pat\n# check status: https://www.githubstatus.com/\n</code></pre>"},{"location":"DevOps/GitHub/Auth/#check-pat","title":"check pat","text":"<pre><code>curl -H \"Authorization: token &lt;my-pat&gt;\" https://api.github.com/user\n</code></pre>"},{"location":"DevOps/GitHub/Auth/#error","title":"Error:","text":"<p>The PAT expired and should be updated.</p>"},{"location":"DevOps/GitHub/Auth/#add-pat-personal-access-token","title":"Add PAT (Personal Access Token)","text":"<p>GitHub account, go to Settings * =&gt; Developer Settings * =&gt; Personal Access Token * =&gt; Generate New Token (Give your password) * =&gt; Fill up the form * =&gt; Click Generate token * =&gt; Copy the generated Token</p>"},{"location":"DevOps/GitHub/Auth/#clone-with-pat","title":"clone with PAT","text":"<pre><code>git clone https://username:ghp_MY-PERSONAL-ACCESS-TOKEN@github.com/username/my-repo.git\n</code></pre>"},{"location":"DevOps/GitHub/Auth/#linux","title":"linux","text":"<p>Configure the local GIT client with a username and email address <pre><code>$ git config --global user.name \"your_github_username\"\n$ git config --global user.email \"your_github_email\"\n$ git config -l\n#after first use cache token\n#git config --global credential.helper cache #not good\ngit config credential.helper store\n#delete cache record if needed\ngit config --global --unset credential.helper\ngit config --system --unset credential.helper\n#pull verify\ngit pull -v\n</code></pre></p>"},{"location":"DevOps/GitHub/Auth/#windows","title":"windows","text":"<p>Credential Manager from Control Panel * =&gt; Windows Credentials * =&gt; find git:https://github.com =&gt; Edit * =&gt; On Password replace with GitHub Personal Access Token</p> <p>If you cannot find git:https://github.com * =&gt; Click on Add a generic credential * =&gt; Internet address will be git:https://github.com * =&gt; type in username and password (GitHub Personal Access Token)</p>"},{"location":"DevOps/GitHub/Auth/#macos","title":"macos","text":"<p>launch app eychain access * =&gt; In Keychain Access, search for github.com * =&gt; Find the internet password entry for github.com * =&gt; Edit or delete the entry accordingly</p>"},{"location":"DevOps/GitHub/Auth/#gitconfig","title":".gitconfig","text":"<p>Configure Git to use the GitHub CLI (gh) as the credential helper. <pre><code>[credential \"https://github.com\"]\n    helper =\n    helper = !/usr/bin/gh auth git-credential\n[credential \"https://gist.github.com\"]\n    helper =\n    helper = !/usr/bin/gh auth git-credential\n</code></pre></p> <p>check the credentials <pre><code>gh auth status\ngh auth status --show-token\n</code></pre></p>"},{"location":"DevOps/GitHub/Blog/","title":"Blog","text":""},{"location":"DevOps/GitHub/Blog/#how-to-setup-github-pages-for-blogging","title":"how to setup github pages for blogging","text":"<ul> <li>https://chadbaldwin.net/2021/03/14/how-to-build-a-sql-blog.html</li> <li>https://github.com/chadbaldwin/chadbaldwin.github.io</li> </ul>"},{"location":"DevOps/GitHub/Blog/#test-github-pages-locally","title":"test github pages locally","text":"<p>https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/testing-your-github-pages-site-locally-with-jekyll - install <code>Ruby</code>, <code>Gem</code>, and <code>Bundler</code> - create Gemfile in root folder   - https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/creating-a-github-pages-site-with-jekyll <pre><code>source \"https://rubygems.org\"\ngem \"github-pages\", \"~&gt; 232\", group: :jekyll_plugins\n</code></pre> - run   - <code>sudo apt update &amp; sudo apt install ruby3.2-dev build-essential</code>   - <code>bundle config set --local path 'vendor/bundle'</code>   - <code>bundle install</code> and   - <code>bundle exec jekyll serve</code></p>"},{"location":"DevOps/GitHub/Blog/#install-rougify","title":"install rougify","text":"<pre><code>sudo apt install ruby-rouge\n</code></pre>"},{"location":"DevOps/GitHub/Blog/#generate-rouge-stylesheet-github-style","title":"Generate Rouge Stylesheet (github style)","text":"<pre><code>rougify style github &gt; assets/css/syntax.css\n</code></pre>"},{"location":"DevOps/GitHub/Blog/#integrate-the-stylesheet","title":"Integrate the Stylesheet","text":"<p>Link the generated CSS file in website's HTML  section (_layouts/post.html): <pre><code>&lt;link href=\"/assets/css/syntax.css\" rel=\"stylesheet\"&gt;\n</code></pre>"},{"location":"DevOps/GitHub/Blog/#rouge-sample-highlighter","title":"Rouge sample highlighter","text":"<p>http://rouge.jneen.net/v3.26.0/r/</p> <p>_config.yml <pre><code>markdown: kramdown\nhighlighter: rouge\n\nkramdown:\n  input: GFM\n  syntax_highlighter: rouge\n</code></pre></p>"},{"location":"DevOps/GitHub/Blog/#template","title":"template","text":"<ul> <li>https://github.com/gangdong/gangdong.github.io (https://dqdongg.com/)</li> <li>https://github.com/purplediane/purplediane.github.io (https://purplediane.github.io/)</li> </ul>"},{"location":"DevOps/GitHub/Blog/#trainingbook-template","title":"training/book template","text":"<ul> <li>https://github.com/olohmann/azure-hands-on-labs/blob/master/labs/03_terraform/terraform.md (https://olohmann.github.io/azure-hands-on-labs/labs/01_serverless/serverless.html)</li> </ul>"},{"location":"DevOps/GitHub/Blog/#jekyll","title":"jekyll","text":"<ul> <li>http://simpleprimate.com/jekyll-for-designers/blog/css/</li> </ul>"},{"location":"DevOps/GitHub/Blog/#hits-and-page-views","title":"hits and page views","text":"<p>https://wxguy.in/posts/how-to-enable-no-of-hits-page-view-counter-on-jekyll-github-blog-posts/</p>"},{"location":"DevOps/GitHub/BlogIdea/","title":"blog layout idea","text":"<p>Is it possible to create github blogs like this page so the left shows the blogs by category and we can also switch to another page showing the blogs by datetime: https://learn.microsoft.com/en-us/azure/virtual-network/tutorial-filter-network-traffic?tabs=portal?</p> <p>Yes \u2705 it\u2019s possible to build a GitHub Pages blog that behaves like that Microsoft Docs page \u2014 with a left-hand navigation tree (by category) and a switchable view (by date, tags, etc.).</p> <p>GitHub Pages is just Jekyll (or any static site generator) behind the scenes, so you can structure your blog like a documentation site.</p>"},{"location":"DevOps/GitHub/BlogIdea/#how-to-achieve-it","title":"\ud83d\udd11 How to achieve it","text":""},{"location":"DevOps/GitHub/BlogIdea/#1-left-sidebar-by-category","title":"1. Left Sidebar by Category","text":"<ul> <li>Use Jekyll Collections or Categories.</li> <li>In <code>_config.yml</code>, enable categories:</li> </ul> <pre><code>collections:\n  posts:\n    output: true\n\ndefaults:\n  - scope:\n      path: \"\"\n      type: \"posts\"\n    values:\n      layout: \"post\"\n</code></pre> <ul> <li>Then create a sidebar include (<code>_includes/sidebar.html</code>) that loops through categories:</li> </ul> <pre><code>&lt;nav class=\"sidebar\"&gt;\n  &lt;h3&gt;Categories&lt;/h3&gt;\n  &lt;ul&gt;\n    {% for category in site.categories %}\n      &lt;li&gt;\n        &lt;a href=\"/categories/{{ category[0] }}\"&gt;{{ category[0] }}&lt;/a&gt;\n        &lt;ul&gt;\n          {% for post in category[1] %}\n            &lt;li&gt;&lt;a href=\"{{ post.url }}\"&gt;{{ post.title }}&lt;/a&gt;&lt;/li&gt;\n          {% endfor %}\n        &lt;/ul&gt;\n      &lt;/li&gt;\n    {% endfor %}\n  &lt;/ul&gt;\n&lt;/nav&gt;\n</code></pre> <p>This will look like Microsoft Docs\u2019 expandable navigation.</p>"},{"location":"DevOps/GitHub/BlogIdea/#2-switch-between-by-category-and-by-date","title":"2. Switch Between \u201cBy Category\u201d and \u201cBy Date\u201d","text":"<ul> <li> <p>Create two index pages:</p> </li> <li> <p><code>categories.html</code> \u2192 loops by category (as above).</p> </li> <li><code>archive.html</code> \u2192 loops by date:</li> </ul> <pre><code>&lt;h3&gt;Posts by Date&lt;/h3&gt;\n&lt;ul&gt;\n  {% for post in site.posts %}\n    &lt;li&gt;{{ post.date | date: \"%Y-%m-%d\" }} \u2013 &lt;a href=\"{{ post.url }}\"&gt;{{ post.title }}&lt;/a&gt;&lt;/li&gt;\n  {% endfor %}\n&lt;/ul&gt;\n</code></pre> <ul> <li>Add navigation links (e.g., in <code>_layouts/default.html</code> or a navbar) so users can toggle between views.</li> </ul>"},{"location":"DevOps/GitHub/BlogIdea/#3-layouttheme-options","title":"3. Layout/Theme Options","text":"<p>To make it look more like Microsoft Docs, you can:</p> <ul> <li> <p>Use a docs-like Jekyll theme, for example:</p> </li> <li> <p>just-the-docs (clean sidebar, like MS Docs).</p> </li> <li>minimal-mistakes (already supports category/date archives).</li> <li>doxygen-awesome (if you want a docset-like feel).</li> </ul>"},{"location":"DevOps/GitHub/BlogIdea/#4-optional-enhancements","title":"4. Optional Enhancements","text":"<ul> <li>Search bar: add jekyll-toc or lunr.js search.</li> <li>Collapsible sidebar: use Bootstrap or pure CSS/JS to expand/collapse categories.</li> <li>Tabs (like \"Portal / CLI / PowerShell\"): can be done with tabbed content in Markdown.</li> </ul> <p>\ud83d\udc49 If you want something exactly like learn.microsoft.com, the closest starter is just-the-docs \u2014 it already has:</p> <ul> <li>Sidebar navigation</li> <li>Categories</li> <li>Search</li> <li>Mobile-friendly layout</li> </ul> <p>Would you like me to show you how to adapt your current blog repo to add a sidebar and archive page, or would you prefer to start fresh with just-the-docs and migrate posts in?</p>"},{"location":"DevOps/GitHub/BlogLayout/","title":"Blog Layout","text":"<p><code>Jekyll</code>-powered GitHub blog repository: - <code>_posts</code>: The content folder for blog posts. - <code>_layouts</code>: This folder contains the HTML templates (or layouts) that wrap around the content. I'll explain how it defines the structure of a page. - <code>_includes</code>: This folder is for reusable code snippets that can be included in layouts or pages. I'll give examples like headers, footers, or social media links. - <code>_sass</code>: This is for styling. I'll explain that it contains Sass/SCSS files that are compiled into CSS for the site's design.</p>"},{"location":"DevOps/GitHub/BlogLayout/#_posts","title":"_posts","text":"<p>The <code>_posts</code> folder is where all of your blog content resides. Each blog post is a Markdown (<code>.md</code>) file, and the file name must follow a specific format: <code>yyyy-mm-dd-blog-title.md</code>.</p> <p>The front matter at the top of each file (YAML data between triple-dashed lines) defines metadata like the post's title, author, and date. <pre><code>---\ntitle: \"My First Post\"\ntags: [intro, blog]\n---\nThis is the content of my first post.\n</code></pre></p>"},{"location":"DevOps/GitHub/BlogLayout/#_layouts","title":"_layouts","text":"<p>The <code>_layouts</code> folder contains the <code>page templates</code> that wrap around your content. A layout is a reusable HTML file that defines the structure and common elements of a page, such as the header, footer, and sidebar.</p> <p>Example layouts include <code>default.html</code>, <code>post.html</code>, <code>page.html</code>, to define the overall structure and look of different types of pages on your site. When you create a new blog post, you specify which layout to use in the front matter: <pre><code>---\nlayout: post\n---\n</code></pre> Jekyll takes your content and inserts it into that layout via <code>{{ content }}</code>.</p>"},{"location":"DevOps/GitHub/BlogLayout/#_includes","title":"_includes","text":"<p>The <code>_includes</code> folder is for storing small, <code>reusable snippets</code> of code or content, like headers, footers, social media links, or a comment section. These are fragments that can be inserted into a layout, post, or page to avoid code duplication.</p> <p>File: <code>_includes/footer.html</code> <pre><code>&lt;footer&gt;\u00a9 {{ site.time | date: \"%Y\" }} My Blog&lt;/footer&gt;\n</code></pre></p> <p>Then in a layout: <pre><code>&lt;body&gt;\n{{ content }}\n{% include footer.html %}\n&lt;/body&gt;\n</code></pre></p>"},{"location":"DevOps/GitHub/BlogLayout/#_sass","title":"_sass","text":"<p>The <code>_sass</code> folder is where you keep reusable Sass/SCSS stylesheets (CSS with variables &amp; nesting), which are used to style your website. Sass is a preprocessor that adds powerful features like variables, nesting, and mixins to CSS.</p> <p>Sass (<code>.sass</code>) or SCSS (<code>.scss</code>) files are compiled into a standard CSS file by Jekyll before the site is built and published.</p> <p>Inside <code>_sass</code>, you create <code>.scss</code> files (partials like <code>_variables.scss</code>, <code>_buttons.scss</code>). Then you import them into your main stylesheet (<code>assets/css/style.scss</code> or similar): <pre><code>---\n---\n@import \"variables\";\n@import \"buttons\";\n</code></pre> Jekyll will compile Sass --&gt; CSS when building the site. It\u2019s for styling structure and theming.</p>"},{"location":"DevOps/GitHub/BlogLayout/#header-pages","title":"header pages","text":"<p>By default all <code>.md</code> files with a title under the root folder will be added to the header nav pages. By settng the <code>header_pages</code> in <code>_config.yml</code> file, we can control which file to be included and the order. These <code>.md</code> files will be added in the <code>header.html</code> file. More details: https://talk.jekyllrb.com/t/minima-header-html-functionality/6721</p>"},{"location":"DevOps/GitHub/BlogLayout/#width-constraint-only-for-wrapper-container","title":"width constraint only for <code>.wrapper</code> container","text":"<p>In Minima\u2019s layout, the width constraint (using <code>$content-width</code>) only applies to elements that are inside the <code>.wrapper</code> container. <pre><code>&lt;header class=\"site-header\"&gt;\n  &lt;div class=\"wrapper\"&gt;\n    &lt;!-- nav, title, etc. --&gt;\n  &lt;/div&gt;\n&lt;/header&gt;\n</code></pre></p>"},{"location":"DevOps/GitHub/CLI/","title":"Github CLI","text":""},{"location":"DevOps/GitHub/CLI/#install","title":"install","text":"<p>https://github.com/cli/cli/blob/trunk/docs/install_linux.md <pre><code>type -p curl &gt;/dev/null || sudo apt install curl -y\ncurl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg \\\n&amp;&amp; sudo chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg \\\n&amp;&amp; echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | sudo tee /etc/apt/sources.list.d/github-cli.list &gt; /dev/null \\\n&amp;&amp; sudo apt update \\\n&amp;&amp; sudo apt install gh -y\n</code></pre></p>"},{"location":"DevOps/GitHub/GitHub/","title":"GitHub","text":"<ul> <li>Collaborative Coding</li> <li>Automation / CI &amp; CD</li> <li>Security including additional features for enterprise customers</li> <li>Project Management</li> </ul> <p>Features: - branch protection rules</p>"},{"location":"DevOps/GitHub/GitHub/#github-vs-gitlab","title":"Github vs Gitlab","text":"<p>https://automateinfra.com/2023/05/05/gitlab-vs-github</p>"},{"location":"DevOps/GitHub/GitHub/#pr-review","title":"PR review","text":"<p>https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/reviewing-changes-in-pull-requests/reviewing-proposed-changes-in-a-pull-request <pre><code>#search for review request\nreview-requested:[USERNAME]\n</code></pre></p>"},{"location":"DevOps/GitHub/GitHub/#sync-proj","title":"Sync proj","text":"<pre><code>#clone project\ngit clone https://github.com/usr/proj.git\ncd repo\n#list forked repo\ngit remote -v\n#sync from upstream\ngit fetch upstream\n#checkout to master\ngit checkout master\ngit merge upstream/master\n</code></pre>"},{"location":"DevOps/GitHub/GitHub/#add-proj-to-github","title":"Add proj to GitHub","text":"<pre><code>cd proj-folder #move to proj folder\ngit init -b main #init local dir as git repo\ngh repo create proj-name #create proj repo on github\ngit pull --set-upstream origin main #pull changes from new github repo\ngit add . &amp;&amp; git commit -m \"initial commit\" &amp;&amp; git push #stage, commit, and push all files local proj\n\n#old git version\ngit init\ngit add . &amp;&amp; git commit -m \"initial commit\"\ngit remote add origin https://github.com/yourusername/your-repo-name.git\ngit pull origin master\ngit push origin master\n</code></pre>"},{"location":"DevOps/GitHub/GitHub/#one-line-add-commit-and-push","title":"one line add commit and push","text":"<ul> <li>creat a makefile in repo folder</li> <li>add the lines to the file (tab not spaces)   <pre><code> git:\n  git add .\n  git commit -m \"$m\"\n  git push -u origin main\n</code></pre></li> <li>make git m=\"message\"</li> </ul>"},{"location":"DevOps/GitHub/GitHub/#pr-not-shown","title":"pr not shown","text":"<p>open the link to create a pr: https://xyz.github.com/usr//pull/new/"},{"location":"DevOps/GitHub/GitHub/#delete-release","title":"delete release","text":"<ul> <li>Delete the release from the Github UI</li> <li>Delete local tag if the release failed: <code>git tag -d 0.1.0</code></li> <li>Delete the remote tag from local master: <code>git push --delete upstream 0.1.0</code></li> </ul>"},{"location":"DevOps/GitHub/Learn/","title":"Learn","text":"<p>https://speakerdeck.com/alicebartlett/git-for-humans</p> <p>https://bitbucket.org/blog/get-git-guru-gold-with-bitbuckets-glorious-git-guides</p> <p>https://try.github.io/</p> <p>An intro to the GitHub workflow: https://adrin.info/gitgithub-how-to-contribute-to-an-open-source-project-on-github.html</p> <p>A more in depth intro to the git workflow: https://gist.github.com/Chaser324/ce0505fbed06b947d962</p> <p>https://livecodestream.dev/post/2020-08-21-git-concepts-and-workflow-for-beginners/</p> <p>Interactive git tutorial: https://learngitbranching.js.org/</p>"},{"location":"DevOps/GitHub/Search/","title":"Search","text":"<p>https://docs.github.com/en/search-github/searching-on-github/searching-issues-and-pull-requests</p>"},{"location":"DevOps/GitHub/Search/#unmerged-pr","title":"unmerged pr","text":"<pre><code>is:pr is:open\nis:pr is:unmerged\n</code></pre>"},{"location":"DevOps/GitHub/Search/#users-pr","title":"user's pr","text":"<pre><code>is:pr author:&lt;user-name&gt;\n</code></pre>"},{"location":"DevOps/GitHub/Search/#dependencies-in-yaml","title":"dependencies in *.yaml","text":"<pre><code>dependencies path:*.yaml\n</code></pre>"},{"location":"DevOps/GitHub/Search/#content-has","title":"content has <code>:</code>","text":"<p>How???</p>"},{"location":"DevOps/Helm/CLI/","title":"CLI","text":""},{"location":"DevOps/Helm/CLI/#repo","title":"repo","text":"<pre><code>helm repo list\nhelm repo add kured https://kubereboot.github.io/charts\nhelm repo remove kured\nhelm repo update\n</code></pre>"},{"location":"DevOps/Helm/CLI/#list-chart","title":"list chart","text":"<pre><code>helm search repo kured\nhelm search repo kured --versions\n</code></pre>"},{"location":"DevOps/Helm/CLI/#install","title":"install","text":"<pre><code>helm install [NAME] [CHART] [flags]\nhelm install &lt;release-name&gt; &lt;chart-name&gt; #chart reference\nhelm install jenkins ./jenkins-1.2.3.tgz #packaged chart\nhelm install jenkins ./jenkins-archive   #unpacked chart folder\nhelm install jenkins https://example.com/charts/jenkins-1.2.3.tgz   #url\nhelm install jenkins --repo https://example.com/charts/jenkins-prd  #repo url\n</code></pre>"},{"location":"DevOps/Helm/CLI/#check-deployment-status","title":"check deployment status","text":"<pre><code>helm status &lt;release-name&gt;\n</code></pre>"},{"location":"DevOps/Helm/CLI/#list-releases-installed-on-cluster","title":"list releases installed on cluster","text":"<pre><code>helm list --namespace &lt;namespace&gt;\nhelm list --all-namespaces\n</code></pre>"},{"location":"DevOps/Helm/CLI/#uninstall","title":"uninstall","text":"<pre><code>helm uninstall &lt;release-name&gt; [...] [flags]\nhelm uninstall --namespace &lt;namespace&gt; ingress-nginx\n</code></pre>"},{"location":"DevOps/Helm/CSIDriver/","title":"Secrets Store CSI Driver","text":"<p>https://azure.github.io/secrets-store-csi-driver-provider-azure/charts</p> <p>Azure Secrets Store CSI Driver allows for the integration of an Azure key vault as a secret store with an AKS cluster via a CSI volume.</p>"},{"location":"DevOps/Helm/Chart/","title":"Chart","text":"<p>https://helm.sh/docs/topics/charts/</p>"},{"location":"DevOps/Helm/Chart/#put-a-chart-into-a-web-repo","title":"put a chart into a web repo","text":"<p><pre><code>mv jupyterhub-0.1.0.tgz ./jupyterhub/charts/ #copy chart package to the charts folder\nhelm repo index ./jupyterhub/charts/ --url https://my.github.io/jupyterhub/charts/ #create index.yaml\n</code></pre> <code>helm repo index ...</code> will create the <code>index.yaml</code> file inside the <code>charts</code> directory</p>"},{"location":"DevOps/Helm/Chart/#create-a-helm-chart-and-deploy-it","title":"create a helm chart and deploy it","text":"<p>https://learn.microsoft.com/en-us/azure/aks/quickstart-helm?tabs=azure-cli <pre><code>helm create &lt;chart-name&gt;\ncd &lt;chart-name&gt;\nhelm dependency update\n# update values.yaml for image info etc.\nhelm install &lt;release-name&gt; .\n\n# alternatively execute out of the folder\n# helm install &lt;release-name&gt; &lt;chart-name&gt;/ --values &lt;chart-name&gt;/values.yaml\n</code></pre></p>"},{"location":"DevOps/Helm/Chart/#disable-chart","title":"disable chart","text":"<p>in <code>values.yaml</code> add <code>enabled: false</code>. and then update deployment yaml files. <pre><code>{{- if .Values.enabled }}\napiVersion: apps/v1\nkind: Deployment\n# ... rest of your deployment definition ...\n{{- end }}\n</code></pre></p>"},{"location":"DevOps/Helm/Chart/#dependencies","title":"dependencies","text":"<p>https://helm.sh/docs/chart_best_practices/dependencies/#helm</p> <p>local repository: Chart.yaml <pre><code>dependencies:\n  - name: mychart\n    version: 3.1.1\n    repository: file://.\n</code></pre> The value of the <code>repository</code> should be the path to the folder in which there is a chart file called <code>&lt;name&gt;-&lt;version&gt;.tgz</code>. In the above example, the <code>mychart-3.1.1.tgz</code> file should be in the same folder for the <code>Chart.yaml</code> file.</p> <p>If there is a <code>charts</code> folder within the same folder of <code>Chart.yaml</code> the repository should be <code>repository: file://./charts/mychart</code> - the <code>mychart</code> should be a folder containing all the unzipped chart files. </p> <p>oci repository Store charts as OCI artifacts - only available in helm 3. <pre><code>repository: oci://my-acr.azurecr.io/helm\n</code></pre></p>"},{"location":"DevOps/Helm/FlowControl/","title":"Flow control","text":"<p>https://helm.sh/docs/chart_template_guide/control_structures/</p>"},{"location":"DevOps/Helm/FlowControl/#whilespace-control","title":"whilespace control","text":"<p>https://pkg.go.dev/text/template#hdr-Text_and_spaces - <code>{{-</code>: left whitespace will be chomped - <code>-}}</code>: right whitespace will be consumed - Be careful! Newlines are whitespace!</p>"},{"location":"DevOps/Helm/Function/","title":"Function","text":"<p>https://helm.sh/docs/chart_template_guide/function_list/#trimprefix</p>"},{"location":"DevOps/Helm/Function/#string","title":"string","text":"<ul> <li>trimPrefix</li> <li>lower</li> </ul> <p>remove namespace from name: - namespace: dev - name: dev-test-app - app: {{ trimPrefix (printf \"%s-\" .Values.namespace) .Chart.Name }}</p>"},{"location":"DevOps/Helm/Helm/","title":"Helm","text":""},{"location":"DevOps/Helm/Helm/#helm","title":"Helm","text":"<p>Template cheatsheet: https://lzone.de/cheat-sheet/Helm%20Templates</p>"},{"location":"DevOps/Helm/Helm/#helm-playground","title":"Helm playground","text":"<p>https://paulvollmer.net/helm-playground</p>"},{"location":"DevOps/Helm/Helm/#helm-vs-kustomize","title":"Helm vs Kustomize","text":"<p>Kustomize: - Simple deployments with straightforward configuration needs. - Applying environment-specific adjustments to existing manifests. - Integrating with GitOps workflows for declarative configuration management. Helm: - Complex applications with multiple components and dependencies. - Standardized packaging and deployment across environments. - Leveraging pre-built charts from public repositories.</p>"},{"location":"DevOps/Helm/Helm/#install","title":"install","text":"<p>https://helm.sh/docs/intro/install/ - download it: <code>wget https://get.helm.sh/helm-v3.0.0-linux-amd64.tar.gz</code> from https://github.com/helm/helm/releases - unpack it: <code>tar -zxvf helm-v3.0.0-linux-amd64.tar.gz</code> - move to desired destination: <code>mv linux-amd64/helm /usr/local/bin/helm</code></p>"},{"location":"DevOps/Helm/Helm/#type-casting","title":"type casting","text":"<p>https://github.com/yaml/YAML2/wiki/Type-casting <pre><code>ports:\n- containerPort: !!int {{ .Values.containers.app.port }}\n</code></pre></p>"},{"location":"DevOps/Helm/Helm/#if-else","title":"if else","text":"<p>https://github.com/bitnami/charts/blob/master/bitnami/apache/templates/hpa.yaml <pre><code>rollingUpdate:\n  maxSurge: 1\n  {{ if gt .Values.replicaCount 3.0 }}\n  maxUnavailable: 0\n  {{ else }}\n  maxUnavailable: 1\n  {{ end }}\n</code></pre></p>"},{"location":"DevOps/Helm/Helm/#remove-whitespace-xyz","title":"remove whitespace <code>{{- xyz }}</code>","text":"<pre><code>{{- 3 }} means \"trim left whitespace and print 3\"\n</code></pre>"},{"location":"DevOps/Helm/Issue/","title":"Issue","text":""},{"location":"DevOps/Helm/Issue/#helm-connection-reset-by-peer","title":"helm <code>connection reset by peer</code>","text":"<p><pre><code>looks like \"https://jupyterhub.github.io/helm-chart\" is not a valid chart repository or cannot be reached:\nGet \"https://hub.jupyter.org/helm-chart/index.yaml\": read tcp xxx.xxx.xxx.xxx:55360-&gt;xxx.xxx.xxx.xxx:443: read:\nconnection reset by peer\n</code></pre> - an outgoing firewall that needs to be added in place for <code>hub.jupyter.org</code> (check blocked by firewall)</p>"},{"location":"DevOps/Helm/Issue/#index-issue","title":"index issue","text":"<p>Need to create the <code>index.yaml</code> in the folder. <pre><code>helm repo index ./charts/ --url https://my-helm-repo-server/charts/\n</code></pre></p> <p>The <code>index.yaml</code> file can be created in any folder with the same archived <code>.tgz</code> helm chart files. <pre><code>rpc error: code = Unknown desc = Manifest generation error (cached):\n`helm dependency build` failed exit status 1:\nError: no cached repository for helm-manager-...c5f4a2 found. (try 'helm repo update'):\nopen /helm-working-dir/repository/helm-manager-...c5f4a2-index.yaml: no such file or directory\n</code></pre></p>"},{"location":"DevOps/Helm/Learn/","title":"Learn","text":""},{"location":"DevOps/Helm/Learn/#helm-chart-terraform-kubernetes-template","title":"helm chart, terraform, kubernetes template","text":"<p>https://github.com/dungdm93/shipyard/tree/4701279cbd0aa7397213c37992392364ff93d601/helm/buildkitd</p>"},{"location":"DevOps/Helm/Repo/","title":"Chart repo","text":"<p>https://helm.sh/docs/topics/chart_repository/</p>"},{"location":"DevOps/Helm/Repo/#use-local-chart","title":"use local chart","text":"<pre><code>dependencies:\n  - name: jupyterhub\n    version: 3.1.0\n    repository: file://./charts/jupyterhub\n</code></pre>"},{"location":"DevOps/Helm/Repo/#use-github-page-as-private-helm-repo","title":"use github page as private helm repo","text":"<p>https://im5tu.io/article/2022/01/creating-a-private-helm-repository-using-github-pages-enterprise/</p>"},{"location":"DevOps/Helm/Repo/#use-azure-blob-storage-as-private-helm-repo","title":"use azure blob storage as private helm repo","text":"<p>Setting up secure helm chart repository on Azure Blob Storage:  https://cwienczek.com/2017/10/setting-up-secure-helm-chart-repository-on-azure-blob-storage/</p>"},{"location":"DevOps/Helm/Repo/#use-azure-container-registry-as-private-helm-repo","title":"use azure container registry as private helm repo","text":"<p>https://learn.microsoft.com/en-us/azure/container-registry/container-registry-helm-repos</p> <p>https://sbulav.github.io/azure/using-argocd-with-azure-acr/ <pre><code>az acr login --name $ACR_NAME\nhelm push ingress-nginx-3.20.1.tgz oci://$ACR_NAME.azurecr.io/helm\naz acr repository list --name $ACR_NAME\n</code></pre> Helm Chart.yaml <pre><code>dependencies:\n  - name: ingress-nginx\n    version: 3.20.1\n    repository: oci://$ACR_NAME.azurecr.io/helm\n</code></pre> Get error using helm 3.8.0 due to bug: https://github.com/helm/helm/issues/10624 <pre><code>rpc error: code = Unknown desc = Manifest generation error (cached): \n`helm dependency build` failed exit status 1:\nError: could not retrieve list of tags for repository oci://$ACR_NAME.azurecr.io/helm: \nGET \"https://$ACR_NAME.azurecr.io/v2/helm/ingress-nginx/tags/list\": \nGET \"https://$ACR_NAME.azurecr.io/oauth2/token?scope=repository%3Ahelm%2Fingress-nginx\n  %3Ametadata_read%2Cpull&amp;service=$ACR_NAME.azurecr.io\": \nunexpected status code 401: unauthorized: authentication required,\nvisit https://aka.ms/acr/authorization for more information.\n</code></pre> Solution: update to the latest helm version?</p>"},{"location":"DevOps/Helm/Repo/#private-repo-with-self-signed-cert","title":"private repo with self-signed cert","text":"<p>not usable: https://github.com/helm/helm/issues/8868</p>"},{"location":"DevOps/Helm/Repo/#disable-ssl-cert","title":"disable ssl cert","text":"<pre><code>helm repo add jh https://jupyterhub.github.io/charts --insecure-skip-tls-verify\n</code></pre>"},{"location":"DevOps/Helm/Repo/#use-ca-file","title":"use <code>--ca-file</code>","text":"<pre><code>helm repo add jh https://jupyterhub.github.io/charts --ca-file /path/to/ca-file\n</code></pre>"},{"location":"DevOps/Helm/Secret/","title":"Secret","text":""},{"location":"DevOps/Helm/Secret/#secret","title":"Secret","text":"<p>see also: Docker Container: SecretProviderClass</p> <p>https://kubernetes.io/docs/concepts/configuration/secret/#overview-of-secrets</p> <p>https://stackoverflow.com/questions/69260696/how-to-pass-azure-key-vault-secrets-to-kubernetes-pod-using-file-in-helm-charts</p> <p>A secret can be used with a pod in three ways: - As files in a volume mounted on one or more of its containers - As container environment variable - By the kubelet when pulling images for the Pod</p>"},{"location":"DevOps/Helm/Secret/#map-key-vault-secret-to-traefic-tlsstore","title":"map Key Vault Secret to traefic TLSStore","text":"<p>https://blog.baeke.info/2020/12/07/azure-key-vault-provider-for-secrets-store-csi-driver/ - settings in the <code>parameters</code> section are actually sufficient to mount the secret/certificate in a pod - With the <code>secretObjects</code> section, we can also ask for the creation of regular Kubernetes secrets. - Note: though the objectType is <code>secret</code> the certificate is uploaded into the Azure Key Vault <code>Certificates</code> not <code>Secrets</code></p> <pre><code>apiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: dev-cert\n  namespace: \"{{ .Values.namespace }}\"\nspec:\n  provider: azure\n  secretObjects:\n    - secretName: dev-cert\n      type: kubernetes.io/tls\n      data:\n        - objectName: dev\n          key: tls.key\n        - objectName: dev\n          key: tls.crt\n  parameters:\n    tenantId: \"{{ .Values.tenantId }}\"\n    keyvaultName: \"{{ .Values.keyVaultName }}\"\n    usePodIdentity: \"true\"\n    objects: |\n      array:\n        - |\n          objectName: dev\n          objectType: secret\n          objectVersion: \"\"\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: TLSStore\nmetadata:\n  name: default\n  namespace: \"{{ .Values.namespace }}\"\nspec:\n  defaultCertificate:\n    secretName: dev-cert\n</code></pre>"},{"location":"DevOps/Helm/Secret/#mounted-volume-file","title":"mounted volume file","text":"<ul> <li>Mount files to a path via Kubernetes Secret</li> <li>Mount Azure Key Vault Secrets to another path via <code>SecretProviderClass</code></li> </ul> <p>secret.yaml <pre><code>apiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: \"{{ .Chart.Name }}-kv-secret\"\n  namespace: \"{{ .Values.namespace }}\"\nspec:\n  provider: azure\n  parameters:\n    usePodIdentity: \"true\"\n    keyvaultName: \"{{ .Values.keyVaultName }}\"\n    objects:  |\n      array:\n        - |\n          objectName: secret--a\n          objectType: secret\n          objectAlias: secret_a.json\n          objectVersion: \"\"\n        - |\n          objectName: secret--b\n          objectType: secret\n          objectAlias: secret_b.json\n          objectVersion: \"\"\n    tenantId: \"{{ .Values.tenantId }}\"\n</code></pre></p> <p>deployment.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name:  \"{{ .Chart.Name }}\"\n  namespace: \"{{ .Values.namespace }}\"\nspec:\n  replicas: {{ .Values.replicaCount }}\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  selector:\n    matchLabels:\n      app: \"{{ .Chart.Name }}\"\n  template:\n    metadata:\n      labels:\n        app: \"{{ .Chart.Name }}\"\n        aadpodidbinding: \"{{ .Values.azureIdentity.name }}\"\n    spec:\n      enableServiceLinks: False\n      containers:\n        - name: \"{{ .Chart.Name }}\"\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n          imagePullPolicy: \"{{ .Values.image.pullPolicy }}\"\n          ports:\n            - containerPort: 5000\n          resources:\n            requests:\n              cpu: \"0.25\"\n              memory: 2Gi              \n            limits:\n              cpu: \"0.5\"\n              memory: 4Gi              \n          volumeMounts:\n            - name: my-file-mount\n              mountPath: /opt/app-path/\n              readOnly: true\n            - name: my-azure-kv-secrets-mount\n              mountPath: \"/home/&lt;user&gt;/my-secret/\"\n              readOnly: true\n          env:\n            - name: MY_DEV_API_URL\n              value: https://example.com/api\n      volumes:\n        - name: my-file-mount\n          secret:\n            secretName: my-file-secret\n        - name: my-azure-kv-secrets-mount\n          csi:\n            driver: secrets-store.csi.k8s.io\n            readOnly: true\n            volumeAttributes:\n              secretProviderClass: \"{{ .Chart.Name }}-kv-secret\"\n</code></pre></p>"},{"location":"DevOps/Helm/Var/","title":"Variable","text":""},{"location":"DevOps/Helm/Var/#variable-name","title":"variable name","text":"<p>Hyphen in helm chart var/subchart names - Hyphen <code>-</code> is not allowed in variable and subchart names and <code>index</code> is a workaround - https://github.com/helm/helm/issues/2192 - https://stackoverflow.com/questions/63853679/helm-templating-doesnt-let-me-use-dash-in-names - example <pre><code>mysub-chart:\n  servicename: mysubchart-service\n# solution\nvalue: {{ index .Values \"mysub-chart\" \"servicename\" }}\n\n# another example\nx.y-def: '{{ index .Values.x \"y-def\" }}'\n</code></pre></p>"},{"location":"DevOps/Helm/App/Jupyterhub/","title":"Jupyterhub","text":"<p>https://jupyterhub.github.io/helm-chart/</p> <p>Create a jupyterhub in the kubernetes cluster.</p>"},{"location":"DevOps/Helm/App/Jupyterhub/#dedicated-node-pool-for-users-taints-and-tolerations","title":"dedicated node pool for users (taints and tolerations)","text":"<p>https://zero-to-jupyterhub.readthedocs.io/en/latest/administrator/optimization.html</p>"},{"location":"DevOps/Helm/App/Jupyterhub/#helm-chart-setup","title":"helm chart setup","text":"<p>https://github.com/jupyterhub/zero-to-jupyterhub-k8s/blob/HEAD/jupyterhub/values.yaml</p>"},{"location":"DevOps/Helm/App/Jupyterhub/#persistent-storage","title":"persistent storage","text":"<ul> <li>https://kienmn97.medium.com/persistent-storage-in-jupyterhub-on-kubernetes-cluster-running-on-minikube-4b469bdb1b86</li> <li>https://docs.microsoft.com/en-us/azure/aks/azure-files-volume</li> <li>https://docs.openshift.com/container-platform/3.10/install_config/persistent_storage/persistent_storage_azure_file <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: jhub-storage\n  namespace: jhub\nspec:\n  storageClassName: \"standard\" # it will be default storage class if unspecified.\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre></li> </ul> <p>We then need to mount the volume to a path in the hub pod where the database is stored, in <code>volumes</code> and <code>volumeMounts</code> sections.</p>"},{"location":"DevOps/Helm/App/Jupyterhub/#failed-to-list-v1beta1poddisruptionbudget-the-server-could-not-find-the-requested-resource","title":"failed to list *v1beta1.PodDisruptionBudget: the server could not find the requested resource","text":"<p>https://discourse.jupyter.org/t/readiness-probe-fails-on-docker-kubernetes-deploy/17517/4 <pre><code>coming from a newer k8s version and a depreciation of v1beta1\npolicy/v1beta1 PodDisruptionBudget is deprecated in aks v1.21+, unavailable in aks v1.25+; use policy/v1 PodDisruptionBudget\n</code></pre> This typically occurs when there's a problem with the Kubernetes <code>API server</code> or when the requested resource version is not available or supported. check the api server status <pre><code>kubectl get componentstatuses\n</code></pre></p>"},{"location":"DevOps/Helm/App/Jupyterhub/#spawn-timeout-and-solution","title":"spawn timeout and solution","text":"<p>https://blog.51cto.com/u_15946369/6035967</p>"},{"location":"DevOps/Helm/App/Jupyterhub/#hook-iamge-puller-imagepullbackoff","title":"hook-iamge-puller ImagePullBackOff","text":"<p><pre><code>connection reset by peer\n</code></pre> https://www.modb.pro/db/179200</p>"},{"location":"DevOps/Helm/App/Jupyterhub/#authenticate-via-rbac","title":"authenticate via rbac","text":"<p>403 Forbidden Error when using Azure AD for authentication: https://discourse.jupyter.org/t/403-forbidden-error-when-using-azure-ad-for-authentication/16737/7</p> <p>When set authentication via rbac, after the app registration secret update,  the secret should be updated in the Kubernetes Secret as the Secret has been created by terraform. </p>"},{"location":"DevOps/Helm/App/Jupyterhub/#user-user-name-not-allowed-error","title":"<code>User 'user-name' not allowed</code> error","text":"<p>https://discourse.jupyter.org/t/403-forbidden-error-when-using-azure-ad-for-authentication/16737/18</p> <p>Solution: <code>c.AzureAdOAuthenticator.allow_all = True</code> Allow all authenticated users to login.</p>"},{"location":"DevOps/Helm/App/Jupyterhub_yaml/","title":"Jupyterhub values.yaml file","text":""},{"location":"DevOps/Helm/App/Jupyterhub_yaml/#singleuser","title":"singleuser","text":"<pre><code>singleuser:\n  uid: 1000\n  fsGid: 100\n  image:\n    name: jupyterhub/k8s-singleuser-sample\n    tag: \"1.0.0\"\n    pullPolicy:\n    pullSecrets: []\n  cpu:\n    guarantee: 0.25 #must be number\n    limit: 1.5      #must be number\n  memory:\n    guarantee: 4G\n    limit: 16G\n  cloudMetadata:\n    # block set to true will append a privileged initContainer using the\n    # iptables to block the sensitive metadata server at the provided ip.\n    blockWithIptables: true\n    ip: 169.254.169.254\n  extraLabels:\n    hub.jupyter.org/network-access-hub: \"true\"\n  storage:\n    type: static\n    static:\n      pvcName: my-pvc\n      subPath: \"{username}\"\n    homeMountPath: /home/user\n</code></pre>"},{"location":"DevOps/Helm/App/Jupyterhub_yaml/#hub","title":"hub","text":"<pre><code>hub:\n  redirectToServer: false\n  allowNamedServers: false\n  activeServerLimit: 1\n</code></pre>"},{"location":"DevOps/Helm/App/Jupyterhub_yaml/#hubimage","title":"hub.image","text":"<pre><code>hub:\n  image:\n    name: jupyterhub/k8s-hub\n    tag: \"3.1.1\"\n    pullPolicy:\n    pullSecrets: []\n</code></pre>"},{"location":"DevOps/Helm/App/Jupyterhub_yaml/#hubconfig","title":"hub.config","text":"<pre><code>hub:\n  config:\n    JupyterHub:\n      admin_access: true\n      authenticator_class: dummy #azuread\n    Authenticator:\n      auto_login: true\n      admin_users:\n        - admin-user-name\n</code></pre>"},{"location":"DevOps/Helm/App/Jupyterhub_yaml/#hubdb","title":"hub.db","text":"<pre><code>hub:\n  db:\n    type: sqlite-pvc\n</code></pre>"},{"location":"DevOps/Helm/App/Jupyterhub_yaml/#hubextraenv","title":"hub.extraEnv","text":"<p>Set envvar from kubernetes secret. - https://z2jh.jupyter.org/en/latest/resources/reference.html#hub-extraenv - https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues/1103 - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#envvar-v1-core <pre><code>hub:\n  extraEnv:\n    JHUB_ACCESS_CLIENT_ID\n      valueFrom:\n        secretKeyRef:\n          name: jupyter-user-secret\n          key: client_id\n    JHUB_ACCESS_CLIENT_SECRET\n      valueFrom:\n        secretKeyRef:\n          name: jupyter-user-secret\n          key: client_secret\n</code></pre></p>"},{"location":"DevOps/Helm/App/Jupyterhub_yaml/#hubextraconfig","title":"hub.extraConfig","text":"<p>https://z2jh.jupyter.org/en/latest/resources/reference.html#schema-hub-extraconfig</p> <p>Arbitrary extra python based configuration that should be in jupyterhub_config.py. <pre><code>hub:\n  extraFiles:\n    my_config:\n      mountPath: /usr/local/etc/jupyterhub/jupyterhub_config.d/my_config.py\n      stringData: |\n        import os\n        c.KubeSpawner.http_timeout = 60\n        c.AzureAdOAuthenticator.allow_all = True\n        c.AzureAdOAuthenticator.client_secret = os.environ['JHUB_ACCESS_CLIENT_SECRET']\n</code></pre></p>"},{"location":"DevOps/Helm/App/Jupyterhub_yaml/#hubextrafiles","title":"hub.extraFiles","text":"<p>To embed Python code inside a YAML file, consider using <code>hub.extraFiles</code> and  mounting a file to <code>/usr/local/etc/jupyterhub/jupyterhub_config.d</code> in order to load your extra configuration logic.</p> <p>The <code>hub.extraFiles</code> allows you to inject additional files into the JupyterHub Hub pod.  These files can be used to customize the look and feel of the Hub, add new features, or provide additional functionality.</p> <p>Here are some examples of how you can use the hub.extraFiles configuration option: - Inject custom CSS styling to override the default styling of the Hub. - Inject custom JavaScript code to add new features or functionality to the Hub. - Inject configuration files for custom authentication providers or other JupyterHub extensions. - Inject static files, such as images or fonts, to be used in the Hub's user interface.</p>"},{"location":"DevOps/Helm/App/Jupyterhub_yaml/#scheduling","title":"scheduling","text":"<pre><code>scheduling:\n  userScheduler:\n    enabled: false\n  userPlaceholder:\n    enabled: true\n    image:\n      name: registry.k8s.io/pause\n      tag: \"3.9\"\n  userPods:\n    nodeAffinity:\n      matchNodePurpose: prefer #require\n</code></pre>"},{"location":"DevOps/Helm/App/Jupyterhub_yaml/#userscheduler","title":"userScheduler","text":"<p>If you have users starting new servers while the total number of active users is decreasing, how will you free up a node so it can be scaled down? <code>user scheduler</code> will schedule new user pods to the most utilised node.</p> <p>Only activate the user scheduler if you have an <code>autoscaling</code> node pool.</p>"},{"location":"DevOps/Helm/App/Jupyterhub_yaml/#userplaceholder","title":"userPlaceholder","text":"<p>The UserPlaceholders is a powerful feature that can improve the performance and scalability of JupyterHub.</p> <p>The purpose of userPlaceholder in JupyterHub is to pre-allocate resources for users before they need them. This can improve the startup time of Jupyter sessions and reduce the load on the Kubernetes cluster.</p> <p>UserPlaceholders are small, lightweight pods that are deployed to the Kubernetes cluster. They are configured to consume the same resources as a typical Jupyter session. When a user requests a Jupyter session, JupyterHub will schedule the session on a userPlaceholder pod. This allows the Jupyter session to start immediately, without having to wait for a new pod to be deployed.</p> <p>UserPlaceholders are also useful for scaling JupyterHub. When the cluster is idle, JupyterHub can scale down the number of userPlaceholder pods. This saves resources and reduces costs. When new users request Jupyter sessions, JupyterHub can quickly scale up the number of userPlaceholder pods to meet the demand.</p> <p>Here are some of the benefits of using userPlaceholders in JupyterHub: - Reduced startup time: Jupyter sessions can start immediately, without having to wait for a new pod to be deployed. - Reduced load on the Kubernetes cluster: UserPlaceholders are small and lightweight pods, so they do not consume as many resources as a typical Jupyter session. - Improved scalability: JupyterHub can quickly scale up or down the number of userPlaceholder pods to meet the demand.</p>"},{"location":"DevOps/Helm/App/Jupyterhub_yaml/#prepuller","title":"PrePuller","text":"<p>https://test-zerotojh.readthedocs.io/en/edit-awseks/optimization.html <pre><code>prePuller:\n  hook:\n    enabled: false\n  continuous:\n    enabled: false\n  pause:\n    image:\n      name: /registry.k8s.io/pause\n      tag: \"3.9\"\n</code></pre></p> <p>Pre-pulling the images on all the nodes can cut this wait time to a few seconds. - hook: user\u2019s container image is pulled on all nodes whenever a helm install or helm upgrade is performed - continues: the user\u2019s container image will be pre-pulled when a new node is added</p>"},{"location":"DevOps/Helm/App/Kafka/","title":"Kafka","text":"<p>https://piotrminkowski.com/2022/06/28/manage-kubernetes-cluster-with-terraform-and-argo-cd/</p>"},{"location":"DevOps/Helm/App/Kafka/#statefulset","title":"statefulset","text":"<p>https://github.com/LinkedInLearning/kafka-on-kubernetes-2899691/blob/02_04e/statefulset-kafka.yaml</p>"},{"location":"DevOps/Helm/App/Kafka/#multi-broker-clsuter","title":"multi-broker clsuter","text":"<p>https://github.com/LinkedInLearning/kafka-on-kubernetes-2899691/tree/03_04</p>"},{"location":"DevOps/Helm/App/Kafka/#zookeeper","title":"zookeeper","text":"<p>https://github.com/LinkedInLearning/kafka-on-kubernetes-2899691/tree/03_05e</p>"},{"location":"DevOps/Helm/App/Kafka/#health-probles","title":"health probles","text":"<p>https://github.com/LinkedInLearning/kafka-on-kubernetes-2899691/tree/04_01b</p>"},{"location":"DevOps/Helm/App/Kafka/#readness-probes","title":"readness probes","text":"<p>https://github.com/LinkedInLearning/kafka-on-kubernetes-2899691/tree/04_02e</p>"},{"location":"DevOps/Helm/App/Kured/","title":"Kured","text":"<p>KUbernetes REboot Daemon: https://github.com/weaveworks/kured</p> <p>https://learn.microsoft.com/en-us/azure/aks/node-updates-kured</p> <p>To protect clusters, security updates are automatically applied to Linux nodes in AKS. Some of these updates require a node reboot to complete the process. </p> <p>With kured, a DaemonSet is deployed that runs a pod on each Linux node in the cluster. These pods in the DaemonSet watch for existence of the /var/run/reboot-required file, and then initiate a process to reboot the nodes.</p>"},{"location":"DevOps/Helm/App/Kured/#cli","title":"cli","text":"<pre><code>helm install kured stable/kured --namespace kured --set extraArgs.reboot-days=\"Mon\\,Tue\"\n</code></pre>"},{"location":"DevOps/Helm/App/Kured/#check-logs","title":"check logs","text":"<p><pre><code>kubectl -n &lt;namespace&gt; logs &lt;kured-pod-name&gt;\n</code></pre> Result <pre><code>\"Reboot schedule: SunMonTueWedThuFriSat between 00:00 and 23:59 UTC\"\n</code></pre></p>"},{"location":"DevOps/Helm/App/Kured/#reboot-schedule","title":"reboot schedule","text":"<ul> <li><code>--start-time</code>: string, schedule reboot only after this time of day (default \"0:00\")</li> <li><code>--end-time</code>: string, schedule reboot only before this time of day (default \"23:59:59\")</li> <li><code>--reboot-days</code>: strings, schedule reboot on these days (default [su,mo,tu,we,th,fr,sa] <pre><code>--reboot-days=mon,tue,wed,thu,fri\n--start-time=9am\n--end-time=5pm\n--time-zone=America/Los_Angeles\n</code></pre> Times can be formatted in numerous ways, including <code>5pm</code>, <code>5:00pm</code>, <code>17:00</code>, and <code>17</code>.</li> </ul>"},{"location":"DevOps/Helm/App/Kured/#configuration","title":"configuration","text":"<p>https://kured.dev/docs/configuration/</p> <p>In values.yaml <pre><code>extraArgs:\n  start-time: \"0:00\"\n  end-time: \"23:59\"\n  reboot-days: \"Mon,Tue\"\n</code></pre> becomes <pre><code>/usr/bin/kured ... --start-time=\"0:00\" --end_time=\"23:59\" --reboot-days=\"Mon\\,Tue\"\n</code></pre></p>"},{"location":"DevOps/Helm/App/Kured/#error-daemonsetapps-kured-is-invalid-specselector-field-is-immutable","title":"error: DaemonSet.apps \"kured\" is invalid: spec.selector: ... field is immutable","text":"<p>https://github.com/kubereboot/kured/issues/610</p> <p>Solution: uninstall the release and install it again - cannot simply upgrade</p>"},{"location":"DevOps/Helm/App/Redis/","title":"Redis","text":"<p>https://github.com/bitnami/charts/tree/main/bitnami/redis</p> <p>https://ep.gnt.md/index.php/how-to-deploy-redis-in-kubernetes-with-helm-chart/</p>"},{"location":"DevOps/Helm/App/Redis/#helm-chart","title":"helm chart","text":"<p>https://github.com/bitnami/charts/tree/main/bitnami/redis/#installing-the-chart</p> <p>chart url: https://charts.bitnami.com/bitnami</p>"},{"location":"DevOps/Helm/App/Redis/#deploy","title":"deploy","text":"<ul> <li>deployment: used to deploy the redis pods</li> <li>service: let other apps access the redis pod via the service name</li> <li>secret: some settings and access control</li> </ul> <p>https://github.com/yifenggit/helm-redis/tree/main/templates - deployment - service</p> <p>https://github.com/sherlock28/helmcharts/tree/main/redis/templates - deployment - service - secret</p> <p>https://github.com/spy86/redis/tree/main/templates - deployment - service - serviceaccount - hpa</p> <p>https://github.com/bytedaring/helm-redis/tree/main/templates - master-statefulset - master-svc - role - rolebinding - serviceaccount - secret</p> <p>https://github.com/kubees/redis-helm-chart/tree/main/redis-helm-chart/templates - azure-sc - pv - pvc - service monitor - storage class</p>"},{"location":"DevOps/Helm/App/Seq/","title":"Seq","text":"<p>Seq log server</p> <p>https://github.com/datalust/helm.datalust.co/blob/main/charts/seq/Chart.yaml</p> <p>https://doc.traefik.io/traefik/routing/providers/kubernetes-crd/#kind-ingressroute</p>"},{"location":"DevOps/Helm/App/Seq/#filter-by-timestamp","title":"filter by timestamp","text":"<pre><code>@Timestamp &gt;= DateTime('2024-01-16T12:55:00Z') and @Timestamp &lt;= DateTime('2024-01-16T12:56:00Z')\n</code></pre>"},{"location":"DevOps/Helm/App/Traefik/","title":"Traefik","text":"<p>https://doc.traefik.io/traefik/</p> <p>https://traefik.io/blog/install-and-configure-traefik-with-helm/</p> <p>Traefik is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease.</p> <p>An open-source Edge Router receives requests on behalf of your system and finds out which components are responsible for handling them.</p> <p>Traefik is a popular Kubernetes ingress controller that acts as a reverse proxy and load balancer for routing external traffic to services running within a cluster.</p>"},{"location":"DevOps/Helm/App/Traefik/#routing","title":"routing","text":"<p>https://doc.traefik.io/traefik/routing/providers/kubernetes-crd/</p>"},{"location":"DevOps/Helm/App/Traefik/#tlsstore","title":"TLSStore","text":"<p>https://doc.traefik.io/traefik/routing/providers/kubernetes-crd/#kind-tlsstore - TLSStore is the CRD implementation of a Traefik \"TLS Store\". - Register the TLSStore kind in the Kubernetes cluster before creating TLSStore objects.</p>"},{"location":"DevOps/Helm/App/Traefik/#tsl","title":"tsl","text":"<p>https://doc.traefik.io/traefik/https/tls/</p>"},{"location":"DevOps/Helm/App/Traefik/#allowcrossnamespace","title":"allowCrossNamespace","text":"<p>Allow IngressRoute to reference resources in namespaces other than theirs.</p> <p>https://doc.traefik.io/traefik/providers/kubernetes-crd/ <pre><code>providers:\n  kubernetesCRD:\n    allowCrossNamespace: true\n</code></pre></p> <p>The default value is false and will get this error when the <code>auth</code> MiddleWare is in the traefik namespace <pre><code>{\"ingress\":\"dev-dashboard\",\"level\":\"error\",\n\"msg\":\"Failed to create middleware keys: middleware traefik/auth is not in the IngressRoute namespace dev\",\n\"namespace\":\"dev\",\"providerName\":\"kubernetescrd\",\"time\":\"2023-06-22T22:37:12Z\"}\n</code></pre></p>"},{"location":"DevOps/Helm/App/Traefik/#self-signed-certificate","title":"self-signed certificate","text":"<p>https://community.traefik.io/t/internal-server-error-when-proxing-to-https-with-self-signed-cert/11087/5</p> <p>When proxying to a server that uses a self-signed certificate, we need to configure a <code>serversTransport</code> to tell Traefik how to handle the certificate. </p>"},{"location":"DevOps/Helm/App/Traefik/#ingressroute","title":"IngressRoute","text":"<p>IngressRoutes expose Kubernetes applications externally to the public. Ingresses associate with Pods through references to Services. <pre><code># https://doc.traefik.io/traefik/routing/providers/kubernetes-crd/#kind-ingressroute\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: \"my-api\"\n  namespace: \"dev\"\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - kind: Rule\n      match: \"Host(`test.example.com`) &amp;&amp; PathPrefix(`/dev/api`)\"\n      middlewares:\n        - name: \"my-api-stripprefix\"\n      services:\n        - kind: Service\n          name: \"my-api\"\n          namespace: \"dev\"\n          passHostHeader: true\n          port: 5000\n  tls: {}\n</code></pre></p>"},{"location":"DevOps/Helm/App/Traefik/#middleware","title":"MiddleWare","text":"<p>https://doc.traefik.io/traefik/middlewares/overview/</p> <p>The middleware, attached to the router, provides a means of tweaking the requests before they are sent to the service (or before the answer from the service are sent to the client).</p> <p>In Traefik, there are several available middleware, some can modify the request, the headers, some are in charge of redirections, some add authentication, and so on.</p> <p>Use a <code>StripPrefix</code> middleware if your backend listens on the root path (/) but should be exposed on a specific prefix.</p> <pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: \"my-api-stripprefix\"\n  namespace: \"dev\"\nspec:\n  stripPrefix:\n    prefixes:\n      - \"/dev/api\"\n</code></pre>"},{"location":"DevOps/Helm/App/Traefik/#ingress","title":"Ingress","text":"<p>The previous IngressRoute and MiddleWare is equavalent to  <pre><code>apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: my-api\n  namespace: dev\n  annotations:\n    kubernetes.io/ingress.class: \"traefik\"\n    traefik.ingress.kubernetes.io/rule-type: \"PathPrefixStrip\"\nspec:\n  rules:\n    - host: test.example.com\n      http:\n        paths:\n        - path: /dev/api\n          backend:\n            serviceName: my-api\n            servicePort: http\n  tls:\n    - secretName: test.example.com-tls-cert\n</code></pre></p>"},{"location":"DevOps/Helm/App/aad_pod_identity/","title":"aad-pod-identity","text":"<p>https://github.com/Azure/aad-pod-identity/tree/master/charts/aad-pod-identity</p> <p><code>aad-pod-identity</code> enables Kubernetes applications to access cloud resources securely with <code>Azure Active Directory</code> (AAD).</p>"},{"location":"DevOps/Helm/Redis/Redis/","title":"Redis","text":""},{"location":"DevOps/Helm/Redis/Redis/#python-app","title":"Python app","text":"<pre><code>import redis\n\n# Connect to Redis using the service name and port\nredis_client = redis.StrictRedis(host='redis-service', port=6379, decode_responses=True)\n\n# Example: set and get a key\nredis_client.set('example_key', 'example_value')\nvalue = redis_client.get('example_key')\nprint(f'Redis Value: {value}')\n</code></pre>"},{"location":"DevOps/Helm/Redis/Redis/#service","title":"service","text":"<p>The service selects pods with the label <code>app: standalone-redis</code></p>"},{"location":"DevOps/Helm/Redis/Redis/#redis_1","title":"redis","text":"<p>set memory limit <pre><code>containers:\n  - name: redis\n    image: public.ecr.aws/docker/library/redis:7.0.4-alpine\n    ports:\n      - containerPort: 6379\n    command: [\"redis-server\"]\n    args: [\"--save\", \"\", \"--appendonly\", \"no\", \"--maxmemory\", \"8G\"]\n</code></pre></p>"},{"location":"DevOps/Helm/Traefik/ApiAuthorization/","title":"api-authorization","text":""},{"location":"DevOps/Helm/Traefik/ApiAuthorization/#concept","title":"concept","text":"<p>https://medium.com/etermax-technology/api-authorization-with-kubernetes-traefik-and-open-policy-agent-23647fc384a1</p>"},{"location":"DevOps/Helm/Traefik/ForwardAuth/","title":"Forward auth","text":"<p>https://doc.traefik.io/traefik/middlewares/http/forwardauth/</p>"},{"location":"DevOps/Helm/Traefik/ForwardAuth/#azure-ad-authentication-proxy","title":"Azure AD authentication proxy","text":"<p>https://msazure.club/using-traefik-ingress-with-cert-manager-and-aad-authentication/</p> <p>To integrate the application with Azure AD authentication, we need to register an application in Azure AD, to do that - Azure Active Directory - App Registrations - Endpoints - Application ID - Certificates &amp; secrets (OAuth client secret)</p>"},{"location":"DevOps/Helm/Traefik/Middleware/","title":"Middleware","text":""},{"location":"DevOps/Helm/Traefik/Middleware/#prefix-removing","title":"prefix-removing","text":"<p><pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: my-server-stripprefix\n  namespace: test\nspec:\n  stripPrefix:\n    prefixes:\n      - \"/my-prefix\"\n</code></pre> For this middleware, if the path does start with the configured prefix (\"/my-prefix\"),  Traefik removes that prefix from the request path before forwarding it to the backend service.</p> <p>Useful when we have multiple services running behind Traefik,  - It can simplify backend service configuration by allowing the service to assume a clean path structure regardless of the actual routing prefix used by Traefik. - It can improve code maintainability if backend code expects a specific URL structure that might be different from the public-facing path.</p>"},{"location":"DevOps/Kubernetes/API/","title":"API","text":""},{"location":"DevOps/Kubernetes/API/#no-kind-kubeschedulerconfiguration-is-registered-for-version-kubeschedulerconfigk8siov1","title":"no kind \"KubeSchedulerConfiguration\" is registered for version \"kubescheduler.config.k8s.io/v1\"","text":"<p>no kind \"KubeSchedulerConfiguration\" is registered for version \"kubescheduler.config.k8s.io/v1\" in scheme \"k8s.io/kubernetes/pkg/scheduler/apis/config/scheme/scheme.go:30\"</p> <p>Possible reasons: - not use the correct version of the KubeSchedulerConfiguration API.   Check the cluster supported KubeSchedulerConfiguration API: <code>kubectl get apiservices -o jsonpath='{.items[?(@.metadata.name==\"kubescheduler.config.k8s.io\")].apiVersion}'</code> - the custom KubeSchedulerConfiguration file is not registered with the Kubernetes scheduler.   To register a custom KubeSchedulerConfiguration file called <code>kube-scheduler.yaml</code>: <code>kube-scheduler --config=kube-scheduler.yaml</code></p> <p>If the command does not show any supported API, most likely - using an older version of Kubernetes that does not include the KubeSchedulerConfiguration API. - deleted the KubeSchedulerConfiguration API from your Kubernetes cluster. - there is a problem with your Kubernetes cluster configuration.</p> <p>Trouble shooting - Check the version of Kubernetes <code>kubectl version</code> - Check the Kubernetes cluster configuration <code>kubectl config view</code>   Look for the <code>kubescheduler.config.k8s.io</code> API in the apiservices section. If the API is not listed, you will need to add it. - Restart the Kubernetes scheduler service <code>kubectl rollout restart deployment kube-scheduler -n kube-system</code></p>"},{"location":"DevOps/Kubernetes/AssignPods/","title":"Assign Pods to Nodes","text":"<p>https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/</p>"},{"location":"DevOps/Kubernetes/AssignPods/#nodeselector-field-matching-against-node-labels","title":"<code>nodeSelector</code> field matching against node <code>labels</code>","text":"<p>Pod <code>nodeSelector</code> <pre><code>nodeSelector:\n    disktype: ssd\n</code></pre> Node <code>label</code> <pre><code>node_labels {\n    \"disktype\" = \"ssd\"\n}\n</code></pre></p>"},{"location":"DevOps/Kubernetes/AssignPods/#affinity-and-anti-affinity","title":"<code>Affinity</code> and <code>anti-affinity</code>","text":"<ul> <li>nodeName field</li> <li>Pod topology spread constraints</li> <li>Node labels</li> </ul>"},{"location":"DevOps/Kubernetes/Autoscaler/","title":"Kube Autoscaler","text":"<p>https://www.replex.io/blog/kubernetes-in-production-best-practices-for-cluster-autoscaler-hpa-and-vpa</p>"},{"location":"DevOps/Kubernetes/Autoscaler/#cluster-autoscaler-status","title":"cluster-autoscaler status","text":"<p>Basic info - not really useful <pre><code>&gt;&gt; kubectl get configmap -n kube-system cluster-autoscaler-status\nNAME                        DATA   AGE\ncluster-autoscaler-status   1      37h\n\nkubectl get cm cluster-autoscaler-status -n kube-system\nkubectl get cm cluster-autoscaler-user-values -n kube-system\n</code></pre> All info <pre><code>kubectl describe configmap --namespace kube-system cluster-autoscaler-status\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Autoscaler/#cluster-austoscaler-ca","title":"cluster austoscaler (CA)","text":"<p>CA increases or decreases the size of a Kubernetes cluster (by adding or removing nodes), based on the presence of pending pods and node utilization metrics. - Adds nodes to a cluster whenever it detects pending pods that could not be scheduled due to resource shortages - Removes nodes from a cluster, whenever the utilization of a node falls below a certain threshold defined by the cluster administrator</p> <p>Best practice: - Ensure cluster nodes have the same capacity - Ensure every pod has resource requests defined   - Utilization is calculated as the sum of requested resources divided by the capacity.   - https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-the-parameters-to-ca</p>"},{"location":"DevOps/Kubernetes/Autoscaler/#horizontal-pod-autoscaler-hpa","title":"horizontal pod autoscaler (HPA)","text":"<p>https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale</p> <p>Scaling formula: https://github.com/kubernetes/kubernetes/issues/78761 <pre><code>desiredMetricValue = ceil[(currentReplicas * currentMetricValue) / desiredReplicas]\n</code></pre></p> <p>Kubernetes uses the horizontal pod autoscaler (HPA) to monitor the resource demand and automatically scale the number of replicas.</p> <pre><code>kubectl autoscale deployment &lt;hpa-name&gt; \\\n  --cpu-percent=50 --min=3 --max=10            #create hpa\nkubectl create -f ./hpa.yaml                   #create hpa from yaml\nkubectl get hpa                                #check hpa\nkubectl get hpa &lt;hpa-name&gt; --watch             #watch hpa\nkubectl get deployment &lt;deployment-name&gt;       #deployment\nkubectl describe hpa &lt;hpa-name&gt; -n &lt;namespace&gt; #hpa conditions\nkubectl delete hpa &lt;hpa-name&gt; -n &lt;namespace&gt;   #delete hpa\nkubectl scale deployment &lt;deployment-name&gt; \\\n  --replicas=&lt;replica-count&gt; -n &lt;namespace&gt;    #scale replicasets\n</code></pre> <p>HPA yaml file: <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: &lt;hpa-name&gt;\n  namespace: &lt;deployment-namespace&gt;\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: &lt;deployment-name&gt;\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: Utilization\n          averageUtilization: 80\n    - type: Resource\n      resource:\n        name: cpu\n        target:\n          type: AverageValue\n          averageValue: 1000m\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n        - type: Pods\n          value: 4\n          periodSeconds: 60\n        - type: Percent\n          value: 10\n          periodSeconds: 60\n      selectPolicy: Max\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Autoscaler/#vertical-pod-autoscaler-vpa","title":"vertical pod autoscaler (VPA)","text":"<p>https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler</p> <p>VPA will set the requests automatically based on usage and thus allow proper scheduling onto nodes so that appropriate resource amount is available for each pod. It will also maintain ratios between limits and requests that were specified in initial containers configuration.</p>"},{"location":"DevOps/Kubernetes/Autoscaler/#scale-rules","title":"Scale rules","text":"<p>https://docs.giantswarm.io/advanced/cluster-autoscaler/</p>"},{"location":"DevOps/Kubernetes/CPU/","title":"CPU","text":"<p>https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/</p> <p>Pod scheduling is based on requests. A Pod is scheduled to run on a Node only if the Node has enough CPU resources available to satisfy the Pod CPU request.</p>"},{"location":"DevOps/Kubernetes/CPU/#cpu-count","title":"cpu count","text":"<p>Inside a pod, <pre><code>grep -c ^processor /proc/cpuinfo #should agree with multiprocessing.cpu_count()\n</code></pre></p>"},{"location":"DevOps/Kubernetes/CPU/#cpu-stalls","title":"cpu stalls","text":"<p>https://www.brendangregg.com/blog/2017-05-09/cpu-utilization-is-wrong.html</p> <p>When the cpu is high, most likely the cpu stalls. Temperature trips stalling the processor???</p> <p>Stalled means the processor was not making forward progress with instructions, and usually happens because it is waiting on memory I/O.</p> <p>\"I think many people interpret high %CPU to mean that the processing unit is the bottleneck, which is wrong.\"</p>"},{"location":"DevOps/Kubernetes/CPU/#should-set-the-cpu-limits","title":"should set the cpu limits?","text":""},{"location":"DevOps/Kubernetes/Config/","title":"KubeConfig","text":""},{"location":"DevOps/Kubernetes/Config/#show-config-path","title":"show config path","text":"<p>run any kubectl command with verbose level 6+ to see the kubeconfig in use <pre><code>kubectl get node -v6\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Config/#show-config-view","title":"show config view","text":"<pre><code>kubectl config view\n</code></pre>"},{"location":"DevOps/Kubernetes/Config/#install-kubectl-on-win","title":"install kubectl on win","text":"<ul> <li>copy file to your folder: https://dl.k8s.io/release/v1.25.0/bin/windows/amd64/kubectl.exe</li> <li>add the exe path to PATH var</li> </ul>"},{"location":"DevOps/Kubernetes/Config/#set-config-envvar","title":"set config envvar","text":"<pre><code>export KUBECONFIG=\"/mnt/c/users/usr/.config/.kube/config\"\n#default\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\nexport KUBECONFIG=$HOME/.kube/config\n</code></pre>"},{"location":"DevOps/Kubernetes/Config/#modify-config-files","title":"modify config files","text":"<pre><code>kubectl config view\nkubectl config get-clusters\naz aks get-credentials -n &lt;aks-name&gt; -g &lt;resource-group-name&gt;\nkubectl config get-contexts\nkubectl config unset users.usr\nkubectl config delete-cluster &lt;cluster-name&gt;\nkubectl config delete-context &lt;cluster-name&gt;\n</code></pre>"},{"location":"DevOps/Kubernetes/Config/#merge-kubeconfig-files","title":"merge kubeconfig files","text":"<pre><code>cp ~/.kube/config ~/.kube/config.bak #make a copy\nKUBECONFIG=\"~/.kube/config:~/.kube/config2\" kubectl config view --flatten &gt; /tmp/config #merge to a new file\nmv /tmp/config ~/.kube/config #replace the config file\nrm ~/.kube/config.bak #delete backup\n</code></pre>"},{"location":"DevOps/Kubernetes/Config/#switch-cluster","title":"switch cluster","text":"<pre><code>kubectl config get-contexts #display all contexts\nkubectl config current-context #display current context\nkubectl config use-context &lt;cluster-name&gt; #set default context\n</code></pre>"},{"location":"DevOps/Kubernetes/ConfigMap/","title":"ConfigMap","text":""},{"location":"DevOps/Kubernetes/ConfigMap/#edit-configmap","title":"edit configmap","text":"<p>will show the <code>cm</code> in interactive mode <pre><code>kubectl edit cm &lt;my-configmap&gt;\n</code></pre></p>"},{"location":"DevOps/Kubernetes/DaemonSet/","title":"DaemonSet","text":"<p>In Kubernetes, a DaemonSet is a type of workload controller that ensures that a copy of a specified pod runs on every node within a cluster.  DaemonSets are primarily used for running system daemons or background tasks that need to be scheduled on every node,  such as log collectors, monitoring agents, or networking components.</p> <p>Here are some key characteristics and use cases for DaemonSets in Kubernetes: - One Pod Per Node: DaemonSets are designed to create exactly one instance (pod) of the specified workload on each node within a cluster.   This ensures that the workload is distributed across all nodes. - Node-Affinity: You can configure node affinity and anti-affinity rules to control which nodes the DaemonSet pods   should be scheduled on or avoid scheduling on. This allows you to target specific nodes or node groups for running your pods. - Replacement and Scaling: DaemonSets automatically manage the pod lifecycle.   When nodes are added or removed from the cluster, DaemonSets will create or delete pods to maintain the desired number of pods per node. - Rolling Updates: You can update the DaemonSet by changing its pod template, which will trigger rolling updates to each node.   This allows you to roll out changes to all nodes in a controlled manner. - Use Cases: Common use cases for DaemonSets include running cluster-wide services like container runtime monitoring agents   (e.g., Prometheus Node Exporter), logging agents (e.g., Fluentd), or networking components (e.g., CNI plugins).</p> <p>Here's an example of a simple DaemonSet YAML configuration: <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: my-daemonset\nspec:\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n      - name: my-container\n        image: my-image:latest\n</code></pre></p> <p>In this example: - The <code>metadata.name</code> field specifies the name of the DaemonSet. - The <code>spec.selector.matchLabels</code> field defines the labels that are used to select nodes where the pods should be scheduled. - The <code>spec.template.metadata.labels</code> field specifies labels for pods created by the DaemonSet. - The <code>spec.template.spec.containers</code> field defines the container specification for the pods.</p> <p>When you apply this configuration, Kubernetes will automatically schedule one pod of <code>my-container</code> on each node in the cluster that matches the selector.</p> <p>DaemonSets are a powerful tool for ensuring that certain pods run on every node in a Kubernetes cluster,  which can be crucial for managing infrastructure and system-level services.</p>"},{"location":"DevOps/Kubernetes/Deployment/","title":"Deployment","text":""},{"location":"DevOps/Kubernetes/Deployment/#get-pod-associated-with-a-deployment","title":"get pod associated with a deployment","text":"<pre><code>kubectl get pods --show-labels\n# single label\nkubectl get pods -l=app=http-svc\nkubectl get pods --selector=app=http-svc\n# multiple labels\nkubectl get pods --selector key1=value1,key2=value2\n</code></pre>"},{"location":"DevOps/Kubernetes/Deployment/#change-replicas","title":"Change Replicas","text":"<pre><code>kubectl scale --replicas=3 deployment/demo-deployment\n</code></pre>"},{"location":"DevOps/Kubernetes/Deployment/#update-the-yaml-directly","title":"update the yaml directly","text":"<pre><code>kubectl edit deployment &lt;deployment_name&gt; -n &lt;namespace&gt; # then save the changes and exit the editor\n</code></pre>"},{"location":"DevOps/Kubernetes/Deployment/#get-and-update-the-yaml","title":"get and update the yaml","text":"<pre><code># download deployment yaml\nkubectl get deployment &lt;deployment_name&gt; -n &lt;namespace&gt; -o yaml &gt; deployment.yaml\n# make changes and apply changes\nkubectl apply -f deployment.yaml \n</code></pre>"},{"location":"DevOps/Kubernetes/Diagnosis/","title":"Kube diagnosis","text":"<p>https://thenewstack.io/living-with-kubernetes-12-commands-to-debug-your-workloads/</p> <p>https://kubernetes.io/docs/tasks/debug/debug-cluster/</p> <p>https://kubernetes.io/docs/tasks/debug/debug-application/determine-reason-pod-failure/</p>"},{"location":"DevOps/Kubernetes/Diagnosis/#run-every-2-seconds","title":"run every 2 seconds","text":"<pre><code>watch -n 2 kubectl top node\n</code></pre>"},{"location":"DevOps/Kubernetes/Diagnosis/#check","title":"check","text":"<pre><code>kubectl version\nkubectl get cm #ConfigMaps\nkubectl get nodes --no-headers | wc -l #cluster node count\nkubectl top nodes\nkubectl top pods -A\nkubectl get events\nkubectl get events -o wide\nkubectl get events | grep &lt;this-string&gt;\nkubectl get events -A --sort-by='.lastTimestamp' | sed -r 's/ {2,}/,/g' &gt; /mnt/c/tmp/aks_events.csv #fixed col to csv\nkubectl get jobs/&lt;job-name&gt;\nkubectl describe job &lt;job-name&gt;\nkubectl describe jobs/&lt;job-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Excel <code>7s</code>, <code>8m</code> or <code>5m8s</code> to seconds <pre><code>=IFERROR(LEFT(B2,FIND(\"m\",B2)-1)*60 + IF(LEN(RIGHT(B2,LEN(B2)-FIND(\"m\",B2)))=0, 0, LEFT(RIGHT(B2,LEN(B2)-FIND(\"m\",B2)), LEN(RIGHT(B2,LEN(B2)-FIND(\"m\",B2)))-1)), LEFT(B2,FIND(\"s\",B2)-1))\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Diagnosis/#get-node-events","title":"get node events","text":"<pre><code>kubectl get events --field-selector involvedObject.kind=Node,involvedObject.name=&lt;node-name&gt; --sort-by=.metadata.creationTimestamp\n</code></pre>"},{"location":"DevOps/Kubernetes/Diagnosis/#drain-a-node","title":"drain a node","text":"<p>Safely Drain a Node: https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/</p>"},{"location":"DevOps/Kubernetes/Diagnosis/#ssh-to-a-node","title":"ssh to a node","text":"<p>https://docs.microsoft.com/en-us/azure/aks/node-access</p>"},{"location":"DevOps/Kubernetes/Diagnosis/#pending-pods","title":"pending pods","text":"<p>https://www.datadoghq.com/blog/debug-kubernetes-pending-pods/ <pre><code>kubectl get pods --all-namespaces=true --field-selector=status.phase=Pending\nkubectl delete pods -A  --field-selector='status.phase=Failed' #delete failed pods\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Disk/","title":"Disk","text":""},{"location":"DevOps/Kubernetes/Disk/#kubernetes-node-events","title":"kubernetes node events","text":"<p><pre><code>Type     Reason                Age                     Message\n----     ------                ----                    -------\nWarning  FreeDiskSpaceFailed   20m (x2195 over 8d)     Failed to garbage collect required amount of images.\n                                                       Attempted to free 5852591718 bytes, but only found 1164052028 bytes eligible to free.\nWarning  FreeDiskSpaceFailed   10m                     Failed to garbage collect required amount of images.\n                                                       Attempted to free 11461842534 bytes, but only found 0 bytes eligible to free.\nWarning  EvictionThresholdMet  9m28s (x20 over 7d17h)  Attempting to reclaim ephemeral-storage\n</code></pre> This is because the node disk is full. </p> <p>How to identify which pod used most of the disk? - if we cannot ssh into the node machine - we can check the disk usage for each pod one by one <code>du -sh</code> - this might include mounted storage as well</p>"},{"location":"DevOps/Kubernetes/FieldSelector/","title":"Field Selector","text":"<p>https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/</p> <pre><code>kubectl get pods --field-selector status.phase=Running\nkubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always\nkubectl get services --all-namespaces --field-selector metadata.namespace!=default\nkubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default\n</code></pre>"},{"location":"DevOps/Kubernetes/Ingress/","title":"Ingress","text":"<p>Ingress is an API object that manages external access to services within a Kubernetes cluster. It acts as a smart router or entry point for incoming traffic and provides advanced routing and load balancing capabilities.</p> <p>Ingress typically works at the application layer (Layer 7) of the OSI model and can perform tasks such as URL-based routing, SSL termination, and request rewriting.</p> <p>It allows you to define rules and configuration for handling different types of incoming traffic and directing them to appropriate services or paths within the cluster.</p> <p>Compared to NodePort, Ingress is a more advanced and flexible way to expose services, providing application-layer routing and load balancing capabilities.</p>"},{"location":"DevOps/Kubernetes/Ingress/#ingress-controller","title":"Ingress Controller","text":"<p>Ingress relies on an <code>Ingress Controller</code>, which is a separate component responsible for implementing the Ingress rules defined in the Kubernetes API.</p> <p>Popular Ingress Controllers include - Nginx Ingress Controller, - Traefik, and - HAProxy Ingress.</p> <p>To use Ingress, you need to - have an Ingress Controller deployed in your cluster, and - the controller must be properly configured and integrated with an external load balancer or an ingress controller that supports bare metal environments.</p>"},{"location":"DevOps/Kubernetes/Ingress/#troubleshooting-ingress-and-services-traffic-flows","title":"troubleshooting ingress and services traffic flows","text":"<p>https://medium.com/@ManagedKube/kubernetes-troubleshooting-ingress-and-services-traffic-flows-547ea867b120 <pre><code>Internet &lt;-&gt; Load Balancer &lt;-&gt; Ingress &lt;-&gt; Middleware &lt;-&gt; Service &lt;-&gt; Pods\n</code></pre> - check pod - check service - check ingress - check load balancer</p>"},{"location":"DevOps/Kubernetes/Ingress/#ingress-controller-logs","title":"Ingress controller logs","text":"<p>Find the type of Ingress Controller used. Then get the log of that Ingress Controller pod. <pre><code>kubectl logs --tail=20 &lt;pod-name&gt;  #display only most recent 20 lines of logs\nkubectl logs --since=1h &lt;pod-name&gt; #display all logs written in the last hour\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Ingress/#ingress-middleware","title":"Ingress Middleware","text":"<p>In Kubernetes, an IngressRoute is a custom resource definition (CRD) introduced by the Traefik Proxy for defining ingress routes in a declarative manner. If you have multiple apps defined on the same route path in an IngressRoute, the behavior depends on how the Ingress controller (such as Traefik) and the backend applications are configured.</p> <p>If you're using the Traefik IngressRoute and you have middleware configured with StripPrefix, it will remove a specified prefix from the request URL before forwarding the request to the backend service. This is useful when your backend service expects requests without a certain prefix.</p> <pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: my-ingressroute\nspec:\n  entryPoints:\n    - web\n  routes:\n  - match: Host(`example.com`) &amp;&amp; PathPrefix(`/app1`)\n    kind: Rule\n    services:\n    - name: app1-service\n      port: 80\n    middlewares:\n    - name: strip-app1-prefix\n\n  - match: Host(`example.com`) &amp;&amp; PathPrefix(`/app2`)\n    kind: Rule\n    services:\n    - name: app2-service\n      port: 80\n    middlewares:\n    - name: strip-app2-prefix\n\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: strip-app1-prefix\nspec:\n  stripPrefix:\n    prefixes:\n      - /app1\n\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: strip-app2-prefix\nspec:\n  stripPrefix:\n    prefixes:\n      - /app2\n</code></pre>"},{"location":"DevOps/Kubernetes/Job/","title":"Job","text":""},{"location":"DevOps/Kubernetes/Job/#create-job","title":"create job","text":"<pre><code>kubectl -n &lt;namespace&gt; create job --from=cronjob/&lt;crojob-name&gt; &lt;job-name&gt; #create a job\nkubectl -n &lt;namespace&gt; get job --sort-by=.status.startTime                #show job status\n</code></pre>"},{"location":"DevOps/Kubernetes/Job/#create-job-from-yaml","title":"create job from yaml","text":"<pre><code>kubectl apply -f &lt;job-name&gt;.yaml\nkubectl get jobs -n &lt;namespace&gt;\nkubectl delete -f &lt;job-name&gt;.yaml #kubectl delete job &lt;job-name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"DevOps/Kubernetes/Job/#job-deployment","title":"job deployment","text":"<p>https://kubernetes.io/docs/concepts/workloads/controllers/job/ <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello-date\nspec:\n  schedule: \"*/30 * * * *\"\n  concurrencyPolicy: \"Forbid\"\n  failedJobsHistoryLimit: 5\n  successfulJobsHistoryLimit: 3\n  jobTemplate:\n    spec:\n      backoffLimit: 5\n      ttlSecondsAfterFinished: 100\n      template:\n        spec:\n          containers:\n          - name: hello-date\n            image: my-image\n            imagePullPolicy: IfNotPresent\n            args:\n            - -e\n            - \"echo date;\"\n          restartPolicy: OnFailure\n      parallelism: 1\n      completions: 1\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Job/#suspend-cronjob","title":"suspend cronjob","text":"<pre><code>kubectl patch cronjobs &lt;cronjob-name&gt; -p '{\"spec\" : {\"suspend\" : true }}'\nkubectl patch cronjobs &lt;cronjob-name&gt; -p '{\"spec\" : {\"suspend\" : false }}'\n</code></pre>"},{"location":"DevOps/Kubernetes/Job/#concurrencypolicy","title":"<code>concurrencyPolicy</code>","text":"<ul> <li>Allow (default): allow concurrently running jobs</li> <li>Forbid: if previous job run hasn't finished yet, the cron job skips the new job run</li> <li>Replace: if previous job run hasn't finished yet, the cron job replaces the previous running job with a new job run</li> </ul>"},{"location":"DevOps/Kubernetes/Job/#restartpolicy","title":"<code>restartPolicy</code>","text":"<ul> <li><code>Never</code>: it won't restart on failure</li> <li><code>OnFailure</code>: the pod will be restarted if it fails</li> <li>with <code>backoffLimit</code>, a restartPolicy of <code>Never</code> ensures that when a pod fails it will eventually propagate and cause the job to fail</li> </ul>"},{"location":"DevOps/Kubernetes/Job/#specbackofflimit","title":"<code>.spec.backoffLimit</code>","text":"<p>The number of <code>retries</code> before considering a Job as failed. Default is 6.</p> <p>Failed Pods associated with the Job are recreated by the Job controller with an exponential back-off delay (10s, 20s, 40s ...) capped at six minutes. The back-off count is reset when a Job's Pod is deleted or successful without any other Pods for the Job failing around that time.</p> <p>The number of retries is calculated in two ways: - The number of Pods with <code>.status.phase = \"Failed\"</code> - When using <code>restartPolicy = \"OnFailure\"</code>, the number of retries in all the containers of Pods with <code>.status.phase</code> equal to <code>Pending</code> or <code>Running</code> If either of the calculations reaches the .spec.backoffLimit, the Job is considered failed.</p> <p>When the <code>JobTrackingWithFinalizers</code> feature is disabled, the number of failed Pods is only based on Pods that are still present in the API.</p> <p>Note: If the job has restartPolicy = \"OnFailure\", the Pod running the Job will be terminated once the job backoff limit has been reached. This can make debugging the Job's executable more difficult. We suggest setting restartPolicy = \"Never\" when debugging the Job or using a logging system to ensure output from failed Jobs is not lost inadvertently.</p>"},{"location":"DevOps/Kubernetes/Job/#degraded-job-reached-the-specified-backoff-limit","title":"Degraded job reached the specified backoff limit","text":"<p>The pod will be deleted and this makes it harder to debug the issue. Change the <code>restartPolicy</code> to \"Never\" for debugging.</p>"},{"location":"DevOps/Kubernetes/Kubectl/","title":"Kubectl","text":"<p>https://kubernetes.io/docs/reference/kubectl/</p>"},{"location":"DevOps/Kubernetes/Kubectl/#install","title":"install","text":"<p>https://kubernetes.io/docs/tasks/tools/ <pre><code>curl -LO https://dl.k8s.io/release/v1.24.9/bin/windows/amd64/kubectl.exe\n\ncurl -LO https://dl.k8s.io/release/v1.24.9/bin/linux/amd64/kubectl\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n</code></pre> The kubectl versions can be found at: https://kubernetes.io/releases</p> <p>https://github.com/Azure/kubelogin <pre><code>export KUBECONFIG=/path/to/kubeconfig\nkubelogin convert-kubeconfig\nkubectl get no\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Kubectl/#show-all-info","title":"show all info","text":"<pre><code>kubectl get node -v=7\nkubectl get node -v=10\n</code></pre>"},{"location":"DevOps/Kubernetes/Kubectl/#list-all-resources-in-a-namespace","title":"list all resources in a namespace","text":"<pre><code>kubectl get all -n &lt;namespace&gt; #not list all\nkubectl api-resources\nkubectl api-resources --verbs=list --namespaced -o name \\\n  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n &lt;namespace&gt;\n</code></pre>"},{"location":"DevOps/Kubernetes/Kubectl/#copy-files","title":"copy files","text":"<p>copy files from local folder to k8s pod: - https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cp - Requires that the <code>tar</code> binary is present in your container image <code>sudo apt-get update; sudo apt-get install -y tar</code> <pre><code>kubectl cp my_code.py my-namespace/my-pod-name:/tmp/my_code.py\nkubectl cp ./my_folder my-namespace/my-pod-name:/tmp/my_folder -c my-container\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Kubernetes/","title":"Kubernetes","text":"<p>kubectl Cheat Sheet:</p> <p>https://kubernetes.io/docs/reference/kubectl/cheatsheet/</p> <p>https://unofficial-kubernetes.readthedocs.io/en/latest/user-guide/kubectl-cheatsheet/</p> <p>kubectl commands:</p> <p>https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#exec</p> <p>Container orchestration tool.</p> <p>https://github.com/kelseyhightower/intro-to-kubernetes-workshop/blob/master/labs/cluster-add-on-ui.md</p>"},{"location":"DevOps/Kubernetes/Kubernetes/#install-kubectl-on-wsl2","title":"install kubectl on wsl2","text":"<pre><code># kubectl will be installed in `/usr/local/bin/kubectl`\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.22.8/bin/linux/amd64/kubectl\nchmod +x ./kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl\n\n# open file ~/.profile update the path for kubectl, also add\nexport KUBECONFIG=/mnt/c/users/$WindowsUSER/.kube/kube_config_filename\n\n# Reopen wsl2 to re-read .profle file\n</code></pre>"},{"location":"DevOps/Kubernetes/Kubernetes/#credential","title":"credential","text":"<pre><code>az login #login\n$env:KUBECONFIG=\"C:\\Users\\&lt;user-name&gt;\\.kube\\&lt;your_aks_name&gt;\" # setup config in windows\naz aks get-credentials --resource-group &lt;aks-rg&gt; --name &lt;aks-name&gt; --overwrite-existing #create credential\n</code></pre>"},{"location":"DevOps/Kubernetes/Kubernetes/#get-pod-info","title":"get pod info","text":"<pre><code>kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"DevOps/Kubernetes/Kubernetes/#open-a-shell-in-a-running-pod-container","title":"open a shell in a running pod container","text":"<pre><code>kubectl exec -it &lt;pod-name&gt; -n &lt;namespace&gt; -- /bin/bash\nkubectl exec -it &lt;pod-name --container &lt;container-name&gt; -n &lt;namespace&gt; -- /bin/bash\n</code></pre>"},{"location":"DevOps/Kubernetes/Kubernetes/#log","title":"log","text":"<pre><code>kubectl logs my-pod                                 #dump pod logs (stdout)\nkubectl logs -l name=myLabel                        #dump pod logs, with label name=myLabel (stdout)\nkubectl logs my-pod --previous                      #dump pod logs (stdout) for previous container\nkubectl logs my-pod -c my-container                 #dump pod container logs (stdout, multi-container case)\nkubectl logs -l name=myLabel -c my-container        #dump pod logs, with label name=myLabel (stdout)\nkubectl logs my-pod -c my-container --previous      #dump pod container logs (stdout, multi-container case) for previous container\n</code></pre>"},{"location":"DevOps/Kubernetes/Label/","title":"Label and Annotation","text":"<p>https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/</p> <p>https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/</p> <ul> <li>Labels can be used to <code>select objects</code> and to <code>find collections of objects</code> that satisfy certain conditions.</li> <li>Annotations are not used to identify and select objects - to <code>attach arbitrary non-identifying metadata to objects</code>.</li> </ul>"},{"location":"DevOps/Kubernetes/Label/#get-node-labels","title":"get node labels","text":"<pre><code>kubectl get nodes --show-labels | sed 's/,/\\n  /g'  #works\nkubectl get nodes --show-labels | tr , '\\n  '       #can't add space\n</code></pre>"},{"location":"DevOps/Kubernetes/Label/#well-known-labels-annotations-and-taints","title":"Well-Known Labels, Annotations and Taints","text":"<p>https://kubernetes.io/docs/reference/labels-annotations-taints/</p>"},{"location":"DevOps/Kubernetes/Learn/","title":"Learn","text":""},{"location":"DevOps/Kubernetes/Learn/#kubernetes-the-hard-way","title":"Kubernetes The Hard Way","text":"<p>walks you through setting up Kubernetes the hard way. </p> <p>https://github.com/kelseyhightower/kubernetes-the-hard-way</p>"},{"location":"DevOps/Kubernetes/Logs/","title":"Logs","text":""},{"location":"DevOps/Kubernetes/Logs/#describe-node","title":"describe node","text":"<pre><code>kubectl describe node &lt;node-name&gt;\n</code></pre>"},{"location":"DevOps/Kubernetes/Logs/#logs_1","title":"logs","text":"<pre><code>kubectl logs &lt;pod-name&gt; -n &lt;namespace&gt;\nkubectl logs -p -c &lt;container-name&gt; &lt;pod-name&gt; #logs from previous terminated container\nkubectl logs -f -c &lt;container-name&gt; &lt;pod-name&gt; #begin streaming logs from container\nkubectl logs --tail=20 &lt;pod-name&gt; #display only the most recent 20 lines of logd\nkubectl logs --since=1h &lt;pod-name&gt; #display all logs written in the last hour, 5 minutes (5m)\n</code></pre>"},{"location":"DevOps/Kubernetes/Logs/#how-long-to-keep-pod-logs","title":"how long to keep pod logs","text":"<p>https://unofficial-kubernetes.readthedocs.io/en/latest/concepts/cluster-administration/logging/ - output to stdout and stderr is handled and redirected to a file in json format, on node's SSD - kubernetes performs log rotation daily (up to 5) per container, or if the log file grows beyond 10MB in size - if a container restarts, keeps one terminated container with its logs - if a pod is evicted, all corresponding containers are also evicted, along with their logs</p>"},{"location":"DevOps/Kubernetes/Memory/","title":"Memory","text":"<p>https://stackoverflow.com/questions/68888889/does-kubernetes-pods-provide-memory-back-after-acquiring-more-than-the-requeste</p> <p>Kubernetes resource requests come into effect at basically three times: - When new pods are being initially scheduled, the resource requests (only) are used to find a node with enough space. The sum of requests must be less than the physical size of the node. Limits and actual utilization aren't considered. - If the process allocates memory, and this would bring its total utilization above the pod's limit, the allocation will fail. - If the node runs out of memory, Kubernetes will look through the pods on that node and evict the pods whose actual usage most exceeds their requests.</p>"},{"location":"DevOps/Kubernetes/Memory/#node-reserved-memory","title":"Node reserved memory","text":"<p>Memory - reserved memory includes the sum of two values.</p> <p>The kubelet daemon is installed on all Kubernetes agent nodes to manage container creation and termination.  By default on AKS, this daemon has the following eviction rule:  - memory.available &lt; 750Mi, which means a node must always have at least 750 Mi allocatable at all times.  - When a host is below that threshold of available memory, the kubelet will terminate one of the running pods to free memory on the host machine and protect it.</p> <p>The second value is a progressive rate of memory reserved for the kubelet daemon to properly function (kube-reserved). <pre><code>- 25% of  0-  4 GB (  4 GB)\n- 20% of  4-  8 GB (  4 GB)\n- 10% of  8- 16 GB (  8 GB)\n- 06% of 16-128 GB (112 GB)\n- 02% of   &gt;128 GB\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Monitor/","title":"Monitor","text":""},{"location":"DevOps/Kubernetes/Monitor/#install-krew","title":"Install <code>krew</code>","text":"<p><code>krew</code> is the plugin manager: https://krew.sigs.k8s.io/docs/user-guide/setup/install/</p> <p>Make sure <code>git</code> is installed and run <pre><code>(\n  set -x; cd \"$(mktemp -d)\" &amp;&amp;\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\n  KREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\n  tar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n  ./\"${KREW}\" install krew\n)\n</code></pre> Add $HOME/.krew/bin directory to PATH by appending the folowing line to <code>.bashrc</code>: <pre><code>export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n</code></pre></p> <p>The restart the shell and check the version: <pre><code>kubectl krew version\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Monitor/#plugin-resource-capacity","title":"Plugin <code>resource-capacity</code>","text":"<p>Resource usage using <code>resource-capacity</code> plugin: https://www.middlewareinventory.com/blog/cpu-memory-usage-nodes-k8s/</p>"},{"location":"DevOps/Kubernetes/Monitor/#install-the-plugin","title":"Install the plugin","text":"<pre><code>kubectl krew install resource-capacity\nkubectl krew list #list all installed plugins\n</code></pre>"},{"location":"DevOps/Kubernetes/Monitor/#get-node-cpu-and-memory-usage","title":"Get node cpu and memory usage","text":"<pre><code>kubectl resource-capacity\nkubectl resource-capacity --sort cpu.limit\nkubectl resource-capacity --sort cpu.util --util                     #include utilization\nkubectl resource-capacity --sort cpu.util --util --pods              #pod level report\nkubectl resource-capacity --sort cpu.util --util --pods --containers #container level report\n</code></pre>"},{"location":"DevOps/Kubernetes/Monitor/#get-namespace-cpu-and-memory-usage","title":"Get namespace cpu and memory usage","text":"<pre><code>kubectl resource-capacity -n kube-system -p -c\nkubectl resource-capacity -n kube-system --pods --containers\n</code></pre>"},{"location":"DevOps/Kubernetes/Monitor/#sort-flags","title":"Sort flags","text":"<pre><code>cpu.util, cpu.request, cpu.limit, mem.util, mem.request, mem.limit, name\n</code></pre>"},{"location":"DevOps/Kubernetes/Monitor/#-node-labels-flag","title":"<code>--node-labels</code> flag","text":"<pre><code>kubectl resource-capacity --namespace-labels &lt;namespace-label&gt;\nkubectl resource-capacity --node-labels &lt;node-label&gt;\nkubectl resource-capacity --pod-labels &lt;pod-label&gt;\n</code></pre>"},{"location":"DevOps/Kubernetes/Monitor/#output-formats","title":"Output formats:","text":"<ul> <li>yaml</li> <li>json</li> <li>table ( default) <pre><code>kubectl resource-capacity -o yaml\n</code></pre></li> </ul>"},{"location":"DevOps/Kubernetes/Multithreading/","title":"Multithreading","text":"<p>https://stackoverflow.com/questions/53276398/kubernetes-cpu-multithreading</p> <p>https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/</p>"},{"location":"DevOps/Kubernetes/Multithreading/#cpu-count","title":"cpu count","text":"<pre><code>grep -c ^processor /proc/cpuinfo #should agree with multiprocessing.cpu_count()\n</code></pre>"},{"location":"DevOps/Kubernetes/Multithreading/#cpu-settings","title":"cpu settings","text":"<p>Threads = 2 does not mean the cpu requests should be 2. Should be tested?</p> <p>The <code>args</code> section of the configuration file provides arguments for the container when it starts. The -cpus \"2\" argument tells the Container to attempt to use 2 CPUs. <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: cpu-2\n  namespace: cpu-test\nspec:\n  containers:\n  - name: cpu-ctr-2\n    image: vish/stress\n    resources:\n      requests:\n        cpu: 500m\n      limits:\n        cpu: \"1\"\n    args:\n    - -cpus\n    - \"2\"\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Node/","title":"Node","text":""},{"location":"DevOps/Kubernetes/Node/#show-node-agentpool","title":"show node agentpool","text":"<p>Every node pool has a label <code>agentpool</code>. Get the node agentpool label <pre><code>kubectl get nodes -L agentpool\n</code></pre> <code>agentpool</code> is a label on the nodepool. Can view them using <code>kubectl get nodes --show-labels</code>. <pre><code>spec:\n  containers:\n  - name: &lt;container-name&gt;\n    image: &lt;image-name&gt;\n  nodeSelector:\n    agentpool: &lt;pool-name&gt;\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Node/#manually-delete-a-node","title":"manually delete a node","text":"<p>scaling up nodes first to make sure the cluster has enough nodes to accomodate the workloads <pre><code>kubectl get nodes\nkubectl cordon &lt;node-name&gt;   #mark a node unschedulable\nkubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-emptydir-data  #safely evict all pods from a node before perform maintenance on the node\nkubectl delete node &lt;node-name&gt;   #delete the node after all pods are evicted from the node\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Node/#cordon-a-node","title":"cordon a node","text":"<pre><code># set node to be unschedulable\nkubectl cordon &lt;node-name&gt;\nkubectl drain &lt;node-name&gt; --ignore-daemonsets\n\n# clean docker disk or do other things\ndocker system prune --all\n\n# reset node as schedulable\nkubectl uncordon &lt;node-name&gt;\n</code></pre>"},{"location":"DevOps/Kubernetes/Node/#find-the-mount-of-a-path","title":"find the mount of a path","text":"<pre><code>findmnt --target /var/lib/docker\n</code></pre>"},{"location":"DevOps/Kubernetes/Node/#error-varlibdockeroverlay2xxx-no-such-file-or-directory","title":"error: /var/lib/docker/overlay2/xxx: no such file or directory","text":"<p>This might be caused <code>docker system prune</code>. Solution: <pre><code>systemctl stop docker\numount /var/lib/docker/overlay2\nrm -rf /var/lib/docker\nsystemctl start docker\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Node/#node-stays-on-readyschedulingdisabled","title":"Node stays on <code>Ready,SchedulingDisabled</code>","text":"<p>Solution <pre><code>kubectl uncordon &lt;node-name&gt;\n</code></pre> https://github.com/kubereboot/kured/issues/63 - incompatibility between the version of kubectl in the kured images you're using and AKS??? - when there is only 1 node in AKS, because the pod cannot be re-created after node rebooted - this happens when the reboot can not occur because a <code>Pod Disruption Budget</code> does not allow pods to be killed on the node <code>kured</code> is trying to drain   <code>error when evicting pods/\"user-scheduler-xxxxx-xxx\" -n \"jhub\" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.</code>   check the <code>pdb</code> settings - some settings are invalid: <code>kubectl get pdb -A</code>, <code>kubectl get pdb -n &lt;namespace&gt; &lt;pdb-name&gt; -o yaml</code> - scheduling disabled represents that node got into maintenance mode - due to maintainance???</p>"},{"location":"DevOps/Kubernetes/Node/#connect-to-aks-node","title":"connect to aks node","text":"<p>https://learn.microsoft.com/en-us/azure/aks/node-access</p>"},{"location":"DevOps/Kubernetes/NodePort/","title":"NodePort","text":"<p>NodePort is another mechanism to expose a Kubernetes service externally. It opens a static port on each node in the cluster, and any traffic coming to that port is forwarded to the corresponding service.</p> <p>NodePort works at the transport layer (Layer 4) of the OSI model and can be considered a simple form of load balancing.</p> <p>When you expose a service using NodePort, Kubernetes automatically assigns a port from the range 30000-32767, which is accessible on all nodes in the cluster. For example, if a service is assigned NodePort 30080, any traffic coming to any node's IP address on port 30080 will be forwarded to the service.</p> <p>NodePort is often used for development and testing purposes or when you have a specific requirement to expose a service on a known port across all nodes. However, it has some limitations, such as exposing the service on a high port range, which might not be desirable in certain production scenarios.</p> <p>NodePort is a simple way to expose services by opening a port on each node in the cluster, but it lacks the advanced features of Ingress.</p>"},{"location":"DevOps/Kubernetes/NodeReboot/","title":"Reboot Node","text":""},{"location":"DevOps/Kubernetes/NodeReboot/#get-node-info","title":"Get node info","text":"<pre><code>kubectl get nodes\nkubectl describe node &lt;node-name&gt;\n</code></pre>"},{"location":"DevOps/Kubernetes/NodeReboot/#drain-the-node-optional-but-recommended","title":"Drain the Node (optional but recommended)","text":"<p>Draining a node will safely evict all the pods from the node and schedule them onto other nodes in the cluster.  This helps to avoid service disruption.</p> <p><pre><code>kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-local-data\n</code></pre>  - <code>--ignore-daemonsets</code>: Skips daemonsets which might be running on the node.  - <code>--delete-local-data</code>: Removes pods with local storage.</p> <p>If the node is part of a high-availability setup, you may want to wait until the cluster has rescheduled the pods to other nodes before proceeding.</p>"},{"location":"DevOps/Kubernetes/NodeReboot/#restart-the-node","title":"Restart the Node","text":"<p>On a VM or Physical Server: <pre><code>sudo reboot\n</code></pre></p> <p>On Cloud Providers: use the cloud provider's management console or CLI tools to restart the instance - Azure: <pre><code>az vm restart --resource-group &lt;resource-group&gt; --name &lt;vm-name&gt;\n</code></pre></p> <ul> <li>AWS EC2: <pre><code>aws ec2 reboot-instances --instance-ids &lt;instance-id&gt;\n</code></pre></li> <li>Google Cloud: <pre><code>gcloud compute instances reset &lt;instance-name&gt; --zone &lt;zone&gt;\n</code></pre></li> </ul>"},{"location":"DevOps/Kubernetes/NodeReboot/#uncordon-the-node-if-drained","title":"Uncordon the Node (if drained)","text":"<p>After the node has restarted, we need to mark it as schedulable again so that new pods can be scheduled onto it. <pre><code>kubectl uncordon &lt;node-name&gt;\n</code></pre></p>"},{"location":"DevOps/Kubernetes/NodeReboot/#verify-node-status","title":"Verify Node Status","text":"<p>Ensure the node has returned to a Ready state and that it is functioning properly. - Check Node Status: <pre><code>kubectl get nodes\n</code></pre> - Check Node Logs:   Depending on your setup, you might want to check system logs or Kubernetes logs to ensure everything is functioning correctly.</p>"},{"location":"DevOps/Kubernetes/NodeTaint/","title":"Taints and Tolerations","text":"<p>https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/</p> <p>https://blog.kubecost.com/blog/kubernetes-taints/</p>"},{"location":"DevOps/Kubernetes/NodeTaint/#taints","title":"Taints","text":"<ul> <li>A node property</li> <li>Allow a node to repel a set of pods</li> </ul>"},{"location":"DevOps/Kubernetes/NodeTaint/#tolerations","title":"Tolerations","text":"<ul> <li>A pod property, applied to pods</li> <li>Allow scheduling pods on node with matching taints, but don't guarantee scheduling</li> <li><code>Equal</code> operator requires the taint value and will not match if the value is different</li> <li><code>Exists</code> operator will match any value as it only considers if the taint is defined regardless of the value</li> </ul>"},{"location":"DevOps/Kubernetes/NodeTaint/#node-affinity","title":"Node affinity","text":"<p>Taints and tolerations can be combined with node affinity to create dedicated nodes. - Node affinity sets the node to schedule pods using labels by specifying the nodeAffinity in PodSpec - Taints on the node ensure that all pods without a matching toleration get repelled</p>"},{"location":"DevOps/Kubernetes/NodeTaint/#node-selector","title":"Node Selector","text":"<p>Node selectors are a simpler way to constrain pod scheduling to nodes with specific labels. - They use the nodeSelector field in a pod's spec to specify node label requirements. - A pod will only be scheduled on nodes that satisfy all of the specified node selector requirements.</p>"},{"location":"DevOps/Kubernetes/NodeTaint/#pod-anti-affinity","title":"Pod anti-affinity","text":"<ul> <li>Can prevent schedulers from locating new pods on nodes where the label selector on the new pod matches the label on an existing pod</li> <li>Relies on label key-value pairs on other pods to determine its scheduling behavior</li> </ul>"},{"location":"DevOps/Kubernetes/NodeTaint/#add-taint-to-node","title":"Add taint to node","text":"<pre><code>kubectl taint node &lt;node-name&gt; &lt;taint-key&gt;=&lt;taint-value&gt;:&lt;taint-effect&gt;\n</code></pre>"},{"location":"DevOps/Kubernetes/NodeTaint/#get-tainted-nodes","title":"Get tainted nodes","text":"<pre><code>kubectl get node -o custom-columns=NAME:.metadata.name,TAINT:.spec.taints[*].effect\nkubectl get node -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints #include keys\nkubectl get node -o=jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.taints[*].key}{\"=\"}{.spec.taints[*].value}{\":\"}{.spec.taints[*].effect}{\"\\n\"}{end}'\n</code></pre> <p>Node taints example: <pre><code>node_taints = [\n \"key=value:effect\"\n]\n</code></pre></p> <p>Pod tolerations example: <code>yaml tolerations: - key: key   operator: Equal   value: value   effect: NoSchedule - key: key2   operator: Exists   effect: NoSchedule</code></p>"},{"location":"DevOps/Kubernetes/PDB/","title":"PDB (Pod Disruption Budget)","text":"<p>https://kubernetes.io/docs/tasks/run-application/configure-pdb/</p> <p>A PodDisruptionBudget has three fields: - A label selector <code>.spec.selector</code> to specify the set of pods to which it applies - <code>.spec.minAvailable</code> which is a description of the number of pods from that set that must still be available after the eviction, even in the absence of the evicted pod. absolute number or a percentage. - <code>.spec.maxUnavailable</code> which is a description of the number of pods from that set that can be unavailable after the eviction. absolute number or a percentage.</p>"},{"location":"DevOps/Kubernetes/PDB/#get-pdb-apiversion-in-kubernetes","title":"get pdb apiversion in kubernetes","text":"<pre><code>kubectl api-resources | grep pdb\n</code></pre>"},{"location":"DevOps/Kubernetes/PDB/#minavailable-or-maxunavailable","title":"<code>minAvailable</code> or <code>maxUnavailable</code>","text":"<ul> <li><code>minAvailable</code>:  the number of Pods that must be available during a disruption</li> <li><code>maxUnavailable</code>:  the number of Pods that may be disrupted</li> <li>only one of <code>minAvailable</code> and <code>maxUnavailable</code> in a single PodDisruptionBudget can be specified</li> <li>an integer represents the number of Pods. If <code>minAvailable = 10</code>, then 10 Pods must always be available, even during a disruption.</li> <li>a percentage as a string (e.g. \"50%\") represents a percentage of total Pods. If <code>minAvailable = 50%\"</code>, then at least 50% of the Pods (rounded) remain available during a disruption.</li> </ul>"},{"location":"DevOps/Kubernetes/PDB/#allowed-disruptions","title":"<code>ALLOWED DISRUPTIONS</code>","text":"<ul> <li><code>ALLOWED DISRUPTIONS = 0</code> when there are not any pods matching <code>app: my-app</code> in the namespace   <code>ALLOWED DISRUPTIONS = 1</code> disruption controller has seen the pods, counted the matching pods, and updated the status of the PDB <pre><code>kubectl -n &lt;namespace&gt; get pdb &lt;pdb-name&gt;\nkubectl -n &lt;namespace&gt; get pdb &lt;pdb-name&gt; -o yaml\n</code></pre></li> </ul>"},{"location":"DevOps/Kubernetes/PDB/#cannot-evict-pod-as-it-would-violate-the-pods-disruption-budget","title":"Cannot evict pod as it would violate the pod's disruption budget","text":"<p><pre><code>NAMESPACE  NAME              MIN AVAILABLE  MAX UNAVAILABLE  ALLOWED DISRUPTIONS  AGE\njhub       user-placeholder  0              N/A              0                    10d\njhub       user-scheduler    N/A            1                1                    10d\n</code></pre> PDBs are a feature for HA deployments, and HA deployments imply 2+ replicas. To avoid the error and failure of node reboot, the replicaSet must be larger than 1.</p>"},{"location":"DevOps/Kubernetes/PersistentVolume/","title":"Persistent Volume","text":"<p>https://kubernetes.io/docs/concepts/storage/persistent-volumes/</p> <ul> <li>Volumes cannot mount within other volumes (but see Using subPath for a related mechanism)</li> <li>A volume cannot contain a hard link to anything in a different volume</li> </ul>"},{"location":"DevOps/Kubernetes/PersistentVolume/#volume-types","title":"Volume types","text":"<p>Ephemeral volume types have a lifetime of a pod, but persistent volumes exist beyond the lifetime of a pod. </p>"},{"location":"DevOps/Kubernetes/PersistentVolume/#reclaim-policy","title":"Reclaim Policy","text":"<p>https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/#:~:text=PersistentVolumes%20can%20have%20various%20reclaim,user%20deletes%20the%20corresponding%20PersistentVolumeClaim. - delete - recycle - retain</p>"},{"location":"DevOps/Kubernetes/Pod/","title":"Pod","text":""},{"location":"DevOps/Kubernetes/Pod/#general","title":"General","text":"<pre><code>kubectl get pods                #check status\nkubectl get pods -o wide        #show more info such node\nkubectl describe pod &lt;pod-name&gt; #check events\nkubectl get event --field-selector involvedObject.name=&lt;pod-name&gt;\nkubectl logs &lt;pod-name&gt;         #check logs\nkubectl exec -it &lt;pod-name&gt; -n &lt;namespace&gt; -- bash      #connect to pod, only one container in pod\nkubectl exec -it &lt;pod-name&gt; -c &lt;container-name&gt; -- bash #connect to pod, mutiple containers in pod\n</code></pre>"},{"location":"DevOps/Kubernetes/Pod/#pending-pods","title":"pending pods","text":"<pre><code>kubectl get pods --field-selector=status.phase=Pending\n</code></pre>"},{"location":"DevOps/Kubernetes/Pod/#delete-failed-pods","title":"delete failed pods","text":"<pre><code>kubectl delete pods -A  --field-selector='status.phase=Failed'\n</code></pre>"},{"location":"DevOps/Kubernetes/Pod/#forece-delete","title":"forece delete","text":"<pre><code>kubectl delete pod &lt;pod-name&gt; --grace-period=0 --force\nkubectl delete pod &lt;pod-name&gt; --grace-period=0 --force -n &lt;namespace&gt;\n</code></pre>"},{"location":"DevOps/Kubernetes/Pod/#pod-stuck-on-terminating","title":"pod stuck on terminating","text":"<ul> <li>https://stackoverflow.com/questions/35453792/pods-stuck-in-terminating-status</li> <li>https://github.com/kubernetes/kubernetes/issues/51835 <pre><code>kubectl delete pod &lt;pod-name&gt; --grace-period=0  --force # if does not work, try\nkubectl patch pod &lt;pod-name&gt; -n &lt;namespace&gt; -p '{\"metadata\":{\"finalizers\":null}}' \n</code></pre></li> </ul>"},{"location":"DevOps/Kubernetes/Pod/#port-forward","title":"port forward","text":"<p>https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/</p> <p>Use Port Forwarding to Access Applications in a Cluster. <pre><code>kubectl port-forward pod/&lt;pod-name&gt; &lt;local-port&gt;:&lt;pod-port&gt;\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Pod/#pod-cpu-and-memory-usage","title":"pod cpu and memory usage","text":"<p>run into the pod <pre><code>cat /sys/fs/cgroup/cpu/cpuacct.usage  #nanosecond\ncat /sys/fs/cgroup/memory/memory.usage_in_bytes | awk '{ mem = $1 / 1024 / 1024 / 1024 ; print mem \"GB\" }'\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Pod/#check-pod-throttling-rate","title":"check pod throttling rate","text":"<ul> <li><code>nr_periods</code>: Total schedule period</li> <li><code>nr_throttled</code>: Total throttled period out of nr_periods</li> <li><code>throttled_time</code>: Total throttled time in ns <pre><code>#run into the pod\ncat /sys/fs/cgroup/cpu/cpu.stat\n</code></pre></li> </ul>"},{"location":"DevOps/Kubernetes/Pod/#kubectl-run-command","title":"Kubectl run command","text":"<p>https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes When you override the default Entrypoint and Cmd for a Container, these rules apply: - do not supply <code>command</code> or <code>args</code>, the defaults defined in the Docker image are used. - supply a <code>command</code> but no <code>args</code>, only the supplied command is used. The default EntryPoint and the default Cmd defined in the Docker image are ignored. - supply only <code>args</code>, the default Entrypoint defined in the Docker image is run with the args that you supplied. - supply a <code>command</code> and <code>args</code>, the default Entrypoint and the default Cmd defined in the Docker image are ignored. Your command is run with your args.</p>"},{"location":"DevOps/Kubernetes/Pod/#create-a-pod-by-passing-env-vars","title":"create a pod by passing env vars","text":"<pre><code>kubectl run &lt;pod-name&gt; -n &lt;namespace&gt; \\\n  --image=&lt;acr-name&gt;.azurecr.io/dev/app:latest --env=\"PREFIX_UPPER_CASE_PARAM=xyz\"\n</code></pre>"},{"location":"DevOps/Kubernetes/Pod/#use-another-entrypoint-and-let-the-pod-run-so-can-get-into-the-container","title":"use another entrypoint and let the pod run so can get into the container","text":"<p>https://stackoverflow.com/questions/59248318/kubectl-run-command-vs-arguments <pre><code>kubectl run &lt;pod-name&gt; -n &lt;namespace&gt; --image=&lt;image-path&gt; \\\n  --restart=Never -o yaml --dry-run -- /bin/sh -c \"echo hello;sleep 3600\"\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Pod/#sleep-pod","title":"sleep pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntu\n  labels:\n    app: ubuntu\nspec:\n  containers:\n  - image: ubuntu\n    command:\n      - \"sleep\"\n      - \"604800\"\n    imagePullPolicy: IfNotPresent\n    name: ubuntu\n  restartPolicy: Always\n</code></pre>"},{"location":"DevOps/Kubernetes/Pod/#temporally-create-a-pod-and-delete-it-when-it-exits","title":"temporally create a pod and delete it when it exits","text":"<p>option 1 <pre><code>kubectl run -it --rm &lt;pod-name&gt; --namespace=&lt;namespace&gt; --image=alpine -- bash\n</code></pre></p> <p>option 2: using a yaml file <pre><code>kubectl apply -f &lt;debug-pod&gt;.yaml\nkubectl exec -it &lt;pod-name&gt; --namespace=&lt;namespace&gt; -- bash\nkubectl delete pod &lt;pod-name&gt; --namespace=&lt;namespace&gt;\n</code></pre></p> <p>yaml file <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: debug-pod\n  namespace: default\nspec:\n  containers:\n  - name: debug-container\n    image: alpine\n    command: [\"/bin/bash\", \"-c\", \"tail -f /dev/null\"]\n    volumeMounts:\n    - name: my-volume\n      mountPath: /path/in/container\n  volumes:\n  - name: my-volume\n    hostPath:\n      path: /path/on/host\n</code></pre> running the <code>tail -f /dev/null</code> command to keep the container running.</p>"},{"location":"DevOps/Kubernetes/Pod/#lifecycle","title":"Lifecycle","text":"<p>https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#example-states</p> <ul> <li>Pending</li> <li>Running</li> <li>Succeeded/Failed</li> </ul>"},{"location":"DevOps/Kubernetes/Pod/#status","title":"Status","text":"<ul> <li><code>Completed</code>: when the running process/app/container returns exit code 0.</li> <li><code>Error</code>: when return a non-zero exit code the status usually set to Error.</li> </ul>"},{"location":"DevOps/Kubernetes/Pod/#pod-and-container-failures","title":"Pod and container failures","text":""},{"location":"DevOps/Kubernetes/Pod/#container","title":"Container","text":"<p>A container in a Pod may fail for a number of reasons, such as - the process in it exited with a non-zero exit code, or - the container was killed for exceeding a memory limit, etc. If this happens, and the .spec.template.spec.restartPolicy = \"OnFailure\", then the Pod stays on the node, but the container is re-run.</p> <p>Therefore, your program needs to handle the case when it is restarted locally, or else specify .spec.template.spec.restartPolicy = \"Never\".</p>"},{"location":"DevOps/Kubernetes/Pod/#pod_1","title":"Pod","text":"<p>An entire Pod can also fail, for a number of reasons, such as - when the pod is kicked off the node (node is upgraded, rebooted, deleted, etc.), or - if a container of the Pod fails and the .spec.template.spec.restartPolicy = \"Never\". When a Pod fails, then the Job controller starts a new Pod.</p> <p>This means that your application needs to handle the case when it is restarted in a new pod. In particular, it needs to handle temporary files, locks, incomplete output and the like caused by previous runs.</p> <p>Note that even if you specify .spec.parallelism = 1 and .spec.completions = 1 and .spec.template.spec.restartPolicy = \"Never\", the same program may sometimes be started twice.</p> <p>If you do specify .spec.parallelism and .spec.completions both greater than 1, then there may be multiple pods running at once. Therefore, your pods must also be tolerant of concurrency.</p>"},{"location":"DevOps/Kubernetes/Port/","title":"Port","text":"<p>https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-ports-targetport-nodeport-service.html</p> <p>https://serverfault.com/questions/1070654/how-to-deploy-docker-container-and-do-port-mapping-forward-using-kubernetes-yaml</p>"},{"location":"DevOps/Kubernetes/PrivateEndpoint/","title":"Private Endpoint","text":"<p>https://samcogan.com/accessing-a-private-aks-cluster-with-additional-private-endpoints/</p>"},{"location":"DevOps/Kubernetes/Python/","title":"Python","text":"<p>https://kubernetes.io/blog/2019/07/23/get-started-with-kubernetes-using-python/</p>"},{"location":"DevOps/Kubernetes/ReplicaSet/","title":"Kube ReplicaSet","text":"<p>Note: ReplicaSet will not be updated automatically for the mounted objects such as key vault secrets - Need to delete the ReplicaSet to force creating a new pod.</p> <p>https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#:~:text=A%20ReplicaSet's%20purpose%20is%20to,specified%20number%20of%20identical%20Pods</p> <p>A <code>ReplicaSet</code>'s purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods.</p> <p>A <code>Deployment</code> is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features.</p> <pre><code>kubectl -n &lt;namespace&gt; get rs #get current replicasets\nkubectl -n &lt;namespace&gt; describe rs/&lt;replicaset-name&gt; #check the status\n</code></pre>"},{"location":"DevOps/Kubernetes/ReplicaSet/#run-into-replicaset","title":"run into replicaset","text":"<pre><code>kubectl exec -it -n &lt;name-space&gt; &lt;replicaset-name&gt; -- /bin/bash\n</code></pre>"},{"location":"DevOps/Kubernetes/RoleBinding/","title":"Role Binding","text":""},{"location":"DevOps/Kubernetes/RoleBinding/#role-and-clusterrole","title":"Role and ClusterRole","text":"<p>ClusterRole and Role are resources used for defining sets of permissions within a cluster: - ClusterRole: Applies cluster-wide and can be used to define permissions across all namespaces within the cluster. - Role: Applies to a specific namespace and defines permissions for resources within that namespace.</p>"},{"location":"DevOps/Kubernetes/RoleBinding/#rolebinding","title":"RoleBinding","text":"<p>Defines access within the Kubernetes cluster, such as the ability to list, get, or watch specific resources (pods, services, etc.).</p>"},{"location":"DevOps/Kubernetes/SecretProvider/","title":"SecretProviderClass","text":""},{"location":"DevOps/Kubernetes/SecretProvider/#azure-usepodidentity","title":"azure usePodIdentity","text":"<pre><code>apiVersion: secrets-store.csi.x-k8s.io/v1alpha1\nkind: SecretProviderClass\nmetadata:\n  name: key-vault-secret-file\nspec:\n  provider: azure\n  parameters:\n    usePodIdentity: \"true\"\n    keyvaultName: \"&lt;key-vault-name&gt;\"\n      objects: |\n        array:\n          - |\n            objectName: &lt;secret-name&gt;\n            objectType: secret\n      tenantId: \"&lt;tenant-id&gt;\"\n</code></pre>"},{"location":"DevOps/Kubernetes/Secrets/","title":"Secrets","text":""},{"location":"DevOps/Kubernetes/Secrets/#secrets","title":"Secrets","text":"<p>https://kubernetes.io/docs/tasks/configmap-secret/managing-secret-using-kubectl/</p> <p>https://howchoo.com/kubernetes/read-kubernetes-secrets</p>"},{"location":"DevOps/Kubernetes/Secrets/#secret-types","title":"secret types","text":"<p>https://kubernetes.io/docs/concepts/configuration/secret/ - Opaque Secrets - ServiceAccount token Secrets - Basic authentication Secret - SSH authentication Secrets - TLS Secrets</p>"},{"location":"DevOps/Kubernetes/Secrets/#get-secret","title":"get secret","text":"<pre><code>kubectl get secrets                   #only summary like name and type etc.\nkubectl describe secret &lt;secret-name&gt; #more details such as filenames\nkubectl get secret &lt;secret-name&gt; -n &lt;namespace&gt; -o yaml #base64\u2011encoded contents\nkubectl get secret &lt;secret-name&gt; -o jsonpath=\"{.data.username}\" | base64 --decode\nkubectl get secret &lt;secret-name&gt; -o jsonpath=\"{.data.user\\.name}\" | base64 --decode\nkubectl get secret &lt;secret-name&gt; -o jsonpath=\"{.data['user\\.name']}\" | base64 --decode\n</code></pre>"},{"location":"DevOps/Kubernetes/Secrets/#delete-secret","title":"delete secret","text":"<pre><code>kubectl delete secret &lt;secret-name&gt;\n</code></pre>"},{"location":"DevOps/Kubernetes/Secrets/#ways-of-creating-secrets","title":"ways of creating secrets","text":"<p>Kubernetes provides three ways of creating secrets: - from files - from the command line - from YAML or JSON definitions</p>"},{"location":"DevOps/Kubernetes/Secrets/#create-secret-from-files","title":"create secret from files","text":"<pre><code>#opaque secret: the schema of the contents is unknown\nkubectl create secret generic &lt;secret-name&gt; \\\n  --from-file=./username.txt \\\n  --from-file=./password.txt\n\nkubectl create secret generic &lt;secret-name&gt; \\\n  --from-file=username=./username.txt \\\n  --from-file=password=./password.txt\n\nkubectl create secret &lt;secret-name&gt; \\\n  --key /etc/certs/{name}.com.key \\\n  --cert /etc/certs/{name}.com.crt \\\n  -n &lt;namespace&gt;\n\nkubectl create secret tls &lt;secret-name&gt; \\\n  --namespace &lt;namespace&gt; \\\n  --key tls.key \\\n  --cert tls.crt\n</code></pre>"},{"location":"DevOps/Kubernetes/Secrets/#create-secret-using-literals","title":"create secret using literals","text":"<pre><code>kubectl create secret generic &lt;secret-name&gt; \\\n  --namespace=default \\\n  --from-literal=&lt;key&gt;=&lt;val&gt; \\\n  --from-literal=password='S!X*hz$z='\n</code></pre>"},{"location":"DevOps/Kubernetes/Secrets/#create-secret-from-yaml","title":"create secret from yaml","text":"<p><code>kubectl create -f secret.yaml</code> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: myapiurltoken-yaml\ntype: Opaque\ndata:\n  url: &lt;base64-encoded values&gt;\n  token: &lt;base64-encoded values&gt;\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Secrets/#edit-secret","title":"edit secret","text":"<pre><code>EDITOR=\"vi\"  kubectl edit secrets &lt;secret-name&gt;\n\n#from file\ndk=$(base64 &lt; \"./a.key\" | tr -d '\\n')\ndv=$(base64 &lt; \"./a.crt\" | tr -d '\\n')\n\n#from string\ndk=$(echo -n 'key-string' | base64)\ndv=$(echo -n 'crt-string' | base64)\n\nkubectl get secrets &lt;secret-name&gt; -n &lt;namespace&gt; -o json \\\n    | jq \".data[\\\"a.key\\\"] |= \\\"$dk\\\"\" \\\n    | jq \".data[\\\"a.crt\\\"] |= \\\"$dv\\\"\" \\\n    | kubectl apply -f -\n\nkubectl patch secret &lt;secret-name&gt; -n &lt;namespace&gt; \\\n    -p \"{\\\"data\\\":{\\\"a.key\\\":\\\"${dk}\\\",\\\"a.crt\\\":\\\"${dv}\\\"}}\"\n</code></pre> <p>delete a key in data <pre><code>kubectl patch configmap myconfigmap --type=json -p='[{\"op\": \"remove\", \"path\": \"/data/mykey\"}]'\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Secrets/#ways-of-consuming-secrets","title":"ways of consuming secrets","text":"<p>Kubernetes provides two ways of consuming secrets: env var and mounted file (preferred).</p>"},{"location":"DevOps/Kubernetes/Secrets/#why-mounted-file-is-preferred","title":"Why mounted file is preferred","text":"<p>It is more secure to mount secrets as files. - Kubernetes treats secrets as environment variables securely - but the container runtime doesn't treat them securely <pre><code>#InstanceID: vmssxxxxxx\nkubectl describe pod &lt;pod-name&gt; | grep Node\n#DockerID of running pod: docker://&lt;DockerID&gt;\nkubectl describe pod &lt;pod-name&gt; | grep 'Container ID'\n\nINSTANCE=&lt;InstanceID&gt;\nDOCKERID=&lt;DockerID&gt;\nVMSS=$(az vmss list --query '[].name' -o tsv) #node scale set name\nRGNAME=$(az vmss list --query '[].resourceGroup' -o tsv) #resource group name\n\n#Kubernetes version &lt;= 1.18 (Docker)\naz vmss run-command invoke -g $RGNAME -n $VMSS \\\n  --command-id RunShellScript --instance-id $INSTANCE \\\n  --scripts \"docker inspect -f '{{ .Config.Env }}' $DOCKERID\" -o yaml | grep SECRET\n\n#Kubernetes version &gt;= 1.19 (containerd)\naz vmss run-command invoke -g $RGNAME -n $VMSS \\\n  --command-id RunShellScript --instance-id $INSTANCE \\\n  --scripts \"crictl inspect --output yaml $DOCKERID\" -o yaml | grep SECRET\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Secrets/#using-secrets-as-an-environment-variable","title":"Using secrets as an environment variable","text":"<p>https://blog.nillsf.com/index.php/2020/02/24/dont-use-environment-variables-in-kubernetes-to-consume-secrets/</p> <p>two options: - will convert all to env vars <pre><code>envFrom:\n- secretRef:\n    name: test-secret\n</code></pre></p> <ul> <li>Explicitely set them one by one <pre><code>env:\n- name: VSTS_AGENT_INPUT_POOL\n    valueFrom:\n    secretKeyRef:\n        name: tfs\n        key: pool\n</code></pre></li> </ul> <p>why is not recommended to mount secretes as env vars:  - the secret value is accessible within the running pod - any app can use the secret values by referencing the appropriate env var - value of the env var will not be updated when the secret itself is updated <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-using-env\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n  env:\n  - name: SECRET_URL\n    valueFrom:\n      secretKeyRef:\n        name: &lt;secret-name&gt;\n        key: secreturl.txt\n  - name: SECRET_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: &lt;secret-name&gt;\n        key: secrettoken.txt\nrestartPolicy: Never\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Secrets/#mounting-secrets-as-a-file-in-a-pod-preferred","title":"Mounting secrets as a file in a pod (preferred)","text":"<ul> <li>the secret value is accessible within the running pod</li> <li>can limit which processes can get access to the contents of these files via file system permissions</li> <li>secrets mounted as files will be dynamically updated as the secrets are updated <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-using-volume\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n  volumeMounts:\n  - name: &lt;volume-mount-name&gt;\n    mountPath: \"/etc/secrets\"\n    readOnly: true\nvolumes:\n- name: &lt;volume-mount-name&gt;\n  secret:\n    secretName: &lt;secret-name&gt;\n</code></pre></li> </ul>"},{"location":"DevOps/Kubernetes/Secrets/#mount-a-secret-as-a-file-and-then-using-bash-to-set-the-env-vars","title":"Mount a secret as a file and then using bash to set the env vars","text":"<p>Create a <code>SecretProviderClass</code>: This will configure how to fetch the secret from Azure Key Vault. <pre><code>apiVersion: secrets-store.csi.k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: keyvault-secret-provider\nspec:\n  provider: azure\n  parameters:\n    usePodIdentity: \"false\"  # Set to \"true\" if using managed identity\n    keyvaultName: \"&lt;your-key-vault-name&gt;\"\n    cloudName: \"AzurePublicCloud\"  # Adjust as necessary\n    objects: |\n      array:\n        - |\n          objectName: \"&lt;your-secret-name&gt;\"\n          objectType: secret\n    tenantId: \"&lt;your-tenant-id&gt;\"\n</code></pre></p> <p>Create a <code>Deployment</code> with Volume Mount: Update your deployment to include the SecretProviderClass as a volume. <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: example\n  template:\n    metadata:\n      labels:\n        app: example\n    spec:\n      containers:\n        - name: example-container\n          image: your-image\n          volumeMounts:\n            - name: secrets-store\n              mountPath: /mnt/secrets-store\n          command: [\"/bin/bash\", \"-c\"]\n          args:\n            - |\n              # Install jq if it's not available\n              apt-get update &amp;&amp; apt-get install -y jq &amp;&amp; \\\n              # Read the JSON file and set environment variables\n              for key in $(jq -r 'keys[]' /mnt/secrets-store/&lt;your-secret-name&gt;.json); do\n                  value=$(jq -r \".\\\"$key\\\"\" /mnt/secrets-store/&lt;your-secret-name&gt;.json)\n                  export \"$key=$value\"\n              done\n              # Your application command here, e.g., ./start-app\n      volumes:\n        - name: secrets-store\n          csi:\n            driver: secrets-store.csi.k8s.io\n            volumeAttributes:\n              secretProviderClass: \"keyvault-secret-provider\"\n</code></pre></p> <p>But not available to all processes! too bad!</p>"},{"location":"DevOps/Kubernetes/Secrets/#use-a-third-party-hel-chart","title":"use a third party hel chart","text":"<p>k8s secret from azue key vault - https://stackoverflow.com/questions/75662594/create-kubernetes-secrets-from-azure-keyvault - example: https://github.com/SparebankenVest/azure-key-vault-to-kubernetes/issues/96</p>"},{"location":"DevOps/Kubernetes/SecretsCSI/","title":"Secrets Store CSI Driver","text":"<p>https://secrets-store-csi-driver.sigs.k8s.io/</p> <p>Allows Kubernetes to mount multiple secrets, keys, and certs stored in enterprise-grade external secrets stores into their pods as a volume.</p>"},{"location":"DevOps/Kubernetes/SecretsCSI/#csi-secret-driver","title":"csi-secret driver","text":"<p>To deploy this SecretProviderClass in aks do we need to install the csi-secret driver. - Enable the CSI Secrets Store Driver Add-on:   - When creating a new AKS cluster, include the <code>--enable-addons azure-keyvault-secrets-provider</code> flag in the <code>az aks create</code> command. - Install in Existing Clusters:   - If the cluster already exists, install it using the following command:   <pre><code>az aks enable-addons --addons azure-keyvault-secrets-provider --name myAKSCluster --resource-group myResourceGroup\n</code></pre>   - The <code>secrets-store-csi-driver</code> pods should be in the <code>kube-system</code> namespace: <code>kubectl get pods -n kube-system</code> - Install the csi-secret driver using helm chart   - <code>https://azure.github.io/secrets-store-csi-driver-provider-azure/charts</code></p>"},{"location":"DevOps/Kubernetes/SecretsCSI/#secretproviderclass","title":"SecretProviderClass","text":"<p>Example for azure <pre><code>apiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: my-azure-keyvault\n  namespace: &lt;namespace&gt;\nspec:\n  provider: azure\n  parameters:\n    usePodIdentity: \"true\"       # Use pod identity for authentication\n    keyvaultName: my-keyvault    # Name of the Azure Key Vault\n    cloudName: AzurePublicCloud  # Optional, for Azure Stack or other clouds\n    tenantId: &lt;your-tenant-id&gt;   # Tenant ID containing the Key Vault\n    objects:                     # List of secrets to be mounted\n      - objectName: my-secret        # Name of the secret in Key Vault\n        objectType: secret           # Type of secret (secret, key, or cert)\n        objectAlias: my-secret.json  # file name alias\n        objectVersion: \"\"            # Optional, specify a particular version\n      - objectName: my-certificate\n        objectType: cert\n      - objectName: my-key\n        objectType: key\n</code></pre> Same using <code>array</code>: <pre><code>apiVersion: secrets-store.csi.x-k8s.io/v1alpha1\nkind: SecretProviderClass\nmetadata:\n  name: my-azure-keyvault\n  namespace: &lt;namespace&gt;\nspec:\n  provider: azure\n  parameters:\n    usePodIdentity: \"true\"\n    keyvaultName: my-keyvault\n    cloudName: AzurePublicCloud\n    tenantId: &lt;your-tenant-id&gt;\n    objects: |\n      array:\n        - |\n          objectName: my-secret\n          objectType: secret\n          objectVersion: \"\"\n        - |\n          objectName: my-certificate\n          objectType: cert\n        - |\n          objectName: my-key\n          objectType: key\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Service/","title":"Service","text":""},{"location":"DevOps/Kubernetes/Service/#internal-dns-name","title":"Internal DNS name","text":"<p>For communicating inside a cluster, the DNS names are like <code>&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code>.</p>"},{"location":"DevOps/Kubernetes/Service/#service-internal-traffic-policy","title":"Service Internal Traffic Policy","text":"<ul> <li>enable the internal-only traffic policy for a Service, by setting its <code>.spec.internalTrafficPolicy</code> to <code>Local</code></li> <li>this tells kube-proxy to only use node local endpoints for cluster internal traffic</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: &lt;service-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  ports:\n    - port: 8000\n  selector:\n    app: &lt;app-name&gt;\n  internalTrafficPolicy: Local\n</code></pre>"},{"location":"DevOps/Kubernetes/Service/#nodeport","title":"NodePort","text":"<p>A NodePort is a type of service that allows you to expose your application outside of the cluster by assigning a static port on each cluster node. It's one of the ways to make your application accessible from outside the Kubernetes cluster.</p> <ul> <li>The assigned <code>NodePort</code> should be open in the firewall rules of your cluster's network configuration to allow incoming traffic.</li> <li>We can access the application using any node's IP address and the assigned NodePort, such as <code>http://192.168.0.100:30007</code></li> <li>default HTTP port (port 80)</li> <li>default HTTPS port (port 443) <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: argocd-server-nodeport\n  namespace: argocd\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/part-of: argocd\nspec:\n  type: NodePort\n  selector:\n    app.kubernetes.io/name: argocd-server\n  ports:\n    - name: http\n      port: 80\n      protocol: TCP\n      targetPort: 8080\n      nodePort: 30007\n    - name: https\n      port: 443\n      protocol: TCP\n      targetPort: 8080\n      nodePort: 30008\n  sessionAffinity: None\n</code></pre></li> </ul>"},{"location":"DevOps/Kubernetes/ServiceAccount/","title":"ServiceAccount","text":"<p>A ServiceAccount is an object that provides an identity for a pod or a group of pods running within a cluster. It allows the pods to interact with the Kubernetes API server and other authorized resources securely.</p>"},{"location":"DevOps/Kubernetes/SortBy/","title":"Sort By","text":"<pre><code>kubectl top pod &lt;pod-name&gt; --sort-by=cpu                 #cpu/memory\nkubectl get services --sort-by=.metadata.name            #name\nkubectl get events --sort-by=.metadata.creationTimestamp #timestamp\nkubectl get pv --sort-by=.spec.capacity.storage          #capacity\nkubectl get pods --sort-by='.status.containerStatuses[0].restartCount'\n</code></pre>"},{"location":"DevOps/Kubernetes/StatefulSet/","title":"StatefulSet","text":"<p>https://medium.com/avmconsulting-blog/deploying-statefulsets-in-kubernetes-k8s-5924e701d327</p> <p>StatefulSet Components - A Headless Service - A StatefulSet - A PersistentVolume</p>"},{"location":"DevOps/Kubernetes/StatefulSet/#scaling-statefulset","title":"Scaling StatefulSet","text":"<p>https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/ <pre><code>kubectl scale sts web --replicas=5                 #scale up\nkubectl patch sts web -p '{\"spec\":{\"replicas\":3}}' #scale down\n</code></pre></p> <p>https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/ - You cannot scale down a StatefulSet when any of the stateful Pods it manages is <code>unhealthy</code>. - Scaling down only takes place after those stateful Pods become running and ready.</p>"},{"location":"DevOps/Kubernetes/StatefulSet/#rollout-restart-statefulset","title":"rollout restart StatefulSet","text":"<p>Stateful set are removed following their ordinal index with the highest ordinal index first.</p> <p><code>kubectl rollout restart</code> is applicable for deployments, daemonsets and statefulsets. - <code>partitions</code>  if in StatefulSet we have <code>updateStrategy.rollingUpdate.partition: 1</code> it will restart all pods with index 1 or higher. <pre><code>kubectl rollout restart statefulset demo -n dev\nkubectl rollout status statefulset demo -n dev\n</code></pre></p>"},{"location":"DevOps/Kubernetes/StatefulSet/#force-delete-statefulset-pod","title":"Force delete StatefulSet pod","text":"<p>https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/ graceful pod deletion <pre><code>kubectl delete pods &lt;pod&gt;\n</code></pre></p> <p>A Pod is not deleted automatically when a node is unreachable. The Pods running on an unreachable Node enter the <code>Terminating</code> or <code>Unknown</code> state after a timeout. Pods may also enter these states when the user attempts graceful deletion of a Pod on an unreachable Node.</p> <p>The only ways in which a Pod in such a state can be removed from the apiserver are as follows: - The Node object is deleted (either by you, or by the Node Controller). - The kubelet on the unresponsive Node starts responding, kills the Pod and removes the entry from the apiserver. - Force deletion of the Pod by the user.</p> <p>Force delete StatefulSet pod <pre><code>kubectl delete pods &lt;pod&gt; --grace-period=0 --force\n#if the pod is stuck on Unknown state after delete, remove the pod from the cluster using\nkubectl patch pod &lt;pod&gt; -p '{\"metadata\":{\"finalizers\":null}}'\n</code></pre></p>"},{"location":"DevOps/Kubernetes/App/PostgreSql/","title":"postgresql","text":""},{"location":"DevOps/Kubernetes/App/PostgreSql/#deploy-postgresql-in-aks","title":"deploy postgresql in aks","text":"<p>https://learn.microsoft.com/en-us/azure/aks/deploy-postgresql-ha</p>"},{"location":"DevOps/Kubernetes/App/Registry/","title":"Registry","text":""},{"location":"DevOps/Kubernetes/App/Registry/#kubelet-pull-images-from-internal-registry-using-local-url-not-possible","title":"kubelet pull images from internal registry using local url not possible","text":"<ul> <li>https://github.com/k3s-io/k3s/issues/1581</li> <li>Containerd runs containers for the Kubelet, but runs on the host and is not itself part of the cluster nor aware of its existence.</li> <li>If the host you're running K3s on cannot resolve in-cluster DNS names, then neither can containerd.</li> <li>hack: use <code>NodePort</code> - expose static port to k8s node: https://medium.com/@lumontec/running-container-registries-inside-k8s-6564aed42b3a</li> </ul>"},{"location":"DevOps/Kubernetes/App/Registry/#local-k8s-local-registry","title":"local k8s + local registry","text":"<p>https://hackernoon.com/kubernetes-cluster-setup-with-a-local-registry-and-ingress-in-docker-using-kind</p>"},{"location":"DevOps/Kubernetes/App/Registry/#aks-on-prem-registry-self-signed-cert","title":"aks + on-prem-registry + self-signed-cert","text":"<p>Add self-signed-cert to the aks nodes: - https://discuss.kubernetes.io/t/aks-on-prem-registry-self-signed-cert/9308/2 - solution: copy the cert file via a daemonset to the aks nodes - details: https://github.com/coreos/tectonic-docs/blob/master/Documentation/admin/add-registry-cert.md - other discussion: https://stackoverflow.com/questions/59723924/azure-kubernetes-service-self-signed-cert-on-private-registry</p>"},{"location":"DevOps/Kubernetes/App/Registry/#delete-docker-images-from-registry","title":"delete docker images from registry","text":"<p>https://teplyheng.medium.com/how-to-completely-remove-docker-images-from-a-docker-registry-v2-76d8a26847ff</p>"},{"location":"DevOps/Kubernetes/Design/Mistake/","title":"common mistakes","text":"<p>https://medium.com/@seifeddinerajhi/most-common-mistakes-to-avoid-when-using-kubernetes-anti-patterns-%EF%B8%8F-f4d37586528d - resources: cpu, mem - ignore health check - avoid using <code>latest</code> tag - overprivileged containers - lack of monitoring and logging - default namespace for all objects - missing security configurations: Authorization, Networking, Storage - missing poddisruptionbudget: avoid unnecessary service outages due to draining nodes</p>"},{"location":"DevOps/Kubernetes/Issues/Auth/","title":"Auth","text":""},{"location":"DevOps/Kubernetes/Issues/Auth/#kubernetes-client-api-error-401-unauthorized","title":"Kubernetes client API error 401 - Unauthorized","text":"<p>add configuration to cluster config <pre><code>kubectl create clusterrolebinding serviceaccounts-cluster-admin --clusterrole=cluster-admin --group=system:serviceaccounts\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Issues/Auth/#you-must-be-logged-in-to-the-server-the-server-has-asked-for-the-client-to-provide-credentials","title":"You must be logged in to the server (the server has asked for the client to provide credentials)","text":"<p>maybe due to issues in <code>.kube/config</code> file.  Solution: update the credentials if using aks <code>az aks get-credentials -n &lt;aks-name&gt; -g &lt;resource-group&gt;</code></p>"},{"location":"DevOps/Kubernetes/Issues/Auth/#the-connection-to-the-server-xxxxxxxxport-was-refused-did-you-specify-the-right-host-or-port","title":"The connection to the server xx.xx.xx.xx:port was refused - did you specify the right host or port?","text":"<p>Possible reasons - config not set: <code>export KUBECONFIG=$HOME/.kube/config</code> - certs expired: <code>kubeadm alpha certs check-expiration</code></p>"},{"location":"DevOps/Kubernetes/Issues/Auth/#kubectl-connect-connection-refused","title":"Kubectl connect: connection refused","text":"<p>detailed steps to check the issues: https://medium.com/@texasdave2/troubleshoot-kubectl-connection-refused-6f5445a396ed</p>"},{"location":"DevOps/Kubernetes/Issues/Auth/#kube-apiserver-log-but-certs-are-renewed","title":"kube-apiserver log, but certs are renewed","text":"<p>Unable to authenticate the request due to an error apiserver x509: certificate has expired or is not yet valid</p> <p>solution: after cert renewal restart the master node</p>"},{"location":"DevOps/Kubernetes/Issues/Auth/#check-component-status","title":"check component status","text":"<pre><code>kubectl get componentstatuses\n</code></pre>"},{"location":"DevOps/Kubernetes/Issues/Auth/#check-ports","title":"check ports","text":"<pre><code>sudo netstat -lnpt | grep kube\nnetstat -a | grep 6443\n</code></pre>"},{"location":"DevOps/Kubernetes/Issues/Auth/#check-firewall","title":"check firewall","text":"<pre><code>#ubuntu\nsudo ufw status verbose\nsudo ufw disable\nsudo ufw enable\n#make sure kube-apiserver can get through port 6443\nsudo ufw allow 6443/tcp\n#try telnet from another host to your apiserver\ntelnet MASTER-IP 6443\n</code></pre>"},{"location":"DevOps/Kubernetes/Issues/Auth/#check-kubeconfig-file","title":"check kubeconfig file","text":"<pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\ncd .kube\nls -la\n</code></pre>"},{"location":"DevOps/Kubernetes/Issues/Auth/#check-master-node-server-address-that-kubectl-running-from","title":"check master node server address that kubectl running from","text":"<pre><code>ifconfig\ncat config\n</code></pre>"},{"location":"DevOps/Kubernetes/Issues/Auth/#restart-kubelet","title":"restart kubelet","text":"<pre><code>### do not do this on a dev or production install\nsudo systemctl stop kubele\n\nsudo systemctl start kubelet\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre>"},{"location":"DevOps/Kubernetes/Issues/Auth/#restart-docker","title":"restart docker","text":"<pre><code>sudo systemctl stop docker\nsudo systemctl start docker\n</code></pre>"},{"location":"DevOps/Kubernetes/Issues/CronJob/","title":"CronJob","text":""},{"location":"DevOps/Kubernetes/Issues/CronJob/#startingdeadlineseconds","title":"<code>startingDeadlineSeconds</code>","text":"<p>Error: Cannot determine if job needs to be started: Too many missed start time (&gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.</p> <p>https://stackoverflow.com/questions/51065538/what-does-kubernetes-cronjobs-startingdeadlineseconds-exactly-mean/51080214#51080214</p> <ul> <li>if <code>startingDeadlineSeconds</code> = 200, it will count how many missed jobs occurred in the last 200 seconds. </li> <li>if <code>startingDeadlineSeconds</code> is not set, the job will be attempted to start by the CronJob controller without checking if it's later or not.</li> </ul>"},{"location":"DevOps/Kubernetes/Issues/CronJob/#cronjob-controller","title":"<code>CronJob controller</code>","text":"<ul> <li>The CronJob controller will check the every 10 seconds the list of cronjobs in the given Kubernetes Client.</li> <li>For every CronJob, it checks how many schedules it missed in the duration from the lastScheduleTime till now.    If there are more than 100 missed schedules, then it doesn't start the job and records the event.</li> </ul>"},{"location":"DevOps/Kubernetes/Issues/Debug/","title":"Debug","text":"<p>https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/#create-a-simple-pod-to-use-as-a-test-environment</p> <p>Sometimes there are network issue we need to solve but the existing pod does not have the diagnostic tools installed. In this case, we can create a test pod in the same namespace.</p>"},{"location":"DevOps/Kubernetes/Issues/Debug/#create-a-simple-pod-to-use-as-a-test-environment","title":"Create a simple Pod to use as a test environment","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnsutils\n  namespace: default\nspec:\n  containers:\n  - name: dnsutils\n    image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3\n    command:\n      - sleep\n      - \"infinity\"\n    imagePullPolicy: IfNotPresent\n  restartPolicy: Always\n</code></pre> <p>Create the pod and run into it <pre><code>kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml\nkubectl get pods dnsutils\nkubectl exec -it dnsutils -- bash\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Issues/Job/","title":"Job","text":""},{"location":"DevOps/Kubernetes/Issues/Job/#job-invalid-spectemplate-field-is-immutable","title":"Job invalid: spec.template field is immutable","text":"<p>The job is already deployed and cannot be deployed again. Should delete it first.</p>"},{"location":"DevOps/Kubernetes/Issues/Kubectl/","title":"Kubectl","text":""},{"location":"DevOps/Kubernetes/Issues/Kubectl/#the-server-could-not-find-the-requested-resource","title":"the server could not find the requested resource","text":"<p>when run <code>kubectl version</code> on worker node - the client version might be too far alway from the server version - what else?</p>"},{"location":"DevOps/Kubernetes/Issues/Network/","title":"Network","text":""},{"location":"DevOps/Kubernetes/Issues/Network/#dns_probe_finished_nxdomain","title":"DNS_PROBE_FINISHED_NXDOMAIN","text":"<p>DNS could not be resolved - https://github.com/Azure/AKS/issues/1320 - https://github.com/projectcalico/calico/issues/8284 - private DNS server or private link was deleted?</p>"},{"location":"DevOps/Kubernetes/Issues/Node/","title":"Node","text":""},{"location":"DevOps/Kubernetes/Issues/Node/#taint","title":"taint","text":"<p>The node controller automatically taints a Node when certain conditions are true. </p>"},{"location":"DevOps/Kubernetes/Issues/Node/#0x-nodes-are-available-y-nodes-were-unschedulable","title":"0/x nodes are available y node(s) were unschedulable","text":"<p>Node might have issues: - <code>Taint: node.kubernetes.io/unschedulable:NoSchedule</code> - <code>Unschedulable:true</code> <pre><code>kubectl get nodes #check status\nkubectl describe node &lt;node-name&gt; #check details\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Issues/Node/#state-not-ready","title":"state not ready","text":"<p>https://www.containiq.com/post/debugging-kubernetes-nodes-in-not-ready-state <pre><code>kubectl get pods -n kube-system -o wide\nkubectl describe pod &lt;pod-name&gt; -n kube-system\nkubectl logs &lt;pod-name&gt; -n kube-system\nkubectl describe daemonset kube-proxy -n kube-system\n\nkubectl describe node nodeName\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Issues/Pod/","title":"Pod","text":""},{"location":"DevOps/Kubernetes/Issues/Pod/#imagepullbackoff","title":"ImagePullBackOff","text":"<p>The name of the image might be not correct. Test it: <pre><code>docker pull &lt;repository&gt;/&lt;image&gt;:&lt;tag&gt;\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Issues/Pod/#new-pods-stuck-in-pending-without-events","title":"New pods stuck in Pending without events","text":"<p>If <code>kube-scheduler</code> is not running on the master, Kubernetes will report everything as okay but will be unable to start containers in a pod.</p> <p>Check <code>kube-scheduler</code> log <pre><code>kubectl get po -A | grep kube-schedule\nkubectl logs &lt;kube-scheduler-name&gt; -n kube-system --tail=20 --timestamps=true\n</code></pre> if related to certificate and certificate not expire - after renew certs restart master node.</p>"},{"location":"DevOps/Kubernetes/Issues/Pod/#cronjob-pod-get-killed-while-executing","title":"Cronjob pod get killed while executing","text":"<p>https://stackoverflow.com/questions/68643381/why-does-my-kubernetes-cronjob-pod-get-killed-while-executing</p> <p>The scale down would fail, because it was always preceded by a scale up, in which point a new node was in the cluster.</p> <p>This node had nothing running on it, so the next Job was scheduled on it. The Job started on the new node, told Azure to scale down the new node to 0, and subsequently the Kubelet killed the job as it was running.</p> <p>Fix: I changed the spec and added a NodeSelector so that the Job would always run on the system pool, which is more stable than the user pool</p>"},{"location":"DevOps/Kubernetes/Issues/Pod/#error-determining-status-error-response-from-daemon-readlink-varlibdockeroverlay2lxxxxx-no-such-file-or-directory","title":"error determining status: Error response from daemon: readlink /var/lib/docker/overlay2/l/xxxxx: no such file or directory","text":"<p>https://icode.best/i/32712543535032</p> <p>https://www.cnblogs.com/haoprogrammer/p/11534092.html</p> <p>Most likely due to disk error, solution (go to the node with issues) <pre><code># set node to be unschedulable\nkubectl cordon &lt;node-name&gt;\nkubectl drain &lt;node-name&gt; --ignore-daemonsets\n\n# check file number\ncd /var/lib/docker\nls -l overlay2 | wc -l\n\n# clean docker disk\ndocker system prune --all\n\n# check file number again\nls -l overlay2 | wc -l\n\n# reset node as schedulable\nkubectl uncordon &lt;node-name&gt;\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Issues/QA/","title":"QA","text":""},{"location":"DevOps/Kubernetes/Issues/QA/#how-to-mount-azure-key-vault-as-volume","title":"how to mount azure key-vault as volume","text":"<p>A: see <code>Azure/KeyVault</code>, <code>DevOps/Helm/Secret</code></p>"},{"location":"DevOps/Kubernetes/Issues/RepoMigration/","title":"legacy-package-repository-deprecation","text":"<p>https://kubernetes.io/blog/2023/08/31/legacy-package-repository-deprecation/</p>"},{"location":"DevOps/Kubernetes/Issues/RepoMigration/#check-if-using-the-legacy-package-repo","title":"check if using the legacy package repo","text":"<p><pre><code>cat /etc/apt/sources.list.d/kubernetes.list\n</code></pre> If the repository definition not use <code>pkgs.k8s.io</code> we need to migrate</p>"},{"location":"DevOps/Kubernetes/Issues/RepoMigration/#migrate-to-the-new-community-operated-repositories","title":"migrate to the new community-operated repositories","text":"<p>https://kubernetes.io/blog/2023/08/15/pkgs-k8s-io-introduction/</p> <ol> <li> <p>Replace the apt repository definition so that apt points to the new repository <pre><code>echo \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\n</code></pre></p> </li> <li> <p>Download the public signing key for the Kubernetes package repositories <pre><code>mkdir /etc/apt/keyrings\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n</code></pre></p> </li> <li> <p>Update the apt package index <pre><code>sudo apt-get update\n</code></pre></p> </li> </ol>"},{"location":"DevOps/Kubernetes/Issues/RepoMigration/#fix-redis-repo-issue","title":"fix redis repo issue","text":"<p>https://answers.launchpad.net/ubuntu/+source/redis/+question/708354 <pre><code>curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n</code></pre></p> <p>Previous fix not relavent to this error: <pre><code>E: Repository 'https://packages.redis.io/deb focal InRelease' changed its 'Origin' value from '' to 'packages.redis.io'\nN: Repository 'https://packages.redis.io/deb focal InRelease' changed its 'Suite' value from '' to 'focal'\nN: This must be accepted explicitly before updates for this repository can be applied. See apt-secure(8) manpage for details.\n</code></pre> Solution: <pre><code>sudo apt-get update --allow-releaseinfo-change\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Network/ClusterIP/","title":"ClusterIP","text":""},{"location":"DevOps/Kubernetes/Network/ClusterIP/#k8s-clusterip-nodeport-and-loadbalancer","title":"k8s: ClusterIp, NodePort, and LoadBalancer","text":"<p>https://kodekloud.com/blog/clusterip-nodeport-loadbalancer/</p> <ul> <li>ClusterIp: pod to pod</li> <li>NodePort: expose port to node</li> <li>LoadBalancer: scaling</li> </ul>"},{"location":"DevOps/Kubernetes/Network/DNS/","title":"DNS","text":"<p>https://learn.microsoft.com/en-us/troubleshoot/azure/azure-kubernetes/troubleshoot-dns-failure-from-pod-but-not-from-worker-node</p>"},{"location":"DevOps/Kubernetes/Network/DNS/#how-it-works","title":"How it works","text":"<p>For DNS resolution, the pods send requests to the <code>CoreDNS</code> pods in the kube-system namespace - If the DNS query is for an internal component, such as a service name, the <code>CoreDNS</code> pod responds by itself  - If the request is for an external domain, the CoreDNS pod sends the request to the <code>upstream DNS server</code></p> <p>The <code>upstream DNS servers</code> are obtained based on the <code>resolv.conf</code> file of the worker node in which the pod is running.  The resolv.conf file (/run/systemd/resolve/resolv.conf) is updated based on the DNS settings of the virtual network on which the worker node is running.</p>"},{"location":"DevOps/Kubernetes/Network/DNS/#debug-dns","title":"debug DNS","text":"<p>https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/</p> <p>First create a test pod in the same namespace <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnsutils\n  namespace: &lt;namespace&gt;\nspec:\n  containers:\n  - name: dnsutils\n    image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3\n    command:\n      - sleep\n      - \"infinity\"\n    imagePullPolicy: IfNotPresent\n  restartPolicy: Always\n</code></pre></p> <p>Then run commands <pre><code>kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml\nkubectl get pods dnsutils\nkubectl exec -it dnsutils -- nslookup kubernetes.default\nkubectl exec -it dnsutils -- cat /etc/resolv.conf\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Topic_Cert/KubernetesCertificate/","title":"Kubernetes Certificate","text":""},{"location":"DevOps/Kubernetes/Topic_Cert/SelfSignedCertificate/","title":"Self-signed Certificate","text":"<p>How to deploy a website such as web api using self-signed certificate?</p>"},{"location":"DevOps/Kubernetes/Topic_Cert/SelfSignedCertificate/#tls-secret","title":"tls <code>secret</code>","text":"<p>We need to put the private/public tls certificate to a Kubernetes Secret.</p>"},{"location":"DevOps/Kubernetes/Topic_Cert/SelfSignedCertificate/#how-to-add-certificate-to-the-pod","title":"how to add certificate to the pod","text":"<p>To update CA certificates within a specific container running in a pod, you should consider  - incorporating the necessary CA certificates directly into your container image or - mounting them as a volume into your pod.</p> <p>Update the Container Image: When building your container image, include the CA certificates you need. You can typically add them to the <code>/etc/ssl/certs/</code> directory in your image. This way, your container already has the necessary CA certificates when it starts.</p>"},{"location":"DevOps/Kubernetes/Topic_Cert/SelfSignedCertificate/#mount-as-volume","title":"mount as volume","text":"<ul> <li>Use a ConfigMap or Secret: Store your CA certificates in a Kubernetes ConfigMap or Secret.</li> <li>Then, mount this ConfigMap or Secret as a volume inside your pod.</li> <li>Your application within the container can use these certificates from the mounted volume.</li> </ul> <p>mount <code>ConfigMap</code> as volume into a pod <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ca-certificates\ndata:\n  my-ca.crt: |\n    -----BEGIN CERTIFICATE-----\n    ...\n    -----END CERTIFICATE-----\n\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-container\n    image: my-image:tag\n    volumeMounts:\n    - name: ca-certs\n      mountPath: /etc/ssl/certs\n      readOnly: true\n  volumes:\n  - name: ca-certs\n    configMap:\n      name: ca-certificates\n</code></pre> In this example, we create a ConfigMap (ca-certificates) with your CA certificate(s). Then, we mount it into the container at <code>/etc/ssl/certs</code>.</p> <p>Remember that this approach assumes that your application inside the container is configured to use the CA certificates from the specified directory.</p> <p>If you need to update CA certificates dynamically in a running pod, you'll need to do it within your application itself, as Kubernetes pods are designed to be immutable, and system-level operations like update-ca-certificates aren't directly supported.</p> <p>why not in <code>/usr/local/share/ca-certificates/extra/</code>? - The choice of where to store CA certificates within your container depends on your application's requirements and how the application expects to find and use CA certificates. /etc/ssl/certs/ is commonly used in many Linux distributions, so it's often a good choice for compatibility. - If your application is specifically configured to look in /usr/local/share/ca-certificates/extra/ for CA certificates, then you can certainly use that directory instead. Just make sure to configure your application accordingly so that it knows where to find the CA certificates.</p> <p>Mount Secret as volume in a pod</p> <p>create a secret <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: ca-certificates-secret\ndata:\n  my-ca.crt: BASE64_ENCODED_CERTIFICATE_DATA\n</code></pre></p> <p>mount secret as volume <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-container\n    image: your-image:tag\n    volumeMounts:\n    - name: ca-certs\n      mountPath: /etc/ssl/certs\n      readOnly: true\n  volumes:\n  - name: ca-certs\n    secret:\n      secretName: ca-certificates-secret\n</code></pre> Using a Secret is a good approach for storing sensitive data like certificates because it provides encryption at rest. Just make sure to base64-encode the certificate data before storing it in the Secret, as shown in the example above. You can easily update the Secret when the certificates expire or need updating without modifying the Docker image or the pod definition. The updated certificates will be available to your pods.</p>"},{"location":"DevOps/Kubernetes/Topic_Cert/SelfSignedCertificate/#mount-multiple-certificates-to-the-same-path","title":"mount multiple certificates to the same path","text":"<p>create two secrets. Another option is put multiple certificates in the same secret. <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: certificate-secret-1\ndata:\n  certificate.crt: BASE64_ENCODED_CERTIFICATE_DATA_1\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: certificate-secret-2\ndata:\n  certificate.crt: BASE64_ENCODED_CERTIFICATE_DATA_2\n</code></pre></p> <p>Create a Volume that combines these Secrets or ConfigMaps using a projected volume: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-container\n    image: your-image:tag\n    volumeMounts:\n    - name: certificates-volume\n      mountPath: /etc/ssl/certs\n  volumes:\n  - name: certificates-volume\n    projected:\n      sources:\n      - secret:\n          name: certificate-secret-1\n          items:\n            - key: certificate.crt\n              path: certificate1.crt\n      - secret:\n          name: certificate-secret-2\n          items:\n            - key: certificate.crt\n              path: certificate2.crt\n</code></pre> In this example, we create a projected volume named certificates-volume that combines two Secrets (certificate-secret-1 and certificate-secret-2) and maps their certificate data to different file paths (certificate1.crt and certificate2.crt) within the /etc/ssl/certs directory. You can access these certificates in your application using the respective file paths.</p>"},{"location":"DevOps/Kubernetes/Topic_Cert/SelfSignedCertificate/#mount-a-secret-to-an-existing-path-in-docker-image","title":"mount a secret to an existing path in docker image","text":"<p>If your Docker image already includes files at a specific path, and you want to mount a Kubernetes Secret at the same path within a running container, it depends on whether the mount point in your pod's definition conflicts with the existing files in your Docker image. Here's what you should consider: - Mount Conflict: If the path you want to mount the Secret to already contains files within your Docker image, Kubernetes will typically mount the Secret over the existing files, effectively replacing them during the pod's execution. -  File Overwrite: When you mount a Secret or ConfigMap into a directory that already contains files within the container, the files from the Secret or ConfigMap will overwrite the existing files within that directory for the duration of the pod's lifecycle.</p> <p>Here's an example:</p> <p>Suppose your Docker image has a certificate file at <code>/etc/ssl/certs/my-cert.crt</code>, and you mount a Secret or ConfigMap to the same path in your pod's definition. In that case, the certificate file from the Secret or ConfigMap will replace the existing <code>my-cert.crt</code> file within the container.</p> <p>If your application expects specific files to be present in that directory and requires those files to have specific content, be cautious when overwriting them with mounted Secrets or ConfigMaps. Make sure the certificates in your Secret or ConfigMap are compatible with what your application expects.</p> <p>If you want to merge the contents of the mounted Secret or ConfigMap with the existing files, you may need to handle the merge logic within your application or entrypoint script.</p> <p>It's generally a good practice to ensure that the contents of your Secrets or ConfigMaps are compatible with the expectations of your application and won't lead to unexpected issues when files are overwritten.</p>"},{"location":"DevOps/Kubernetes/Topic_Volume/ConfigMap/","title":"ConfigMap","text":"<p>https://kubernetes.io/docs/concepts/storage/volumes/</p> <ul> <li>A ConfigMap is always mounted as <code>readOnly</code></li> <li>A container using a ConfigMap as a <code>subPath</code> volume mount will not receive ConfigMap updates</li> <li>Text data is exposed as files using the UTF-8 character encoding. For other character encodings, use binaryData</li> </ul>"},{"location":"DevOps/Kubernetes/Topic_Volume/ConfigMap/#configmap-mount-example","title":"ConfigMap mount example","text":"<p>The <code>log-config</code> ConfigMap is mounted as a volume, and all contents stored in its <code>log_level</code> entry are mounted into the Pod at path <code>/etc/config/log_level</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: configmap-pod\nspec:\n  containers:\n    - name: test\n      image: busybox:1.28\n      command: ['sh', '-c', 'echo \"The app is running!\" &amp;&amp; tail -f /dev/null']\n      volumeMounts:\n        - name: config-vol\n          mountPath: /etc/config\n  volumes:\n    - name: config-vol\n      configMap:\n        name: log-config\n        items:\n          - key: log_level\n            path: log_level\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Topic_Volume/PersistentVolume/","title":"persistent-volume","text":"<p>https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/</p> <ul> <li>use <code>PersistentVolume</code> to create a persistent-volume</li> <li>use <code>PersistentVolumeClaim</code> to request physical storage for pods</li> </ul>"},{"location":"DevOps/Kubernetes/Topic_Volume/PersistentVolume/#types","title":"types","text":"<p>https://kubernetes.io/docs/concepts/storage/persistent-volumes/ - csi - nfs - local - azureFile (replaced by azureFile CSI migration)</p>"},{"location":"DevOps/Kubernetes/Topic_Volume/PersistentVolume/#delete-pvpvc","title":"delete pv/pvc","text":"<p><pre><code>PersistentVolumeClaim   is invalid: spec: Forbidden:\nspec is immutable after creation except resources.requests for bound\n</code></pre> https://stackoverflow.com/questions/60175522/kubernetes-persistentvolumeclaim-error-forbidden-is-immutable-after-creation</p> <p>solution: comment out <pre><code>finalizers:\n  - kubernetes.io/pv-protection\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Topic_Volume/PersistentVolume/#pvc-volume-already-bound-to-a-different-claim","title":"pvc volume already bound to a different claim","text":"<p>https://vmninja.wordpress.com/2021/08/10/kubernetes-volume-x-already-bound-to-a-different-claim/</p> <p>solution: edit <code>resourceVersion</code> and <code>uid</code> in <code>pv</code> section <code>claimRef</code> using the value in <code>pvc</code>.</p>"},{"location":"DevOps/Kubernetes/Topic_Volume/Secret/","title":"Secret","text":"<p>https://kubernetes.io/docs/concepts/configuration/secret/</p> <p>Secrets are similar to <code>ConfigMaps</code> but are specifically intended to hold confidential data.</p> <ul> <li>A secret volume is used to pass sensitive information, such as passwords, to Pods.</li> <li>A Secret is always mounted as readOnly.</li> <li>A container using a Secret as a subPath volume mount will not receive Secret updates.</li> </ul>"},{"location":"DevOps/Kubernetes/Topic_Volume/Secret/#security","title":"security","text":"<ul> <li>Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd</li> <li>Anyone who is authorized to create a Pod in a namespace can use that access to read any Secret in that namespace; this includes indirect access such as the ability to create a Deployment.</li> </ul>"},{"location":"DevOps/Kubernetes/Topic_Volume/Secret/#mount-example","title":"mount example","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-dotfiles-pod\nspec:\n  containers:\n    - name: dotfile-test-container\n      image: registry.k8s.io/busybox\n      command:\n        - ls\n        - \"-l\"\n        - \"/etc/secret-volume\"\n      volumeMounts:\n        - name: secret-volume          \n          mountPath: \"/etc/secret-volume\"\n          readOnly: true\n  volumes:\n    - name: secret-volume\n      secret:\n        secretName: dotfile-secret\n</code></pre> <p>In this example, the secret is a Kuberneters Secret resource. The following terraform code creats a k8s secret. <pre><code># https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/secret\nresource \"kubernetes_secret\" \"main\" {\n  depends_on = [kubernetes_namespace.ns]\n  metadata {\n    name        = \"k8s-secret\"\n    labels      = {}\n    annotations = {}\n    namespace   = var.namespace\n  }\n  data = {\n    \"dev.user\" = file(\"//example.com/config/dev.user\")\n    \"dev.pass\" = file(\"//example.com/config/dev.pass\")\n  }\n}\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Topic_Volume/StorageClass/","title":"StorageClass","text":"<p>https://kubernetes.io/docs/concepts/storage/storage-classes/</p>"},{"location":"DevOps/Kubernetes/Topic_Volume/StorageClass/#default-storageclass","title":"default storageclass","text":"<p>When a PVC does not specify a storageClassName, the default StorageClass is used.</p>"},{"location":"DevOps/Kubernetes/Topic_Volume/StorageClass/#set-default-storageclass","title":"set default storageclass","text":"<p>https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/</p> <p>Mark a StorageClass as default: <pre><code>kubectl patch storageclass &lt;class-name&gt; -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n</code></pre></p> <p>Mark a StorageClass as non-default: <pre><code>kubectl patch storageclass &lt;class-name&gt; -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}'\n</code></pre></p>"},{"location":"DevOps/Kubernetes/Topic_WebApi/WebApi/","title":"WebApi","text":"<p>Traffic flow: <pre><code>Internet &lt;-&gt; Load Balancer &lt;-&gt; Ingress &lt;-&gt; Middleware &lt;-&gt; Service &lt;-&gt; Pods\n</code></pre></p> <p>To ensure an api is accessible from the public internet, we should provide <code>ingress</code> and <code>service</code>, the <code>load balancer</code> is provided on the Kubernetes level.</p>"},{"location":"DevOps/Kubernetes/k8s/APIServer/","title":"kube-apiserver","text":""},{"location":"DevOps/Kubernetes/k8s/APIServer/#cluster-info","title":"cluster info","text":"<p>detailed information about the overall health of the cluster <pre><code>kubectl cluster-info\nkubectl cluster-info dump\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/APIServer/#check-kube-apiserver-is-running-and-hasnt-crashed","title":"check kube-apiserver is running and hasn't crashed","text":"<p>https://discuss.kubernetes.io/t/the-connection-to-the-server-localhost-8080-was-refused-did-you-specify-the-right-host-or-port/1464/3 <pre><code>docker ps | grep kube-apiserver\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/APIServer/#check-kube-apiserver-ssl-cert","title":"check kube-apiserver SSL cert","text":"<p>https://stackoverflow.com/questions/67132520/kubelet-service-cant-access-kube-apiserver-at-port-6443-with-https-due-to-error <pre><code>curl -kv https://172.24.4.159:6443\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/APIServer/#check-kube-apiserver-health","title":"check kube-apiserver health","text":"<p>https://kubernetes.io/docs/reference/using-api/health-checks/ <pre><code>curl -k https://localhost:6443/livez?verbose\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/APIServer/#why-disable-swap-on-kubernetes","title":"Why disable swap on kubernetes","text":"<p>https://serverfault.com/questions/881517/why-disable-swap-on-kubernetes</p> <ul> <li>Support for swap is non-trivial.</li> <li>Guaranteed pods should never require swap.</li> <li>Burstable pods should have their requests met without requiring swap.</li> <li>BestEffort pods have no guarantee.</li> <li>The kubelet right now lacks the smarts to provide the right amount of predictable behavior here across pods.</li> </ul>"},{"location":"DevOps/Kubernetes/k8s/APIServer/#the-connection-to-the-server-ip6443-was-refused-did-you-specify-the-right-host-or-port","title":"The connection to the server ip:6443 was refused - did you specify the right host or port?","text":"<p>https://discuss.kubernetes.io/t/the-connection-to-the-server-host-6443-was-refused-did-you-specify-the-right-host-or-port/552</p> <p>https://www.reddit.com/r/kubernetes/comments/10aya7n/master_node_not_accessible_after_a_few_minutes</p> <ul> <li>certs expired   <pre><code>kubeadm alpha certs check-expiration\n</code></pre></li> <li>has config setup correctly?   <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\nexport KUBECONFIG=$HOME/.kube/config\n</code></pre></li> <li>disable SWAP <code>swapoff -a</code> <pre><code>#to make it permanent go to /etc/fstab\nsudo -i\nswapoff -a\nexit\nstrace -eopenat kubectl version\nsudo systemctl restart kubelet.service\n</code></pre>   further steps?   <pre><code>#remove the &amp;&amp; and \\ and copy and paste each line in the command line, this will ensure swap is disabled and persistent across a reboot.\nsudo dphys-swapfile swapoff &amp;&amp; \\\nsudo dphys-swapfile uninstall &amp;&amp; \\\nsudo systemctl disable dphys-swapfile\n</code></pre></li> <li>check if docker was running</li> <li>checked if kubelet was running, any log errors?</li> <li>disk is under 20GB but it is 100GB already <code>df -H</code></li> <li>reboot the master</li> <li>firewall blocks the port   <pre><code>sudo systemctl status firewalld #redhat centos\nsudo systemctl stop firewalld #redhat, centos\nsudo ufw status verbose #ubuntu\nsudo ufw disable #ubuntu\n</code></pre></li> <li>looking with netstat to see if something is running on port 6443</li> <li><code>kubeadm reset</code> on master and worker nodes?</li> </ul>"},{"location":"DevOps/Kubernetes/k8s/Certificate/","title":"Certificate","text":"<p>https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/</p> <p>Kubernetes can be configured to certificate rotation.  However, there are cases the certificate rotation does not work. In this situation, we need to check and renew the certificates manually.</p> <p>If not mentioned specifically, all the following commands are executed in the k8s master node.</p>"},{"location":"DevOps/Kubernetes/k8s/Certificate/#cert-types","title":"Cert types","text":"<p>There are two types of certs in kubernetes - <code>kubeadm</code> for master\u3001etcd: renew via <code>kubeadm alpha certs renew all</code> - <code>kubelet</code> for kubelet: connection between node and master (usually 1 year expiration)</p>"},{"location":"DevOps/Kubernetes/k8s/Certificate/#check-certs","title":"check certs","text":"<p>https://serverfault.com/questions/1068751/var-lib-kubelet-pki-kubelet-crt-is-expired-how-to-renew-it</p> <p>check <code>/var/lib/kubelet/pki/kubelet.crt</code>, or really for the certificate signed by the kube-apiserver, not specifically the kubeadm or kubectl certificates? <pre><code>echo -n | openssl s_client -connect localhost:10250 2&gt;&amp;1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep -A 2 Validity\n</code></pre></p> <p>why this command gave the wrong info? <pre><code>sudo openssl x509 -in /var/lib/kubelet/pki/kubelet.crt -text -noout  | grep -A 2 Validity\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/Certificate/#enabling-signed-kubelet-serving-certificates","title":"Enabling signed kubelet serving certificates","text":"<p>https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/</p> <p>If you have already created the cluster you must adapt it by doing the following: - Find and edit the <code>kubelet-config-1.xx</code> ConfigMap in the kube-system namespace.   In that ConfigMap, the kubelet key has a KubeletConfiguration document as its value.   Edit the KubeletConfiguration document to set <code>serverTLSBootstrap: true</code>. - On each node, add the <code>serverTLSBootstrap: true</code> field in <code>/var/lib/kubelet/config.yaml</code> and restart the kubelet with <code>systemctl restart kubelet</code>.</p>"},{"location":"DevOps/Kubernetes/k8s/Certificate/#certificate-rotation-auto-renewal-settings","title":"Certificate rotation (auto renewal) settings","text":"<ul> <li>https://kubernetes.io/docs/tasks/tls/certificate-rotation/</li> <li>https://github.com/kubernetes/kubeadm/issues/2024</li> </ul> <p>https://www.leiyawu.com/2020/10/11/Untitled/ - from v1.8, to auto reload certs when start kubelet add option <code>\u2013rotate-certificates</code> - kubelet auto renew certs, when start add <code>\u2013feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true</code> - controller manager auto approve CSRs, when start add <code>\u2013feature-gates=RotateKubeletServerCertificate=true</code> and bind RBAC rules</p> <p>If the certificate rotation is not enabled or not setup correctly, we should check and setup the configuration.</p> <p>After enable the rotation <code>/var/lib/kubelet/pki/kubelet.crt</code> is no longer used,  instead the symbolic link <code>/var/lib/kubelet/pki/kubelet-server-current.pem</code> is used and points to the latest rotated certificate.</p>"},{"location":"DevOps/Kubernetes/k8s/Certificate/#config-kubelet","title":"config kubelet","text":"<p>Make sure the following options are in <code>/var/lib/kubelet/config.yaml</code> <pre><code>rotateCertificates: true #enable client cert rotation\nserverTLSBootstrap: true #enable server cert rotation\nfeatureGates:\n  RotateKubeletClientCertificate: true\n  RotateKubeletServerCertificate: true\n</code></pre> <pre><code>--rotate-certificates\n--feature-gates=RotateKubeletClientCertificate=true\n--feature-gates=RotateKubeletServerCertificate=true\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/Certificate/#config-kube-controller-manager","title":"config kube-controller-manager","text":"<p>Make sure the folowing options are in <code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code> <pre><code>spec:\n  containers:\n  - command:\n    - --experimental-cluster-signing-duration=17520h0m0s  #2 years\n    - --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true #auto approve csr\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/Certificate/#create-server-clusterrole-and-clusterrolebinding","title":"Create Server ClusterRole and ClusterRoleBinding","text":"<p>system:certificates.k8s.io:certificatesigningrequests:selfnodeserver <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver\nrules:\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - certificatesigningrequests/selfnodeserver\n  verbs:\n  - create\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: kubeadm:node-autoapprove-certificate-server\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n</code></pre> create ClusterRole <code>kubectl apply -f ca-update.yaml</code>. show all ClusterRoles <code>k get ClusterRole -A</code>.</p> <p>We can also create the ClusterRoleBinding using cli: certificatesigningrequests:selfnodeserver: auto renew certs for <code>kubelet 10250 api</code> in <code>system:nodes</code> group <pre><code>kubectl create clusterrolebinding kubeadm:node-autoapprove-certificate-server \\\n  --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes\n</code></pre> Included in <code>ca-update.yaml</code>.</p>"},{"location":"DevOps/Kubernetes/k8s/Certificate/#create-client-clusterrolebinding","title":"Create Client ClusterRoleBinding","text":"<p>certificatesigningrequests:nodeclient: auto approve TLS bootstrapping first request by <code>kubelet-bootstrap</code> users <pre><code>kubectl create clusterrolebinding kubeadm:node-autoapprove-bootstrap \\\n  --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --user=kubelet-bootstrap\n</code></pre></p> <p>certificatesigningrequests:selfnodeclient: auto renew certs for <code>kubelet</code> and <code>apiserver</code> in <code>system:nodes</code> group <pre><code>kubectl create clusterrolebinding kubeadm:node-autoapprove-certificate-rotation \\\n  --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes\n</code></pre></p> <p>Edit kubelet-config-1.xx <pre><code>k get cm -A\nkubectl edit configmap -n &lt;namespace&gt; &lt;config-map-name&gt; -o yaml\n</code></pre></p> <p>Restart <code>kube-controller-manager</code> and <code>kubelet</code> services <pre><code>#restart controller-manager\nsystemctl restart kube-controller-manager\n\n#delete kubelet certs in ssl\nrm -f kubelet-client-current.pem kubelet-client-2021-05-10-09-45-21.pem kubelet.key kubelet.crt\n\n#restart kubelet, new certs will be renewed\nsystemctl restart kubelet\n\n#check renewed certs\nopenssl x509 -noout -text -in kubelet-client-current.pem | grep \"Not\"\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/Certificate/#control-plane-approval","title":"Control plane approval","text":"<p><code>journalctl -xeu kubelet -n 100 --no-pager</code> error: no serving certificate available for the kubelet <pre><code>#get the list of CSRs\nkubectl get csr -A\n\n#approve a CSR with kubectl\nkubectl certificate approve &lt;csr-name&gt;\n\n#deny a CSR\nkubectl certificate deny &lt;csr-name&gt;\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/Certificate/#manually-approve-all-pending-csrs","title":"manually approve all pending CSRs","text":"<p>If there are errors like <code>error: no serving certificate available for the kubelet from journalctl -xeu kubelet -n 100 --no-pager</code>, run <pre><code>kubectl get csr -A | grep Pending | awk '{print $1}' | xargs kubectl certificate approve\nkubectl get csr -A | grep Pending | tr -s ' ' | cut -d' ' -f1 | while IFS= read -r csr; do kubectl certificate approve $csr; done\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/Certificate/#get-a-certificate-signing-request-every-15-minutes","title":"get-a-certificate-signing-request-every-15-minutes","text":"<p>https://serverfault.com/questions/1112910/i-get-a-certificate-signing-request-every-15-minutes-kubernetes</p> <p>kubelet log: certificate_manager.go:451] certificate request was not signed: timed out waiting for the condition</p> <p>kubernetes CSR in pending status <pre><code>user@example:$ k get csr -A\nNAME        AGE     SIGNERNAME                                    REQUESTOR               CONDITION\ncsr-2444s   13h     kubernetes.io/kube-apiserver-client-kubelet   system:node:node1       Pending\n</code></pre> <code>kubernetes.io/kube-apiserver-client-kubelet</code>: signs client certificates that will be honored as client certificates by the API server. May be auto-approved by <code>kube-controller-manager</code>.</p> <p>starting the management cluster!</p>"},{"location":"DevOps/Kubernetes/k8s/Certificate/#auto-approval-for-server-csrs-was-removed","title":"auto approval for server CSRs was removed","text":"<p>https://github.com/kubernetes/kubernetes/issues/73356</p> <p>The CSR approving controllers implemented in core Kubernetes do not approve node serving certificates for security reasons. To use RotateKubeletServerCertificate operators need to run a custom approving controller, or manually approve the serving certificate requests.</p>"},{"location":"DevOps/Kubernetes/k8s/ClientCert/","title":"Client Cert","text":""},{"location":"DevOps/Kubernetes/k8s/ClientCert/#kubelet-client-cert","title":"Kubelet client certificate rotation fails","text":"<p>https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#kubelet-client-cert</p> <p>https://github.com/kubernetes/website/blob/6ea53189723803c363d4de1c0cc76dd0cd98ff39/content/en/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm.md#kubelet-client-certificate-rotation-fails-kubelet-client-cert</p> <p>By default, kubeadm configures a kubelet with automatic rotation of client certificates by using the /var/lib/kubelet/pki/kubelet-client-current.pem symlink specified in /etc/kubernetes/kubelet.conf. If this rotation process fails you might see errors such as x509: certificate has expired or is not yet valid in kube-apiserver logs.</p> <p>To fix the issue you must follow these steps: - Backup and delete /etc/kubernetes/kubelet.conf and /var/lib/kubelet/pki/kubelet-client* from the failed node. - From a working control plane node in the cluster that has /etc/kubernetes/pki/ca.key   execute kubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE &gt; kubelet.conf.   $NODE must be set to the name of the existing failed node in the cluster.   Modify the resulted kubelet.conf manually to adjust the cluster name and server endpoint, or pass kubeconfig user --config (it accepts InitConfiguration).   If your cluster does not have the ca.key you must sign the embedded certificates in the kubelet.conf externally. - Copy this resulted kubelet.conf to /etc/kubernetes/kubelet.conf on the failed node. - Restart the kubelet (systemctl restart kubelet) on the failed node and wait for /var/lib/kubelet/pki/kubelet-client-current.pem to be recreated. - Manually edit the kubelet.conf to point to the rotated kubelet client certificates, by replacing client-certificate-data and client-key-data with:   <pre><code>client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem\nclient-key: /var/lib/kubelet/pki/kubelet-client-current.pem\n</code></pre> - Restart the kubelet. - Make sure the node becomes Ready.</p>"},{"location":"DevOps/Kubernetes/k8s/Deploy/","title":"Deploy","text":"<p>A kubernetes cluster can be deployed as system services (kubeadm version) or containers.</p>"},{"location":"DevOps/Kubernetes/k8s/Health/","title":"Health","text":""},{"location":"DevOps/Kubernetes/k8s/Health/#check-pods-not-running-completed","title":"check pods not running / completed","text":"<pre><code>kubectl get po -A -o wide | egrep -v 'Running|Completed'\n</code></pre>"},{"location":"DevOps/Kubernetes/k8s/Health/#check-containers-of-running-pods-are-ready","title":"check containers of running pods are ready","text":"<pre><code>kubectl get po -A -o wide | egrep 'Running'\n</code></pre>"},{"location":"DevOps/Kubernetes/k8s/Health/#check-status-of-nodes","title":"check status of nodes","text":"<pre><code>kubectl get nodes\n</code></pre>"},{"location":"DevOps/Kubernetes/k8s/Health/#check-storage-health","title":"check storage health","text":"<pre><code>kubectl -n &lt;namespace&gt; get pvc\n</code></pre>"},{"location":"DevOps/Kubernetes/k8s/Health/#check-kubelet","title":"check kubelet","text":"<pre><code>systemctl status kubelet\n</code></pre>"},{"location":"DevOps/Kubernetes/k8s/Health/#clusterrole","title":"clusterrole","text":"<pre><code>kubectl describe clusterrole system:kube-scheduler\n</code></pre>"},{"location":"DevOps/Kubernetes/k8s/Health/#check-k8s-health-status","title":"check k8s health status","text":"<p><pre><code>kubectl get componentstatuses\n</code></pre> This command will be the easiest way to discover if your <code>scheduler</code>, <code>controller-manager</code> and <code>etcd</code> node(s) are healthy. <pre><code>Warning: v1 ComponentStatus is deprecated in v1.19+\nNAME               STATUS    MESSAGE      ERROR\nscheduler          Unhealthy Get \"https://127.0.0.1:10259/healthz\":\n                                dial tcp 127.0.0.1:10259: connect: connection refused\ncontroller-manager Unhealthy Get \"https://127.0.0.1:10257/healthz\":\n                                dial tcp 127.0.0.1:10257: connect: connection refused\netcd-0             Healthy   {\"health\":\"true\"}\n</code></pre></p> <p>Solution - check component logs <pre><code>kubectl logs -n kube-system &lt;scheduler-pod-name&gt;\nkubectl logs -n kube-system &lt;controller-manager-pod-name&gt;\n</code></pre> - Check Service Status: ensure the services are running and have valid ip <pre><code>kubectl get svc -n kube-system\n</code></pre> - Check Component Health Probes: A healthy component should return an HTTP 200 OK response. <pre><code>curl http://&lt;scheduler-ip&gt;:10251/healthz\ncurl http://&lt;controller-manager-ip&gt;:10252/healthz\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/KubeAdm/","title":"KubeAdm","text":""},{"location":"DevOps/Kubernetes/k8s/KubeAdm/#connect-to-k8s-master-ubuntu","title":"connect to k8s master (ubuntu)","text":"<pre><code>ssh username@machine.example.com\n</code></pre>"},{"location":"DevOps/Kubernetes/k8s/KubeAdm/#list-images","title":"list images","text":"<pre><code>kubeadm config images list\n</code></pre>"},{"location":"DevOps/Kubernetes/k8s/KubeAdm/#sudo-ls-etckubernetesmanifests","title":"sudo ls /etc/kubernetes/manifests/","text":"<ul> <li>etcd.yaml</li> <li>kube-apiserver.yaml</li> <li>kube-controller-manager.yaml</li> <li>kube-scheduler.yaml</li> </ul>"},{"location":"DevOps/Kubernetes/k8s/KubeAdm/#ca-path-etckubernetespki","title":"CA path <code>/etc/kubernetes/pki</code>","text":"<p>The <code>apiserver-kubelet-client</code> is the client cert that API server will present to a kubelet. check to determine that kubelet is configured to trust clients that are signed by the k8s CA <code>cat /var/lib/kubelet/config.yaml</code>. <pre><code>$ sudo ls -al /etc/kubernetes/pki\ntotal 68\ndrwxr-xr-x 3 root root 4096 Aug  5  2020 .\ndrwxr-xr-x 4 root root 4096 Aug  5  2020 ..\n-rw-r--r-- 1 root root 1025 Aug  5  2020 ca.crt\n-rw------- 1 root root 1679 Aug  5  2020 ca.key\ndrwxr-xr-x 2 root root 4096 Aug  5  2020 etcd\n-rw-r--r-- 1 root root 1038 Aug  5  2020 front-proxy-ca.crt\n-rw------- 1 root root 1679 Aug  5  2020 front-proxy-ca.key\n-rw------- 1 root root 1675 Aug  5  2020 sa.key\n-rw------- 1 root root  451 Aug  5  2020 sa.pub\n-rw-r--r-- 1 root root 1220 Feb  1 11:08 apiserver.crt\n-rw------- 1 root root 1679 Feb  1 11:08 apiserver.key\n-rw-r--r-- 1 root root 1090 Feb  1 11:08 apiserver-etcd-client.crt\n-rw------- 1 root root 1679 Feb  1 11:08 apiserver-etcd-client.key\n-rw-r--r-- 1 root root 1099 Feb  1 11:08 apiserver-kubelet-client.crt\n-rw------- 1 root root 1679 Feb  1 11:08 apiserver-kubelet-client.key\n-rw-r--r-- 1 root root 1058 Feb  1 11:08 front-proxy-client.crt\n-rw------- 1 root root 1675 Feb  1 11:08 front-proxy-client.key\n</code></pre></p> <p><code>kubeadm alpha certs check-expiration</code> will check certs in <code>/etc/kubernetes/pki</code> and cert in <code>KUBECONFIG</code> file used by kubeadm\uff08admin.conf, controller-manager.conf and scheduler.conf.</p>"},{"location":"DevOps/Kubernetes/k8s/KubeAdm/#cert-path-varlibkubeletpkikubelet","title":"cert path <code>/var/lib/kubelet/pki/kubelet.*</code>","text":"<pre><code>$ sudo ls -al /var/lib/kubelet/pki\ntotal 27\ndrwxr-xr-x 2 root root 4096 Mar 13  2023 .\ndrwx------ 8 root root 4096 Aug  5  2021 ..\n-rw------- 1 root root 1070 May 20  2022 kubelet-client-2022-05-20-14-29-52.pem\n-rw------- 1 root root 1070 Mar 10  2023 kubelet-client-2023-03-10-03-43-08.pem\nlrwxrwxrwx 1 root root   59 Mar 10  2023 kubelet-client-current.pem -&gt; /var/lib/kubelet/pki/kubelet-client-2022-03-10-03-43-08.pem\n-rw-r--r-- 1 root root 2173 Aug  5  2021 kubelet.crt\n-rw------- 1 root root 1679 Aug  5  2021 kubelet.key\n</code></pre>"},{"location":"DevOps/Kubernetes/k8s/KubeAdm/#kubeadm-certs","title":"kubeadm-certs","text":"<ul> <li>https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs</li> <li>https://www.chernsan.com/2021/02/09/etcd-certificates-expired/</li> </ul> <p>Kubernetes certificates expire after one year.</p> <p>kubeadm creates certs under <code>/var/lib/kubelet/pki/kubelet.*</code> signed with a different CA from the one under <code>/etc/kubernetes/pki/ca.pem</code>.</p>"},{"location":"DevOps/Kubernetes/k8s/KubeAdm/#check-certs","title":"check certs","text":"<pre><code>kubeadm alpha certs check-expiration\nkubeadm certs check-expiration        #new might not work\n</code></pre>"},{"location":"DevOps/Kubernetes/k8s/KubeAdm/#renew-certs","title":"renew certs","text":"<p><code>kubelet.conf</code> will not be renewed by <code>kubeadm alpha certs renew all</code>. <pre><code>#do we need this before renewal?\nkubeadm init phase kubelet-finalize all #'symlink' kebelet.conf to kubelet cert rotation\n\nkubeadm alpha certs renew all           #renew all certs\nkubeadm alpha certs renew &lt;CERTIFICATE&gt; #renew one cert\nkubeadm certs renew &lt;CERTIFICATE&gt;       #new might not work\n\n#do we need this after renewal?\ncd /etc/kubernetes\nkubeadm alpha kubeconfig user --org system:nodes --client-name system:node:$(hostname) &gt; kubelet.conf\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/KubeAdm/#update-config","title":"update config","text":"<p>https://www.oak-tree.tech/blog/k8s-cert-yearly-renewwal <pre><code>cd ~/.kube\nrm config.old\nmv config config.old                      #archive old config\nsudo cp /etc/kubernetes/admin.conf config #copy new config\nsudo chown $(id -u):$(id -g) config       #aplly permissions to admin user and group\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/KubeAdm/#copy-the-config-file-to-all-worker-nodes","title":"copy the config file to all worker nodes","text":"<p>The updated <code>admin.conf</code> file should also be copied to <code>.kube/config</code> in all worker nodes with the right permissions being applied.</p>"},{"location":"DevOps/Kubernetes/k8s/KubeAdm/#do-we-need-to-do-more-to-fix-the-issue","title":"do we need to do more to fix the issue?","text":"<p>https://serverfault.com/questions/1065444/how-can-i-find-which-kubernetes-certificate-has-expired</p> <p>https://github.com/kubernetes/kubernetes/issues/102556</p> <p>Kubeadm does not manage the kubelet cert rotation. It is automatic by default unless the user opts out.</p>"},{"location":"DevOps/Kubernetes/k8s/Learn/","title":"Learn","text":"<p>install k8s: https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/06-4.kubelet.md</p> <p>add new node: https://blog.csdn.net/qq_27156945/article/details/102599336</p> <p>cert renewal: https://ost.51cto.com/posts/12458</p> <p>deploy: https://blog.51cto.com/juestnow/2435471</p>"},{"location":"DevOps/Kubernetes/k8s/Logs/","title":"Logs","text":"<p>https://theithollow.com/2020/02/12/kubernetes-logs-for-troubleshooting</p>"},{"location":"DevOps/Kubernetes/k8s/Logs/#kubelet-service-log","title":"<code>kubelet</code> service log","text":"<p>Any components deployed as systemd services can be found in the Linux journal on the host in which the service resides. This could include etcd logs, kubelet logs, or any of the components running as a service.</p> <p>The <code>kubelet</code> runs on every node in the cluster. Logs can be accessed by using the <code>journalctl</code> command on the linux host: <pre><code>journalctl -xeu kubelet\njournalctl -xeu kubelet -n 100 --no-pager\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/Logs/#container-log","title":"container log","text":"<p>If using <code>kubectl</code> is possible <pre><code>kubectl logs -h\nkubectl logs &lt;pod-name&gt;\nkubectl logs &lt;pod-name&gt; --tail=20\nkubectl logs &lt;pod-name&gt; --since=1h\nkubectl logs &lt;pod-name&gt; --timestamps=true\n</code></pre></p> <p>Get the logs through container runtime if <code>kubectl</code> is not available <pre><code>docker logs &lt;container-id&gt;\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/Logs/#kube-sheduler-pod-log","title":"<code>kube-sheduler</code> pod log","text":"<ul> <li><code>kube-scheduler</code> pod's log: same as using <code>kubeclt logs</code></li> <li><code>kube-scheduler</code> container's log: same as kube-scheduler pod's log.</li> <li><code>kube-controller-manager</code></li> <li><code>kube-apiserver</code></li> </ul>"},{"location":"DevOps/Kubernetes/k8s/MetricsServer/","title":"MetricsServer","text":"<p>https://github.com/kubernetes-sigs/metrics-server</p> <p>Kubernetes Metrics Server measures CPU and memory usage and shows resource usage statistics across the cluster.</p>"},{"location":"DevOps/Kubernetes/k8s/MetricsServer/#install","title":"install","text":"<p><pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n</code></pre> Installed conponents <pre><code>service/metrics-server\nserviceaccount/metrics-server\ndeployment.apps/metrics-server\napiservice.apiregistration.k8s.io/v1betal.metrics.k8s.io\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader\nclusterrole.rbac.authorization.k8s.io/system: aggregated-metrics-reader\nclusterrole.rbac.authorization.k8s.io/system: metrics-server\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server: system: auth-delegator\nclusterrolebinding.rbac.authorization.k8s.io/system: metrics-server\n</code></pre></p>"},{"location":"DevOps/Kubernetes/k8s/MetricsServer/#error-the-server-could-not-find-the-requested-resource-get-services-httpheapster","title":"error: the server could not find the requested resource (get services http:heapster:)","text":"<p>Error from <code>kubectl top nodes</code>, indicating that the metrics server <code>apiserver</code> is missing or not work correctly.</p>"},{"location":"DevOps/Kubernetes/k8s/Overview/","title":"Overview","text":"<ul> <li>A <code>Kubernetes cluster</code> consists of a set of worker machines, called nodes, that run containerized applications.</li> <li>The <code>worker node</code>(s) host the Pods that are the components of the application workload.</li> <li>The <code>control plane</code> manages the worker nodes and the Pods in the cluster.</li> </ul>"},{"location":"DevOps/Kubernetes/k8s/Overview/#control-plane-components","title":"Control Plane Components","text":"<ul> <li><code>etcd</code>: Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.</li> <li><code>kube-apiserver</code>:  API server is the front end for the Kubernetes control plane.</li> <li><code>kube-scheduler</code>: Watche for newly created Pods with no assigned node, and selects a node for them to run on.</li> <li><code>kube-controller-manager</code>: Node, Job, EndpointSlice, ServiceAccount controllers.</li> <li><code>cloud-controller-manager</code>: Link cluster into cloud provider's API,    and separate out the components that interact with that cloud platform from components that only interact with your cluster.</li> </ul>"},{"location":"DevOps/Kubernetes/k8s/Overview/#node-components","title":"Node Components","text":"<ul> <li><code>kubelet</code>: Make sure that containers are running in a Pod.</li> <li><code>kube-proxy</code>: Maintain network rules on nodes.</li> <li><code>Container runtime</code>: Responsible for running containers.</li> </ul>"},{"location":"DevOps/Kustomize/Kustomize/","title":"Kustomize","text":"<ul> <li>Helm excels in managing complex applications with multiple configurations, while</li> <li>Kustomize is a solid choice for fine-tuning and <code>patching</code> existing manifests</li> </ul>"},{"location":"DevOps/MonitorPerf/Dynatrace/","title":"Dynatrace","text":""},{"location":"DevOps/MonitorServer/Datadog/","title":"Datadog","text":""},{"location":"DevOps/MonitorServer/Grafana/","title":"Grafana","text":"<p>Grafana is a dashboarding solution that can be used to visualize Prometheus metrics. It allows you to create custom dashboards that track the health of your Kubernetes cluster.</p>"},{"location":"DevOps/MonitorServer/Splunk/","title":"Splunk","text":""},{"location":"DevOps/MonitorServer/Tool/","title":"Tool","text":"<p>pprof: a tool for visualization and analysis of profiling data. pprof reads a collection of profiling samples in profile.</p>"},{"location":"DevOps/Network/Authorization/","title":"Authorization","text":""},{"location":"DevOps/Network/Authorization/#add-auth2-to-a-web-api-deployed-in-aks","title":"Add auth2 to a web api deployed in aks","text":"<ul> <li>install <code>thomseddon/traefik-forward-auth</code> forward service: Deployment, Service, IngressRoute, Middleware</li> <li>create an <code>app registration</code> in azure for the api</li> <li>access the api using the token extracted with the app registration </li> </ul>"},{"location":"DevOps/Network/tcp/","title":"tcp","text":""},{"location":"DevOps/Network/tcp/#test-tcp-connection","title":"test tcp connection","text":"<pre><code>cat &lt; /dev/tcp/127.0.0.1/21\n</code></pre>"},{"location":"DevOps/Nginx/FileServer/","title":"File server","text":""},{"location":"DevOps/Nginx/FileServer/#nginxconf","title":"nginx.conf","text":"<pre><code>events {}\nhttp {\n    server {\n        location / {\n            autoindex on; # automatically generate a directory listing when a user requests a directory\n            root /data;   # all file paths within the location / block will be relative to this root\n        }\n    }\n}\n</code></pre>"},{"location":"DevOps/Nginx/Nginx/","title":"nginx","text":""},{"location":"DevOps/Nginx/Nginx/#config-guide","title":"config guide","text":"<p>https://nginx.org/en/docs/beginners_guide.html</p>"},{"location":"DevOps/Python/Conda/","title":"Conda","text":""},{"location":"DevOps/Python/Conda/#cache-whl-and-conda-packages","title":"cache whl and conda packages","text":"<p>cache the packages so will not extract from remote</p>"},{"location":"DevOps/Python/Conda/#cache-conda-env-in-ci","title":"cache conda env in CI","text":"<p>https://stackoverflow.com/questions/69542082/dependency-caching-in-python-ci-pipeline-in-azure-devops</p> <p>The installation of new package duiring a CI build will take a long time. We can cache the env and reuse it to reduce the build time.</p>"},{"location":"DevOps/SSL/Authority/","title":"Authority","text":""},{"location":"DevOps/SSL/Authority/#authority-info-access-with-comma","title":"Authority Info Access with comma","text":"<p>https://mta.openssl.org/pipermail/openssl-users/2021-January/013291.html</p> <p>put the uri in another section <pre><code>authorityInfoAccess = @v3_aia\n\n[ v3_aia ]\ncaIssuers;URI=ldap:///CN=Example%20Certification%20Authority,CN=AIA,CN=Public%20Key%20Services,CN=Services,CN=Configuration,DC=EXAMPLE,DC=com?cACertificate?base?objectClass=certificationAuthority\n</code></pre></p>"},{"location":"DevOps/SSL/CA/","title":"CA","text":""},{"location":"DevOps/SSL/CA/#create-key","title":"create key","text":"<pre><code>openssl genrsa -out ca_test.key 2048\n</code></pre>"},{"location":"DevOps/SSL/CA/#create-crt","title":"create crt","text":"<pre><code>openssl req -new -x509 -days 365 -key ca_test.key -out ca_test.crt -config ca.cnf\n</code></pre>"},{"location":"DevOps/SSL/CA/#cacnf","title":"ca.cnf","text":"<p>A full example here:  https://mta.openssl.org/pipermail/openssl-users/2021-January/013291.html <pre><code># OpenSSL configuration for Root CA\n\noid_section = oids\n\n[ oids ]\ncertificateTemplate     = 1.3.6.1.4.1.311.20.2\ncaVersion               = 1.3.6.1.4.1.311.21.1\ncertificateTemplateName = 1.3.6.1.4.1.311.21.2\n\n[ req ]\nprompt             = no\nstring_mask        = default\n\ndefault_bits       = 2048\ndistinguished_name = req_distinguished_name\nx509_extensions    = x509_ext\n\n[ req_distinguished_name ]\nDC   = com\n1.DC = Example\nCN   = Example Certification Authority\n\n[ x509_ext ]\ncertificateTemplate     = ASN1:BMP:CA\nkeyUsage                = digitalSignature, keyCertSign, cRLSign\nbasicConstraints        = critical, CA:true\nsubjectKeyIdentifier    = hash\ncaVersion               = ASN1:INTEGER:0\ncertificateTemplateName = ASN1:PRINTABLESTRING:X?......o.....5K...c\n#certificateTemplateName = ASN1:BMP:EXAMPLE-CA\n</code></pre></p>"},{"location":"DevOps/SSL/CAIntermediate/","title":"Intermediate CA","text":""},{"location":"DevOps/SSL/CAIntermediate/#ordering-certificate-when-using-intermediates","title":"Ordering certificate when using intermediates","text":"<p>When using intermediate certificate(s), we need to make sure that: - the application using the certificate is sending the complete chain (<code>server</code> certificate and <code>intermediate</code> certificate).  - and usually we have to create a file containing the <code>server</code> certificate file and the <code>intermediate</code> certificate file.  - it is required to put the <code>server</code> certificate file first, and then the <code>intermediate</code> certificate file(s).  - we can create the correct file for the chain using: <code>cat cert.pem intermediate.pem &gt; chain.pem</code></p>"},{"location":"DevOps/SSL/CAMicrosoft/","title":"CA microsoft","text":""},{"location":"DevOps/SSL/CAMicrosoft/#oids-when-issued-by-windows-enterprise-ca","title":"OIDs when issued by Windows enterprise CA","text":"<p>https://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-wcce/c8bec234-0a53-4a7c-859d-2bb7b2537da5 - <code>1.3.6.1.4.1.311.21.7</code> Certificate Template Information   - This extension value MUST be <code>DER-encoded</code>   - https://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-wcce/9da866e5-9ce9-4a83-9064-0d20af8b2ccf - <code>1.3.6.1.4.1.311.21.10</code> Application Policies</p>"},{"location":"DevOps/SSL/CAMicrosoft/#certificate-template","title":"certificate template","text":"<p>https://social.technet.microsoft.com/Forums/Lync/en-US/decc0a03-4d53-40e4-b05c-494c13551fda/how-to-duplicate-a-certificate-template-using-a-command?forum=winserversecurity</p>"},{"location":"DevOps/SSL/Cert/","title":"Cert","text":""},{"location":"DevOps/SSL/Cert/#check-cert","title":"check cert","text":"<pre><code>ll /etc/ssl/certs/ | grep my-cert\nll /usr/local/share/ca-certificates/ | grep my-certs\nll /usr/local/share/ca-certificates/extra | grep my-certs\n</code></pre>"},{"location":"DevOps/SSL/Cert/#command","title":"command","text":"<pre><code>openssl x509 -req -days 1000 -in tls.csr -signkey tls.key -out tls.crt -extfile v3.ext\n</code></pre>"},{"location":"DevOps/SSL/Cert/#v3-extension","title":"v3 extension","text":"<pre><code># OpenSSL configuration for v3 extension\n\n#1.3.6.1.4.1.311.21.7 = ASN1:PRINTABLESTRING:0..&amp;+.....x............./...{....&amp;.......v..d...\nextendedKeyUsage = serverAuth\nkeyUsage = digitalSignature, Key Encipherment\n1.3.6.1.4.1.311.21.10 = ASN1:PRINTABLESTRING:0.0 #???\nsubjectKeyIdentifier = hash\nsubjectAltName = @alt_names\nauthorityKeyIdentifier = keyid, issuer\ncrlDistributionPoints = @crldp_section\n#not working\nauthorityInfoAccess = caIssuers;URI:ldap:///CN=Example%20Certification%20Authority,CN=AIA,CN=Public%20Key%20Services,CN=Services,CN=Configuration,DC=EXAMPLE,DC=com?cACertificate?base?objectClass=certificationAuthority\n\n[ alt_names ]\nDNS.1 = *.my.example.com\nDNS.2 = *.dev.my.example.com\n\n[ crldp_section ]\n#fullname=URI: obsolete should not be used: https://www.mail-archive.com/openssl-users@openssl.org/msg55684.html\nURI.1 = ldap:///CN=Example%20Certification%20Authority,CN=DC1PXXCS01,CN=CDP,CN=Public%20Key%20Services,CN=Services,CN=Configuration,DC=EXAMPLE,DC=com?certificateRevocationList?base?objectClass=cRLDistributionPoint\n</code></pre>"},{"location":"DevOps/SSL/Config/","title":"Config","text":"<p>https://gist.github.com/oz9un/60cb7d491989002578d7d763dfda333c</p>"},{"location":"DevOps/SSL/Docker/","title":"Docker","text":""},{"location":"DevOps/SSL/Docker/#x509-certificate-signed-by-unknown-authority","title":"x509: certificate signed by unknown authority","text":"<p>After adding the tls.crt to the path <code>/etc/ssl/certs</code> or <code>/usr/local/share/ca-certificates</code> and run <code>update-ca-certificates</code>. The command <code>curl -kv https://my.example.com</code> shows <code>SSL certificate verify ok</code> but docker build shows <code>x509: certificate signed by unknown authority</code>.</p> <p>solution: https://stackoverflow.com/questions/50768317/docker-pull-certificate-signed-by-unknown-authority</p> <p>After updating OS certificates, we typically need to restart the docker service to get it to detect that change.  This is usually done with: <pre><code>sudo systemctl restart docker\n</code></pre> or for non-systemd environments: <pre><code>sudo service docker restart\n</code></pre></p>"},{"location":"DevOps/SSL/Error/","title":"Error","text":""},{"location":"DevOps/SSL/Error/#unable-to-get-local-issuer-certificate","title":"Unable to get Local Issuer Certificate","text":"<p>The root certificates on the system are not working correctly, misconfigured, such as using a root ca cert that is not used to create the cert.</p>"},{"location":"DevOps/SSL/Error/#it-department-installed-a-firewall-which-intercepts-ssl-connections","title":"IT department installed a firewall which intercepts SSL connections.","text":"<p>https://serverfault.com/questions/1100480/wsl-docker-curl-60-unable-to-get-local-issuer-certificate</p>"},{"location":"DevOps/SSL/Error/#ssl-certificate_verify_failed-unable-to-get-local-issuer-certificate-_sslc1108","title":"[SSL: CERTIFICATE_VERIFY_FAILED] unable to get local issuer certificate (_ssl.c:1108)","text":"<p>https://stackoverflow.com/questions/62952004/local-issuer-certificate-error-uniquely-in-docker-with-python</p> <p>Solution: <code>export SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt</code></p> <p>Another case:\\ https://levelup.gitconnected.com/solve-the-dreadful-certificate-issues-in-python-requests-module-2020d922c72f</p> <p>The <code>consolidate.pem</code> should include all level crt files (root, intermediate, and local) in the chain <pre><code>response = requests.post(url, files=files, headers=headers, verify='consolidate.pem')\n</code></pre></p>"},{"location":"DevOps/SSL/Error/#ssl-certificate_verify_failed-unable-to-get-local-issuer-certificate-_sslc1129","title":"[SSL: CERTIFICATE_VERIFY_FAILED] unable to get local issuer certificate (_ssl.c:1129)","text":"<p>Cause: Could not find the root/intermediate ca crt files in the server - Possible solution: <code>export REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt</code> - Possible solution, if we have both root and intermediate ca files, both ca crt files should be installed in the trust store <pre><code>sudo cp root-ca.crt intermediate-ca.crt /usr/local/share/ca-certificates/extra/\nsudo update-ca-certificates\n</code></pre></p>"},{"location":"DevOps/SSL/Error/#certificate-verify-failed-self-signed-certificate-in-certificate-chain","title":"certificate verify failed: self signed certificate in certificate chain","text":"<p>Solution: - must put the server crt and the intermediate crt into one file - server crt first then intermediate crt: <code>cat server.crt intermediate-ca.crt &gt; tls.crt</code></p>"},{"location":"DevOps/SSL/Error/#verify-errornum21unable-to-verify-the-first-certificate","title":"verify error:num=21:unable to verify the first certificate","text":"<p>https://pavolkutaj.medium.com/unable-to-verify-the-first-certificate-with-openssl-47eecb652a9b</p> <p>This error means that the certificate chain is broken for OpenSSL \u2014 but does not have to be for browsers. - The crt is not added to /etc/ssl/certs/ca-certificates.crt - The pem converted from crt does not exist in /etc/ssl/certs</p>"},{"location":"DevOps/SSL/Error/#ssl-unsafe_legacy_renegotiation_disabled-unsafe-legacy-renegotiation-disabled","title":"[SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled","text":"<p>disabled in OpenSSL 3: https://www.openssl.org/docs/manmaster/man7/migration_guide.html</p> <p>https://pipeawk.com/index.php/2022/05/19/openssl-enable-legacy-renegotiation/</p> <p>Cause: <code>openssl</code> binaries are compiled with legacy renegotiation disabled by default. This disables any non TLS 1.3 libraries and certificates renegotiation to a lower standard.</p> <p>Solution: Update file <code>/etc/ssl/openssl.cnf</code>. Note: UnsafeLegacyRenegotiation (allows for man in the middle attacks) is a workaround, and should not be deployed to a production environment. - At the very beginning, insert   <pre><code>openssl_conf = openssl_init\n</code></pre> - At the end, add   <pre><code>[openssl_init]\nssl_conf = ssl_sect\n\n[ssl_sect]\nsystem_default = system_default_sect\n\n[system_default_sect]\nMinProtocol = TLSv1.2\nCipherString = DEFAULT@SECLEVEL=1\nOptions = UnsafeLegacyRenegotiation  \n</code></pre> - <code>export OPENSSL_CONF=path_to_openssl.cnf</code></p> <p>The previous solution might work  - but we might not realise that there are other issues,  - so we can get other errors like: <code>unable to get local issuer certificate</code> - it's possible the legacy checked first then check the local issuer certificate</p> <p>Even for cert that is not self-siged, we need to add the root and intermediat certs.  In dockerfile add: <pre><code>COPY docker/root-ca.crt docker/itmd-ca.crt docker/server.crt /usr/local/share/ca-certificates/extra/\nRUN update-ca-certificates\n</code></pre></p>"},{"location":"DevOps/SSL/Extension/","title":"Extension","text":""},{"location":"DevOps/SSL/Extension/#x509-extensions-for-different-types-of-certificates","title":"x509 Extensions for different types of certificates","text":"<p>https://www.golinuxcloud.com/add-x509-extensions-to-certificate-openssl/</p>"},{"location":"DevOps/SSL/Extension/#x509v3-config","title":"x509v3 config","text":"<p>https://www.openssl.org/docs/man1.0.2/man5/x509v3_config.html <pre><code>extendedKeyUsage=serverAuth\nkeyUsage=digitalSignature, Key Encipherment\nsubjectKeyIdentifier=hash\nsubjectAltName=DNS:*.my.example.com, DNS:*.dev.my.example.com\nauthorityKeyIdentifier=keyid,issuer\nauthorityInfoAccess=caIssuers;URI:http://my.ca/ca.html\ncrlDistributionPoints=crldp1_section\n\n[crldp1_section]\n\nfullname=URI:http://myhost.com/myca.crl\n\ncertificatePolicies=1.2.4.5, 1.1.3.4\n</code></pre></p>"},{"location":"DevOps/SSL/Extension/#key-usage","title":"Key usage","text":"<p>https://superuser.com/questions/738612/openssl-ca-keyusage-extension</p>"},{"location":"DevOps/SSL/Extension/#crl-distribution-points","title":"CRL distribution points","text":"<p>https://security.stackexchange.com/questions/226242/where-do-you-get-the-digicert-crl-distribution-point - Root certificates do not include a CRL (Certificate Revocation List) distribution point, because root certificates cannot be revoked. - The CRL distribution point URL is included in (all) certificates issued by that root certificate.</p> <p>Purpose - The CR is a cryptographically signed list that contains the serial number of revoked certificates issued by that particular root or intermediate CA. - Browsers nowadays do no longer use the CRL to check for revoked certificates, instead they rely on a mechanism called OCSP or OCSP Stapling. - Some browsers also ship their own list of revoked certificates with every browser update (e.g. OneCRL from Mozilla). - Other software, like Microsoft Windows, still fetches, caches and honors CRLs.</p> <p>https://stackoverflow.com/questions/11966123/howto-create-a-certificate-using-openssl-including-a-crl-distribution-point</p> <p>You can get the crlDistributionPoints into your certificate in (at least) these two ways: - Use openssl ca rather than x509 to sign the request. Pass -config as needed if your config is not in a default location. Most of your provided command can be used if you omit the options starting with -CA <pre><code>openssl ca -in $NAME.csr -out certs/$NAME.pem -days 3650\n</code></pre> - Use the command as you've provided in your question, but first create a file containing your v3 extensions (ie mycrl.cnf); add the option -extfile mycrl.cnf to your call to openssl x509 <pre><code>openssl x509 -req -in $NAME.csr -out certs/$NAME.pem -days 3650 \\\n  -CAcreateserial -CA cacert.pem -CAkey private/cakey.pem \\\n  -CAserial serial -extfile mycrl.cnf\n</code></pre> Where mycrl.cnf contains the following: <pre><code>crlDistributionPoints=URI:http://example.com/crl.pem\n</code></pre></p>"},{"location":"DevOps/SSL/Extension/#issue-different-from-subject","title":"Issue different from subject","text":"<p>https://stackoverflow.com/questions/21666516/openssl-how-do-i-set-the-issuer-details</p> <p>See this example: https://pages.cs.wisc.edu/~zmiller/ca-howto/</p> <p>Need to use the certificate authority (ca)</p>"},{"location":"DevOps/SSL/Issuer/","title":"Issuer","text":"<p>Issuer to be different from subject\\ https://pages.cs.wisc.edu/~zmiller/ca-howto/</p>"},{"location":"DevOps/SSL/Kubernetes/","title":"Kubernetes","text":""},{"location":"DevOps/SSL/Kubernetes/#show-cert-details","title":"show cert details","text":"<pre><code>openssl x509 -noout -text -in tls.crt\n</code></pre>"},{"location":"DevOps/SSL/Kubernetes/#validate-cert","title":"validate cert","text":"<p>Will show server certificate details <pre><code>curl https://my.example.com -kv\n</code></pre></p>"},{"location":"DevOps/SSL/Kubernetes/#kube-ingress-tls-certificate-issues","title":"kube ingress tls certificate issues","text":"<p>The Kubernetes Secret and the Ingress controller must be in the same namepsace: https://stackoverflow.com/questions/66469622/kubernetes-ingress-controller-not-able-to-find-the-certificate-secret</p>"},{"location":"DevOps/SSL/Kubernetes/#tls-failed-to-parse-private-key","title":"tls: failed to parse private key","text":"<p>For cetificate issues, check the ingress controller pod logs <pre><code>k -n &lt;namespace&gt; logs &lt;ingress-controller-pod&gt; | grep certificate\n</code></pre> Solution: the private key has a passphrase. The private key should be unencrypted. Note that tls.crt should include the intermediate ca. <pre><code>openssl pkcs12 -in tls.pfx -nocerts -out tls-encrypted.key\nopenssl pkcs12 -in tls.pfx -clcerts -nokeys -out tls_raw.crt\nopenssl rsa -in tls-encrypted.key -out tls.key\n</code></pre></p> <p>Not show the details about creating the cert: https://devopscube.com/configure-ingress-tls-kubernetes/</p>"},{"location":"DevOps/SSL/Kubernetes/#kube-certi-3-methods","title":"kube certi 3 methods","text":"<p>https://kubernetes.io/docs/tasks/administer-cluster/certificates/</p>"},{"location":"DevOps/SSL/Kubernetes/#kubernetes-ssl-certificates","title":"kubernetes-ssl-certificates","text":"<p>https://phoenixnap.com/kb/kubernetes-ssl-certificates</p>"},{"location":"DevOps/SSL/Kubernetes/#kubernetes-ssltls-certificates","title":"kubernetes-ssl/tls-certificates","text":"<p>https://medium.com/avmconsulting-blog/how-to-secure-applications-on-kubernetes-ssl-tls-certificates-8f7f5751d788</p>"},{"location":"DevOps/SSL/Kubernetes/#generate-certificate-files","title":"Generate Certificate Files","text":"<p>produce a 2048-bit RSA encrypted key: <pre><code>openssl genrsa -out ca.key 2048\n</code></pre></p> <p>use <code>ca.key</code> to generate <code>ca.crt</code>: <pre><code>openssl req -x509 -new -nodes -key ca.key -subj \"/CN=[my.example.com]\" -days &lt;ndays&gt; -out ca.crt\n</code></pre></p> <p>generate <code>server.key</code> file with 2048-bit RSA encryption: <pre><code>openssl genrsa -out server.key 2048\n</code></pre></p>"},{"location":"DevOps/SSL/Kubernetes/#create-certificate-configuration-file","title":"Create Certificate Configuration File","text":"<p>generate certificate signing request (csr): <pre><code>openssl req -new -key server.key -out server.csr -config csr.conf\n</code></pre></p> <p><code>csr.conf</code> <pre><code>[ req ]\ndefault_bits = 2048\nprompt = no\ndefault_md = sha256\nreq_extensions = req_ext\ndistinguished_name = dn\n\n[ dn ]\nC = [country]\nST = [state]\nL = [city]\nO = [company]\nOU = [organization-unit]\nCN = [common-name]\n\n[ req_ext ]\nsubjectAltName = @alt_names\n\n[ alt_names ]\nDNS.1 = kubernetes\nDNS.2 = kubernetes.default\nDNS.3 = kubernetes.default.svc\nDNS.4 = kubernetes.default.svc.cluster\nDNS.5 = kubernetes.default.svc.cluster.local\nIP.1 = [MASTER_IP]\nIP.2 = [MASTER_CLUSTER_IP]\n\n[ v3_ext ]\nauthorityKeyIdentifier=keyid,issuer:always\nbasicConstraints=CA:FALSE\nkeyUsage=keyEncipherment,dataEncipherment\nextendedKeyUsage=serverAuth,clientAuth\nsubjectAltName=@alt_names\n</code></pre></p>"},{"location":"DevOps/SSL/Kubernetes/#generate-the-certificate","title":"Generate the Certificate","text":"<p>create a server certificate <code>server.crt</code> with <code>ca.key</code>, <code>ca.crt</code> and <code>server.csr</code> <pre><code>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \\\n  -CAcreateserial -out server.crt -days 1000 \\\n  -extensions v3_ext -extfile csr.conf\n</code></pre></p> <p>view the created certificate <pre><code>openssl x509 -noout -text -in ./server.crt\n</code></pre></p>"},{"location":"DevOps/SSL/Kubernetes/#_1","title":"Kubernetes","text":"<p>https://kuberty.io/blog/how-to-generate-a-self-signed-certificate-for-kubernetes-2/</p>"},{"location":"DevOps/SSL/LDAP/","title":"ldap","text":"<p>https://idez.biz/connect-to-ad-using-ssl/</p>"},{"location":"DevOps/SSL/Link/","title":"Link","text":""},{"location":"DevOps/SSL/Link/#how-ssl-works-chinese","title":"how ssl works (Chinese)","text":"<p>https://www.cnblogs.com/sslwork/p/5986985.html</p>"},{"location":"DevOps/SSL/Link/#certificate-chain-config","title":"certificate chain config","text":"<p>https://medium.com/@superseb/get-your-certificate-chain-right-4b117a9c0fce</p> <p>if some root or intermediate certs are missing - browsers are smart enough to get the missing cert chain - but command line not</p>"},{"location":"DevOps/SSL/OpenSSL/","title":"OpenSSL","text":"<p>https://medium.com/@superseb/get-your-certificate-chain-right-4b117a9c0fce</p>"},{"location":"DevOps/SSL/OpenSSL/#version","title":"version","text":"<pre><code>openssl version -d\n</code></pre>"},{"location":"DevOps/SSL/OpenSSL/#openssl-30-unsafelegacyrenegotiation-disabled","title":"openssl 3.0 UnsafeLegacyRenegotiation disabled","text":"<ul> <li>https://stackoverflow.com/questions/75763525/curl-35-error0a000152ssl-routinesunsafe-legacy-renegotiation-disabled</li> <li>https://github.com/openssl/openssl/issues/21296</li> <li>python fix? https://stackoverflow.com/questions/71603314/ssl-error-unsafe-legacy-renegotiation-disabled/73519818#73519818</li> </ul>"},{"location":"DevOps/SSL/OpenSSL/#check-cert","title":"check cert","text":"<pre><code>openssl rsa -check -in server.key \nopenssl x509 -noout -text -in server.crt\nopenssl req -noout -text -verify -in server.csr\n</code></pre>"},{"location":"DevOps/SSL/OpenSSL/#check-key-cert-match","title":"check key / cert match","text":"<pre><code>openssl rsa -noout -modulus -in server.key | openssl md5\nopenssl x509 -noout -modulus -in server.crt | openssl md5\n</code></pre>"},{"location":"DevOps/SSL/OpenSSL/#verify-certificate-chain","title":"verify certificate chain","text":"<p><code>-untrusted</code> to provide intermediate certificate <pre><code>openssl verify cert.pem\nopenssl verify -CAfile ca.pem cert.pem\nopenssl verify -CAfile RootCert.crt -untrusted Intermediate.crt UserCert.crt\n</code></pre></p>"},{"location":"DevOps/SSL/OpenSSL/#verify-certificate","title":"verify certificate","text":"<pre><code>openssl s_client -connect &lt;hostname&gt;:&lt;port&gt;\nopenssl s_client -connect dev.example.com:443 -CApath /etc/ssl/certs\n</code></pre>"},{"location":"DevOps/SSL/OpenSSL/#with-config-file","title":"with config file","text":"<pre><code>openssl genrsa -out ca.key 2048\nopenssl req -x509 -new -nodes -days 365 -key ca.key -out ca.crt -subj \"/CN=my.example.com\"\n\nopenssl genrsa -out server.key 2048\nopenssl req -new -key server.key -out server.csr -config csr.conf\nopenssl x509 -req -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -extensions v3_ext -extfile csr.conf\n</code></pre>"},{"location":"DevOps/SSL/OpenSSL/#use-passphrase","title":"use passphrase","text":"<pre><code>openssl genrsa -des3 -out ca.key 2048\nopenssl req -x509 -new -nodes -days 365 -key ca.key -sha256 -out ca.pem\n\nopenssl req -new -nodes -out server.csr -newkey rsa:2048 -keyout server.key\nopenssl x509 -req -days 365 -in server.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out server.crt -sha256 -extfile v3.ext\nopenssl pkcs12 -inkey server.key -in server.crt -export -out server.pfx\n</code></pre>"},{"location":"DevOps/SSL/OpenSSL/#combine-req-and-crt-in-one-command","title":"combine req and crt in one command","text":"<pre><code>...\n</code></pre>"},{"location":"DevOps/SSL/OpenSSL/#export-csr-from-crt-and-key","title":"export csr from crt and key","text":"<pre><code>#generate root ca\nopenssl genrsa -out server.key 4096\nopenssl req -new -key server.key -out server.csr\nopenssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt\nopenssl x509 -noout -text -in server.crt\n\n#export csr from crt and key\nopenssl x509 -x509toreq -in server.crt -signkey server.key -out new-server.csr\nopenssl req -noout -text -in new-server.csr #check server.csr    \n\n#generate crt from key and new csr\nopenssl x509 -req -days 365 -in new-server.csr -signkey server.key -out new-server.crt -extfile v3.ext\nsha256sum server.crt #verify server.crt   \n</code></pre>"},{"location":"DevOps/SSL/OpenSSL/#get-issuersubject","title":"get issuer/subject","text":"<pre><code>openssl x509 -noout -issuer -in cert.pem\nopenssl x509 -noout -subject -in ca.pem\n</code></pre>"},{"location":"DevOps/SSL/Pfx/","title":"Pfx","text":""},{"location":"DevOps/SSL/Pfx/#pfx-to-key-and-crt","title":"pfx to key and crt","text":"<pre><code>openssl pkcs12 -in &lt;filename.pfx&gt; -nocerts -out &lt;keyfilename-encrypted.key&gt;\nopenssl pkcs12 -in &lt;filename.pfx&gt; -clcerts -nokeys -out &lt;certificatename.crt&gt;\nopenssl rsa -in &lt;keyfilename-encrypted.key&gt; -out &lt;keyfilename-decrypted.key&gt;\n</code></pre>"},{"location":"DevOps/SSL/Renew/","title":"Renew","text":"<p>https://www.golinuxcloud.com/renew-ssl-tls-server-certificate-openssl/</p>"},{"location":"DevOps/SSL/Renew/#server","title":"server","text":""},{"location":"DevOps/SSL/SSL/","title":"SSL","text":"<p>https://www.keyfactor.com/resources/how-to-check-ssl-certificate/</p>"},{"location":"DevOps/SSL/SSL/#types-of-ssl-certificates","title":"Types of SSL certificates","text":"<ul> <li>Domain Validated Certificate (DV)</li> <li>Organization Validated Certificate (OV)</li> <li>Extended Validation Certificate (EV)</li> </ul>"},{"location":"DevOps/SSL/SSL/#create-self-signed-ssl-certificate","title":"Create self-signed SSL certificate","text":"<p>create a <code>my-site.crt</code> file based on input <code>my-site.csr</code> <pre><code>openssl x509 -signkey my-site.key -in my-site.csr -req -days 365 -out my-site.crt\n</code></pre></p> <p>create a <code>cert.pem</code> based on pwd and pfx <pre><code>SET pwd=mypwd\nSET pfx_filepath=C:/tmp/cert.pfx\nopenssl req -x509 -sha256 -nodes -days 730 -newkey rsa:2048 -keyout private_key.key -out cert.crt -subj /CN=*.example.com\nopenssl pkcs12 -export -out %pfx_filepath% -inkey private_key.key -in cert.crt -password pass:%pwd%\nopenssl pkcs12 -in %pfx_filepath% -out cert.pem -nodes\n</code></pre></p>"},{"location":"DevOps/SSL/SSL/#renew-ssl-certificate","title":"Renew SSL Certificate","text":"<ul> <li>Generate a CSR (Certificate Signing Request)</li> <li>Select your SSL certificate and enter the required details like the validity period you need and other details and submit it to the CA. </li> <li>You will get renewed certificate files which you can use on your server. </li> <li>Renewing SSL certificates will require you to complete the same procedures you did for getting a new SSL certificate. These could be domain validation, organizational validation, and other verifications as needed for the level of certificate you are applying to the CA for.</li> </ul>"},{"location":"DevOps/SSL/SSL/#view-all-ssl-certificates-in-a-bundle","title":"view all ssl certificates in a bundle","text":"<p>https://serverfault.com/questions/590870/how-to-view-all-ssl-certificates-in-a-bundle/1079893#1079893 <pre><code>openssl storeutl -noout -text -certs bundle.crt\n</code></pre> The <code>storeutl</code> command can be used to display the contents fetched from the given URIs. - <code>-noout</code> prevents output of the PEM data - <code>-text</code> prints out the objects in text form, like the -text output from openssl x509 - <code>-certs</code> Only select the certificates from the given URI</p>"},{"location":"DevOps/SSL/SSL/#ssltsl-in-kubernetes","title":"ssl/tsl in kubernetes","text":"<p>https://medium.com/avmconsulting-blog/how-to-secure-applications-on-kubernetes-ssl-tls-certificates-8f7f5751d788 <pre><code>#create ca private key\nopensslL genrsa -out ca.key 2048\n#create self-signed certificate\nopenssl req -x509 -new -nodes -days 365 -key ca.key -out ca.crt -subj \"/CN=yourdomain.com\"\n#create tls secret using kubectl command\nkubectl create secret tls my-tls-secret --key ca.key --cert ca.crt\n</code></pre></p>"},{"location":"DevOps/SSL/SelfSigned/","title":"Browser SSL self-signed","text":"<p>renew ssl\\ https://www.golinuxcloud.com/renew-self-signed-certificate-openssl/</p> <p>create browser SSL self-signed certificate with details:\\ https://medium.com/@tbusser/creating-a-browser-trusted-self-signed-ssl-certificate-2709ce43fd15</p>"},{"location":"DevOps/SSL/SelfSigned/#why-need-tlscrt-in-the-client-machine","title":"why need tls.crt in the client machine","text":"<p>The package you received contains the certificate from that website.  The tls.crt in the client machine is used to verify that the package is not modified in the middle.</p> <p>For a self-signed certificate, if we do not have the local tls.crt we will get the error <code>self signed certificate in certificate chain</code>. To avoid the issue, we need to copy the tls.crt to <code>/etc/ssl/certs</code> or <code>/usr/local/share/ca-certificates/extra</code> and run <code>update-ca-certificates</code>.</p>"},{"location":"DevOps/SSL/SelfSigned/#create-root-certificate","title":"Create root certificate","text":""},{"location":"DevOps/SSL/SelfSigned/#create-a-private-key","title":"create a private key","text":"<pre><code>openssl genrsa -des3 -passout pass:&lt;pwd&gt; -out root.key 2048\n</code></pre>"},{"location":"DevOps/SSL/SelfSigned/#generate-root-certificate","title":"generate root certificate","text":"<p><pre><code>openssl req -x509 -new -nodes -key root.key -sha256 -days 1000 -out root.pem\n</code></pre> Input info required: - Country Name (2 letter code) [AU]:NL - State or Province Name (full name) [Some-State]:. - Locality Name (eg, city) []: - Organization Name (eg, company) [Internet Widgits Pty Ltd]: - Organizational Unit Name (eg, section) []: - Common Name (e.g. server FQDN or YOUR name) []: - Email Address []:&lt;&gt;"},{"location":"DevOps/SSL/SelfSigned/#create-ssl-certificate","title":"Create SSL certificate","text":""},{"location":"DevOps/SSL/SelfSigned/#v3ext","title":"v3.ext","text":"<p><pre><code>authorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\nsubjectAltName = @alt_names\n[alt_names]\nDNS.1 = example.com\nDNS.2 = example2.com.dev\nDNS.3 = *.example.com\n</code></pre> <code>alt_names</code> can add one or more DNSs.  </p>"},{"location":"DevOps/SSL/SelfSigned/#create-private-key-for-ssl-certificate-and-certificate-signing-request","title":"create private key for SSL certificate and certificate signing request","text":"<p><pre><code>openssl req -new -nodes -out server.csr -newkey rsa:2048 -keyout server.key\n</code></pre> Also requires some info input.</p>"},{"location":"DevOps/SSL/SelfSigned/#issue-the-certificate-with-the-previously-generated-root-certificate","title":"issue the certificate with the previously generated root certificate","text":"<pre><code>openssl x509 -req -in server.csr -CA root.pem -CAkey root.key -CAcreateserial -out server.crt -days 500 -sha256 -extfile v3.ext  \n</code></pre>"},{"location":"DevOps/SSL/SelfSigned/#combine-serverkey-and-servercrt-into-single-file","title":"combine <code>server.key</code> and <code>server.crt</code> into single file","text":"<pre><code>openssl pkcs12 -inkey server.key -in server.crt -export -out server.pfx  \n</code></pre>"},{"location":"DevOps/SSL/Subject/","title":"Subject","text":""},{"location":"DevOps/SSL/Subject/#common-name-will-be-depreciated","title":"Common Name (will be depreciated)","text":"<p>The Common Name (CN) represents the server name protected by the SSL certificate.  The certificate is valid only if the request hostname matches the certificate common name.  Most web browsers display a warning message when connecting to an address that does not match the common name in the certificate.</p>"},{"location":"DevOps/SSL/Subject/#cn-exemplecom","title":"CN = exemple.com","text":"<p>This cert will only be valid for domain <code>exemple.com</code></p>"},{"location":"DevOps/SSL/Subject/#cn-exemplecom-cn-exemplecom","title":"CN = exemple.com CN = *.exemple.com","text":"<p>In this case the cert will be valid for the domain <code>exemple.com</code> and all subdomains such as <code>blog.example.com</code>.</p> <p>It does not matter whether the SN has the star or not. The DNS should include both.</p>"},{"location":"DevOps/SSL/Subject/#subject-alternative-name","title":"Subject Alternative Name","text":"<p>The Subject Alternative Name (SAN) is an extension to the X.509 specification that allows users to specify additional host names for a single SSL certificate.  The use of the SAN extension is standard practice for SSL certificates, and it\u2019s on its way to replacing the use of the common name (CN).</p>"},{"location":"DevOps/SSL/curl/","title":"curl","text":""},{"location":"DevOps/SSL/curl/#check-certificates-using-curl","title":"check certificates using curl","text":"<pre><code>curl -kv https://www.example.com\ncurl --verbose https://www.example.com\n</code></pre>"},{"location":"DevOps/SSL/Topic/IngressTsl/","title":"Ingress TSL","text":"<p>how to update kubernetes ingress tsl</p>"},{"location":"DevOps/SSL/Topic/IngressTsl/#generate-self-signed-tls","title":"generate self signed tls","text":""},{"location":"DevOps/SSL/Topic/IngressTsl/#extract-certificate-key-pair","title":"Extract certificate key pair","text":"<p>Self signed certificate in pfx format is usually stored in the Azure Key-vault Certificates store. A Kubernetes Secret can be created directly in terraform to load the cert from Azure Key-vault.</p> <p>The pfx cert include both the private key and certificate.  Use the following commands to extract the key pair (tls.key and tls-raw.crt) from the pfx file: <pre><code>openssl pkcs12 -in tls.pfx -nocerts -out tls-encrypted.key\nopenssl pkcs12 -in tls.pfx -clcerts -nokeys -out tls-raw.crt\nopenssl rsa -in tls-encrypted.key -out tls.key\n</code></pre> Note that when extracting the key pair, a passphrase is required which should be recorded when generating the cert. The tls.crt file should include all level (local, intermediate, and root from top to bottome) certificate files in the chain.  After downloading the CA certificates from the web browser, run the following commands to concatenation the certificates to one file: <pre><code>cp tls-raw.crt tls.crt\ncat intermediate-ca.crt &gt;&gt;tls.crt\ncat root-ca.crt &gt;&gt;tls.crt\n</code></pre></p>"},{"location":"DevOps/SSL/Topic/IngressTsl/#update-server-certificate-in-kubernetes","title":"Update server certificate in Kubernetes","text":"<p>In the Kubernetes cluster, the server certificate key pair is installed into the cluster Secrets store for the <code>traefik</code> ingress controller application.  To directly update the key pair in the cluster, run <pre><code>key=$(base64 &lt; \"./tls.key\" | tr -d '\\n')\ncrt=$(base64 &lt; \"./tls.crt\" | tr -d '\\n')\nkubectl patch secret tls-cert -n traefik \\\n    -p \"{\\\"data\\\":{\\\"tls.key\\\":\\\"${key}\\\",\\\"tls.crt\\\":\\\"${crt}\\\"}}\"\n</code></pre></p>"},{"location":"DevOps/SSL/Topic/IngressTsl/#check-the-cert-works-or-not","title":"check the cert works or not","text":"<ul> <li>do not use a webbrowser - there is cache and delay - not reliable</li> <li>use <code>curl -kv https:example.com</code></li> </ul>"},{"location":"DevOps/SSL/Topic/IngressTsl/#check-the-ingress-controller-logs","title":"check the ingress controller logs","text":"<p>The ingress should be integrated with one of the ingress controller (traefik, nginx, and ?). check the log of the ingress controller pod logs.</p>"},{"location":"DevOps/SSL/Topic/IngressTsl/#tls-failed-to-parse-private-key","title":"tls: failed to parse private key","text":"<p>For cetificate issues, check the ingress controller pod logs <pre><code>k -n &lt;namespace&gt; logs &lt;ingress-controller-pod&gt; | grep certificate\n</code></pre> Solution: the private key has a passphrase. The private key should be unencrypted. Note that tls.crt should include the intermediate ca.</p>"},{"location":"DevOps/SSL/Topic/IngressTsl/#tls-private-key-does-not-match-public-key","title":"tls: private key does not match public key","text":"<p>Used the wrong public key or the server public key is not placed in the file first (should be server, intermediate, root keys)</p>"},{"location":"DevOps/SSL/Topic/IngressTsl/#skipping-addition-of-certificate-for-domains-examplecomexamplecom-to-entrypoint-https-as-it-already-exists-for-this-entrypoint","title":"Skipping addition of certificate for domain(s) \\\"example.com,*.example.com\\\", to EntryPoint https, as it already exists for this Entrypoint","text":"<ul> <li>run <code>curl -kv https://dev.example.com</code> and still get the old certificate.   Restart the ingress controller pods (some report indicates that the old cert can still be used) and check the pod logs, </li> <li><code>Error configuring TLS for ingress dev/sales-api: secret dev/example.com-tls-cert does not exist</code>.   Updated the Secret with the same name in another namespace but not in the ingress controller namespace.</li> </ul>"},{"location":"DevOps/SSL/Topic/IngressTsl/#install-root-and-intermediate-ca-files-for-devexamplecom-domain","title":"Install root and intermediate CA files for dev.example.com domain","text":"<p>For self-signed certificate in the certificate chain, both the root and intermediate CA files  should be installed in the certificate trust store to avoid the error <code>self signed certificate in certificate chain</code>.</p> <p>In the docker base image: - First add all the new certificate files into the docker folder. - Then update the base.docker file add <code>COPY root-ca.crt intermediate-ca.crt /usr/local/share/ca-certificates/extra/</code> and <code>RUN update-ca-certificates</code>. - At last, make a new release of the base and rebuild all applications using this new base image.</p> <p>Question: can we map the cert files to a volume so we do not need to update the docker image after cert renewal? what's the prons and cons?</p>"},{"location":"DevOps/Security/Cert/","title":"Cert","text":""},{"location":"DevOps/Security/Cert/#update-ca-certificates","title":"update-ca-certificates","text":"<p>https://manpages.ubuntu.com/manpages/xenial/man8/update-ca-certificates.8.html</p> <p>Updates the directory <code>/etc/ssl/certs</code> to hold SSL certificates and generates ca-certificates.crt, a concatenated single-file list of certificates.</p>"},{"location":"DevOps/Security/Pass/","title":"Password","text":""},{"location":"DevOps/Security/Pass/#hash-password","title":"hash password","text":"<p>https://snyk.io/learn/password-storage-best-practices/ - generates a random value called a \"salt\" - concatenates the user's password and the salt - hash the combined value using argon2, bcrypt, scrypt - store the hash and the salt in database, not the original password - the generated hash is compared to the stored hash. If they match, the entered password is correct</p>"},{"location":"DevOps/Security/SFTP/","title":"sftp","text":""},{"location":"DevOps/Security/SFTP/#winscp-sftp-sync","title":"winscp sftp sync","text":"<p>sync sftp to local folder <pre><code>@echo off\n\nset dir=%~dp0\nset log=\"%dir%\\my.log\"\nset mask=xxxxx.xxx.xxxx.*\nset path=\"\\\\xx.xx\\test\\\"\nset ftpdir=\"/register/usr/access_grp1/\"\nset winscp=\"%dir%\\WinSCP-5.13.3-Portable\\winscp.com\" /ini=nul /loglevel=-1 /script=\"%dir%\\syn.txt\"\nset datetimenow=\"\"\n\ncall:isodtfunc datetimenow\n(echo %datetimenow% &amp; %winscp% /parameter %mask% %ftpdir% %path% &amp; echo.) &gt;&gt; %log%\n\ngoto:eof\n\n\n:isodtfunc\n:: use wmic to retrieve date and time\nfor /f \"skip=1 tokens=1-6\" %%g in ('wmic path win32_localtime get day^,hour^,minute^,month^,second^,year /format:table') do (\n  if \"%%~l\"==\"\" goto s_done\n  set _yyyy=%%l\n  set _mm=00%%j\n  set _dd=00%%g\n  set _hour=00%%h\n  set _minute=00%%i\n)\n:s_done\n\n:: pad digits with leading zeros\nset _mm=%_mm:~-2%\nset _dd=%_dd:~-2%\nset _hour=%_hour:~-2%\nset _minute=%_minute:~-2%\n\n:: display the date/time\nset %~1=%_yyyy%-%_mm%-%_dd% %_hour%:%_minute%\ngoto:eof\n</code></pre></p> <p>script syn.txt <pre><code>#connect to SFTP server using private key\nopen sftp://usr@sftp-reg.cloud.xxx.com/\n  -hostkey=\"ssh-rsa 2048 Hdkiifvvr1rNc0+jfk+yzcVWxxxnhu8eu/3EA=\"\n  -privatekey=\"usr_ssh_key.ppk\" -passphrase=\"xxxx\"\n\n#download file\n#get -preservetime -filemask=\"%1%\" \"%2%*\" \"%3%\"\n\n#synchronize files\noption batch continue\nsynchronize local -preservetime -filemask=\"%1%\" \"%3%\" \"%2%\"\n\n#exit WinSCP\nexit\n</code></pre></p>"},{"location":"DevOps/Security/SFTP/#run-ssh-add-on-windows","title":"run-ssh-add-on-windows","text":"<p>https://stackoverflow.com/questions/18683092/how-to-run-ssh-add-on-windows</p> <p>If you are using Git Bash, turn on ssh-agent: <pre><code># start the ssh-agent in the background\nssh-agent -s\n# Agent pid 59566\n</code></pre></p>"},{"location":"DevOps/Security/SFTP/#using-gits-start-ssh-agent","title":"using git's start-ssh-agent","text":"<p>Make sure you have Git installed and have git's cmd folder in your PATH. For example, on my computer the path to git's cmd folder is <code>c:\\Program Files\\Git\\cmd</code></p> <p>Make sure your id_rsa file is in the folder <code>c:\\users\\yourusername\\.ssh</code></p> <p>Restart your command prompt if you haven't already, and then run start-ssh-agent. It will find your id_rsa and prompt you for the passphrase</p> <p>Update 2019 - A better solution if you're using Windows 10: OpenSSH is available as part of Windows 10 which makes using SSH from cmd/powershell much easier in my opinion. It also doesn't rely on having git installed, unlike my previous solution. - Open Manage optional features from the start menu and make sure you have Open SSH Client in the list. If not, you should be able to add it. - Open Services from the start Menu - Scroll down to OpenSSH Authentication Agent &gt; right click &gt; properties - Change the Startup type from Disabled to any of the other 3 options. I have mine set to Automatic (Delayed Start) - Open cmd and type where ssh to confirm that the top listed path is in System32. Mine is installed at C:\\Windows\\System32\\OpenSSH\\ssh.exe. If it's not in the list you may need to close and reopen cmd.</p> <p>Once you've followed these steps, ssh-agent, ssh-add and all other ssh commands should now work from cmd. To start the agent you can simply type ssh-agent.</p> <p>Optional step/troubleshooting: If you use git, you should set the GIT_SSH environment variable to the output of where ssh which you ran before (e.g C:\\Windows\\System32\\OpenSSH\\ssh.exe). This is to stop inconsistencies between the version of ssh you're using (and your keys are added/generated with) and the version that git uses internally. This should prevent issues that are similar to this</p> <p>Some nice things about this solution: - You won't need to start the ssh-agent every time you restart your computer - Identities that you've added (using ssh-add) will get automatically added after restarts. (It works for me, but you might possibly need a config file in your c:\\Users\\User.ssh folder) - You don't need git! - You can register any rsa private key to the agent. The other solution will only pick up a key named id_rsa</p>"},{"location":"DevOps/Security/SSH/","title":"SSH","text":""},{"location":"DevOps/Security/SSH/#ssh-key-pair","title":"SSH key pair","text":"<p>The key pair consists of a public and a private key. The public key is kept on the server, while the private key is kept on your computer. When you connect via SSH, a trust relationship between your computer and the server is established using the key pair. If any of the keys is missing or there is a discrepancy between the keys, a connection cannot be established.</p>"},{"location":"DevOps/Security/SSH/#ssh-keygen","title":"ssh-keygen","text":"<ul> <li>The algorithm is selected using the <code>-t</code> option [rsa, dsa, ecdsa, ed25519]</li> <li>The key size using the <code>-b</code> option [2048, 4096]</li> <li>The filename is defined using the <code>-f</code> option <pre><code># create SSH key pair via Linux\nssh-keygen -b 2048 -f id123_ssh_key\n# validate SSH key pair via Linux\nssh-keygen -lf id123_ssh_key\n</code></pre></li> </ul>"},{"location":"DevOps/Security/Security/","title":"Security","text":"<p>https://www.trendmicro.com/vinfo/be/security/news/vulnerabilities-and-exploits/abusing-argo-cd-helm-and-artifact-hub-an-analysis-of-supply-chain-attacks-in-cloud-native-applications</p>"},{"location":"DevOps/Security/Security/#snyk","title":"snyk","text":"<p>Snyk is a developer security platform that enables application and cloud developers to secure their whole application \u2014 finding and fixing vulnerabilities from their first lines of code to their running cloud. - can add your github repo so source code and pr etc can be captured - can also add into your build pipeline (at step) so it will scan vulnerabities after build </p>"},{"location":"DevOps/Security/Security/#sonarcube","title":"sonarcube","text":"<p>Empower development teams with a code quality and security solution that deeply integrates into your enterprise environment; enabling you to deploy clean code consistently and reliably.</p>"},{"location":"DevOps/Security/TSL/","title":"TSL","text":""},{"location":"DevOps/Security/TSL/#upgrade-to-12","title":"upgrade to 1.2","text":"<p>https://learn.microsoft.com/en-us/security/engineering/solving-tls1-problem</p>"},{"location":"DevOps/Terraform/Bicep/","title":"Bicep","text":""},{"location":"DevOps/Terraform/Bicep/#disadvantages","title":"disadvantages","text":"<ul> <li>tend to re-deploy rather than update a single component</li> <li>can only manage single resources, cannot create direct relatioships between multiple resources</li> <li>can only remove one resource not all together as no abliblity to manage relationships</li> </ul>"},{"location":"DevOps/Terraform/Error/","title":"Error","text":""},{"location":"DevOps/Terraform/Error/#trouble-shooting","title":"Trouble shooting","text":"<p>https://developer.hashicorp.com/terraform/tutorials/configuration-language/troubleshooting-workflow</p>"},{"location":"DevOps/Terraform/Error/#error-while-retrieving-oauth-token-code-expired","title":"Error while retrieving OAuth token: Code Expired","text":"<p>https://github.com/hashicorp/terraform-provider-kubernetes/issues/606 Possible solutions - fetch the AKS credentials before run terraform and then try any kubectl cmd. This way you will manage to auth. - when fetch credentials for AKS, fetch the admin ones(param --admin). The admin account does not need to register, or at least in my case. - switch to root before run terraform <code>sudo -i</code> - my solution: do not run <code>az login</code> in vscode terminal - has restricted permisison, run it in a standard terminal</p> <pre><code>az account get-access-token\naz login --tenant &lt;my-tenantid&gt;\n</code></pre>"},{"location":"DevOps/Terraform/Error/#invalid-value-for-path-parameter-no-file-exists-at-valuesyaml","title":"Invalid value for \"path\" parameter: no file exists at \"values.yaml\"","text":"<p>https://stackoverflow.com/questions/62127753/why-do-i-get-call-to-function-file-failed-no-file-exists <pre><code>values = [\n    file(\"values.yaml\")\n]\n</code></pre> When referring to a file in the calling module from a child module, an absolute path based on the calling module's path using <code>path.module</code> should be provided: <pre><code>module \"dev\" {\n    source = \"../modules/dev\"\n    values_path = \"${path.module}/values.yaml\"\n}\n</code></pre> Another solution: provide the entire file as a variable (this is what the <code>file</code> function does): <pre><code>module \"dev\" {\n    source = \"../modules/dev\"\n    values = file(\"${path.module}/values.yaml\")\n}\n</code></pre></p>"},{"location":"DevOps/Terraform/Error/#failed-to-construct-rest-client","title":"Failed to construct REST client","text":"<p>https://github.com/hashicorp/terraform-provider-kubernetes/issues/1775</p> <p>The kubernetes_manifest resource needs access to the API server of the cluster during planning (cluster must exist).</p> <p>This is because, in order to support CRDs in Terraform ecosystem, we need to pull the schema information for each manifest resource at runtime (during planing).</p>"},{"location":"DevOps/Terraform/Error/#error-backend-configuration-changed","title":"Error: <code>Backend configuration changed</code>","text":"<p>The <code>.terraform</code> folder or the cache must be cleared.</p> <p>Linux: <pre><code>rm -rf .terraform\n</code></pre></p> <p>Windows: <pre><code>Get-ChildItem -Recurse -Filter \".terraform\" | Remove-Item -Recurse -Force\n</code></pre></p>"},{"location":"DevOps/Terraform/Install/","title":"Install","text":"<p>https://adamtheautomator.com/terraform-azure-vm/</p>"},{"location":"DevOps/Terraform/Install/#windows","title":"Windows","text":"<ul> <li>download the exe and extract it into a folder</li> <li>add the folder path to the <code>path</code> env</li> <li><code>terraform -version</code></li> </ul>"},{"location":"DevOps/Terraform/Install/#linux","title":"Linux","text":"<p>https://www.terraform.io/downloads <pre><code>sudo curl -O &lt;terraform_download_url&gt; #or sudo wget &lt;url&gt;\nsudo unzip &lt;zip_file_downloaded_in_previous_step&gt;\nsudo mv terraform /usr/bin/\nsudo rm &lt;zip_file_downloaded_in_previous_step&gt;\n</code></pre></p>"},{"location":"DevOps/Terraform/Learn/","title":"Learn","text":""},{"location":"DevOps/Terraform/Learn/#tutorials","title":"tutorials","text":"<ul> <li>https://github.com/HoussemDellai/terraform-course/tree/main</li> <li>https://developer.hashicorp.com/terraform/tutorials/configuration-language/troubleshooting-workflow</li> </ul>"},{"location":"DevOps/Terraform/Learn/#links","title":"links","text":"<p>https://learnk8s.io/terraform-aks</p> <p>https://www.linkedin.com/pulse/one-hour-day-learn-terraform-microsoft-azure-gary-tong</p>"},{"location":"DevOps/Terraform/Learn/#blogs","title":"blogs","text":"<p>https://blog.gruntwork.io/a-comprehensive-guide-to-terraform-b3d32832baca</p> <p>https://blog.gruntwork.io/terraform-tips-tricks-loops-if-statements-and-gotchas-f739bbae55f9</p>"},{"location":"DevOps/Terraform/ResourceManagement/","title":"Resource management","text":""},{"location":"DevOps/Terraform/ResourceManagement/#how-to-manage-resources-created-outside-of-terraform","title":"how to manage resources created outside of terraform","text":"<p>https://stackoverflow.com/questions/63671473/how-to-detect-changes-made-outside-of-terraform - use tags - access policies</p>"},{"location":"DevOps/Terraform/Target/","title":"Target","text":""},{"location":"DevOps/Terraform/Target/#run-plan-and-apply-to-one-resource-only","title":"run plan and apply to one resource only","text":"<p>https://spacelift.io/blog/terraform-target</p> <p>Other unrelevant input variables should be commented out even they are not used!</p> <p>Assume in the current folder there is a file named <code>my-module.tf</code> with  <pre><code>module \"storage-test\" {\n  source = \"../../modules/module-a\"\n  ...\n}\n</code></pre> We can run the following <code>terraform</code> command to only make changes to this module. <pre><code>terraform apply -target module.storage-test\n</code></pre></p>"},{"location":"DevOps/Terraform/Terraform/","title":"Terraform","text":""},{"location":"DevOps/Terraform/Terraform/#scheduled-maintenance","title":"scheduled maintenance","text":"<p>See: https://status.hashicorp.com/</p>"},{"location":"DevOps/Terraform/Terraform/#basic","title":"basic","text":"<pre><code>terraform init\nterraform plan -out main.tfplan\nterraform apply main.tfplan\necho \"$(terraform output resource_group_name)\" #check the name from the outputs\nterraform plan -destroy -out main.destroy.tfplan\nterraform destroy\n</code></pre>"},{"location":"DevOps/Terraform/Terraform/#update","title":"update","text":"<ul> <li>update terraform states to match current state of the actual resources:</li> <li><code>terraform refresh</code> alias of <code>terraform apply -refresh-only -auto-approve</code> depreciated</li> <li><code>terraform apply -refresh-only</code> safer, suggested way</li> <li>add manually created resource under source control: <code>terraform import</code></li> </ul>"},{"location":"DevOps/Terraform/Terraform/#best-practices","title":"best practices","text":"<p>https://cloud.google.com/docs/terraform/best-practices-for-terraform#module-structure</p> <p>environment directory must contain: - <code>main.tf</code>: instantiate the service module - <code>backend.tf</code>: declare the Terraform backend state location (typically, Cloud Storage)</p> <p>module directory should contain: - <code>main.tf</code> - <code>variables.tf</code> - <code>outputs.tf</code></p>"},{"location":"DevOps/Terraform/AzureRM/AKS/","title":"AKS","text":""},{"location":"DevOps/Terraform/AzureRM/AKS/#node-pool","title":"node pool","text":"<p>https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/aks/custom-node-configuration.md</p> <p>Provider: https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/kubernetes_cluster_node_pool</p>"},{"location":"DevOps/Terraform/AzureRM/AKS/#role-assignment","title":"role assignment","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/role_assignment</p>"},{"location":"DevOps/Terraform/AzureRM/AKS/#storage-container","title":"storage container","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/data-sources/storage_container</p>"},{"location":"DevOps/Terraform/AzureRM/AKS/#network_profile-example","title":"network_profile example","text":"<p>https://github.com/fluxcd/source-controller/issues/898 <pre><code>  network_profile {\n    network_plugin = \"azure\"\n    network_policy = \"azure\"\n    outbound_type  = \"loadBalancer\"\n  }\n</code></pre></p>"},{"location":"DevOps/Terraform/AzureRM/AdIdentity/","title":"AdIdentity","text":""},{"location":"DevOps/Terraform/AzureRM/AdIdentity/#azurerm_user_assigned_identity-in-terraform-vs-azureidentity-in-helm","title":"<code>azurerm_user_assigned_identity</code> in terraform vs <code>AzureIdentity</code> in helm","text":"<p>azurerm_user_assigned_identity (Terraform) - Purpose: A Terraform resource that creates and manages user-assigned managed identities in Azure. - Function: It provides a way to create and manage these identities independently of other resources. - Configuration: It's defined in Terraform configuration files using the <code>azurerm_user_assigned_identity</code> resource. - Key Properties:     - <code>name</code>: The name of the managed identity.     - <code>resource_group_name</code>: The resource group where the identity is created.     - <code>location</code>: The Azure region where the identity is created.</p> <p>AzureIdentity (Helm) - Purpose: A configuration parameter within the Azure AD Pod Identity (AAD Pod Identity) Helm chart. - Function: It enables pods in a Kubernetes cluster to access Azure resources using a managed identity. - Configuration: It's defined within the <code>values.yaml</code> file of the AAD Pod Identity chart. - Key Properties:     - <code>resourceID</code>: The ID of the user-assigned managed identity.     - <code>clientID</code>: The client ID of the managed identity.     - <code>name</code>: A name for the identity within the namespace.     - <code>namespace</code>: The namespace where the identity is created.</p> <p>Relationship - Integration: They work together to enable pods in a Kubernetes cluster to access Azure resources using managed identities:     1. Terraform: Creates the user-assigned managed identity using <code>azurerm_user_assigned_identity</code>.     2. Helm: Refers to the created managed identity using <code>AzureIdentity</code> within the AAD Pod Identity chart. - Coordination: Terraform generates the necessary information (resource ID, client ID) for the <code>AzureIdentity</code> configuration in Helm.</p> <p>Terraform code: <pre><code>resource \"azurerm_user_assigned_identity\" \"my_identity\" {\n  name                = \"my-identity\"\n  resource_group_name = \"my-resource-group\"\n  location            = \"eastus\"\n}\n</code></pre></p> <p>Helm values.yaml: <pre><code>azureIdentities:\n  - name: \"my-identity\"\n    resourceID: \"${azurerm_user_assigned_identity.my_identity.id}\"\n    clientID: \"${azurerm_user_assigned_identity.my_identity.client_id}\"\n    # ... other configuration\n</code></pre></p>"},{"location":"DevOps/Terraform/AzureRM/AdIdentity/#access-to-blob-storage","title":"access to blob storage","text":"<p>https://stackoverflow.com/questions/52769758/azure-blob-storage-authorization-permission-mismatch-error-for-get-request-wit</p> <p>For blob storage, to let an app to access a specific blob container, we have to include that blob container in the <code>azurerm_role_assignment</code> scope for that app.</p> <pre><code># https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/role_assignment\nresource \"azurerm_role_assignment\" \"blob_contributor\" {\n  # https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#storage-blob-data-contributor\n  scope                = var.st_blob.container[\"dev-data\"].resource_manager_id\n  role_definition_name = \"Storage Blob Data Contributor\"\n  principal_id         = module.ad_identity.principal_id\n}\n\n# https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/role_assignment\nresource \"azurerm_role_assignment\" \"queue_contributor\" {\n  # https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#storage-blob-data-contributor\n  scope                = var.st_blob.container[\"dev-data\"].resource_manager_id\n  role_definition_name = \"Storage Queue Data Contributor\"\n  principal_id         = module.ad_identity.principal_id\n}\n</code></pre>"},{"location":"DevOps/Terraform/AzureRM/Authenticate/","title":"Authenticate","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/azure_cli</p>"},{"location":"DevOps/Terraform/AzureRM/Authenticate/#authentication-methods","title":"authentication methods","text":"<p>When run locally use Azure CLI; when run on CI server, use the others - using Azure CLI (locally) - using Managed Service Identity - using a Service Principal and a Client Certificate - using a Service Principal and a Client Secret - using a Service Principal and Open ID Connect</p> <pre><code>az login --service-principal --username &lt;client_id&gt; --password &lt;client_secret&gt; --tenant &lt;tenant_id&gt;\n</code></pre>"},{"location":"DevOps/Terraform/AzureRM/Authenticate/#run-terraform-using-service-principle","title":"run terraform using service principle","text":"<pre><code>export ARM_SUBSCRIPTION_ID=\"&lt;subscription_id&gt;\"\nexport ARM_TENANT_ID=\"&lt;tenant_id&gt;\"\nexport ARM_CLIENT_ID=\"&lt;client_id&gt;\"\nexport ARM_CLIENT_SECRET=\"&lt;client_secret&gt;\"\n</code></pre>"},{"location":"DevOps/Terraform/AzureRM/Authenticate/#error-logged-in-using-azure-cli-cannot-use-service-principle","title":"error: logged in using Azure CLI cannot use service principle","text":"<p>solution: - Make sure logged in using the right directory - Must login use the terminal not the vs code terminal as vs code terminal has restricted permission</p>"},{"location":"DevOps/Terraform/AzureRM/Authenticate/#error-while-retrieving-oauth-token-code-expired","title":"Error while retrieving OAuth token: Code Expired","text":"<p><pre><code>Error: \nGet \"https://&lt;aks-name&gt;.hcp.&lt;region&gt;.azmk8s.io:443/api/v1/namespaces/&lt;namespace&gt;\": \n  acquiring a token for authorization header: failed acquiring new token: waiting for device code authentication to complete: \n  autorest/adal/devicetoken: Error while retrieving OAuth token: Code Expired\n</code></pre> Reason: the aks config file not in the right path so could not connetc to the aks.</p>"},{"location":"DevOps/Terraform/AzureRM/AzureRM/","title":"AzureRM","text":""},{"location":"DevOps/Terraform/AzureRM/AzureRM/#group","title":"group","text":"<p>https://registry.terraform.io/providers/hashicorp/azuread/latest/docs/data-sources/group</p>"},{"location":"DevOps/Terraform/AzureRM/AzureRM/#resource-group","title":"resource group","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/data-sources/resource_group</p>"},{"location":"DevOps/Terraform/AzureRM/AzureRM/#subnet","title":"subnet","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/data-sources/subnet</p>"},{"location":"DevOps/Terraform/AzureRM/ContainerRegistry/","title":"acr","text":""},{"location":"DevOps/Terraform/AzureRM/ContainerRegistry/#yaml","title":"yaml","text":"<pre><code>resource \"azurerm_container_registry\" \"acr\" {\n  name                          = var.name\n  resource_group_name           = var.resource_group_name\n  location                      = var.location\n  sku                           = \"Standard\"\n  admin_enabled                 = true\n  public_network_access_enabled = true\n  zone_redundancy_enabled       = var.zone_redundancy_enabled\n\n  retention_policy {\n    enabled = true\n    days    = var.untagged_retention_days\n  }\n  lifecycle {\n    create_before_destroy = true\n  }\n  tags = {\n    environment = local.env\n  }\n}\n</code></pre>"},{"location":"DevOps/Terraform/AzureRM/Issue/","title":"Issue","text":""},{"location":"DevOps/Terraform/AzureRM/Issue/#how-to-find-the-api-version-for-managedclusters-used-in-hashicorpazurerm","title":"how to find the api version for managedClusters used in hashicorp/azurerm","text":"<ul> <li>goto <code>https://github.com/hashicorp/terraform-provider-azurerm/blob/v3.99.0/internal/services/containers/client/client.go</code></li> <li>locate <code>\".../containerservice/2025-07-01/managedclusters\"</code></li> <li>api-version for managedclusters is <code>2025-07-01</code></li> </ul>"},{"location":"DevOps/Terraform/AzureRM/Network/","title":"Network","text":""},{"location":"DevOps/Terraform/AzureRM/Network/#private-dns-zone","title":"Private DNS Zone","text":"<p>private dns zone: https://www.terraform.io/docs/providers/azurerm/r/private_dns_zone.html</p> <p>private dns zone virtual network link: https://www.terraform.io/docs/providers/azurerm/r/private_dns_zone_virtual_network_link.html</p>"},{"location":"DevOps/Terraform/AzureRM/PersistentVolume/","title":"Persistent Volume","text":""},{"location":"DevOps/Terraform/AzureRM/PersistentVolume/#link-azure-file-storage-file-shares-to-a-persistentvolume-pv-in-kubernetes-without-using-a-storageclass","title":"Link Azure File Storage <code>File Shares</code> to a PersistentVolume (PV) in Kubernetes without using a StorageClass","text":"<p>create a secret a terraform including the <code>storage account name</code> and <code>storage account key</code> <pre><code># https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/secret\nresource \"kubernetes_secret\" \"dev_storage\" {\n  depends_on = [kubernetes_namespace.main]\n  metadata {\n    name        = \"dev-storage\"\n    labels      = {}\n    annotations = {}\n    namespace   = var.namespace\n  }\n  data = {\n    \"azurestorageaccountname\" = var.dev_storage_account.name\n    \"azurestorageaccountkey\"  = var.dev_storage_account.key\n  }\n}\n</code></pre></p> <p>create a PersistentVolume. The share name is the <code>File Shares</code> name of the storage account <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: azure-pv\nspec:\n  capacity:\n    storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteMany\n  azureFile:\n    secretName: azure-secret\n    secretNamespace: dev\n    shareName: myshare\n    readOnly: false\n  mountOptions:\n    - dir_mode=0777\n    - file_mode=0777\n    - uid=1000\n    - gid=1000\n    - mfsymlinks\n    - nobrl\n</code></pre></p> <p>create a PersistentVolumeClaim <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: azure-pvc\n  namespace: dev\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 2Gi\n  volumeName: azure-pv\n</code></pre></p>"},{"location":"DevOps/Terraform/AzureRM/PostgreSQL/","title":"PostgreSQL","text":""},{"location":"DevOps/Terraform/AzureRM/PostgreSQL/#server","title":"server","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/postgresql_flexible_server</p>"},{"location":"DevOps/Terraform/AzureRM/PostgreSQL/#database","title":"database","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/postgresql_flexible_server_database</p>"},{"location":"DevOps/Terraform/AzureRM/RoleAssignment/","title":"role assignment","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/role_assignment</p>"},{"location":"DevOps/Terraform/AzureRM/RoleAssignment/#acrpullpush","title":"acrPull/Push","text":"<p>aks accesses acr example https://github.com/fluxcd/test-infra/tree/65e1a901cbb9b3f9f27ffad7f9a32a6366eae1cc/tf-modules/azure/aks</p> <p>AcrPull <pre><code>resource \"azurerm_role_assignment\" \"kubweb_to_acr\" {\n  scope                = azurerm_container_registry.acr.id\n  role_definition_name = \"AcrPull\"\n  principal_id         = azurerm_kubernetes_cluster.aks.kubelet_identity[0].object_id\n}\n</code></pre></p>"},{"location":"DevOps/Terraform/AzureRM/Storage/","title":"Storage","text":"<ul> <li>azurerm_storage_container</li> <li>stoarge_account_network_rules</li> <li>azurerm_storage_container</li> <li>azurerm_private_endpoint: dfs, file, blob</li> <li>azurerm_user_assigned_identity</li> <li>azurerm_role_assignment: Storage Blob Data Contributor, Storage Queue Data Contributor, keyvalut secret etc</li> </ul> <p>Note that the network rules might be in effect after the creation of the storage account. So the storag containers can only be created in a second run. The first run might have errors like: <pre><code>Error: containers.Client#GetProperties: Failure responding to request: StatusCode=403 --\nOriginal Error: autorest/azure: Service returned an error. Status=403\nCode=\"AuthorizationFhorized to perform this operation.\n</code></pre></p>"},{"location":"DevOps/Terraform/AzureRM/Storage/#add-a-manully-created-azure-blob-storage-container-under-terraform-control","title":"Add a manully created azure blob storage container under terraform control","text":"<p>We can manage an existing Azure Blob Storage container using Terraform.  Terraform has an Azure provider that allows you to define and manage Azure resources, including Blob Storage containers.</p> <p>To manage an existing Blob Storage container in Terraform, wed need to <code>import</code> it into the Terraform state.  The import process enables Terraform to track the existing resource and manage its state going forward. </p> <p>Here's how we can do it:</p> <ol> <li> <p>Create a new Terraform configuration file or update an existing one to include the Azure provider and the Blob Storage container resource definition.  <pre><code>provider \"azurerm\" {\n  features {}\n}\n\nresource \"azurerm_storage_container\" \"example\" {\n  name                  = \"&lt;container-name&gt;\"\n  storage_account_name  = \"&lt;storage-account-name&gt;\"\n  resource_group_name   = \"&lt;resource-group-name&gt;\"\n  container_access_type = \"private\"\n}\n</code></pre></p> </li> <li> <p>Replace <code>&lt;container-name&gt;</code>, <code>&lt;storage-account-name&gt;</code>, and <code>&lt;resource-group-name&gt;</code> with the actual names of the  Blob Storage container, storage account, and resource group.</p> </li> <li>Run terraform init to initialize the Terraform configuration.</li> <li> <p>Run the import command to import the existing Blob Storage container into the Terraform state: <pre><code>terraform import azurerm_storage_container.example \\\n  /subscriptions/&lt;subscription-id&gt;/resourceGroups/&lt;resource-group-name&gt;/providers \\\n  /Microsoft.Storage/storageAccounts/&lt;storage-account-name&gt;/blobServices/default/containers/&lt;container-name&gt;\n</code></pre></p> </li> <li> <p>Replace <code>&lt;subscription-id&gt;</code>, <code>&lt;resource-group-name&gt;</code>, <code>&lt;storage-account-name&gt;</code>, and <code>&lt;container-name&gt;</code> with the respective values for the Blob Storage container.</p> </li> <li>Run terraform plan to verify the import was successful and to see any changes that Terraform would apply.</li> <li>Finally, run terraform apply to apply the changes and manage the Blob Storage container through Terraform.</li> </ol> <p>Note that importing an existing resource does not delete or modify the resource itself.  Terraform tracks the resource's state and can manage it going forward.  Therefore, your data in the Blob Storage container will not be lost during this process.</p> <p>However, exercise caution when applying changes to the Terraform configuration that manages the Blob Storage container,  as inappropriate modifications can result in unintended consequences.  Always review the Terraform plan before applying changes to ensure they align with your expectations and requirements.</p>"},{"location":"DevOps/Terraform/AzureRM/Storage/#azurerm_storage_container","title":"azurerm_storage_container","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_container</p>"},{"location":"DevOps/Terraform/AzureRM/Storage/#stoarge_account_network_rules","title":"stoarge_account_network_rules","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_account_network_rules</p> <p>when nfsv3_enabled = true, network_rules must be created together with the storage, otherwise might get errors like <code>Storage Account xxx was not found</code>. see bug: https://github.com/hashicorp/terraform-provider-azurerm/issues/14540</p> <pre><code>resource \"azurerm_storage_account_network_rules\" \"main\" {\n  storage_account_id         = azurerm_storage_account.main.id\n  default_action             = \"Deny\"\n  ip_rules                   = [\"127.0.0.1\"]\n  virtual_network_subnet_ids = [azurerm_subnet.main.id]\n  bypass                     = [\"Metrics\"]\n}\n</code></pre>"},{"location":"DevOps/Terraform/AzureRM/Storage/#azurerm_private_endpoint","title":"azurerm_private_endpoint","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/private_endpoint - subresource_names: dfs - general file system access - subresource_names: file - only for FileStorage - subresource_names: blob - for type != FileStorage</p>"},{"location":"DevOps/Terraform/AzureRM/Subnet/","title":"Subnet","text":""},{"location":"DevOps/Terraform/AzureRM/Subnet/#subnet_1","title":"subnet","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/subnet</p>"},{"location":"DevOps/Terraform/AzureRM/Subnet/#route-table","title":"route table","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/route_table</p>"},{"location":"DevOps/Terraform/AzureRM/Subnet/#subnet-route-table-association","title":"subnet route table association","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/subnet_route_table_association</p>"},{"location":"DevOps/Terraform/AzureRM/Subnet/#network-security-rule","title":"network security rule","text":"<p>https://www.terraform.io/docs/providers/azurerm/r/network_security_rule.html</p>"},{"location":"DevOps/Terraform/AzureRM/Subnet/#network-security-group","title":"network security group","text":"<p>https://www.terraform.io/docs/providers/azurerm/r/network_security_group</p>"},{"location":"DevOps/Terraform/AzureRM/Subnet/#subnet-network-security-group-association","title":"subnet network security group association","text":"<p>https://www.terraform.io/docs/providers/azurerm/r/subnet_network_security_group_association</p>"},{"location":"DevOps/Terraform/AzureRM/VM/","title":"VM","text":""},{"location":"DevOps/Terraform/AzureRM/VM/#vm-extension","title":"vm extension","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/virtual_machine_extension</p> <p>allow running scripts such as installing extra software etc.</p>"},{"location":"DevOps/Terraform/AzureRM/Vnet/","title":"Vnet","text":""},{"location":"DevOps/Terraform/AzureRM/Vnet/#virtual-network","title":"virtual network","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/data-sources/virtual_network</p>"},{"location":"DevOps/Terraform/AzureRM/Vnet/#route-table","title":"route table","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/route_table</p>"},{"location":"DevOps/Terraform/AzureRM/Vnet/#public-ip","title":"public ip","text":"<p>https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/public_ip</p>"},{"location":"DevOps/Terraform/AzureRM/azuread_application/","title":"azuread_application","text":"<p>Manages an application registration within Azure Active Directory. https://registry.terraform.io/providers/hashicorp/azuread/latest/docs/resources/application</p> <p>Lightweighted version: https://registry.terraform.io/providers/hashicorp/azuread/latest/docs/resources/application_registration</p> <pre><code>data \"azuread_client_config\" \"main\" {}\n\n# https://registry.terraform.io/providers/hashicorp/azuread/latest/docs/resources/application\nresource \"azuread_application\" \"main\" {\n  display_name     = var.display_name\n  identifier_uris  = [\"api://example-app\"]\n  logo_image       = filebase64(\"/path/to/logo.png\")\n\n  sign_in_audience = \"AzureADMultipleOrgs\"\n\n  owners = [\n    data.azuread_client_config.main.object_id,\n    data.azuread_service_principal.main.id,\n    data.azuread_user.tst.id,    \n  ]\n\n  api {\n    mapped_claims_enabled          = true\n    requested_access_token_version = 2\n\n    known_client_applications = [\n      azuread_application.known1.application_id,\n      azuread_application.known2.application_id,\n    ]\n\n    oauth2_permission_scope {\n      admin_consent_description  = \"Allow the application '${var.display_name}' to act on behalf of the signed-in user.\"\n      admin_consent_display_name = \"Allow impersonation\"\n      enabled                    = true\n      id                         = random_uuid.main.result\n      type                       = \"User\"\n      user_consent_description   = \"Allow the application '${var.display_name}' to act on your behalf.\"\n      user_consent_display_name  = \"Pass-through credentials\"\n      value                      = \"user_impersonation\"\n    }\n  }\n\n  web {\n    homepage_url  = \"https://app.example.net\"\n    logout_url    = \"https://app.example.net/logout\"\n    redirect_uris = [\"https://app.example.net/account\"]\n\n    implicit_grant {\n      access_token_issuance_enabled = true\n      id_token_issuance_enabled     = true\n    }\n  }  \n\n  # resource_access_scopes = {\n    # \"ms_graph\" = [\n      # \"email\", \"profile\", \"user.read\",\n    # ]\n    # \"azure_key_vault\" = [\"user_impersonate\"]    \n    # \"azure_storage\"   = [\"user_impersonate\"]\n    # \"azure_data_lake\" = [\"user_impersonate\"]\n    # \"oss_database\"    = [\"user_impersonate\"]\n    # \"sql_database\"    = [\"user_impersonate\"]\n  # }\n  dynamic \"required_resource_access\" {\n    for_each = var.resource_access_scopes\n    iterator = permissions\n    content {\n      resource_app_id = lookup(local.resource_ids, permissions.key)\n      dynamic \"resource_access\" {\n        for_each = permissions.value\n        content {\n          type = \"Scope\"\n          id   = lookup(\n            lookup(local.permissions, permissions.key),\n            resource_access.value\n          )\n        }\n      }\n    }\n  }\n\n  # variable \"id_token_claims\" {\n    # type = list(object({\n      # name      = string\n      # essential = bool\n      # source    = optional(string)\n      # additional_properties = optional(list(string))\n    # }))\n    # default = []\n  # }\n  optional_claims {\n    dynamic \"id_token\" {\n      for_each = var.id_token_claims[*]\n      iterator = claim\n      content {\n        name                  = claim.value.name\n        essential             = claim.value.essential\n        source                = claim.value.source\n        additional_properties = claim.value.additional_properties\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"DevOps/Terraform/AzureRM/azuread_application/#azuread_application_password","title":"azuread_application_password","text":"<p>https://discuss.hashicorp.com/t/azuread-application-password-rotation/35141</p> <p>https://registry.terraform.io/providers/hashicorp/azuread/latest/docs/resources/application_password.html</p> <p>password rotation (create a second secret with a different rotation period to avoid interruption) <pre><code>resource \"time_rotating\" \"password_rotate\" {\n  rotation_days = 60\n}\n\nresource \"azuread_application_password\" \"main\" {\n  application_object_id = azuread_application.application.object_id\n  end_date_relative     = \"2400h\"\n  rotate_when_changed = {\n    rotation = time_rotating.password_rotate.id\n  }\n}\n</code></pre></p>"},{"location":"DevOps/Terraform/AzureRM/azuread_application/#service-principal","title":"service principal","text":"<pre><code># https://registry.terraform.io/providers/hashicorp/azuread/latest/docs/resources/service_principal\nresource \"azuread_service_principal\" \"main\" {\n    application_id               = module.main.client_id\n    app_role_assignment_required = false\n    owners                       = module.main.owners\n    tags                         = [\"apiConsumer\", \"webApp\"]\n}\n</code></pre>"},{"location":"DevOps/Terraform/Blog/LearnTerrafrom/","title":"How to learn terraform from zero","text":"<p>learning from creating projects is the best way but we still need to know where to start.</p>"},{"location":"DevOps/Terraform/Blog/LearnTerrafrom/#basic-commands","title":"basic commands","text":"<ul> <li>init</li> <li>plan</li> <li>apply</li> </ul>"},{"location":"DevOps/Terraform/Blog/LearnTerrafrom/#read-the-docs","title":"read the docs","text":"<p>read the docs to understand - how terraform works, - api - state - module</p>"},{"location":"DevOps/Terraform/Blog/LearnTerrafrom/#api","title":"api","text":"<ul> <li>where to include api version</li> </ul>"},{"location":"DevOps/Terraform/Blog/LearnTerrafrom/#module","title":"module","text":"<p>modules are just like functions in other programming languages, they have - inputs: variables - main: code used to create resources - outputs: outputs from the created resources</p>"},{"location":"DevOps/Terraform/Blog/LearnTerrafrom/#state","title":"state","text":"<ul> <li>where to save state</li> <li>how to import state</li> <li>how to sync state</li> </ul>"},{"location":"DevOps/Terraform/Blog/LearnTerrafrom/#data-type","title":"data type","text":"<ul> <li>string</li> <li>number</li> <li>map</li> <li>object</li> </ul>"},{"location":"DevOps/Terraform/Blog/LearnTerrafrom/#expression","title":"expression","text":"<ul> <li>conditional</li> <li>loop</li> </ul>"},{"location":"DevOps/Terraform/Blog/LearnTerrafrom/#optional-properties","title":"optional properties","text":"<ul> <li>null: optional properties</li> <li>dynamic blocks: optional blocks</li> </ul>"},{"location":"DevOps/Terraform/Expression/Conditional/","title":"Conditional","text":""},{"location":"DevOps/Terraform/Expression/Conditional/#count","title":"count","text":"<p>A common issue with <code>count</code> is that once you delete any resource other than the last one,  Terraform will try to force replace all resources that the index doesn't match.</p> <p>Usually only used for enable/disable a resource <pre><code>variable \"add_storage_account\" {\n  description = \"boolean to determine whether to create a storage account or not\"\n  type        = bool\n}\n\nresource \"azurerm_storage_account\" \"main\" {\n  count = var.add_storage_account ? 1 : 0\n\n  resource_group_name      = \"rg-st\"\n  location                 = \"eastus\"\n  account_tier             = \"Standard\"\n  account_replication_type = \"LRS\"\n\n  name = \"stspacelift${count.index}${local.rand_suffix}\"\n}\n</code></pre></p>"},{"location":"DevOps/Terraform/Expression/DynamicBlocks/","title":"Dynamic blocks","text":"<p>https://developer.hashicorp.com/terraform/language/expressions/dynamic-blocks</p>"},{"location":"DevOps/Terraform/Expression/DynamicBlocks/#conditional-dynamic-blocks","title":"conditional dynamic blocks","text":"<p>https://stackoverflow.com/questions/66152036/terraform-conditional-argument-block <pre><code>resource \"azurerm_automation_schedule\" \"example\" {\n  name                    = var.aaname\n  resource_group_name     = azurerm_resource_group.example.name\n  automation_account_name = azurerm_automation_account.example.name\n\n  dynamic \"monthly_occurrence\" {\n    for_each = var.frequency == \"Month\" ? [1] : []\n    content {\n       day = monthly_occurrence.value.day\n       occurrence = monthly_occurrence.value.occurrence\n    }\n  }\n}\n</code></pre></p>"},{"location":"DevOps/Terraform/Expression/Func/","title":"Func","text":""},{"location":"DevOps/Terraform/Expression/Func/#yamldecode","title":"yamldecode","text":"<p>https://developer.hashicorp.com/terraform/language/functions/yamldecode</p> <p>Get value from yaml file <pre><code>module \"example\" {\n  source = \"../modules/example\"\n  value = yamldecode(file(\"${path.module}/example.yaml\")).configs.value\n}\n\noutput \"example_value\" {\n  value = jsondecode(helm_release.argocd.metadata[0].values).configs.value\n}\n</code></pre></p>"},{"location":"DevOps/Terraform/Expression/Func/#replace","title":"replace","text":"<p>replace user/pass url to ssh url: <pre><code>replace(yamldecode(helm_release.argocd.metadata[0].values).configs.repo_url, \"https://github.com/\", \"git@github.com:\")\n</code></pre></p>"},{"location":"DevOps/Terraform/Expression/Loop/","title":"Loop","text":""},{"location":"DevOps/Terraform/Expression/Loop/#for_each","title":"for_each","text":"<p><code>for_each</code> isn't sensitive to changes in the order of resources. <pre><code>variable \"bucket_names\" {\n  type    = list(string)\n  default = [\"bucket-1\", \"bucket-2\"]\n}\n\nresource \"aws_s3_bucket\" \"main\" {\n  for_each = toset(var.bucket_names)\n\n  bucket = each.value\n  acl    = \"private\"\n\n  tags = {\n    Name = each.value\n  }\n}\n</code></pre></p> <p>Use both key anf value <pre><code>variable \"bucket_info\" {\n  type = map(object({\n    acl = string\n  }))\n  default = {\n    \"bucket-1\" = {\n      acl = \"private\"\n    },\n    \"bucket-2\" = {\n      acl = \"public-read\"\n    }\n  }\n}\n\nresource \"aws_s3_bucket\" \"main\" {\n  for_each = var.bucket_info\n\n  bucket = each.key\n  acl    = each.value.acl\n}\n</code></pre></p> <p>Limitations: - the keys in for_each map block must have a known value.   - can't be generated on the fly by functions (like bcrypt or timestamp)   - can't refer to resource-specific attributes that are provided by a cloud provider, like a cloud resource ID - we can't use sensitive values as arguments for for_each. - basically, when using for_each, you need to directly specify the values.</p>"},{"location":"DevOps/Terraform/Expression/Loop/#for_each-with-condition","title":"for_each with condition","text":"<p>https://spacelift.io/blog/terraform-conditionals <pre><code>resource \"azuread_group_member\" \"main\" {\n  for_each = { for key, val in data.azuread_user.main :\n    key =&gt; val if val.user_type == \"Member\" }\n\n  group_object_id  = azuread_group.main.id\n  member_object_id = data.azuread_user.main[each.key].id\n}\n</code></pre></p>"},{"location":"DevOps/Terraform/Expression/Optional/","title":"Optional","text":""},{"location":"DevOps/Terraform/Expression/Optional/#skip-a-property-by-setting-its-value-to-null","title":"skip a property by setting its value to <code>null</code>","text":"<p>https://developer.hashicorp.com/terraform/language/expressions/types#null</p> <p>a value that represents absence or omission. If you set an argument of a resource to null, Terraform behaves as though you had completely omitted it </p> <pre><code>account_tier = var.env == \"prod\" ? var.account_tier : null\n</code></pre>"},{"location":"DevOps/Terraform/Kubernetes/ArgoCD/","title":"ArgoCD","text":""},{"location":"DevOps/Terraform/Kubernetes/ArgoCD/#argocd-setup","title":"argocd setup","text":"<p>https://wikdom.eu/blog/argocd-setup/</p>"},{"location":"DevOps/Terraform/Kubernetes/Manifest/","title":"Manifest","text":""},{"location":"DevOps/Terraform/Kubernetes/Manifest/#set-value-of-variables-in-helm-chart-valuesyaml-from-manifest","title":"set value of variables in helm chart values.yaml from manifest","text":"<p>values.yaml <pre><code>myVar:\n  name: \"var-name\"\n  id: \"\"\nmyVar2: \"\"\n</code></pre></p> <p>manifest.tf <pre><code># https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/manifest\nresource \"kubernetes_manifest\" \"my-app\" {\n  depends_on = [kubernetes_manifest.argo_project]\n  # https://argoproj.github.io/argo-cd/operator-manual/application.yaml\n  manifest = {\n    \"apiVersion\" = \"argoproj.io/v1alpha1\"\n    \"kind\" = \"Application\"\n    \"metadata\" = {\n      \"name\" = \"my-app\"\n      \"namespace\" = \"argocd\"\n    }\n    \"spec\" = {\n      \"destination\" = {\n        \"name\" = \"in-cluster\"\n        \"namespace\" = var.namespace\n      }\n      \"project\" = var.namespace\n      \"source\" = {\n        \"helm\" = {\n          \"parameters\" = [\n            {\n              name = \"myVar.name\"\n              value = module.my-app.name\n            },\n            {\n              name = \"myVar.id\"\n              value = module.my-app.id\n            },\n            {\n              name = \"myVar2\"\n              value = var.var2.name\n            },\n          ]\n          \"valueFiles\" = [\n            \"values.yaml\",\n          ]\n        }\n        \"path\" = \"helm/dev/my-app\"\n        \"repoURL\" = var.argocd_repo_url\n        \"targetRevision\" = \"HEAD\"\n      }\n      \"syncPolicy\" = {\n        \"automated\" = {\n          \"prune\" = true\n          \"selfHeal\" = true\n        }\n        \"syncOptions\" = [\n          \"PruneLast=true\",\n          \"CreateNamespace=true\",\n        ]\n      }\n    }\n  }\n}\n</code></pre></p>"},{"location":"DevOps/Terraform/Kubernetes/Namespace/","title":"Namespace","text":"<pre><code>resource \"kubernetes_namespace\" \"main\" {\n  metadata {\n    name        = var.namespace\n    labels      = {}\n    annotations = {}\n  }\n}\n</code></pre>"},{"location":"DevOps/Terraform/Kubernetes/NodePool/","title":"Node pool","text":""},{"location":"DevOps/Terraform/Kubernetes/NodePool/#node-labels-and-node-taints","title":"node labels and node taints","text":"<ul> <li><code>node_labels</code> - (Optional) A map of Kubernetes labels which should be applied to nodes in this Node Pool.</li> <li><code>node_taints</code> - (Optional) A list of Kubernetes taints which should be applied to nodes in the agent pool (e.g key=value:NoSchedule).   Changing this forces a new resource to be created.</li> </ul> <p>These properties can be used to only allow some pods to be deployed in this node pool. <pre><code># https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/kubernetes_cluster_node_pool\nresource \"azurerm_kubernetes_cluster_node_pool\" \"np01\" {\n...\n  node_labels = {\n    \"hub.jupyter.org/node-purpose\" = \"dev-user\"\n  }\n  node_taints = [\n    \"hub.jupyter.org/dedicated=dev-user:NoSchedule\"\n  ]\n}\n</code></pre></p> <p>And in jupyterhub <code>values.yaml</code> <pre><code>singleuser:\n  nodeSelector:\n    hub.jupyter.org/node-purpose: dev-user\nscheduling:\n  userPods:\n    nodeAffinity:\n      matchNodePurpose: require\n</code></pre></p>"},{"location":"DevOps/Terraform/Kubernetes/Secret/","title":"Secret","text":"<pre><code># https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/secret\nresource \"kubernetes_secret\" \"main\" {\n  depends_on = [kubernetes_namespace.main]\n  metadata {\n    name        = var.name\n    namespace   = var.namespace\n    labels      = {}\n    annotations = {}    \n  }\n  data = {\n    \"mysql.username\" = file(\"//example.com/share/mysql.username.txt\")\n    \"mysql.password\" = file(\"//example.com/share/mysql.password.txt\")\n  }\n}\n</code></pre>"},{"location":"DevOps/Terraform/Module/Dependency/","title":"Dependency","text":""},{"location":"DevOps/Terraform/Module/Dependency/#terragrunt-dependency-vs-terraform-depends_on","title":"terragrunt <code>dependency</code> vs terraform <code>depends_on</code>","text":"<p>Scope - In Terragrunt, we can declare dependencies between modules  - In Terraform, we can only declare dependencies between resources within a single module</p> <p>Execution order - In Terragrunt, dependencies are resolved in a top-down order, where each module is processed before its dependencies - In Terraform, the depends_on argument is used to specify the execution order of resources within a module</p>"},{"location":"DevOps/Terraform/Module/Module/","title":"Module","text":"<p>providers should not be included in modules.</p>"},{"location":"DevOps/Terraform/Output/Output/","title":"Output","text":""},{"location":"DevOps/Terraform/Output/Output/#show-output-values","title":"show output values","text":"<p>After running <code>terrafrom apply</code>, we can use <code>terrafrom output</code> to show the values defined in outputs.</p>"},{"location":"DevOps/Terraform/Output/Output/#export-the-state","title":"export the state","text":"<p><pre><code>terraform state pull &gt; module_test_state.tfstate\n</code></pre> Then search for the <code>key_id</code> of that object.</p>"},{"location":"DevOps/Terraform/Provider/NullProvider/","title":"Null provider","text":""},{"location":"DevOps/Terraform/Provider/Provider/","title":"Provider","text":"<p>https://developer.hashicorp.com/terraform/language/modules/develop/providers</p> <p>Provider configurations can be defined only in a root Terraform module.</p> <p>Providers can be passed down to descendent modules in two ways: - either implicitly through inheritance, or - explicitly via the providers argument within a module block.</p> <p>A module intended to be called by one or more other modules must not contain any provider blocks.</p>"},{"location":"DevOps/Terraform/Provider/Provider/#passing-providers-explicitly","title":"passing Providers Explicitly","text":"<p>When - child modules each need a different configuration of a particular provider, or - where the child module requires a different provider configuration than its parent, you can use the providers argument within a module block to explicitly define which provider configurations are available to the child module.</p> <p>Those with the alias argument set are never inherited automatically by child modules, and so must always be passed explicitly using the providers map.</p>"},{"location":"DevOps/Terraform/Provider/Provider/#determine-what-version-of-a-provider-a-resource-was-created-with","title":"determine what version of a provider a resource was created with","text":"<p>https://www.reddit.com/r/Terraform/comments/17ujw6f/how_can_i_determine_what_version_of_a_provider_a/</p> <p>if get error <code>Resource instance managed by newer provider version</code> we need to figure out the version of the provider used to create the resource. - from state file we can only get the <code>schema_version</code> linked to that resource - then we need to find which version of the provider uses the <code>schema_version</code></p>"},{"location":"DevOps/Terraform/State/Import/","title":"Import Existing Resources","text":"<p>https://developer.hashicorp.com/terraform/cli/commands/import</p> <p>https://dev.to/cloudskills/getting-started-with-terraform-on-azure-importing-existing-infrastructure-43fa</p> <ul> <li>Manage pre-existing infrastructure by terraform</li> <li>Import manually created resources to be under the control of terraform configurations</li> </ul>"},{"location":"DevOps/Terraform/State/Import/#simple-resource","title":"simple resource","text":"<ul> <li>create terraform code</li> <li>import state: <code>terraform init</code> and <code>terraform import &lt;terraform-resource-name&gt;.&lt;resource-label&gt; &lt;azure-resource-id&gt;</code></li> <li>check new resource in state file: <code>cat terraform.tfstate</code></li> <li>validate terraform configuration <code>terraform plan</code> should report no changes</li> </ul>"},{"location":"DevOps/Terraform/State/Import/#multiple-resources","title":"multiple resources","text":"<ul> <li>create terraform code for all resources</li> <li>import state for resources one by one</li> <li>check and validate the states</li> </ul>"},{"location":"DevOps/Terraform/State/Import/#import-module","title":"import module","text":"<ul> <li>in the folder calling the module with <code>backend.tf</code> run <code>terraform init</code></li> <li>then for each resource run <code>terraform import module.&lt;module-name&gt;.&lt;terraform-resource-name&gt;.&lt;resource-label&gt; &lt;azure-resource-id</code></li> </ul>"},{"location":"DevOps/Terraform/State/Import/#import-one-of-for_each-from-module","title":"import one of for_each from module","text":"<p>in unix shell, use single quotes to make the inner address be taken literally <pre><code>terraform import 'module.my_storage.azurerm_storage_container.container[\"&lt;container-name&gt;\"]' \\\nhttps://&lt;storage-account&gt;.blob.core.windows.net/&lt;container-name&gt;\n</code></pre> in windows the double quotes must be escaped with a backslash and use quotes: <pre><code>terraform import 'module.my_storage.azurerm_storage_container.container[\\\"&lt;container-name&gt;\\\"]' \\\nhttps://&lt;storage-account&gt;.blob.core.windows.net/&lt;container-name&gt;\n</code></pre></p>"},{"location":"DevOps/Terraform/State/Import/#import-with-variables","title":"import with variables","text":"<p>https://stackoverflow.com/questions/57187782/how-to-use-terraform-import-with-passed-in-variables</p> <p>Note that even variables in the variables.tf that are not used are still required. <pre><code>terraform import -var 'environment=sandbox' azurerm_storage_account.my_storage foo\nterraform import -var-file='../tfvars/prod.tfvars' 'module.MySystem.azurerm_windows_virtual_machine.windsvm[\"dsvm0003\"]' \\\n\"/subscriptions/xxx-xxx-xxx-xxx-xxx/resourceGroups/myRG/providers/Microsoft.Compute/virtualMachines/DSVM0003\"\n</code></pre></p> <p>Another way is set default values for the variables in the <code>variables.tf</code> file or set the real value from the <code>state</code> file.</p> <p>terragrunt: in <code>terragrunt.hcl</code> the <code>inputs</code> section can set the values for the input variables from dependencies. Therefore the best way to import the existing resource is via <code>terragrunt import ADDR ID</code>.</p>"},{"location":"DevOps/Terraform/State/State/","title":"State","text":""},{"location":"DevOps/Terraform/State/State/#backend","title":"backend","text":"<p>Used to store the states remotely - manage conflicts between team members by locking - keep the states from accessing by other people</p>"},{"location":"DevOps/Terraform/State/State/#manually-update-remote-state","title":"manually update remote state","text":"<p>This is required when properties have been changed between versions</p> <p>https://stackoverflow.com/questions/63427604/how-do-you-fix-terraform-unsupported-attribute-ses-smtp-password-after-upgradi <pre><code>terraform state pull &gt; state.json\n#edit state.json and increment the serial attribute (ex \"serial\": 10, -&gt; \"serial\": 11,)\nterraform state push state.json\n# run terraform plan\nrm state.json #delete it\n</code></pre></p>"},{"location":"DevOps/Terraform/State/State/#error-locking-state-error-acquiring-the-state-lock-state-blob-is-already-locked","title":"Error locking state: Error acquiring the state lock: state blob is already locked","text":"<p>Solution in Azure: https://stackoverflow.com/questions/64690427/error-locking-state-error-acquiring-the-state-lock-state-blob-is-already-locke - navigate to the storage account - then to the container in the Azure portal that holds the state file - the blob will show as <code>Leased</code> under the leased state column - select the state file, and hit the <code>break lease</code> button - Note: need Privileged Identity Management (PIM) to do this</p>"},{"location":"DevOps/Terraform/TFModule/null_resource/","title":"null_resource","text":"<p>https://faun.pub/terraform-null-provider-and-null-resource-explained-6a8d674cad63</p> <p>The <code>null_resource</code> is commonly used to run scripts on a specified trigger.</p> <p>Using <code>local-exec</code> provider within it means that anything that code actually does, will not be held in the Terraform state file.</p>"},{"location":"DevOps/Terraform/TFModule/random_id/","title":"random_id","text":"<p>https://registry.terraform.io/providers/hashicorp/random/latest/docs/resources/id</p> <p>The keepers are seeds for the random string that is generated, to ensure the random string is deterministic - until the keepers changed.</p>"},{"location":"DevOps/Terraform/Variable/List/","title":"List","text":""},{"location":"DevOps/Terraform/Variable/List/#list-vs-map","title":"list vs map","text":"<p>https://sokolovtech.com/terraform/94-lists-vs-maps-in-terraform-variables</p>"},{"location":"DevOps/Terraform/Variable/List/#list-of-objects","title":"List of objects","text":"<pre><code>variable \"shops\" {\n  type = list(object({\n    name      = string\n    address   = optional(string)\n  }))\n  default = []\n}\n\nshops = [\n  {\n    name      = \"flowers\",\n    address   = \"Asia\",\n  }, {\n    name      = \"animals\",\n    address   = \"Africa\",\n  },\n]\n\noptional_shops {\n  dynamic \"my_shop\" {\n    for_each = var.shops[*]\n    iterator = shop\n    content {\n      name      = shop.value.name\n      address   = shop.value.address\n    }\n  }\n}\n</code></pre>"},{"location":"DevOps/Terraform/Variable/String/","title":"String","text":""},{"location":"DevOps/Terraform/Variable/String/#default","title":"default","text":"<pre><code>variable \"string_value\" {\n  type    = string\n  default = null\n}\n\nvar.string_value == null ? 0 : 1\n</code></pre>"},{"location":"DevOps/Terraform/Variable/Variable/","title":"Variable","text":""},{"location":"DevOps/Terraform/Variable/Variable/#map-vs-object","title":"map vs object","text":"<ul> <li><code>map(string)</code> only accepts string value, but cannot validate the key types</li> <li>an object is a map with more explicit keying</li> <li>object does not have default keys, have to specify every key</li> <li>object will not accept any type that does not match the pattern   <pre><code>object({\n  id = string\n  size = number\n  labels = map(string)\n})\n</code></pre></li> </ul>"},{"location":"DevOps/Terraform/Variable/terraform.tfvars/","title":"terraform.tfvars","text":"<ul> <li><code>terraform.tfvars</code> serves as a way to store variable values for your infrastructure configuration. </li> <li>it contains variable names and their corresponding values, separated by an equals sign (<code>=</code>).</li> </ul>"},{"location":"DevOps/Terraform/Variable/terraform.tfvars/#why-we-need-terraformtfvars","title":"why we need <code>terraform.tfvars</code>","text":"<ul> <li>Purpose: separate variable definitions from Terraform code. This promotes code reusability and makes it easier to manage different environments with different variable values.</li> <li>Benefits:<ul> <li>Flexibility: You can easily change variable values by modifying the <code>terraform.tfvars</code> file, without altering your Terraform code.</li> <li>Security: Sensitive information like API keys or passwords can be kept out of your main Terraform code, potentially enhancing security.</li> <li>Environment-specific configurations: You can create multiple <code>terraform.tfvars</code> files for different environments (e.g., development, staging, production), each with its own set of variable values.</li> </ul> </li> <li>Usage: Terraform automatically loads variables defined in <code>terraform.tfvars</code> when you run commands like <code>terraform apply</code>. You can also specify additional <code>tfvars</code> files using the <code>-var-file</code> flag.</li> </ul> <p>Note that the <code>terraform.tfvars</code> file is not mandatory.  We can pass variable values in various ways, such as through command-line flags, environment variables, or interactive prompts.</p>"},{"location":"DevOps/Terragrunt/Backend/","title":"Backend","text":""},{"location":"DevOps/Terragrunt/Dependency/","title":"Dependency","text":""},{"location":"DevOps/Terragrunt/Dependency/#mock-outputs","title":"mock outputs","text":"<p>https://terragrunt.gruntwork.io/docs/features/execute-terraform-commands-on-multiple-modules-at-once/</p> <p>If the module depends on the outputs of another module that hasn't been applied yet, we need to use the <code>mock_outpus</code> when <code>run-all plan/validate</code>.</p>"},{"location":"DevOps/Terragrunt/Dependency/#when-to-run-all-discussion","title":"when to run-all discussion","text":"<p>https://github.com/gruntwork-io/terragrunt/issues/727</p>"},{"location":"DevOps/Terragrunt/Issue/","title":"Issue","text":""},{"location":"DevOps/Terragrunt/Issue/#checksums-mismatch","title":"checksums mismatch","text":"<p>Updating terraform version will get error <code>Terraform does not match any of the checksums recorded in the dependency lock file</code>.</p> <p>Solution: - Delete all <code>.terraform.lock.hcl</code> files: <code>Get-ChildItem -Path . -Filter \".terraform.lock.hcl\" -Recurse | Remove-Item -Force</code> - Delete all <code>.terraform</code> folders: <code>Get-ChildItem -Path . -Filter \".terraform\" -Directory -Recurse -ErrorAction SilentlyContinue | Remove-Item -Recurse -Force</code> - Run <code>terragrunt run-all init -upgrade</code></p>"},{"location":"DevOps/Terragrunt/Provider/","title":"provider","text":"<pre><code># root folder\ngenerate \"provider\" {\n  path      = \"provider.tf\"\n  if_exists = \"overwrite\"\n  contents  = &lt;&lt;EOF\n    terraform {\n      required_providers {\n        azurerm = {\n          source  = \"hashicorp/azurerm\"\n          version = \"3.0.0\"\n        }\n      }\n      #backend \"azurerm\" {}\n    }\n\n    provider \"azurerm\" {\n      features {}\n      subscription_id = \"2445bc57-cb0e-448a-ac3a-c4c41c63eef2\"\n    }\n }\n\n# moduler terragrunt.hcl\ninclude {\n  path = find_in_parent_folders()\n}\n</code></pre>"},{"location":"DevOps/Terragrunt/Terragrunt/","title":"Terragrunt","text":"<p>https://repo1.dso.mil/platform-one/big-bang/customers/template/-/tree/main/terraform</p> <p>https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/</p> <ul> <li>make providers dry</li> <li>make backend dry</li> <li>will automatically determine dependencies</li> </ul>"},{"location":"DevOps/Terragrunt/Terragrunt/#install-in-linux-from-binary","title":"install in linux from binary","text":"<p>find the latest in the assets section in: https://github.com/gruntwork-io/terragrunt/releases <pre><code>wget https://github.com/gruntwork-io/terragrunt/releases/download/v0.42.5/terragrunt_linux_amd64\nmv terragrunt_linux_amd64 terragrunt\nchmod u+x terragrunt\nsudo mv terragrunt /usr/local/bin/terragrunt\nterragrunt --help\n</code></pre></p>"},{"location":"DevOps/Terragrunt/Terragrunt/#commands","title":"commands","text":"<pre><code>terragrunt run-all init            #initialize\nterragrunt run-all init -upgrade   #upgrade providers if init fails\nterragrunt run-all plan            #pre-check\nterragrunt run-all plan *&gt; c:/terragrunt.plan.output #had issues\nterragrunt run-all apply           #deploy, this incurs cost!\nterragrunt run-all destroy         #destroy\nterragrunt run-all refresh         #update state to match remote systems, can sometimes fix errors\nterragrunt run-all show -json planfile\n</code></pre>"},{"location":"DevOps/Terragrunt/Terragrunt/#execute-terraform-command-on-specific-module","title":"execute terraform command on specific module","text":"<ul> <li>cd into that module and run command without run-all, or</li> <li>use <code>--terragrunt-working-dir &lt;module-path&gt;</code> parameter</li> </ul>"},{"location":"DevOps/Terragrunt/Terragrunt/#hcl","title":"hcl","text":"<p>root - remote_state - generate \"provider\"</p> <p>subfolder - include - dependency - inputs</p>"},{"location":"DevOps/Terragrunt/Terragrunt/#dependency","title":"dependency","text":"<p>terragrunt.hcl <pre><code>dependency \"aks\" {\n  config_path = \"../aks\"\n  mock_outputs = {\n    cluster_name           = \"mock-name\"\n    cluster_resource_group = \"mock-rg\"\n  }\n}\n\ninputs = {\n  aks_name           = dependency.aks.cluster_name\n  aks_resource_group = dependency.aks.cluster_resource_group\n}\n</code></pre></p>"},{"location":"DevOps/Terragrunt/Terragrunt/#inputs","title":"inputs","text":"<p>get the dependency outputs as inputs of the variables in the current module</p>"},{"location":"DevOps/Terragrunt/Blog/LearnTerragrunt/","title":"How to learn terragrunt","text":""},{"location":"DevOps/Terragrunt/Blog/LearnTerragrunt/#read-the-blogs","title":"read the blogs","text":"<ul> <li>how it used to link different resources together with orders</li> </ul>"},{"location":"DevOps/Terragrunt/Blog/LearnTerragrunt/#global-provides","title":"global provides","text":"<ul> <li>how to set providers in one place</li> </ul>"},{"location":"DevOps/Terragrunt/Blog/LearnTerragrunt/#dependencies","title":"dependencies","text":"<p>pass variable values - mock - local</p>"},{"location":"DevOps/Test/Locust/","title":"Locust","text":"<p>Locust is an easy to use, scriptable and scalable (load) performance testing tool.</p> <p>Users behaviour can be defined in regular Python code, instead of being stuck in a UI or restrictive domain specific language.</p> <p>Locust primarily works with web sites/services, it can be used to test almost any system or protocol.</p>"},{"location":"DevOps/Test/Selenium/","title":"Selenium","text":""},{"location":"DevOps/Test/Test/","title":"Test","text":""},{"location":"DevOps/Test/Test/#unit-test","title":"Unit test","text":"<p>Check if the small piece of code (can be as small as a function) is doing what it is suppose to do.</p>"},{"location":"DevOps/Test/Test/#python-test-frameworks","title":"Python test frameworks","text":"<ul> <li>unittest</li> <li>pytest</li> <li>nosetest</li> </ul>"},{"location":"DevOps/Test/Test/#integration-test","title":"Integration Test","text":"<p>A level of software testing where individual units are combined and tested as a group.</p>"},{"location":"DevOps/Test/Test/#system-testing","title":"System Testing","text":"<p>Load Testing - whether software solution will perform under real-life loads. Gatling/Locust</p>"},{"location":"DevOps/Test/Test/#acceptance-testing","title":"Acceptance Testing","text":"<p>Performed by the end user or the client to verify/accept the software system before moving the software application to the production environment.</p>"},{"location":"DevOps/Yaml/String/","title":"String","text":""},{"location":"DevOps/Yaml/String/#yaml-folding","title":"YAML folding","text":"<p>https://stackoverflow.com/questions/3790454/how-do-i-break-a-string-in-yaml-over-multiple-lines - <code>&gt;</code>, <code>|</code>: \"clip\": keep the line feed, remove the trailing blank lines - <code>&gt;-</code>, <code>|-</code>: \"strip\": remove the line feed, remove the trailing blank lines - <code>&gt;+</code>, <code>|+</code>: \"keep\": keep the line feed, keep trailing blank lines</p> <p>Example: <pre><code>- name: ENV_NAME\n  value: &gt;-\n    X x\n    Y y\n    Z,123\n</code></pre> Is the same as this: <pre><code>- name: ENV_NAME\n  value: \"X x Y y Z,123\"\n</code></pre></p> <p>Only way to concatenate lines without spaces - <code>double-quoted style</code> (\\ and \" must be escaped by \\, newlines can be inserted with a literal \\n sequence) <pre><code>Key: \"this is my very very \\\"very\\\" loooo\\\n  ng string.\\n\\nLove, YAML.\"\n</code></pre> will be converted to: <pre><code>Key: \"this is my very very \\\"very\\\" loooong string.\\n\\nLove, YAML.\"\n</code></pre></p>"},{"location":"DevOps/Yaml/String/#string-quotes","title":"string quotes","text":"<p>https://leopathu.com/content/string-quotes-yaml-file</p> <p>https://stackoverflow.com/questions/19109912/yaml-do-i-need-quotes-for-strings-in-yaml</p> <p>https://stackoverflow.com/questions/3790454/how-do-i-break-a-string-in-yaml-over-multiple-lines/21699210#21699210</p> <p>Strings containing any of the following characters must be quoted - <code>{ } [ ] &lt; &gt; = ! @ # % &amp; * - ? : , | \\</code> not always true, depending on where the character is - it's more convenient to use single quotes, without escaping any backslash \\ - double quotes provide a way to express arbitrary strings, by using \\ to escape characters and sequences   <code>\"Add new line\\n\"</code></p>"},{"location":"DevOps/Yaml/String/#strings-must-be-quoted","title":"strings must be quoted","text":"<ul> <li>string is <code>true</code> or <code>false</code> (otherwise, it would be treated as a boolean value)</li> <li>string is <code>null</code> or <code>~</code> (otherwise, it would be considered as a null value)</li> <li>string looks like a <code>number</code>, such as integers (11), floats (3.14) and exponential numbers (1e2) (otherwise, it would be treated as a numeric value)</li> <li>string looks like a <code>date</code> (e.g. 2014-12-31) (otherwise it would be automatically converted into a Unix timestamp)</li> </ul> <p>Strings only need quotation if (the beginning of) the value can be misinterpreted as a data type or the value contains a \":\" (could get misinterpreted as key): - <code>foo: \"{{ bar }}\"</code> can be misinterpreted as datatype dict - <code>foo: \"bar:baz\"</code> can be misinterpreted as key - <code>&gt;</code> interior line breaks are stripped out   <pre><code>key: &gt;\n  A long\n  string here.\n</code></pre> - <code>|</code> interior line breaks are preserved as <code>\\n</code> <pre><code>key: |\n  A long\n  string here.\n</code></pre> - <code>&gt;-</code> or <code>|-</code> no line breaks at the end</p>"},{"location":"DevOps/Yaml/Yaml/","title":"Yaml","text":""},{"location":"DevOps/Yaml/Yaml/#yaml","title":"Yaml","text":""},{"location":"DevOps/Yaml/Yaml/#pass-var-to-yaml","title":"pass var to yaml","text":"<p>https://stackoverflow.com/questions/48296082/how-to-set-dynamic-values-with-kubernetes-yaml-file <pre><code>template=`cat \"deploy.yml.template\" | sed \"s/{{ MYVARNAME }}/$MYVARVALUE/g\"`\n\n# apply the yml with the substituted value\necho \"$template\" | kubectl apply -f -\n</code></pre></p>"},{"location":"DevOps/Yaml/Yaml/#pass-variable-by-json","title":"pass variable by json","text":"<p>https://learn.microsoft.com/en-us/azure/devops/pipelines/process/expressions?view=azure-devops#converttojson <pre><code>MY_JSON: ${{ convertToJson(parameters.listOfValues) }}\n</code></pre> https://stackoverflow.com/questions/63502743/pass-variables-to-yaml-template-like-parameters-in-azure-pipelines <pre><code>$obj = ConvertFrom-Json '$(${{ parameters.listOfValuestInJson }})'\n</code></pre></p>"},{"location":"Documentation/Jira/","title":"Jira","text":""},{"location":"Documentation/Jira/#advanced-search","title":"advanced search","text":"<pre><code>project = \"TEST\" AND assignee = currentuser()\n\nstatus=resolved AND (project=\"Teams in Space\" OR assignee=captainjoe)\n\n#contain words Jira and Software, in no particular order\ntext ~ \"Jira Software\"\n\n#exact\ntext ~ \"\\\"Jira Software\\\"\"\n</code></pre>"},{"location":"Documentation/Nginx/","title":"Nginx","text":""},{"location":"Documentation/Nginx/#put-static-web-files-in-nginx","title":"put static web files in Nginx?","text":"<p>Run a nginx service to provide host for all documents.</p>"},{"location":"Documentation/Blog/Learn/","title":"Learm","text":""},{"location":"Documentation/Blog/Learn/#blog-website","title":"blog website","text":"<p>https://www.confessionsofadataguy.com/introduction-to-unit-testing-with-pyspark</p>"},{"location":"Documentation/Blog/SASS/","title":"SASS","text":"<p>Excellent question \ud83d\ude4c \u2014 the way Jekyll themes (like Minima or custom ones on GitHub Pages) organize their Sass (<code>.scss</code>) files can feel a bit mysterious at first. Let\u2019s break down the relationships:</p>"},{"location":"Documentation/Blog/SASS/#the-_sass-folder-in-jekyll","title":"\ud83d\udcc2 The <code>_sass</code> folder in Jekyll","text":"<ul> <li>This folder contains partial Sass files (filenames start with <code>_</code> like <code>_layout.scss</code>, <code>_variables.scss</code>, <code>_base.scss</code>).</li> <li>These files don\u2019t compile into CSS by themselves. Instead, they\u2019re imported into a main stylesheet (often <code>assets/main.scss</code> or <code>assets/css/style.scss</code>).</li> </ul>"},{"location":"Documentation/Blog/SASS/#common-files-their-roles","title":"\ud83e\udde9 Common Files &amp; Their Roles","text":"<ol> <li> <p><code>_variables.scss</code></p> </li> <li> <p>Stores reusable values like:</p> <p><pre><code>$base-font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n$base-font-size: 16px;\n$spacing-unit: 1.5rem;\n$brand-color: #2a7ae2;\n</code></pre>    * These variables get used everywhere else, so if you change <code>$base-font-size</code>, it updates across the site.</p> </li> <li> <p><code>_layout.scss</code></p> </li> <li> <p>Defines the layout structure of your site:</p> <ul> <li>Site header (<code>.site-header</code>)</li> <li>Navigation bar (<code>.site-nav</code>)</li> <li>Posts container (<code>.post-content</code>)</li> <li>Sidebar, footer</li> <li>It typically uses variables from <code>_variables.scss</code> so you don\u2019t hardcode sizes/colors.</li> </ul> </li> <li> <p><code>_base.scss</code> (sometimes called <code>_typography.scss</code> or <code>_reset.scss</code>)</p> </li> <li> <p>Sets global styles:</p> <ul> <li><code>body { font-family: $base-font-family; font-size: $base-font-size; }</code></li> <li>Link styles (<code>a { color: $brand-color; }</code>)</li> <li>Headings (<code>h1, h2, h3 { font-weight: bold; }</code>)</li> <li>Often imports Google Fonts or sets up the baseline typography for posts.</li> </ul> </li> <li> <p><code>_syntax-highlighting.scss</code></p> </li> <li> <p>Controls how code blocks look (important for GitHub Pages blogs).</p> </li> <li> <p>Usually overrides Rouge/Pygments highlighting.</p> </li> <li> <p><code>_posts.scss</code> or <code>_blog.scss</code> (if present)</p> </li> <li> <p>Controls post-specific styles:</p> <ul> <li><code>.post-title</code>, <code>.post-content</code>, <code>.post-meta</code></li> <li>Font size, line-height, spacing for blog text</li> </ul> </li> </ol>"},{"location":"Documentation/Blog/SASS/#how-they-work-together","title":"\ud83d\udcca How They Work Together","text":"<ol> <li>You have a main SCSS entry file like <code>assets/main.scss</code>:</li> </ol> <pre><code>---\n# Only the front matter makes Jekyll process this as SCSS\n---\n@import \"variables\";\n@import \"base\";\n@import \"layout\";\n@import \"syntax-highlighting\";\n@import \"posts\";\n</code></pre> <ol> <li> <p>Jekyll compiles this into one minified CSS file (<code>_site/assets/main.css</code>).</p> </li> <li> <p>Your site\u2019s HTML (<code>_layouts/default.html</code>) links to that CSS.</p> </li> <li> <p>When you tweak <code>_variables.scss</code>, it flows into <code>_layout.scss</code>, <code>_posts.scss</code>, etc., changing styles everywhere.</p> </li> </ol>"},{"location":"Documentation/Blog/SASS/#controlling-post-fonts-styles","title":"\ud83d\udd24 Controlling Post Fonts &amp; Styles","text":"<p>If you specifically want to control how posts look (like font family, size, spacing):</p> <ul> <li>Look inside <code>_base.scss</code> and <code>_posts.scss</code> (or <code>_layout.scss</code> if the theme mixes them).</li> <li>Common selectors:</li> </ul> <pre><code>.post-content {\n  font-family: $base-font-family;\n  font-size: 1rem;   // inherits from $base-font-size\n  line-height: 1.7;\n}\n\n.post-title {\n  font-size: 2rem;\n  font-weight: bold;\n}\n</code></pre> <p>Change the variables in <code>_variables.scss</code> or override directly in <code>_posts.scss</code>.</p> <p>\u2705 Summary:</p> <ul> <li><code>_variables.scss</code> = the \u201csettings\u201d (fonts, colors, sizes).</li> <li><code>_base.scss</code> = the \u201cfoundation\u201d (typography, reset, global body styles).</li> <li><code>_layout.scss</code> = the \u201cstructure\u201d (header, nav, footer, containers).</li> <li><code>_posts.scss</code> = the \u201ccontent styling\u201d (titles, paragraphs, meta).</li> <li>All are imported into <code>main.scss</code>, which GitHub Pages compiles into your site\u2019s CSS.</li> </ul>"},{"location":"Documentation/Dokuwiki/Dockerization/","title":"Dockerization","text":"<p>https://www.annhe.net/article-4339.html</p> <p>Use docker and git to work together with dokuwiki</p>"},{"location":"Documentation/Dokuwiki/LinuxInstall/","title":"Install Docuwiki on Linux","text":""},{"location":"Documentation/Dokuwiki/LinuxInstall/#install-php-70-and-extensions","title":"install php 7.0 and extensions","text":"<pre><code>sudo apt install -y php7.0 php7.0-cli php7.0-fpm php7.0-gd php7.0-xml php7.0-zip\n</code></pre>"},{"location":"Documentation/Dokuwiki/LinuxInstall/#install-apache2","title":"install apache2","text":"<pre><code>sudo apt update\nsudo apt install apache2\n\nsudo apt install libapache2-mod-php7.0\nsudo a2enmod php7.0 #enable php7.0 in apache2\nsudo service apache2 restart\n\nsudo systemctl stop apache2.service\nsudo systemctl start apache2.service\nsudo systemctl enable apache2.service\n</code></pre>"},{"location":"Documentation/Dokuwiki/LinuxInstall/#install-dokuwiki","title":"install dokuwiki","text":"<pre><code>stat -c '%G' /var/www/dokuwiki #check folder owner\nsudo mkdir -p /var/www/dokuwiki #create a document root directory\nsudo chown -R johndoe:johndoe /var/www/dokuwiki #change ownership of the directory to user\ncd /var/www/dokuwiki\nwget https://download.dokuwiki.org/src/dokuwiki/dokuwiki-stable.tgz #download newest stable release of DokuWiki\n\n#Unpack the DokuWiki tarball.\ntar xvf dokuwiki-stable.tgz\nrm dokuwiki-stable.tgz\nsudo mv dokuwiki/dokuwiki-*/ dokuwiki\nrmdir dokuwiki-2018-04-22b/\n\n#Change ownership of the /var/www/dokuwiki directory to www-data\nsudo chown -R www-data:www-data /var/www/dokuwiki\n\n#Change document root in Apache to point to /var/www/dokuwiki\nsudo nano /etc/apache2/sites-enabled/000*.conf #Replace DocumentRoot /var/www/html with DocumentRoot /var/www/dokuwiki\n\n#Change AllowOverrides setting in Apache2 to use .htaccess files for security\nsudo vi /etc/apache2/apache2.conf #For directory /var/www/ replace AllowOverride None with AllowOverride All\n\n#Restart php7.0-fpm.service\nsudo systemctl restart php7.0-fpm.service\n\nsudo rm /var/www/dokuwiki/install.php\n</code></pre>"},{"location":"Documentation/Dokuwiki/Setup/","title":"setup","text":"<p>https://www.dokuwiki.org/devel:css?s%5B%5D=userstyle.css#user_styles</p>"},{"location":"Documentation/Dokuwiki/Setup/#disable-update-messages","title":"disable update messages","text":"<p>in dokuwiki/doku.php, change $updateVersion to a value higher than the unique ID shown in square brackets of the update messages</p>"},{"location":"Documentation/Dokuwiki/Setup/#dokuwiki-admin-blank-how-to-open","title":"dokuwiki admin blank, how to open","text":"<p>http://YOUR_DOMAIN_PATH/doku.php?id=start&amp;do=admin&amp;page=extension</p>"},{"location":"Documentation/Dokuwiki/Setup/#change-default-page-width","title":"change default page width","text":"<p>you need to put style.local.ini into \"../Sites/wiki/lib/tpl/dokuwiki/style.local.ini\" if you make use of the dokuwiki-template.   _site_width__ = \"100%\"</p>"},{"location":"Documentation/Dokuwiki/Setup/#adjust-space-before-list-and-headers","title":"adjust space before list and headers","text":"<p>```css conf/userstyle.css / move list up / p + ul {   margin-top: -20px; }</p> <p>p + ol {   margin-top: -20px; }</p> <p>/ h2,h3 bottom margin / h2, h3 {   margin-bottom: 4px; } ```</p>"},{"location":"Documentation/Dokuwiki/Start/","title":"Start","text":""},{"location":"Documentation/Dokuwiki/Start/#start","title":"Start","text":"<p>{{cc.png|test}}</p> <p>See [[wiki:syntax|syntax]] for Wiki syntax.</p> <pre><code>This is a\nplain text example.\n</code></pre> <p>And a math ($x = ay^2 + b\\sqrt{y} + c$) example: $$x_{\\rm mean} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$$</p> <p>```file txt file_example.txt This is a file example with export title. <pre><code>```py\n#python code example\nimport pandas as pd\ndf = new DataFrame()\ndf['id'] = ['a','b','c']\ndf['val'] = [1,2,3]\n</code></pre></p>"},{"location":"Documentation/Dokuwiki/Start/#setup","title":"Setup","text":"<ul> <li>Go to your Extension Manager page on the Admin screen and install some useful extensions</li> <li>Create a page called \u201csidebar\u201d in the root namespace with this code in it: %%{{indexmenu&gt;..#1|js navbar nocookie id#random}}%%</li> <li>Go to the the Configuration Settings page on the Admin screen and set the following options: <pre><code>useslash = true\nplugin \u2192 indexmenu \u2192 skip_index = /(^wiki$|^playground$)/\nplugin \u2192 indexmenu \u2192 skip_file = /(^sidebar$)/\n</code></pre></li> </ul>"},{"location":"Documentation/Dokuwiki/Start/#extensions","title":"Extensions","text":"<ul> <li>[[https://www.dokuwiki.org/plugin:indexmenu|IndexMenu]] for left side tree navigation with namespaces</li> <li>[[https://www.dokuwiki.org/plugin:plaintext|PlainText]] for keeping newline in text</li> <li>[[https://www.dokuwiki.org/plugin:mathjax|MathJax]] for LaTex style math input</li> </ul>"},{"location":"Documentation/Dokuwiki/Start/#change-dataconf-folder","title":"Change data/conf folder","text":"<ul> <li>move the two folders to a new directory</li> <li>in conf/local.php add: $conf['savedir'] = 'C:/Users/usr/media/wiki/data';</li> <li>dokuwiki/inc/preload.php add: define('DOKU_CONF', 'C:/Users/usr/media/wiki/conf/');</li> <li>Note that the relative path does not work and the bat file should also be updated</li> </ul>"},{"location":"Documentation/Dokuwiki/Start/#change-default-font-size","title":"Change default font size","text":"<pre><code>/* create the file conf/userstyle.css */\nbody {\n    font: normal 75%/1.4 Arial, sans-serif;\n    /* default font size: 100% =&gt; 16px; 93.75% =&gt; 15px; 87.5% =&gt; 14px; 81.25% =&gt; 13px; 75% =&gt; 12px; 68.75% =&gt; 11px */\n    -webkit-text-size-adjust: 100%;\n}\n</code></pre>"},{"location":"Documentation/Dokuwiki/Start/#set-timezone","title":"Set timezone","text":"<pre><code>// create the file config/local.protected.php\n&lt;?php\ndate_default_timezone_set(\"Australia/Brisbane\");\n?&gt;\n</code></pre>"},{"location":"Documentation/Dokuwiki/Start/#change-edit-page-height","title":"Change Edit-Page height","text":"<p>```js conf/userscript.js jQuery(function() {     jQuery('#wiki__text').height('480px'); }); <pre><code>## Remove extra space between paragraph and list\n```css conf/userstyle.css\n.page p + ul {\n    margin-top: -1.3em;\n}\n</code></pre></p>"},{"location":"Documentation/Dokuwiki/Start/#page-width","title":"page width","text":"<p>```css conf/userstyle.css</p>"},{"location":"Documentation/Dokuwiki/Start/#dokuwiki__site","title":"dokuwiki__site {","text":"<pre><code>max-width: 90% !important;\n</code></pre> <p>} ```</p>"},{"location":"Documentation/Dokuwiki/Start/#move-the-wiki-to-another-folder","title":"Move the wiki to another folder","text":"<p>DokuWiki Setup Error: The datadir ('pages') at /pages is not found, isn't accessible or writable. You should check your config and permission settings. Or maybe you want to run the installer?</p> <p>Solution:   * change savedir to the new data path in conf/local.php   * change DOKU_CONF definition in dokuwiki/inc/preload.php to the conf path</p> <p>Dokuwiki warning: \"Unknown:\" failed to open stream windows</p> <p>Solution   * the data path should not have spaces</p>"},{"location":"Documentation/MkDocs/AutoNav/","title":"Create nav automcatically","text":""},{"location":"Documentation/MkDocs/AutoNav/#install-package","title":"install package","text":"<pre><code>pip install mkdocs-awesome-pages-plugin\n</code></pre>"},{"location":"Documentation/MkDocs/AutoNav/#update-mkdocsyml","title":"update mkdocs.yml","text":"<p>remove <code>nav</code> section and add <pre><code>plugins:\n  - awesome-pages\n</code></pre></p>"},{"location":"Documentation/MkDocs/CliDoc/","title":"CLI docs","text":""},{"location":"Documentation/MkDocs/CliDoc/#example","title":"example","text":"<pre><code># My-CLI Reference\n\nDocumentation for my command line tools.\n\n::: mkdocs-click\n    :module: my.module.path.sim.main\n    :command: cli\n    :prog_name: simulation\n    :depth: 1\n\n::: mkdocs-click\n    :module: my.module.path.opt.main\n    :command: cli\n    :prog_name: optimization\n    :depth: 1\n</code></pre>"},{"location":"Documentation/MkDocs/MkDocs/","title":"MkDocs","text":"<ul> <li>https://www.mkdocs.org/user-guide/</li> <li>https://mkdocs.readthedocs.io/en/0.13.3/user-guide/styling-your-docs/</li> <li>https://realpython.com/python-project-documentation-with-mkdocs/</li> </ul> <p>MkDocs is a fast and simple static site generator that\u2019s geared towards building project documentation with Markdown. <pre><code>mkdocs new &lt;proj-name&gt; #create a new docs project\nmkdocs serve           #run in preview mode\nmkdocs build           #build the docs project\nmkdocs build --config-file &lt;config-file&gt; --site-dir &lt;output-path&gt;\n</code></pre></p>"},{"location":"Documentation/MkDocs/MkDocs/#packages","title":"packages","text":"<ul> <li><code>mkdocs</code> (mkdocs): build static pages from Markdown</li> <li><code>mkdocstrings</code> (mkdocstrings-python): auto-generate documentation from docstrings in your code. It's the Python handler for mkdocstrings that allows mkdocstrings to parse Python code.</li> <li><code>material</code> (mkdocs-material): style mkdocs documentation</li> </ul>"},{"location":"Documentation/MkDocs/MkDocs/#navigation","title":"navigation","text":"<p>https://squidfunk.github.io/mkdocs-material/setup/setting-up-navigation/#instant-loading <pre><code>theme:\n  name: material\n  custom_dir: theme\n  language: en\n  features:\n    - navigation.indexes\n    - navigation.tabs\n    - navigation.tabs.sticky\n    - navigation.tracking\n    - search.highlight\n    - search.suggest\n  palette:\n    scheme: preference\n    primary: amber\n    accent: yellow\n  font:\n    text: Roboto\n    code: Roboto Mono\n  icon:\n    logo: material/library\n    repo: fontawesome/brands/git-alt\n</code></pre></p>"},{"location":"Documentation/MkDocs/Setting/","title":"Setting","text":""},{"location":"Documentation/MkDocs/Setting/#mkdocsyml","title":"mkdocs.yml","text":"<p>https://mkdocs.readthedocs.io/en/0.10/user-guide/configuration/</p>"},{"location":"Documentation/MkDocs/Setting/#mkdocs-material-settings","title":"mkdocs-material settings","text":"<p>https://squidfunk.github.io/mkdocs-material/customization/</p> <p>https://squidfunk.github.io/mkdocs-material/setup/setting-up-navigation/#navigation-tabs <pre><code>- header.autohide        #hide header after scrolling a few lines\n- navigation.tabs        #top level sections as tabs after the header\n- navigation.tabs.sticky #navigation tabs will remain visible when scrolling down\n- navigation.indexes     #attach documents to sections\n- navigation.tracking    #url address automatically updated\n- search.highlight       #all saerch occurance will be highlighted\n- search.suggest         #search suggestions\n</code></pre></p>"},{"location":"Documentation/MkDocs/Setting/#md-files-n-root-folder","title":"md files n root folder","text":"<p>solution: use <code>symlink</code> <pre><code>mkdir _docs\nln -s ../README.md _docs/index.md\nln -s ../AI _docs/AI\nln -s ../AWS _docs/AWS\nln -s ../Azure _docs/Azure\nln -s ../Coding _docs/Coding\nln -s ../Cybersecurity _docs/Cybersecurity\nln -s ../Data _docs/Data\nln -s ../DevOps _docs/DevOps\nln -s ../Documentation _docs/Documentation\nln -s ../Energy _docs/Energy\nln -s ../Learn _docs/Learn\nln -s ../ML _docs/ML\nln -s ../Optimization _docs/Optimizaton\nln -s ../PowerBI _docs/PowerBI\nln -s ../Python _docs/Python\nln -s ../SQL _docs/SQL\nln -s ../System _docs/System\n</code></pre></p>"},{"location":"Documentation/Sphinx/Sphinx/","title":"Sphinx","text":"<p>when to use <code>sphinx</code> and when to use <code>mkdocs</code> - sphinx: complicated - for large projects - mkdocs: simple - good for small projects</p>"},{"location":"Documentation/Sphinx/Sphinx/#build-sphinx-docs","title":"Build Sphinx docs","text":"<p>Install <code>sphinx-build</code> first <pre><code>mamba install sphinx myst-parser -y\nmamba install furo # install theme\n</code></pre> Then run <pre><code>cd docs\nsphinx-build -b html . _build\n</code></pre> HTML lands in <code>docs/_build</code>.</p> <p>or run from root <pre><code>sphinx-build -b html docs/api _build/html/api\n</code></pre> The output HTML will be in <code>_build/html/api</code>.</p>"},{"location":"Documentation/Sphinx/Sphinx/#pydata-theme","title":"PyData theme","text":"<pre><code>mamba install sphinx pydata-sphinx-theme sphinx-rtd-theme sphinx-autodoc-typehints sphinx-design numpydoc -y\n</code></pre>"},{"location":"Energy/Inertia/","title":"Inertia","text":"<p>https://australiainstitute.org.au/wp-content/uploads/2021/03/VEPC-system-security-report-FINAL.pdf</p> <p>Electric system inertia is a measure of energy that is stored in large rotating machines that are synchronized to the grid at 60 Hz.</p> <p>Inertia refers to the extent to which the power system resists (in micro-second time scales) changes to demand and supply.</p> <p>System strength refers to the extent to which a stable voltage waveform is maintained after disturbances to the system, such as from a short circuit.</p> <p>Batteries' inertia is not limited by the kinetic energy of coal generators' storage capacity. Whereas coal generators are only able to provide an inertial response equal to its capacity for around three seconds, batteries can provide this for far longer than the inertial response needed.</p> <p>While inertia is most often multi-regional, system strength is mostly a local or regional issue, and so supplies of it need to be sourced locally.</p> <p>The Rate of Change of Frequency (RoCoF) is measured in Hz per second. In the NEM, the automatic access standard is to be able to operate continuously with a RoCoF of 4 Hz/s for 0.25 seconds, and 3 Hz/s for more than one second.</p> <p>Reliable power system frequency operation requires a combination of: - Inertia acting in the first second after a disturbance, used to slow the rate of change of system frequency and allow generation to respond. - Fast-acting generation responding in the first thirty seconds after a disturbance (primary frequency control) and used to keep the frequency in a safe operating range. - Slower acting generation greater than thirty seconds after a disturbance (secondary frequency control), used to bring the system back to 50Hz</p> <p>In NEM, most inertia is provided by coal, gas and hydro generators. A substantia amount of inertia is also provided by the inductance in the transmission system and in the rotating on the demand side. Primary frequency control is provided by automatic generation control (AGC), regulation frequency control ancillary services (FCAS) and 6 Second contingency FCAS.</p>"},{"location":"Energy/Issues/","title":"Issues","text":"<p>NEED UPDATE</p> <p>4 hour batteries + renewables are not sufficient to meet all system peaks - particularly in the late 2030s and beyond. It is entirely possible for a sustained renewables drought to leave the fleet of 4 hour batteries entirely spent.</p> <p>When there are simultaneous low solar and wind over a full day, the optimal operation of the system uses 4 hour battery in the morning and evening peaks. Middle of the day had generation from 12 hr PHES and USE, while simultaneously charging the 4 hour batteries for use in the evening.</p> <p>From an engineering perspective this suggests some peaking gas is required, however the economics of OCGTs are very challenging. The increased LCOE will no doubt further reduce the economic viability of OCGTs as a solution to meet these uncovered peaks. AEMO got around this problem by including deeper storages (up to 12 hours) in their modelling.</p> <p>If we meet USE requirements and no further new entrants are profitable then we should let the prices outcomes be what they are. But perhaps this will change when a model with long term TWPs over $100 per MWh.</p>"},{"location":"Energy/Issues/#new-entrant","title":"new entrant","text":"<p>what is the reason for projects getting off the ground, even modelling doesn't show that they should be economic at least for 5+ years. - input costs are too high? - price forecasts are lower than others? - these projects have additional sources of revenue we don't capture?</p> <p>Many projects getting off the ground are securing PPAs. This limits exposure to wholesale prices. Parties buying electricity via PPAs may not be motivated solely by price (e.g. helps government green and job credentials, helps corporates green credentials). Also investors do not always make smart decisions (dotcom bubble, subprime loans and the GFC, etc.). It is not clear that if they're willingly signing PPAs at a loss or if they found modelling to justify the PPA price as fair.</p>"},{"location":"Energy/Market/","title":"Market","text":""},{"location":"Energy/Market/#market","title":"market","text":"<p>By 2040, an estimated 15 GW of synchronous capacity will exit the market, while 30-40 GW of non-synchronous generation will enter over that time. - frequency control: keep frequency operating within the right limits - system strength: like a major generator or user disconnecting from the system</p> <p>system services were historically provided as a by-product of generating from coal, gas and hydro sources. But as thermal generators are retired, they not only no longer provide these additional services, but there is also an absence of signals to incentivise other parties to provide these services in the NEM.</p>"},{"location":"Energy/Market/#system-strength","title":"system strength","text":"<p>It is an umbrella term that refers to a number of different issues. A strong, stable voltage means this voltage sine wave is very smooth in shape, doesn\u2019t deform too much when there is a disturbance on the system, and typically doesn\u2019t get too big or too small. We say the system is strong if the voltage wave form meets these conditions; it exhibits high system strength.</p>"},{"location":"Energy/Semischd/","title":"Semi-Scheduled","text":"<p>Semi-scheduled generating units are those generating units registered in accordance with NER clause 2.2.7. Generally these generating units will be: - Greater than 30 MW and - Intermittent generation (typically, wind and solar farms)</p> <p>The dispatch instruction for a semi-scheduled generating unit requires that unit\u2019s active power output to be capped at the dispatch level set by AEMO only when its semi-dispatch interval flag is set to \u2018TRUE\u2019. This is called its \u201csemi-dispatch cap\u201d.</p> <p>NER 4.9.5(a)(3) also requires AEMO to specify a ramp rate or a specific target time to reach the outcome specified in the dispatch instruction. Absent a specified ramp rate, the semi-scheduled generating unit is expected to ramp linearly from its initial active power output to its semi-dispatch cap applying at the end of the 5-minute dispatch interval, subject to energy availability.</p> <p>This requirement applies to semi-scheduled generating units that have an active power control system capable of linear ramping as agreed in the relevant performance standard. When the semi-dispatch interval flag is set to \u2018FALSE\u2019 the semi-scheduled generating unit is free to generate at any level.</p>"},{"location":"Energy/Simulation/","title":"Simulation","text":""},{"location":"Energy/Simulation/#price-simulation","title":"price simulation","text":"<p>Price simulations needed for - Value at Risk (forward prices) - Gross Margins at Risk (spot prices) - Credit at Risk (settlements and MtM)</p>"},{"location":"Energy/Simulation/#simulation-methodologies","title":"simulation methodologies","text":"<p>electricity splot price - historical sampling + scaling - structure modeling with bid stack - stochastic models (mean reverting jump defussion)</p> <p>electricity forward price (swap or cap) - historical sampling of returns - stochastic models (brownian motion)</p> <p>electricity future price - combination of spot and forward prices</p> <p>non-standard contract price - option pricing model - equation from forward pricing</p> <p>LGC and STC price - historical sampling of returns - stochastic models (brownian motion) - other</p> <p>load simulation - long term (customer churn) and short term (daily/seasonal shape, flex) - stochastic expression (ornstein-uhlenbeck) - regression approach (average of load + rand) - historical sampling</p>"},{"location":"Energy/Battery/Parameter/","title":"Parameter","text":""},{"location":"Energy/Battery/Parameter/#lifetime-throughput","title":"lifetime throughput","text":"<p>Battery <code>lifetime throughput</code> refers to the total amount of energy a battery can store and release over its entire lifespan.  It's a key metric for understanding a battery's durability and total capacity for work.  - Throughputs vs. Cycle Life:   Traditionally, battery life is measured in cycles (charges and discharges).   Throughput goes beyond cycles and considers the total energy a battery can deliver,   accounting for factors like efficiency losses during charging and discharging. - Impact on Battery Choice:   Throughput helps compare batteries for different applications.   A higher throughput for the same capacity indicates a longer-lasting battery,   making it ideal for scenarios needing frequent charging and discharging, like solar energy storage.  - Units and Calculations:   Throughput is typically measured in kilowatt-hours (kWh) or megawatt-hours (MWh).   While the exact calculation can vary, it factors in the battery's initial capacity,   degradation over time, and efficiency during charging/discharging cycles.</p>"},{"location":"Energy/Bids/Bids/","title":"bids","text":""},{"location":"Energy/Bids/Bids/#scheduled-load","title":"scheduled load","text":"<p>Treatment of Dispatchable Loads.pdf</p> <p>Scheduled load will clear the bids from the highest price band to lowest price band. This is because the band price represents the maximum market clearing price that the market participant is willing to pay before decreasing the electricity consumption of their scheduled load by up to the MW increment in that band for the specified trading interval. This means if the market price is higher than the band price this band will not be scheduled.</p>"},{"location":"Energy/Data/EU/","title":"EU","text":""},{"location":"Energy/Data/EU/#european-electricity-and-gas-market-data","title":"European Electricity and Gas Market Data","text":"<ul> <li>Electricity Market data (Entsoe) https://lnkd.in/eTKfacth</li> <li>Ember (uses Entsoe) https://lnkd.in/estdyutC</li> <li>NordPool Market Data https://lnkd.in/eD-wMJGy</li> <li>Wartsila Energy Lab (Entsoe) https://lnkd.in/eC6AHJWu</li> <li>Gas flow Dashboard (Entsog) https://lnkd.in/evXRzvgy</li> <li>Gas Storage Dash (Entsog) https://lnkd.in/emk_7uXA</li> <li>EU Energy Balances https://lnkd.in/egqw223R</li> <li>Comm Prices (TEconomics) https://lnkd.in/eadFrwdU</li> <li>UK Gas Data (Prevailing view) https://lnkd.in/ey97FTuX</li> <li>EU LNG Data (GIE) https://lnkd.in/ehCV-Wde</li> <li>Oil Product Margins (NESTE) https://lnkd.in/evgdXUN3</li> </ul>"},{"location":"Energy/Data/EU/#eu-energy-and-climate-emissions-data","title":"EU Energy and Climate Emissions Data","text":"<ul> <li>Petrol/Diesel/weekly Prices https://lnkd.in/eWVDFWrw</li> <li>EU ETS Emissions Data https://lnkd.in/e47ADcPz</li> <li>EU GHG Detailed Data by Sector https://lnkd.in/e9mWV4mJ</li> </ul>"},{"location":"Energy/Data/EU/#global-historic-energy-trends-and-data","title":"Global Historic Energy Trends and Data","text":"<ul> <li>BP Statistical Review https://lnkd.in/eucgDQru</li> <li>Our world in Data (energy) https://lnkd.in/eKsGVyHu</li> <li>Gapminder tool https://lnkd.in/eiZE7U_R</li> <li>Energy Balances (IEA) https://lnkd.in/eyS59C35</li> <li>RES Capacities (IRENA) https://lnkd.in/eMDZ7AKH</li> <li>RES profiles (RNinja) https://lnkd.in/e8TB9BMC</li> <li>Global Scenario(IIASA) https://lnkd.in/euiAcCpu</li> <li>Time series heat (Nature) https://lnkd.in/ery8heJn</li> <li>Wind Profiles(Emhires) https://lnkd.in/eu5_buHW</li> <li>30 yr (wind/solar data) https://lnkd.in/ezUfHRhT</li> <li>Future Energy Scenarios https://lnkd.in/eWAuM5Sg</li> <li>Plant locations (WRI) https://lnkd.in/evGDP-X9</li> </ul>"},{"location":"Energy/Data/EU/#ireland-national-data","title":"Ireland National Data","text":"<ul> <li>Fuel sales (NORA) https://lnkd.in/d2BjB4zi</li> <li>Electricity/oil/Gas(SEAI) https://lnkd.in/ee3HtkeQ</li> <li>Energy Balances(SEAI) https://lnkd.in/eMnw7Jhd</li> <li>Gas data (GNI) https://lnkd.in/eA__-4Am</li> <li>Elec prices(SEMOPX) https://lnkd.in/ecDdfc7f</li> <li>Scada Data (EirGrid) https://lnkd.in/dx9UdwsR</li> <li>GHG Inventories (EPA) https://lnkd.in/e4g3PgTg</li> <li>GHG Projections (EPA) https://lnkd.in/efm4V9dA</li> <li>Inventory Subs (EPA) https://lnkd.in/ehFbGdvh</li> <li>Climate Statement(Metie) https://lnkd.in/eZTVHmWb</li> <li>Energy Scenarios (TIM UCC) https://lnkd.in/egpRy2wM</li> </ul>"},{"location":"Energy/Data/NEM/","title":"NEM","text":""},{"location":"Energy/Data/NEM/#nem-data-dashboard","title":"nem data dashboard","text":"<p>https://wa.aemo.com.au/energy-systems/electricity/national-electricity-market-nem/data-nem/data-dashboard-nem</p>"},{"location":"Energy/Dispatch/Dispatch/","title":"dispatch","text":"<p>when a unit failed to conform (compliance) the dispatch target, it may need pay infringement notice.</p> <p>Two trigger mechanisms are utilised to identify the severity of Non-Compliance. These are the Small Error Trigger and the Large Error Trigger.</p>"},{"location":"Energy/Dispatch/Dispatch/#pre-dispatch-process","title":"pre-dispatch process","text":"<p>https://www.aemo.com.au/media/Files/Other/electricityops/0140-0040%20pdf.pdf</p>"},{"location":"Energy/Dispatch/Dispatch/#the-semi-dispatch-process","title":"The semi-dispatch process","text":"<p>why solar farms are only subject to the ramp down constraint, not the ramp up constraint in dispatch?</p> <p>why most of the time of solar farm's output is constrained by the dispatch target, seems there are no binding constraints?</p> <p>https://www.aemo.com.au/-/media/Files/Electricity/NEM/Security_and_Reliability/Power_System_Ops/Procedures/SO_OP_3705---Dispatch.pdf</p> <p>The National Electricity Market Dispatch Engine (NEMDE) will dispatch the semi-scheduled generating unit in a similar manner to a normal scheduled generating unit based on the bid information provided and the UIGF provided from AWEFS/ASEFS, with the only difference being that a semi-scheduled cap flag (semi-dispatch interval flag) will also be provided. This flag may be set on the basis of constraint limitations or bidding reasons.</p> <p>If this cap is set to false then the intermittent generator is not required to follow dispatch targets and will not be considered by the non-compliance monitor. If the flag is set to true then the intermittent generator is required to follow the dispatch target only in that its output must not exceed the dispatch target value and will be monitored by the non-compliance monitor.</p> <p>The \u201csemi-dispatch compliance\u201d requirement flag is set when either one of the following conditions is satisfied:   * Dispatch Cap limited by Binding or Violated Network or FCAS Constraint   * The generating unit\u2019s forecast output (its dispatch cap) is explicitly limited by any binding or violated network or FCAS constraint equation, and if the actual output were to exceed the dispatch cap value this would result in violating (or further violating) that network or FCAS constraint equation; OR   * Dispatch Cap otherwise below the Unit\u2019s unconstrained intermittent generation forecast     * The generating unit\u2019s forecast output (its dispatch cap) is not explicitly limited by a binding or violated network constraint equation, BUT     * The generating unit\u2019s dispatch cap is less than its unconstrained intermittent generation forecast as a result of either a purely inter-regional limitation, or an offer or marketrelated limitation, the latter including:       * Unit Ramp Rate       * Unit Fixed Loading Level       * Non-dispatch of uneconomic price bands       * Marginal dispatch of economic price bands</p> <p>if uigf (forecast, avail) is too low, the target (totalcleared) will be constrainted by ramp down rate, so target &gt; avail. In this interval, the semicap flag = false, the actual generation can be lower or higher than the target.</p>"},{"location":"Energy/Dispatch/Dispatch/#terms","title":"terms","text":"<ul> <li>INITIALMW is the SCADA metered value nearest to the 5-minute interval at the time of running NEMDE.</li> <li>TOTALCLEARED is the dispatch target for the end of this dispatch interval (DI).</li> <li>AVAILABILITY appears to be the Australian Solar Energy Forecasting System (ASEFS) forecast for the next 5-minutes ahead</li> </ul>"},{"location":"Energy/Dispatch/Dispatch/#demand-terms","title":"demand terms","text":"<p>https://www.aemo.com.au/-/media/Files/Electricity/NEM/Security_and_Reliability/Dispatch/Policy_and_Process/Demand-terms-in-EMMS-Data-Model.pdf</p> <p>page 14 table 4: - native demand - operational demand - scheduled demand - cleared supply - total demand (excl. non-scheduled wind/solar/other, except gen, scheduled load/bdu, interconnector loasses)</p>"},{"location":"Energy/Dispatch/Dispatch/#relationship","title":"relationship","text":"<p>https://www.aemo.com.au/-/media/Files/Electricity/NEM/Security_and_Reliability/Dispatch/Policy_and_Process/Demand-terms-in-EMMS-Data-Model.pdf</p> <ul> <li><code>LHS = DispatchableGeneration + Net Interconnector Targets (into the Region)</code></li> <li><code>RHS = TotalDemand + DispatchableLoad - WDR_Dispatched + Allocated Interconnector Losses</code></li> <li><code>LHS = RHS = ClearedSupply</code></li> </ul>"},{"location":"Energy/Dispatch/Predispatch/","title":"Predispatch","text":""},{"location":"Energy/Dispatch/Predispatch/#30-minute-predispatch-predispatch","title":"30-minute Predispatch (Predispatch)","text":"<ul> <li>https://www.aemo.com.au/-/media/files/electricity/nem/security_and_reliability/power_system_ops/procedures/so_op_3704-predispatch.pdf?la=en</li> <li>Runs and is published every 30 minutes</li> <li>forecasts out to the end of the next market day (4 AM the following day). It can cover a period of up to 48 hours or more.</li> </ul>"},{"location":"Energy/Dispatch/Predispatch/#5-minute-predispatch-p5min","title":"5-minute Predispatch (P5MIN)","text":"<ul> <li>Runs and is published every 5 minutes</li> <li>Forecasts for the next 12 dispatch cycles (one hour)</li> </ul>"},{"location":"Energy/Dispatch/Predispatch/#extended-predispatch-pd7day","title":"Extended Predispatch (PD7Day)","text":"<p>https://www.aemo.com.au/-/media/Files/Electricity/NEM/Emergency_Management/2017/Guide-to-Extended-Predispatch-Report.pdf</p>"},{"location":"Energy/Dispatch/Predispatch/#total-demand","title":"Total demand","text":"<p>https://aemo.com.au/-/media/files/electricity/nem/security_and_reliability/dispatch/policy_and_process/demand-terms-in-emms-data-model.pdf?la=en</p> <p>Table <code>dispatchregionsum</code>: - NetInterchange: negative for import - <code>Gen - NetInterchange(export) = TotalDemand + Load - WDR_Dispatched + AllocatedInterconnectorLosses</code></p>"},{"location":"Energy/Dispatch/Predispatch/#energy-reserve","title":"Energy reserve","text":"<p><code>Reserve = AvailGen - NetInterchange (Export) - TotalDemand</code></p> <p>Table <code>predispatchregionsum</code>, <code>p5min_regionsolution</code>, <code>dispatchregionsum</code>:  - <code>TOTALDEMAND</code>: forecast total demand in MW (less normally on loads) - <code>AVAILABLEGENERATION</code>: aggregate generation bid available in the region (MW), the sum of all generation units' available capacity. - <code>AVAILABLELOAD</code>: aggregate load bid available in the region (MW). This primarily refers to <code>dispatchable load</code> or <code>interruptible load</code> that can be reduced to help balance supply and demand. - <code>PREDISPATCHSEQNO</code> format: <code>yyyymmddxx</code> - <code>RUN_DATETIME</code> format: <code>yyyy-mm-dd 24hh:mi:ss</code> - <code>SETTLEMENT_DATE</code> format: <code>yyyy-mm-dd 24hh:mi:ss</code></p>"},{"location":"Energy/Dispatch/Predispatch/#regional-excess-supply","title":"Regional excess supply","text":"<p>https://www.aemo.com.au/-/media/files/electricity/nem/security_and_reliability/power_system_ops/reserve-level-declaration-guidelines.pdf Mainland regions: <pre><code>RXS\n= + Aggregate Non-Energy Limited Capacity\n  + Aggregate Energy Limited Capacity\n  - Aggregate Semi-Scheduled Output\n  - Net Interconnector Export\n  + Aggregate Semi-Scheduled Output\n  - Scheduled Demand\n= + AvailGen\n  - NetInterchange (Export)\n  - TotalDemand\n</code></pre></p>"},{"location":"Energy/Forecast/ML/","title":"ML","text":""},{"location":"Energy/Forecast/ML/#energy-market-deep-learning","title":"energy market deep learning","text":"<p>https://github.com/nick-gorman/energy-market-deep-learning</p>"},{"location":"Energy/Forecast/Price/","title":"Price forecast","text":"<p>Price forecast is harder than demand forecast as price is not directly lined to weather conditions.  Price is more volatile - can have many spikes. Capacity availability and generator and interconnector outages will have a significant role on price spikes.</p>"},{"location":"Energy/Forecast/Price/#weather-and-price-forecast","title":"weather and price forecast","text":"<p>https://montel.energy/blog/predicting-energy-prices-the-role-of-weather-forecasts</p>"},{"location":"Energy/Forecast/Price/#reserve-margin-for-spike-forecasting","title":"reserve margin for spike forecasting","text":"<p>https://en.wikipedia.org/wiki/Electricity_price_forecasting</p>"},{"location":"Energy/Forecast/Price/#price-forecast-papers","title":"price forecast papers","text":"<ul> <li>stochastic-differential: https://www.sciencedirect.com/topics/engineering/stochastic-differential</li> <li>Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx</li> <li>Short-term electricity load and price forecasting by a new optimal LSTM-NN based prediction algorithm</li> </ul>"},{"location":"Energy/Hedge/Financial/","title":"Financial","text":"<p>Chapter 3: Electricity financial markets.pdf</p>"},{"location":"Energy/Hedge/Financial/#market-types","title":"Market types","text":"<p>OTC (over-the-counter): - bilateral arrangements between generators and retailers, or - arranged with the assistance of brokers that post bid (buy) and ask (sell) prices on behalf of their clients - rely on the credit worthiness of electricity market counterparties</p> <p>ASX: - electricity derivative products: futurres and options - Participants (licensed brokers) buy and sell contracts on behalf of clients that include   - generators,   - retailers,   - speculators such as hedge funds, and banks and other financial intermediaries</p>"},{"location":"Energy/Hedge/Financial/#financial-market-instruments","title":"Financial market instruments","text":"<p>Forward contracts  An agreement to exchange the NEM spot price in the future for an agreed fixed price.  - Called <code>swaps</code> in the OTC markets: OTC <code>swap</code> settlements are typically paid or received weekly in arrears (after the spot price is known) based on the difference between the spot price and the previously agreed fixed price. - Called <code>futures</code> on the ASX: ASX electricity <code>futures</code> and <code>options</code> settlements are paid or received daily based on mark-to-market valuations. ASX futures are finally cash settled against the average spot price of the relevant quarter.</p> <p>Options A right, without obligation, to enter into a transaction at an agreed price in the future (exercisable option) or  a right to receive cash flow differences between an agreed price and a floating price (cash settled option). The buyer pays a premium to the option seller. - <code>Cap</code>: A contract through which the buyer earns payments when the pool price exceeds an agreed price. Caps are typically purchased by retailers to place a ceiling on their effective pool purchase price in the future. - <code>Floor</code>: A contract through which the buyer earns payments when the pool price is less than an agreed price. Floors are typically purchased by generators to ensure a minimum effective pool sale price in the future. - <code>Swaptions / futures options</code>: An option to enter a swap or futures contract at an agreed price and time in the future. - <code>Asian options</code>: An option through which the payoff is linked to the average value of an underlying benchmark (usually the NEM spot price) during a defined period. - <code>Profiled volume options for sculpted loads</code>: A volumetric option that gives the holder the right to purchase a flexible volume in the future at a fixed price.</p>"},{"location":"Energy/Hedge/Financial/#swap-otc","title":"SWAP (OTC)","text":"<p>A swap is a financial derivative contract between two parties who agree to exchange cash flows or other financial instruments over a specified period. </p> <p>In electricity markets, swaps are often used to manage price risk associated with fluctuations in electricity prices.</p> <p>Electricity swaps allow market participants, such as generators, retailers, and other stakeholders, to hedge against the volatility of electricity prices.  For example, a generator may enter into a swap agreement to fix the price at which they sell electricity, providing predictability in revenue.</p>"},{"location":"Energy/Hedge/Financial/#cap","title":"CAP","text":"<p>A cap is a financial instrument that sets a maximum limit on the price of a commodity, such as electricity, within a specified period. </p> <p>It provides protection against price spikes beyond a predetermined level.</p> <p>In electricity markets, participants can purchase price caps as a risk management strategy (pay premium in all intervals).  If the market price exceeds the cap, the cap holder receives compensation for the difference. </p> <p>Caps are often used by electricity consumers or retailers to limit their exposure to extremely high prices during periods of peak demand or supply shortages.</p>"},{"location":"Energy/Hedge/Hedging/","title":"Hedging","text":""},{"location":"Energy/Hedge/Hedging/#purpose-of-hedging","title":"Purpose of hedging","text":"<p>future-financial-risk-management-in-the-nem-report-for-the-accc.pdf - help manage the revenue risks to generators - provide retailers with greater energy purchase cost certainty - allow welfare maximising price discrimination - lower entry and exit barriers</p>"},{"location":"Energy/Hedge/Hedging/#ways-of-participants-hedging-their-exposure-to-spot-prices","title":"Ways of participants hedging their exposure to spot prices","text":"<ul> <li>Vertical integration: A retailer that owns a generator</li> <li>Power purchase agreements (PPA) with a generator</li> <li>Financial derivatives such as swap contracts and cap contracts</li> </ul>"},{"location":"Energy/Hedge/Hedging/#financial-hedging","title":"Financial Hedging","text":"<ul> <li>Futures Contracts: Retailers can use futures contracts to lock in the price of electricity for a future delivery date. By entering into a futures contract, retailers can protect themselves from potential price increases in the electricity market.</li> <li>Options Contracts: Options give retailers the right, but not the obligation, to buy or sell electricity at a specified price (strike price) on or before a certain date. Retailers can use options to hedge against adverse price movements while retaining flexibility to benefit from favorable price changes.</li> <li>Swaps: Electricity swaps involve exchanging cash flows based on the difference between fixed and floating electricity prices. Through swaps, retailers can manage their exposure to price fluctuations and reduce the impact of market volatility.</li> </ul>"},{"location":"Energy/Hedge/Hedging/#volume-hedging","title":"Volume Hedging","text":"<ul> <li>Baseload Contracts: Retailers can enter into baseload contracts to secure a fixed volume of electricity supply throughout the day. Baseload contracts provide certainty of supply, making it easier for retailers to plan their electricity needs accurately.</li> <li>Peak Load Contracts: Peak load contracts provide additional electricity supply during peak demand periods, allowing retailers to meet their customers' increased electricity consumption during those hours. These contracts help manage capacity constraints and reduce reliance on the spot market.</li> </ul>"},{"location":"Energy/Hedge/Hedging/#portfolio-diversification","title":"Portfolio Diversification","text":"<ul> <li>Contract Types: Retailers can diversify their electricity supply portfolio by having a mix of contract types, such as spot market purchases, long-term contracts, and power purchase agreements (PPAs) with renewable energy sources. This diversity helps mitigate risks associated with market fluctuations and regulatory changes.</li> <li>Energy Sources: Diversifying the sources of electricity supply is essential for managing risks and supporting sustainability goals. Incorporating a mix of conventional and renewable energy sources ensures a balanced and resilient energy portfolio.</li> <li>Geographical Diversification: Retailers can source electricity from various regions to spread risk and reduce the impact of localized supply disruptions or extreme weather events.</li> </ul>"},{"location":"Energy/Hedge/Link/","title":"Link","text":""},{"location":"Energy/Hedge/Link/#hedge","title":"hedge","text":"<p>https://wattclarity.com.au/articles/2019/10/how-retailers-use-contracts-to-hedge-customers/</p>"},{"location":"Energy/Hedge/Link/#load-following-hedge","title":"Load Following Hedge,","text":"<p>which is a fixed price, variable volume swap, meaning that the retailer has a certain price for all energy consumed.</p>"},{"location":"Energy/Loss/UFE/","title":"global settlement and UFE","text":"<p>https://www.aemc.gov.au/sites/default/files/2018-12/Global%20Settlement%20and%20Market%20Reconciliation%20-%20For%20publication.pdf</p> <p>https://aemo.com.au/initiatives/major-programs/nem-five-minute-settlement--program-and-global-settlement/global-settlement/how-gs-will-work</p> <p>https://aemo.com.au/-/media/files/electricity/nem/data/metering/ufe/2022/ufe-report-june-2022.pdf?la=en</p>"},{"location":"Energy/Loss/UFE/#ufe-calculation","title":"UFE calculation","text":"<p>In accordance with NER 3.15.5, for each local area, the UFE amount for each trading interval is determined by the following formula: $$ UFE = TME \u2013 DDME \u2013 ADME $$ Where: - <code>UFE</code> is total unaccounted for energy for a local area, - <code>TME</code> is total energy inflows into a local area from transmission connection points, - <code>DDME</code> is cross boundary energy flow between adjacent distribution networks. DDME is a positive value for the supplying distribution local area and a negative value for the receiving distribution local area, and - <code>ADME</code> is the aggregate of energy flows for each connection point in a local area. UFE, TME, DDME and ADME information is available from the RM 46 Report for financially responsible Market Participants (FRMPs) and Local Network Service Provides (LNSPs).</p>"},{"location":"Energy/Loss/UFE/#ufe-allocation","title":"UFE allocation","text":"<p>The allocation of UFE for every distribution network connection point in a local area is determined by the following formula: $$ UFEA = UFE x (DME/ADMELA) $$ Where: - <code>UFEA</code> is the allocation of local area unaccounted for energy for a connection point, - <code>DME</code> is the load component (ME- x DLF) at a connection point in the local area, - <code>ME-</code> is load component as recorded in the metering data at a connection point in the local area, - <code>DLF</code> is the distribution loss factor applicable at a connection point in the local area, and - <code>ADMELA</code> is the aggregate of all DME amounts in a local area for which a Market Customer is financially responsible</p>"},{"location":"Energy/Loss/UFE/#ufe-factor-ufef","title":"UFE Factor (UFEF)","text":"<p>The UFE Factor (UFEF) is used to facilitate the allocation of UFE to individual connection points. $$ UFEF = UFE/ADMELA $$ Where: - <code>UFE</code> is total unaccounted for energy for a local area, and - <code>ADMELA</code> is the aggregate of all DME amounts in a local area for which a Market Customer is financially responsible - <code>UFEA = UFE x (DME/ADMELA)</code>, or can be expressed as <code>UFEA = DME x (UFE/ADMELA)</code>, therefore <code>UFEA = DME x UFEF</code> UFEF and ADMELA are available from the RM 46 Report. UFEF is also available from the RM 43 Report.</p>"},{"location":"Energy/MLF/MLF/","title":"MLF (Marginal Loss factors)","text":"<p>Marginal network losses are always a more appropriate basis for computing loss factors than average network losses from the point of view of market design, because they reflect the underlying ideal relationship between locational marginal prices better (remembering that a key principle of the NEM is the adoption of marginal pricing).</p>"},{"location":"Energy/MLF/MLF/#issues","title":"Issues","text":""},{"location":"Energy/MLF/MLF/#how-the-settlement-is-allocated","title":"how the settlement is allocated?","text":"<p>generators are paid by the electricity they provide and customers pay the amount they use. so there are some residues in the settlement. The residues are currently allocated to customers.</p>"},{"location":"Energy/MLF/MLF/#why-it-becomes-important-recently","title":"why it becomes important recently?","text":"<p>More projects are connected to the network with a unpredictable speed, leading to quick change of the mlf in the system.</p> <p>Transparency of the potential projects will help other investors to decide where to develop their projects for a more efficient system.</p>"},{"location":"Energy/MMS/ColTerm/","title":"table column terms","text":""},{"location":"Energy/MMS/ColTerm/#website","title":"website","text":"<p>https://github.com/UNSW-CEEM/NEMOSIS/wiki/Column-Summary</p> <ul> <li>SettlementDate: the date time at the end of the period. </li> </ul>"},{"location":"Energy/MMS/MMS/","title":"mms","text":"<p>AEMO tables</p> <p>https://github-wiki-see.page/m/UNSW-CEEM/NEMOSIS/wiki/AEMO-Tables</p> <p>http://www.nemweb.com.au/#mms-data-model</p> <p>http://www.nemweb.com.au/Data_Archive/Wholesale_Electricity/MMSDM/</p> <p>https://www.aemo.com.au/Electricity/National-Electricity-Market-NEM/Data/Market-Management-System-MMS/Dispatch</p> <p>files: http://nemweb.com.au/Reports/CURRENT/</p> <p>packages: http://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_2.htm</p> <p>table def: https://www.aemo.com.au/energy-systems/electricity/national-electricity-market-nem/data-nem/market-data-nemweb</p> <p>list of tables: https://visualisations.aemo.com.au/aemo/nemweb/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_470_8.htm</p>"},{"location":"Energy/MMS/MMS/#mypasa-duid-availability","title":"mypasa duid availability","text":"<p>https://nemweb.com.au/Reports/CURRENT/MTPASA_DUIDAvailability/</p>"},{"location":"Energy/MMS/MMS/#prdloader-tables","title":"prdLoader tables","text":"<p>https://visualisations.aemo.com.au/aemo/di-help/Content/pdrLoader/Database_Tables.htm</p>"},{"location":"Energy/MMS/MMS/#demand-data","title":"demand data","text":"<p>dispatchis.regionsum\\ marketnet.dispatchregionsum</p> <p>InitialSupply =\\ Sum of InitialMW [Over all Regional Scheduled and Semi-scheduled generating units]\\ + Net MeteredMWFlow into the Region [Over all Interconnectors connected to the region]</p> <p>ClearedSupply =\\ Sum of TotalCleared [Over all Regional Scheduled and Semi-scheduled generating units]\\ + Net MWFlow into the Region [Over all Interconnectors connected to the region]</p>"},{"location":"Energy/MMS/MMS/#dispatchscadaunit_scada","title":"dispatchscada.unit_scada","text":"<p>contains actual generation data for each - scheduled generation unit, - semi-scheduled generation unit, and - non-scheduled generating units or - non-scheduled generating systems (a non-scheduled generating system comprising non-scheduled generating units).</p> <p>This file also contains actual load for each scheduled load.</p> <p>The actual generation and load data in the file is reported under a column headed 'SCADAVALUE'</p>"},{"location":"Energy/MMS/MMS/#next_day_actual_gen","title":"next_day_actual_gen","text":"<p>Non-Scheduled Generation Data:\\ The metered power outputs from most large non-scheduled generating units and generating systems, in a daily file covering all intervals of the previous trading day.</p> <p>The output values are sampled from SCADA at 5-minute intervals and reported under the column headed 'MWH_READING'. These figures should be read as instantaneous power in MW rather than MWh.</p>"},{"location":"Energy/MMS/MMS/#dispatch","title":"dispatch","text":"<p>Public 5 minute dispatch data by region. Data covers interconnector flows, constraints, regional reference price, demand, dispatchable generation, dispatchable load, and ancillary services data.</p> <p>For the energy market, the regional reference price (RRP) is the spot price.</p> <p>The regional original price (ROP) includes the cost of any constraint violations, and can exceed the Market Price Cap (MPC), in which case it will be automatically revised before it is published as the Regional Reference Price (RRP) for the DI.</p>"},{"location":"Energy/MMS/MMS/#trading","title":"trading","text":"<p>Public 30 minute dispatch data by region. Data covers interconnector flows, regional reference price, demand, dispatchable generation, dispatchable load, and ancillary services data.</p>"},{"location":"Energy/MMS/MMS/#predispatch","title":"predispatch","text":"<p>5-minute Predispatch: This provides a 60-minute look-ahead forecast, updated every 5 minutes. 30-minute Predispatch: This provides a 40-hour look-ahead forecast, updated every 30 minutes. \u00a0 7-day Predispatch: This provides a 7-day look-ahead forecast, updated daily.</p>"},{"location":"Energy/MMS/MMS/#p5min","title":"p5min","text":"<p>The five-minute predispatch (P5Min) is a MMS system providing projected dispatch for 12 Dispatch cycles (one hour). The 5-minute Predispatch cycle runs every 5-minutes to produce a dispatch and pricing schedule to a 5-minute resolution covering the next hour, a total of twelve periods.</p> <p>For each interval_datetime there will be 12 runs with the last run_datetime starting from the interval_datetime.</p>"},{"location":"Energy/MMS/MMS/#predispatchis","title":"predispatchis","text":"<p>30 minute predispatch (forecast) data by region to the end of the next market day (from 1 pm why it goes to the next day), and is updated half hourly.</p> <p>Regional demand can be calculated as total demand plus dispatchable load (i.e. Regional demand = Total Demand + Dispatchable Load)</p>"},{"location":"Energy/MMS/MMS/#predispatch-sensitivities","title":"predispatch sensitivities","text":"<p>Pre-dispatch sensitivities \u2013 also known as pre-dispatch scenarios.</p> <p>https://aemo.com.au/-/media/files/electricity/nem/security_and_reliability/dispatch/policy_and_process/pre-dispatch-sensitivities.pdf</p>"},{"location":"Energy/MMS/MMS/#daily","title":"daily","text":"<p>This file is generated daily, shortly after 0400. It contains previous day data for all dispatch and trading intervals. Data provided by generating unit includes initial MW, dispatch targets and ramp rates.</p> <p>tunit: 30min results (trading) dunit: 5min results (dispatch)</p> <p>runno:</p> <p>intervention:</p> <p>https://github.com/UNSW-CEEM/NEMOSIS/wiki/Column-Summary</p>"},{"location":"Energy/MMS/MMS/#bids","title":"bids","text":"<p>https://www.aemo.com.au/-/media/Files/Electricity/NEM/5MS/Systems-Workstream/2019/Format-and-Validation-for-Energy-FCAS-and-MNSP-Bids-and-Offers.pdf</p> <p>For bids data the first period is between 4:00 and 4:05, and usually the datetime is the period_ending time.</p> <p>There are three types of bids or offers to supply \u2013 default bids, daily bids, re-bids:   - Default bids are standing bids that apply where no daily bid has been made. These bids are of a \u2018commercialin-confidence\u2019 nature and, in general, reflect the base operating levels for generators.   - Daily bids are submitted before 12:30 pm on the day before supply is required, and are reflected in pre-dispatch forecasts.   - Generators may submit re-bids up until approximately five minutes prior to dispatch. In doing so, they can change the volume of electricity from what it was in the original offer, but they cannot change the offer price.</p> <p>Closed Bid: The latest bid close to the dispatch interval. For each DI, go through the duid BidDayOffer bids and find the bid with the highest firstdispatch &lt;= di (dispatch interval), this is the closed bid. A bid with null (DateTime.MinValue) firstdispatch cannot be a closed bid.</p> <p>Valid Bid: Scan through the bids, whatever has a bidofferdate less than or equal to the closed bid's bidofferdate is a valid bid (even if the firstdispatch field is null).</p> <p>Closed Bid Capped: The bid quantities are limited by the max avail. For scheduled load units as the bids are in the reverse order (from the highest price to the lowest price) so the bid should be capped from highest to lowest bands. Also, if fixedload is not zero, fixedload will be for the first band and all other bands should be zero.</p> <p>BIDOFFERFILETRK shows an audit trail of all files submitted containing an FCAS bid, including corrupt bids and rebids.</p> <p>BIDDAYOFFER is a child table to BIDOFFERFILETRK. BIDDAYOFFER shows the Energy and Ancillary Service bid data for each Market Day.</p> <p>BIDPEROFFER is a child table of BIDDAYOFFER. BIDPEROFFER shows period-based Energy and Ancillary Service bid data.</p> <p>Empirical observations of bidding patterns in Australia\u2019s National Electricity Market\\ http://www.ceem.unsw.edu.au/sites/default/files/event/documents/Hu_etal_energy_policy2004.pdf\\ Gen can rebid the quantity for a previously bid settlement interval up to 5 min before dispatch, although the price bands are fixed for the entire settlement day.</p> <p>Bidding-in-energy-only-wholesale-electricity-markets-Final-report.PDF</p> <p>bid splitting: - bid a reduced capacity at lower price level to ensure continuous dispatch; and - bid the remaining capacity at the maximum allowable market price</p>"},{"location":"Energy/MMS/MMS/#mcc_dispatch","title":"mcc_dispatch","text":"<p>Comment Results from the Marginal Constraint Cost (MCC) re-run of the dispatch process. The MCC forms part of the AER's \"Electricity transmission network service providers Service target performance incentive Scheme\"</p>"},{"location":"Energy/MMS/MMS/#station-mapping","title":"station mapping","text":"<p>marketnet.dudetailsummary provide duid and station mapping</p>"},{"location":"Energy/MMS/MMS/#terms","title":"terms","text":"<p>DISPATCHABLEGENERATION = Dispatched Generation (Sum of dispatched Scheduled and Semischeduled generation)</p>"},{"location":"Energy/MMS/MMS/#nemreview","title":"nemreview","text":"<p>nemreview uses intervention = 1 data</p>"},{"location":"Energy/MMS/MMS/#participant_registration","title":"PARTICIPANT_REGISTRATION","text":"<p>PUBLIC_DVD_DUDETAIL_201908010000.zip\\ PUBLIC_DVD_DUDETAILSUMMARY_201908010000.zip\\ http://nemweb.com.au/Data_Archive/Wholesale_Electricity/MMSDM/2019/MMSDM_2019_08/MMSDM_Historical_Data_SQLLoader/DATA/</p> <p>table: DUDETAIL \\ DUDETAIL sets out a records specific details for each unit including start type and whether normally on or off load. Much of this data is information only and is not used in dispatch or settlements.</p> <p>table: DUDETAILSUMMARY \\ reducing the need for participants to use the various dispatchable unit detail and owner tables to establish generating unit specific details.</p>"},{"location":"Energy/MMS/NemDataDashboard/","title":"NEM Data Dashboard","text":""},{"location":"Energy/MMS/NemDataDashboard/#website","title":"website","text":"<p>https://aemo.com.au/en/energy-systems/electricity/national-electricity-market-nem/data-nem/data-dashboard-nem</p>"},{"location":"Energy/PASA/STPASA/","title":"ST PASA","text":"<ul> <li>https://www.aemo.com.au/-/media/files/electricity/nem/planning_and_forecasting/pasa/st-pasa-procedure.pdf?rev=d6993d409d774141abd8bdb36a78db2e&amp;sc_lang=en</li> <li>NER 3.7.3 https://energy-rules.aemc.gov.au/ner/175/23961</li> </ul>"},{"location":"Energy/Regulation/Renewable/","title":"Renewable","text":"<p>Renewable Energy (Electricity) Act 2000:  https://www.legislation.gov.au/C2004A00767/latest/text</p>"},{"location":"Energy/Regulation/Renewable/#stc-liability","title":"STC liability","text":"<p>Sections 38AA and 38AE</p>"},{"location":"Energy/Renewable/Project/","title":"Project","text":""},{"location":"Energy/Renewable/Project/#au-renewables-database","title":"AU Renewables Database","text":"<p>https://github.com/akarich73/AURenewablesDb</p> <p>An open source structured PostgreSQL &amp; PostGIS database of renewable energy projects in Australia.</p>"},{"location":"Energy/Renewable/Renewable/","title":"Renewable","text":"<p>PEC: Partial Exemption Certificate\\ A certificate that provides partial exemption for prescribed Emissions Intensive Trade Exposed activities from Large-Scale Renewable Energy Target and the Small-Scale Renewable Energy Scheme liabilities. Eligible Emissions Intensive Trade Exposed activities must apply to the Office of the Renewable Energy Regulator annually for Partial Exemption Certificates. These Partial Exemption Certificates can be on-sold to Large-Scale Renewable Energy Target and the Small-Scale Renewable Energy Scheme liable entities.</p> <p>Operational demand is the demand that must be met by centrally dispatched generation. In other words, it is the total electrical consumption from grid-connected consumers less production from rooftop solar.</p> <p>Residual demand is the operational demand minus large-scale renewable production at each 5-minute period.</p>"},{"location":"Energy/Renewable/Renewable/#solar","title":"Solar","text":""},{"location":"Energy/Renewable/Renewable/#solar-battery","title":"solar-battery","text":"<p>revenues: - arbitrage - energy trading - FCAS markets</p> <p>issues: - paired with the solar farm, which has a large low voltage tariff class, attracts heavy \u201cdemand tariffs\u201d from the local network operator - being paired with a solar farm and having to operate under the constraints of that facility\u2019s connection agreement: This effectively means that the combined solar farm and battery are limited to the 50MW rating, meaning the battery is effectively constrained to using headroom in this connection point from un-utilised solar output [better using self-forecasting to improve accuracy].</p>"},{"location":"Energy/Renewable/Renewable/#battery","title":"Battery","text":"<p>Batteries can be more rapidly deployed than traditional firming capacity that is used to augment renewables when the sun isn\u2019t shining and the wind isn\u2019t blowing and are perhaps more flexible in terms of location.</p>"},{"location":"Energy/Renewable/Renewable/#built-signal","title":"built signal","text":"<p>Locational marginal prices (LMPs), as would be provided under transmission access reform in the NEM, will provide batteries with more granular information in relation to: - concentrations of generation, in particular intermittent generation; - concentrations of load; - constraints on the transmission network; and - volatile prices at specific points on the network as a consequence of the interaction of the first two factors with the third.</p>"},{"location":"Energy/Renewable/Renewable/#battery-markets","title":"battery markets","text":"<p>The investment and operational case for a battery is derived from several markets. Some of the most notable options are: - providing system services, such as frequency regulation, voltage support or reactive power support; - co-located firming of generators such as wind or solar farms; - transmission and distribution investment deferral by managing the load on the transmission or distribution level below its specified maximum level; - arbitrage in the wholesale market for energy, where batteries charge using inexpensive electrical energy and discharge when prices for electricity are high. This option allows large-scale batteries to alleviate congestion on the transmission network (just as they would for transmission deferral), but does so through responses to price incentives whether these are regional prices or locational marginal prices. - peak demand lopping for significant loads; and - providing virtual cap products for significant loads that would otherwise purchase these products directly or indirectly from the contract market.</p>"},{"location":"Energy/Renewable/Renewable/#nem-utility-scale-batteries","title":"NEM utility scale batteries","text":"<p>In Australia, large-scale battery storage deployment is still in its early stages. There are currently five registered battery storage systems in excess of 1 MW capacity.</p> <p>These five systems are: - the Ballarat Energy Storage System in Victoria; - the Gannawarra Energy Storage System in Victoria; - the Hornsdale Power Reserve in South Australia; - the Dalrymple North Battery Energy Storage System in South Australia; and - the Lake Bonney BESS1 in South Australia.</p> <p>The drivers behind each of these large-scale batteries vary. The Ballarat Energy Storage System\u2019s construction was motivated in large part by locational considerations, as it was expected to be able to ease constraints on transmission lines in Western Victoria that reduce the output of wind and solar-generated electricity.  It was also commissioned to provide FCAS services.</p> <p>The Gannawarra Energy Storage System was commissioned in North-Western Victoria to participate in energy price arbitrage and provide FCAS services, though the possibility of using the battery to reduce curtailment of future renewable energy generation on a relatively constrained line has also been noted.</p> <p>The Hornsdale Power Reserve battery in Mid-Northern South Australia, installed as part of the South Australian Government\u2019s energy policy to accelerate the roll-out of grid-scale energy storage,  has been noted to provide a backup source of electricity, provide FCAS services and participate in price arbitrage using spot market prices. Another rationale for the battery was noted to be the extended support for the Heywood interconnector by alleviating system security constraints.</p> <p>The Dalrymple North battery on South Australia\u2019s Yorke Peninsula focuses on providing reliability in the case of islanding and FCAS to reduce constraints on the Heywood interconnector and improve system security by quickly injecting electricity into the transmission network following a disturbance.</p> <p>The Lake Bonney battery in South Australia has been commissioned with the intent of firming nearby renewable generation and providing FCAS services.</p>"},{"location":"Energy/Renewable/Wind/NEMWind/","title":"NEM wind","text":""},{"location":"Energy/Renewable/Wind/NEMWind/#wind-farms","title":"wind farms","text":"<p>https://www.canstarblue.com.au/electricity/wind-farms-australia/</p>"},{"location":"Energy/Retail/Contract/","title":"Contract","text":"<ul> <li>Fixed price</li> <li>Load following book build</li> <li>Non-load following book build</li> <li>Pool price pass through</li> </ul>"},{"location":"Energy/Risk/Derivative/","title":"Derivative","text":""},{"location":"Energy/Risk/Learn/","title":"Learn","text":""},{"location":"Energy/Risk/Learn/#paper","title":"paper","text":"<p>Hedging and Tail Risk in Electricity Markets</p> <p>online companion dataset to 'Hedging and Tail Risk in Electricity Markets' - https://github.com/fnbillimor/tail_hedging_power</p>"},{"location":"Energy/Risk/Learn/#hedging","title":"hedging","text":"<p>https://www.investopedia.com/trading/hedging-beginners-guide/</p>"},{"location":"Energy/Risk/Learn/#eletricity-risk","title":"eletricity risk","text":"<p>https://github.com/xenakas/electricity_risks</p>"},{"location":"Energy/Risk/Learn/#financial-derivatives-and-risk-management-course-materials","title":"Financial derivatives and risk management course materials","text":"<p>https://github.com/lfthwjx/Financial-Derivatives-Risk-Management</p>"},{"location":"Energy/Risk/Learn/#financial-modelling-and-risk-management","title":"Financial modelling and risk management","text":"<p>https://github.com/shreemoyee/FinancialModellingandRiskMAnagement Zero coupon bonds - Options on bonds - Bonds futures and forwards</p> <p>Swaps - Caplets and floorlets - Swaps and swapsons - Forward equations</p>"},{"location":"Energy/Risk/Learn/#financial-models-in-pricing-derivatives-using-python","title":"financial models in pricing derivatives using python","text":"<p>https://github.com/stochasticquant/Derivative-Pricing-in-Python - Risk neutral pricing  - Black Scholes pricing</p>"},{"location":"Energy/Risk/Learn/#stochastic-optimization-with-risk","title":"stochastic optimization with risk","text":"<p>https://www.gurobi.com/events/solving-simple-stochastic-optimization-problems-with-gurobi - Risk - VaR - CVaR</p>"},{"location":"Energy/Risk/Risk/","title":"Risk","text":"<p>Risk related to potential for downside outcomes - consequence - likelihood</p>"},{"location":"Energy/Risk/Risk/#risk-types","title":"risk types","text":"<ul> <li>market risk<ul> <li>Value at Risk (VaR)</li> <li>Marginal Value at Risk (Marginal VaR)</li> <li>Greeks in options trading: Delta, Gamma, Theta, Vega, and Rho</li> </ul> </li> <li>credit risk</li> <li>liquidity risk</li> </ul>"},{"location":"Energy/Stochastic/Non-anticipativity/","title":"non-anticipativity","text":""},{"location":"Energy/Stochastic/Non-anticipativity/#basic","title":"basic","text":"<ul> <li>https://pubsonline.informs.org/doi/pdf/10.1287/educ.1053.0016</li> <li>decisions made at a specific point in time cannot rely on information that is only available in the future</li> <li>non-anticipativity constraint: scenarios with a common history must (logically) have the same set of decisions</li> <li>objective: expected cost is the sum of the weighted cost of each scenario, so the decision is made based on all the scenario posibilities</li> </ul>"},{"location":"Energy/Stochastic/ScenRed/","title":"Scenario Reduction","text":""},{"location":"Energy/Stochastic/ScenRed/#scenario-reduction_1","title":"scenario reduction","text":""},{"location":"Energy/Stochastic/ScenRed/#backward-reduction","title":"backward reduction","text":"<p>delete least important scenarios one by one - step 1: select scenario with smallest distance to other scenario - step 2: merge and update scenario weights - step i: go to step 1 again</p>"},{"location":"Energy/Stochastic/ScenRed/#fast-forward-selection","title":"fast forward selection","text":"<p>keep most important scenarios one by one - step 1:     - determine the scenario that can represent all scenarios with the min probability distance     - keep that scenario - step i:     - for each remaining scenarios, calculate the min probability distance to other remaining scenarios     - keep that scenario as well - do util the number of scenarios to keep reached the provided number</p>"},{"location":"Energy/Stochastic/ScenRed/#scenario-tree","title":"scenario tree","text":"<p>2003: https://www.wias-berlin.de/people/heitsch/ieee03ghr.pdf - relative accuracy: eq. (1)     - when only one scenario is kept, the loss is 100%     - so use that scenario with the minimum sum prob distance as a base reference     - when a scenario linked to a deleted scenario is to be deleted, we must update the distance of the deleted scenario to new preserved scenario set     - as the distance increases so the total loss - optimal reduction problem:     - minimize the total min distance between all deleted scenarios to the remaining scenarios     - each deleted scenario will be merged to one of the remaining scenarios with the shortest distance     - the sum(p_j * d_ji) is the information loss - probability redistribution: eq.(2)     - all deleted scenarios will be merged to one of the scenarios in the remaining set with the shortest distance</p>"},{"location":"Energy/Stochastic/ScenRed/#code","title":"code","text":"<p>c++ version - https://github.com/Jonas-eng/scenred/issues/1 - https://github.com/Jonas-eng/scenred/blob/master/scenario_reduction_c.pyx</p> <p>python version (fast forward selection): - https://github.com/DanieleGioia/ScenarioReducer/blob/main/main_example.py</p> <p>scenario tree - https://gitlab.com/supsi-dacd-isaac/scenred</p>"},{"location":"Energy/Stochastic/ScenRed/#papaers","title":"papaers","text":"<ul> <li> <p>2000: Scenario reduction in stochastic programming: An approach using probability metrics   https://edoc.hu-berlin.de/bitstream/handle/18452/8896/20.pdf?sequence=1</p> </li> <li> <p>2003: Scenario Reduction and Scenario Tree Construction for Power Management Problems   https://www.wias-berlin.de/people/heitsch/ieee03ghr.pdf</p> </li> <li> <p>2003: Scenario reduction algorithms in stochastic programming</p> </li> <li>https://edoc.hu-berlin.de/server/api/core/bitstreams/ed72ccea-3fd6-4e4b-8975-53a27184c758/content</li> <li>backward reduction: eq.(12)</li> <li> <p>forward reduction: eq.(13)</p> </li> <li> <p>2008: Scenario tree modelling for multistage stochastic programs   https://edoc.hu-berlin.de/server/api/core/bitstreams/7f0e1eb1-ca9b-42d5-a213-97b1c0b71764/content</p> </li> <li> <p>2009: Scenario reduction for futures market trading in electricity markets   https://pdfs.semanticscholar.org/6b05/95e54011b6925647adbf837ebb30f77f29d9.pdf</p> </li> </ul>"},{"location":"Energy/Stochastic/Stochastic/","title":"Stochastic","text":""},{"location":"Energy/Stochastic/Stochastic/#nodescenario-based-formulation","title":"node/scenario based formulation","text":"<p><code>node base</code> and <code>scenario based</code> formulated problems are equivalent to each other. - <code>node based</code>: each node has a set of variables and constraints - no nonanticipativity constraints - <code>scenario based</code>:each scenario has a set of variables and constraints - nonanticipativity constraints - difference: <code>node based</code> removes the duplicate variables and constraints in the formulation - difference: <code>scenario based</code> is straightforward and easy to implement</p>"},{"location":"Energy/Trading/Learn/","title":"Learn","text":""},{"location":"Energy/Trading/Learn/#kaggle-competition-optiver-trading-at-the-close","title":"kaggle competition: optiver-trading-at-the-close","text":"<p>https://www.kaggle.com/competitions/optiver-trading-at-the-close/discussion</p> <ul> <li>7th: https://github.com/nimashahbazi/optiver-trading-close</li> <li>https://github.com/beingamanforever/Optiver-Trading-at-the-close</li> <li>blog: https://medium.com/@joehbridges/gauging-the-market-optivers-trading-at-the-close-kaggle-competition-27b73f7789c0</li> </ul>"},{"location":"Energy/Trading/Learn/#books","title":"books","text":"<ul> <li>https://www.oreilly.com/library/view/energy-trading-and/9781118339343/</li> <li>Energy Trading and Risk Management: A Practical Approach to Hedging, Trading and Portfolio Diversification</li> <li>Stochastic Modeling of Electricity and Related Markets</li> </ul>"},{"location":"Energy/Trading/Learn/#ml-design","title":"ml design","text":"<ul> <li>Advances in Financial Machine Learning</li> <li>https://github.com/doda/advances-in-financial-ml-notes</li> </ul>"},{"location":"Energy/Trading/Learn/#algorithm-trading","title":"algorithm trading","text":"<p>https://www.linkedin.com/pulse/challenges-algorithmic-trading-short-term-energy-market-ishutin</p> <p>basic analysis using polars and duckdb: - https://medium.com/@codewithmuse/41ffb7090a38</p>"},{"location":"Energy/Trading/Learn/#blogs","title":"blogs","text":"<ul> <li>https://flex-power.energy/energyblog</li> <li>https://quantfin.net</li> </ul>"},{"location":"Energy/Trading/Learn/#repos","title":"repos","text":"<ul> <li>https://github.com/tradytics/eiten: Algorithmic Investing Strategies for Everyone</li> </ul>"},{"location":"Energy/Trading/Retail/","title":"Retail","text":""},{"location":"Energy/Trading/Retail/#firm-vs-non-firm-load","title":"firm vs non-firm load","text":"<p>The non firm load is just the flexibility part of the contract? So the target volume is the expected total demand.</p> <p>Firm Load: - A \"firm load\" refers to electricity demand that is considered essential and reliable.   It represents a customer's guaranteed and consistent electricity consumption.   Customers with firm loads typically have a high level of reliability and dependability in their electricity supply. - In the context of electricity supply contracts, when a customer has a firm load,   it means that the utility or supplier commits to providing a specified quantity of electricity on a continuous and uninterrupted basis.   The customer can rely on the electricity being available whenever needed. - Customers with firm loads often pay a premium for this level of reliability and   may enter into long-term contracts or agreements to ensure a stable and uninterrupted power supply.</p> <p>Non-firm Load: - A \"non-firm load\" represents electricity demand that is subject to interruption or curtailment by the electricity supplier or grid operator.   It is typically considered non-guaranteed or interruptible load. - In the context of electricity supply contracts, non-firm load customers may have lower priority in terms of electricity supply during times of high demand or emergencies.   The supplier can curtail or interrupt the supply to these customers if necessary to maintain the overall stability of the grid. - Non-firm load customers often pay lower rates for their electricity because they accept the risk of occasional interruptions in their service.   These customers may include industrial users who can temporarily reduce their electricity consumption during peak periods.</p>"},{"location":"Energy/Trading/SharpeRatio/","title":"sharpe ratio","text":"<p>The Sharpe ratio is a measure of risk-adjusted return used to evaluate the performance of an investment, portfolio, or strategy. It shows whether an investment's returns are due to smart investment decisions or just excessive risk.</p>"},{"location":"Energy/Trading/SharpeRatio/#calculation-and-formula","title":"Calculation and Formula","text":"<p>The Sharpe ratio is calculated by taking the excess return of the investment over a risk-free rate and dividing it by the standard deviation of those returns.</p> <p>$$\\text{Sharpe Ratio} = \\frac{R_p - R_f}{\\sigma_p}$$</p> Variable Description $\\mathbf{R_p}$ The average return of the investment or portfolio. $\\mathbf{R_f}$ The risk-free rate of return (e.g., the yield of a short-term U.S. Treasury Bill). $\\mathbf{R_p - R_f}$ The excess return of the investment (the premium for taking risk). $\\mathbf{\\sigma_p}$ The standard deviation of the portfolio's excess return (a measure of its volatility/risk)."},{"location":"Energy/Trading/SharpeRatio/#interpretation","title":"Interpretation","text":"<p>In general, a higher Sharpe ratio is better. It indicates that the investment is generating more return for each unit of risk (volatility) taken.</p> <ul> <li>Sharpe Ratio &gt; 1.0: Considered acceptable or good.</li> <li>Sharpe Ratio &gt; 2.0: Considered very good (often described as excellent).</li> <li>Sharpe Ratio &gt; 3.0: Considered exceptional.</li> </ul> <p>Key takeaway:</p> <p>The ratio is a powerful tool because it penalizes performance that is achieved simply by taking on more risk. If two portfolios have the same return, the one with the lower volatility (and thus higher Sharpe ratio) is the better investment.</p>"},{"location":"Energy/Trading/SharpeRatio/#history","title":"History","text":"<p>The Sharpe ratio was developed by Nobel laureate William F. Sharpe in 1966, originally published as the \"reward-to-variability ratio.\" It is one of the most fundamental metrics in modern portfolio theory (MPT).</p>"},{"location":"Energy/Trading/SharpeRatio/#how-to-use-sharpe-ratio","title":"How to use sharpe ratio","text":"<ul> <li>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5520741</li> <li>This paper reviews the pitfalls of naive Sharpe ratio analysis and provides a comprehensive framework for its proper use. </li> </ul>"},{"location":"Energy/Trading/Trading/","title":"Trading","text":"<ul> <li>gas (gas market)</li> <li>spot (nem)</li> <li>retail (nem)</li> <li>renewable (cer, otc)</li> <li>derivative trading (otc, futures)</li> </ul>"},{"location":"Learn/Book/","title":"Book","text":""},{"location":"Learn/Book/#free-programming-ebooks","title":"Free programming ebooks","text":"<p>https://github.com/EbookFoundation/free-programming-books/blob/main/books/free-programming-books-subjects.md</p>"},{"location":"Learn/Website/","title":"Websites","text":""},{"location":"Learn/Website/#learning-python-c-typescript-angular-sql-etc","title":"Learning Python, C#, TypeScript, Angular, SQL etc","text":"<p>https://www.tutorialsteacher.com</p>"},{"location":"Learn/Website/#test-what-you-learned","title":"Test what you learned","text":"<p>https://www.testdome.com/tests</p>"},{"location":"Learn/Website/#learning-resources-including-videos-for-software-developers-and-devops-engineers","title":"Learning resources including videos for software developers and devops engineers","text":"<p>https://roadmap.sh</p>"},{"location":"Learn/Website/#tech-leader","title":"tech leader","text":"<p>https://www.youtube.com/@LeadDev/videos</p>"},{"location":"Learn/Video/MakeVideo/","title":"Make Video","text":"<p>https://www.techsmith.com/blog/make-youtube-video/#set-up-video-recording</p>"},{"location":"Learn/Video/ScreenRecorder/","title":"Screen Recorder","text":""},{"location":"Learn/Video/ScreenRecorder/#obs-studio","title":"OBS Studio","text":""},{"location":"Learn/Video/ScreenRecorder/#sharex","title":"ShareX","text":""},{"location":"ML/Backtest/","title":"backtest","text":"<p>Marcos Lopez de Prado - Advances in Financial Machine Learning - The purpose of a backtest is to discard bad models, not to improve them.  - Likely be overfitting - history is just one realization, of many. - Use combinatorial corss-val to create more scenarios to reduce overfitting</p>"},{"location":"ML/Classification/","title":"Classification","text":""},{"location":"ML/Classification/#confusion-matrix-and-classification-report","title":"Confusion Matrix and Classification Report","text":"<p>classfication report <pre><code>from sklearn.metrics import (\n    accuracy_score,\n    mean_squared_error,\n    mean_absolute_error,\n    confusion_matrix,\n    classification_report,\n)\nconf_matrix = confusion_matrix(y_actual, y_pred)   # confusion matrix\nclassif_report = classification_report(            # classification report\n    y_actual, y_pred, zero_division=0\n)\nprint(f'Confusion matrix:\\n{conf_matrix}')\nprint(f'Classification report:\\n{classif_report}')\n</code></pre></p>"},{"location":"ML/CrossValidation/","title":"Cross Validation","text":"<p>The number of data for traing should be between 70-80% of the total historical data.  If we have less data, perhaps you should use more data (up to 90%) for training.</p>"},{"location":"ML/CrossValidation/#purpose","title":"purpose","text":"<ul> <li>evaluate how well a model will perform on unseen data, helping to detect and prevent overfitting</li> <li>used to select the model parameters with the best score in hyperparameter tuning</li> <li>ts-cross-val: efficiently use all training data to get the best performance evaluation</li> </ul>"},{"location":"ML/CrossValidation/#sliding-window","title":"sliding window","text":"<p>The total training and test data points are the same but move to the future.</p>"},{"location":"ML/CrossValidation/#expanding-window","title":"expanding window","text":"<p>The training window expands while the test window slides.</p>"},{"location":"ML/ExponentialSmoothing/","title":"Exponential Smoothing","text":"<p>Exponential Smoothing is a popular time series forecasting method used to make predictions based on past data observations.  It is particularly useful when dealing with time series data that exhibits a certain degree of <code>trend</code>, <code>seasonality</code>, or <code>noise</code>.</p> <p>The basic idea behind Exponential Smoothing is to give more weight to recent observations while gradually decreasing the importance of older ones as you move further back in time.  This is achieved by assigning exponentially decreasing weights to the historical data points. The method is called <code>exponential</code> because the weights decrease exponentially as you move back in time.</p> <p>There are several variations of Exponential Smoothing, depending on the number of factors being considered. Some common types include: - Simple Exponential Smoothing (alpha): It considers only the current observation and the past forecast value. It is suitable for time series data with <code>no trend or seasonality</code>. - Double Exponential Smoothing (alpha, beta) (Holt's method): This method incorporates a trend component along with the level component found in simple exponential smoothing. It is useful when the data exhibits a <code>linear trend</code>. - Triple Exponential Smoothing (alpha, beta, gamma) (Holt-Winters method): This method includes a seasonal component in addition to the trend and level components. It is beneficial when the data exhibits both a <code>trend</code> and <code>seasonality</code>.</p> <p>Exponential Smoothing is relatively easy to implement and can provide reasonably accurate forecasts for <code>short-term</code> predictions.  However, it might not be as effective in capturing complex patterns or making long-term predictions where other advanced forecasting methods may be more appropriate.  It's important to analyze the data and choose the appropriate variant of Exponential Smoothing or other forecasting techniques based on the characteristics of the time series data.</p>"},{"location":"ML/ExponentialSmoothing/#parameters","title":"parameters","text":"<ul> <li>alpha ($\\alpha$): The smoothing factor for the level component (<code>smoothing_level</code>). It controls the weight given to the most recent observation in the data. Values of \u03b1 range between 0 and 1, where 0 means no weight is given to the most recent observation (ignoring it entirely), and 1 means only the most recent observation is considered for forecasting.</li> <li>beta ($\\beta$): The smoothing factor for the trend component (<code>smoothing_trend</code>). It determines the weight given to the most recent trend when calculating future trends. Like \u03b1, values of \u03b2 also range between 0 and 1.</li> <li>gamma ($\\gamma$): The smoothing factor for the seasonal component (<code>smoothing_seasonal</code>). It determines the weight given to the most recent seasonal observation when calculating future seasonal patterns. Like \u03b1 and \u03b2, values of \u03b3 also range between 0 and 1.</li> <li>Seasonal Period ($m$): The number of time periods in each season (<code>seasonal_periods</code>). For example, if the data exhibits yearly seasonality, m would be 12 for monthly data.</li> </ul> <p>The selection of appropriate parameter values can be achieved through various methods, such as <code>grid search</code>, <code>cross-validation</code>, or <code>using optimization techniques</code> to minimize forecasting errors. </p>"},{"location":"ML/Forecast/","title":"Forecast","text":"<ul> <li>static / dynamic forecasting</li> <li>short term: 2-week lookahead</li> <li>long term</li> </ul>"},{"location":"ML/Forecast/#data","title":"data","text":"<p>Data aspects: - missing data, - trend, - seasonality, - volatility, - drift and - rare events.</p> <p>https://neptune.ai/blog/arima-vs-prophet-vs-lstm</p>"},{"location":"ML/Forecast/#methods","title":"methods","text":"<p>why does forecast combination work? - https://scholar.cu.edu.eg/?q=amiratiya/files/m4-ijf.pdf - average of diverse or comparable forecasts has a shorter distance to the true values compared to each of the forecast - exclude forecasts that are considerably worse than the best ones in the pool, unless they are very diverse from the rest</p>"},{"location":"ML/Forecast/#retrospect","title":"retrospect","text":"<ul> <li>check accuracy each week</li> <li>target: minimize cost or forecast error?</li> </ul>"},{"location":"ML/Forecast/#underfit-and-overfit","title":"underfit and overfit","text":"<p>https://curiousily.com/posts/hackers-guide-to-fixing-underfitting-and-overfitting-models/</p> <ul> <li>underfit: high bias</li> <li>overfit: high variance</li> <li>mse (mean squared error) = variance + bias^2</li> </ul>"},{"location":"ML/Forecast/#prediction-intervals","title":"prediction intervals","text":"<p>https://otexts.com/fpp2/prediction-intervals.html</p>"},{"location":"ML/ForecastPackage/","title":"Forecast Package","text":""},{"location":"ML/ForecastPackage/#darts-python-lib","title":"Darts Python Lib","text":"<p>Darts supports forecasting approaches from ARIMA and exponential smoothing to novel methods based on machine learning and deep learning.  Darts also includes functions to understand the statistical properties of time series and evaluate the accuracy of forecasting models.</p>"},{"location":"ML/ForecastPackage/#pycaret-python-lib","title":"PyCaret Python Lib","text":"<p>PyCaret automates machine learning workflows.</p>"},{"location":"ML/Learn/","title":"Learn","text":""},{"location":"ML/Learn/#forecasting-example-using-xgb-gru-cnn-lstm-stc-notebook","title":"forecasting example using xgb, gru, cnn, lstm stc (notebook)","text":"<p>https://github.com/ritikdhame/Electricity_Demand_and_Price_forecasting</p>"},{"location":"ML/Learn/#many-forecast-examples","title":"many forecast examples","text":"<p>https://cienciadedatos.net/documentos/py39-forecasting-time-series-with-skforecast-xgboost-lightgbm-catboost</p>"},{"location":"ML/Learn/#probabilistic-forecasting","title":"probabilistic forecasting","text":"<ul> <li>Bootstrapping: https://skforecast.org/latest/user_guides/probabilistic-forecasting-bootstrapped-residuals.html</li> <li>Conformal prediction: https://skforecast.org/latest/user_guides/probabilistic-forecasting-conformal-prediction.html</li> <li>Quantile regression: https://skforecast.org/latest/user_guides/probabilistic-forecasting-quantile-regression.html</li> </ul>"},{"location":"ML/Learn/#kaggle","title":"kaggle","text":"<p>https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/discussion/472793</p> <p>lagged and rolling features (notebook): - https://www.kaggle.com/code/ymatioun/enefit-ym1</p>"},{"location":"ML/Learn/#forecasting-principles-and-practice","title":"Forecasting: Principles and Practice","text":"<p>https://otexts.com/fpp3/index.html</p> <p>https://github.com/zgana/fpp3-python-readalong</p> <p>Python-centered read-along of the excellent Forecasting: Principles and Practice by Rob J Hyndman and George Athanasopoulos</p>"},{"location":"ML/Learn/#httpsgithubcomnixtla","title":"https://github.com/Nixtla","text":"<p>A collection of libraries for time series forecasting, optimized for speed and developer experience.</p>"},{"location":"ML/Learn/#httpsgithubcomcuge1995awesome-time-series","title":"https://github.com/cuge1995/awesome-time-series","text":"<p>A list of papers, datasets, Kaggle competitions for various tasks in time series.</p>"},{"location":"ML/Learn/#books","title":"books","text":"<ul> <li>python machine learning: https://github.com/rasbt/python-machine-learning-book-3rd-edition</li> <li>demand forecast best practices</li> <li>Introduction to Time Series Analysis and Forecasting, 2nd Edition</li> </ul>"},{"location":"ML/MarkovChain/","title":"Markov Chain","text":""},{"location":"ML/MarkovChain/#how-it-works","title":"how it works","text":"<ul> <li>disrectize values into different buckets (states)</li> <li>get the probability from one state to another using historical data</li> <li>do the forecast using the matrix</li> </ul>"},{"location":"ML/MarkovChain/#when-to-use-it","title":"when to use it","text":"<ul> <li>unpredictable things like wind speed (not good for solar)</li> <li>depedent on current state</li> <li>good for long term, not good for short term</li> </ul>"},{"location":"ML/Method/","title":"Method","text":"<p>https://www.reddit.com/r/MachineLearning/comments/ob2rll/d_how_often_is_lightgbm_used_for_forecasting_in/</p>"},{"location":"ML/Method/#statistics","title":"statistics","text":"<ul> <li>Prophet and ARIMA-type models are great for forecasting trends with steady growth and stable seasonality</li> <li>but struggle with time-series that have complex dependencies on exogenous variables (e.g. \u2018shocks\u2019 relating to weather)</li> </ul>"},{"location":"ML/Method/#ensemble","title":"ensemble","text":"<p>Use an ensemble of  - Prophet to capture the underlying seasonality, and - XGBoost to capture the weather/seasonality shock impact</p>"},{"location":"ML/Metric/","title":"Metric","text":"<p>https://otexts.com/fpp2/accuracy.html</p> <ul> <li>(MAE) mean absolute error </li> <li>(RMSE) root mean squared error </li> <li>(MAPE) mean absolute percentage error: not good when value is close to zero</li> <li>(MAAPE) mean arctangent absolute percentage error : not consistent with the other metrics, why???</li> </ul> <p>For a combination of parameters, if a metric has different patterns compared to the other metrics, it's not consistent and should not be used. For example, all other metrics show a min for a specific combination of parameters but that metric does not show a min.</p>"},{"location":"ML/Parameter/","title":"Parameter","text":""},{"location":"ML/Parameter/#cyclical-learning-rate","title":"Cyclical learning rate","text":"<p>https://github.com/brianmanderson/Cyclical_Learning_Rate - vary the learning rate between a lower bound (min_lr) and an upper bound (max_lr) instead of using a fixed or monotonically decaying learning rate. - this can help improve training performance and speed up convergence by allowing the model to explore different areas of the loss landscape.</p>"},{"location":"ML/Regularization/","title":"Regularization","text":"<p>Regularization is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting (Wikipedia Regularization).</p> <p>Advantages for applying regularization to regression: - Preventing overfitting - Variable selection and removal of correlated variables - Converting ill-posed problems to well-posed by adding additional information via penalty parameter lambda</p> <p>Variable selection methods: - Ridge method: shrink coefficients of correlated variables - LASSO (Least Absolute Shrinkage and Selection Operator) method: pick one variable and discard the others - Elastic Net Penalty: mixture of Ridge and LASSO methods</p>"},{"location":"ML/TimeSeries/","title":"Time Series","text":""},{"location":"ML/TimeSeries/#methods","title":"methods","text":"<p>https://news.ycombinator.com/item?id=37877443 - On extremely high dimensional data: <code>MLP</code> that just uses lagged values as features - On mid-dimensional data: <code>LightGBM/Xgboost</code> is by far the best and generally performs at or better than any deep learning model - On low-dimensional data: <code>(V)ARIMA/ETS/Factor</code> models are still king</p>"},{"location":"ML/TimeSeries/#model-history","title":"model history","text":"<p>https://medium.com/@ycwong.joe/a-brief-history-of-time-series-models-38455c2cd78d</p>"},{"location":"ML/TimeSeries/#summary","title":"summary","text":"<p>https://chartexpo.com/blog/time-series-forecasting</p>"},{"location":"ML/TimeSeries/#how-to-feed-forecast-into-next-steps-input-feature","title":"How to feed forecast into next step's input feature","text":"<p>https://www.kaggle.com/code/ryanholbrook/forecasting-with-machine-learning - multioutput model - each model for one step - one model, update feature based on prevous forecast - each model for one step, update feature based on prevous forecast</p>"},{"location":"ML/ARIMA/Arima/","title":"Arima","text":"<ul> <li>for data shows clear trends or seasonal patterns</li> <li>ideal for short-term forecasts</li> <li>can use differencing to remove non-stationary trend</li> </ul>"},{"location":"ML/ARIMA/Arima/#sarimax-example","title":"SARIMAX example","text":"<p>https://github.com/SamWachira/Electricity-Demand-Forecasting/blob/main/Time_Series_Electricity.ipynb</p>"},{"location":"ML/ARIMA/Arima/#arimax-autoregressive-integrated-moving-average-with-exogenous-variables","title":"ARIMAX (autoregressive integrated moving average with exogenous variables)","text":"<ul> <li>The ARIMAX model is an extension of the standard ARIMA model, and </li> <li>it allows for the inclusion of one or more exogenous variables in the model equation. </li> <li>The ARIMAX model can be estimated using maximum likelihood estimation, and </li> <li>the parameters of the model can be estimated using numerical optimization techniques.</li> </ul>"},{"location":"ML/ARIMA/Arima/#components","title":"components","text":"<ul> <li>Auto-Regressive (AR):</li> <li>determine the present value based on historical values</li> <li>only for data with stationary trend</li> <li>Moving Average (MA):</li> <li>smoothen variations to reveal the underlying trend</li> <li>only for data with stationary trend</li> <li>Integrated (I):</li> <li>integrate the AR and MA elements by computing differences between current and past values</li> <li>transform the dataset into a stationary trend</li> </ul>"},{"location":"ML/ARIMA/Arima/#parameters","title":"parameters","text":"<ul> <li>p: the number of past values considered for the AR component</li> <li>q: the number of moving averages applied</li> <li>d: the number of past values subject to differentiation</li> </ul>"},{"location":"ML/ARIMA/Arima/#limit","title":"limit","text":"<p>lacking adaptability</p>"},{"location":"ML/Feature/Cyclical/","title":"Cyclical","text":""},{"location":"ML/Feature/Cyclical/#cyclical-features","title":"cyclical features","text":"<p>links: - https://mlpills.substack.com/p/issue-89-encoding-cyclical-features - https://skforecast.org/0.16.0/faq/cyclical-features-time-series.html</p> <pre><code>df['cyc_sncs'] = df['cyc_sin'] * df['cyc_cos']\n# Add squared terms to capture non-linear relationships\ndf['cyc_sin2'] = df['cyc_sin'] ** 2\ndf['cyc_cos2'] = df['cyc_cos'] ** 2\n</code></pre> <p>encoding types: - one-hot - sin/cos - spine - rbf</p> <p>pros and cons: - nn: sin/cos is good - tree-based models (Random Forest, XGBoost, LightGBM): split features one at a time - cannot do that for sin/cos together - prophet: automatically models seasonality using Fourier transformations - no need to encode cyclical features</p>"},{"location":"ML/Feature/Datetime/","title":"Datetime","text":"<ul> <li>https://medium.com/data-science-at-microsoft/introduction-to-feature-engineering-for-time-series-forecasting-620aa55fcab0</li> <li>https://medium.com/@karanbhutani477/feature-engineering-for-time-series-data-a-deep-yet-intuitive-guide-b544aeb26ec2</li> </ul> <p>features: - month/day/hour - weekday/weekend/holiday - lag (shift): use previous data - windows (rolling average): mean for the last day/week etc. - expanding</p>"},{"location":"ML/Feature/Datetime/#timeseries-feature-engineering","title":"timeseries feature engineering","text":"<ul> <li>https://www.kaggle.com/code/pinardogan/time-series-using-lightgbm-with-explanations/notebook</li> <li>https://www.kaggle.com/code/zhikchen/forecasting-prediction-using-lightgbm-model</li> </ul> <p>note: - lag - rolling - ewma - lasso feature selection - customized evaluation function <code>feval</code></p> <p>Feature types: | Feature Type       | Description                              | Effect on Accuracy             | |--------------------|------------------------------------------|-------------------------------| | Lag Features       | Values from previous time steps          | High for short-term trends    | | Rolling Statistics | Moving averages or standard deviations   | Medium for identifying trends | | Time-based         | Cyclical encodings (e.g., sine/cosine for time) | High for seasonal patterns    |</p>"},{"location":"ML/Feature/Datetime/#trend-decomposition","title":"trend decomposition","text":"<pre><code>from statsmodels.tsa.seasonal import seasonal_decompose\ndecompose_result = seasonal_decompose(decom_data, period=5, model='additive')\n\nobserved = decompose_result.observed\ntrend    = decompose_result.trend\nseasonal = decompose_result.seasonal\nresidual = decompose_result.resid\n</code></pre>"},{"location":"ML/Feature/Datetime/#datetime-feature","title":"datetime feature","text":"<p>Usually the <code>day of year</code>, <code>day of week</code> and <code>hour</code> will cover all the season patterns. Each will encoded into the <code>sine</code> and <code>cosine</code> features.</p>"},{"location":"ML/Feature/Datetime/#sincos-features","title":"sin/cos features","text":"<pre><code># sin(2 * pi * x / max(x))\n# cos(2 * pi * x / max(x))\n\n# hour\ndf['h_sin'] = np.sin(np.pi * df['hour'] / 12)\ndf['h_cos'] = np.cos(np.pi * df['hour'] / 12)\n# month\ndf['m_sin'] = np.sin(np.pi * df['month'] / 6)\ndf['m_cos'] = np.cos(np.pi * df['month'] / 6)\n# day of year\ndf['y_sin'] = np.sin(np.pi * df['day_of_year'] / 183)\ndf['y_cos'] = np.cos(np.pi * df['day_of_year'] / 183)\n</code></pre>"},{"location":"ML/Feature/Datetime/#radial-basis-function","title":"radial basis function","text":"<p>sin/cos vs radial basis function (rbf): https://developer.nvidia.com/blog/three-approaches-to-encoding-time-information-as-features-for-ml-models/</p> <p>diff: - sin/cos: cyclical or periodic data, such as time series data - rbf: non-linear relationships and capturing complex patterns in data, such as images - rbf can introduce more complexity to the model, which may lead to overfitting if not carefully tuned - rbf can be computationally more expensive than sine/cosine features, especially for large datasets</p>"},{"location":"ML/Feature/Datetime/#lag-and-lag-diffauto-correlation","title":"lag and lag diff/auto correlation","text":"<p>lag features: https://www.kaggle.com/code/ryanholbrook/time-series-as-features <pre><code>df['x_lag1'] = df.groupby(['id'])['x'].shift(1)\ndf['x_lag1_diff'] = df['x'] - df['x'].groupby(['id']).shift(1)\ndf['x_lag1_autocorr'] = df['x'].corrwith(df['x'].shift(1))\n</code></pre></p>"},{"location":"ML/Feature/Datetime/#percentage-diff","title":"percentage diff","text":"<pre><code>df['pct_diff_x_y'] = (df['x'] - df['y']) / df['y'] * 100\n</code></pre>"},{"location":"ML/Feature/Datetime/#diff-firstmedian","title":"diff first/median","text":"<pre><code>df['x_diff_first'] = df['x'] - df['x'].groupby(['id']).transform('first') #median\n</code></pre>"},{"location":"ML/Feature/Datetime/#imbalance","title":"imbalance","text":"<pre><code>df['imb_x_y'] = (df['x'] - df['y']).divide(df['x'] + df['y'], fill_value=np.nan)\n</code></pre>"},{"location":"ML/Feature/Datetime/#rolling","title":"rolling","text":"<p>mean/var/std/min/max/med/skew/kurt/cumsum/range <pre><code>df['x_mean'] = df.groupby(['id'])['x'].agg('mean') # std etc\ndf['x_cumsum'] = df.groupby(['id'])['x'].cumsum()\ndf['x_range'] = df.groupby(['id'])['x'].max(axis=1) - df.groupby(['id'])['x'].min(axis=1)\n</code></pre></p>"},{"location":"ML/Feature/Feature/","title":"Feature","text":""},{"location":"ML/Feature/Feature/#time-series-features","title":"time series features","text":"<ul> <li><code>observed features</code>:<ul> <li>\"target-dependent\" features</li> <li>can only be lagged to the latest available date</li> <li>or used to calculate statistical rolling features</li> <li>so in forecast we have and can use these (lagged /rolling) features</li> </ul> </li> <li><code>forecast features</code>:<ul> <li>can be included directly or lagged, as long as they are available at time of forecasting</li> </ul> </li> </ul>"},{"location":"ML/Feature/Feature/#feature-importance-assessment","title":"Feature importance assessment","text":"<p>Feature\u2019s relative importance can be assessed through two approaches: - Maybe first use <code>featurewiz-polars</code> - Shapley (python package <code>shap</code>) decompositions - Marginal performance loss through feature removal</p>"},{"location":"ML/Feature/Feature/#feature-selection","title":"feature selection","text":"<p>Best approach: use <code>featurewiz</code> to reduce feature size and then use <code>shap</code> to explain feature importance.</p> <p>Featurewiz: Recursive XGBoost Feature Selection - Automated feature selection - Extensive feature engineering <pre><code>import featurewiz as fw\nwiz = fw.FeatureWiz(verbose=1)\nX_train, y_train = wiz.fit_transform(train[preds], train[target])\nX_test = wiz.transform(test[preds])\n</code></pre> Featurewiz-Polars: https://github.com/AutoViML/featurewiz_polars/tree/main <pre><code>from featurewiz_polars import FeatureWiz\n\n# Initialize FeatureWiz for classification\nwiz = FeatureWiz(\n    model_type=\"Classification\",\n    estimator=None,\n    corr_limit=0.7,\n    category_encoders='onehot',\n    classic=True,\n    verbose=0,\n)\n\n# Fit and transform the training data\nX_transformed, y_transformed = wiz.fit_transform(X_train, y_train)\n\n# Transform the test data\nX_test_transformed = wiz.transform(X_test)\n\n# Transform the test target variable\ny_test_transformed = wiz.y_encoder.transform(y_test)\n\n# View results\nprint(\"Selected Features:\")\nprint(wiz.selected_features)\n# Example Output: ['col1', 'col2', 'category_A', 'category_B']\n\nprint(\"\\nTransformed DataFrame head:\")\nprint(X_transformed.head())\n# Example Output: Polars DataFrame with only the selected features\n</code></pre></p>"},{"location":"ML/Feature/FracDiff/","title":"Fractionally Differentiated Features","text":"<p>Fractionally Differentiated Features are a technique used to transform time series data in a way that - reduces short-term noise and - preserves long-term trends (memory)</p> <p>Example, fractional differencing $y_t - \\gamma y_{t-1}$ where $\\gamma$ is a fractional value like 0.5.</p> <p>Why use fractional differencing? - Preserve Long-Term Trends: Fractional differencing allows you to remove short-term noise while maintaining the long-range dependencies that are crucial for accurate forecasts, especially in financial, economic, or stock market data. - Stationarity Without Over-Differencing: It can help make the series stationary without over-differencing, which can lead to loss of information. For example, first differencing (subtracting one previous observation) may eliminate too much of the useful data, but fractional differencing avoids this problem. - Better Model Performance: For certain time series models, like machine learning models (LightGBM, XGBoost), maintaining long-term dependencies can be crucial for predictive accuracy. Fractional differencing can improve model performance by preserving these important correlations.</p> <p>Note that integer differencing will: - make the data stationary, - it also often results in over-differencing (no memory), which can remove valuable information and hurt model performance</p>"},{"location":"ML/Feature/Imbalanced/","title":"Imbalanced","text":""},{"location":"ML/Feature/Imbalanced/#imbalanced-data-should-i-use-smote","title":"imbalanced data - should i use <code>smote</code>","text":"<ul> <li>https://mindfulmodeler.substack.com/p/dont-fix-your-imbalanced-data</li> <li>https://mindfulmodeler.substack.com/p/imbalanced-data-do-nothing-should</li> </ul> <p>Problem: - oversampling, will break calibration - undersampling, will lost some information</p> <p>Solutions: - weight data points: give underrepresented data points a higher and overrepresented a lower weight   - lgb: <code>sample_weight</code> and <code>class_weight = balanced</code> in classification - use cost-sensitive machine learning to train models - do threshold tuning on validation data - smote works for a weak classifier - undersampling for extremely large data and assigned larger weights to the sampled examples</p>"},{"location":"ML/Feature/Imbalanced/#lightgbm-imbalanced-data","title":"lightgbm imbalanced data","text":"<pre><code>model = lgb.train(\n    params={'class_weight': 'balanced'},\n    train_set=train_data,\n    valid_sets=valid_sets,\n    num_boost_round=num_boost_round,\n)\n</code></pre>"},{"location":"ML/Feature/Imbalanced/#xgboost-imbalanced-data","title":"xgboost imbalanced data","text":"<p>https://xgboosting.com/xgboost-imbalanced-multi-class-classification-set-sample_weight-using-compute_sample_weight/ <pre><code>from sklearn.utils import compute_sample_weight\nsample_weight = compute_sample_weight('balanced', y_train)\ntrain_data = xgb.DMatrix(\n    data=X_train,\n    label=y_train,\n    weight=sample_weight,\n)\n\n# sklearn\nmodel = XGBClassifier(random_state=42)\nmodel.fit(X_train, y_train, sample_weight=sample_weight)\n</code></pre></p>"},{"location":"ML/Feature/Importance/","title":"feature importance","text":"<p>The methods, like PFI or SHAP importance, are not designed for feature selection. They are for feature importance - explain features</p> <p>Purpose of feature selection: - Improve predictive performance - Speed up model training and prediction - Reduce feature space for comprehensibility - Cost reduction</p>"},{"location":"ML/Feature/Importance/#feature-importance-example","title":"feature importance example","text":"<pre><code>import xgboost as xgb\nimport lightgbm as lgb\n\n# XGBoost feature importance\nimport xgboost as xgb\n# model = model = xgb.XGBClassifier(\n#     objective='binary:logistic',  # Default for binary classification\n#     eval_metric='logloss',        # Metric for evaluation\n# )\nmodel = xgb.XGBRegressor(\n    objective='reg:squarederror',  # Default for regression, minimizes squared error\n    eval_metric='rmse',            # Metric for evaluation\n)\nmodel.fit(X, y)\nimportance = model.feature_importances_\nprint(importance)\n\n# LightGBM feature importance\ntrain_data = lgb.Dataset(data=X.to_arrow(), label=y.to_arrow())\n# for small number of features, reduce `num_leaves` to avoid warnings\n# for small number of data points, reduce `min_data_in_leave` to avoid warnings\n# warnings #1: [Warning] No further splits with positive gain, best gain: -inf\n# warnings #2: There are no meaningful features which satisfy the provided configuration.\n# Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\nparams = {'objective': 'regression', 'metric': 'rmse', 'num_leaves': 2, 'min_data_in_leaf': 2}\nmodel = lgb.train(params, train_data, num_boost_round=100)\nimportances = model.feature_importance(importance_type='gain')\nprint(importance)\n\n# Catboost feature importance, lgb: 35x faster, xgb: 15x faster\n# model = ctb.CatBoostClassifier(verbose=0)\nmodel = ctb.CatBoostRegressor(verbose=0)\nmodel.fit(X.to_pandas(), y.to_pandas())\nimportance = model.get_feature_importance()\nprint(importance)\n</code></pre>"},{"location":"ML/Feature/Importance/#shap-feature-explainer","title":"shap feature explainer","text":"<p>```py import shap import numpy as np import matplotlib.pyplot as plt from statsmodels.graphics.tsaplots import (     plot_acf,     plot_pacf, )</p>"},{"location":"ML/Feature/Selection/","title":"feature selection","text":"<p>Summary of all feature selection methods (including python code): - https://github.com/Yimeng-Zhang/feature-engineering-and-feature-selection - A Short Guide for Feature Engineering and Feature Selection.pdf</p> <p>Feature selection implementations: - https://github.com/jundongl/scikit-feature</p> <p>pros and cons of different methods: - https://www.paypalobjects.com/ecm_assets/Feature%20Selection%20WP-PP-v1.pdf - voting: time consuming - union: leading to to many features with many methods - union of xgb, lgb and ctb: best result with low compution time</p> <p>links: - https://neptune.ai/blog/feature-selection-methods - https://www.kaggle.com/code/prashant111/comprehensive-guide-on-feature-selection/notebook - https://www.kaggle.com/code/kanncaa1/feature-selection-and-data-visualization - methods: https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00905-w</p> <p>good disscusion: - https://www.reddit.com/r/datascience/comments/1gsa6aj/lightgbm_feature_selection_methods_that_operate - suggest <code>Permutation Importance</code> - others: NFE and then RFE - Correlation-based feature selection with Mutual Information is a good starting point, LightGBM can handle it efficiently</p> <p>three widely used feature selection methods: - ANOVA - Mutual Information - Recursive Feature Elimination</p>"},{"location":"ML/Feature/Selection/#tree-based-method","title":"tree based method","text":"<p>Limitation: - correlated features show similar importance - correlated features importance is lower than real importance, when tree is build without its correlated counterparts - high carinal variable tend to show higher importance</p> <p>Effect of Correlated Features: - Lasso tends to pick one feature out of a correlated group and zero out the rest. - Boosted trees often split on multiple correlated features, sharing importance among them.</p> <p>use model build-in importance feature: - xgboost - lightgbm - random forrest</p>"},{"location":"ML/Feature/Selection/#simple-methods","title":"Simple methods","text":""},{"location":"ML/Feature/Selection/#variance-threshold","title":"Variance Threshold","text":"<p>Remove constant/near-constant features: <pre><code>from sklearn.feature_selection import VarianceThreshold\nX_reduced = VarianceThreshold(threshold=0.01).fit_transform(X)\n</code></pre></p>"},{"location":"ML/Feature/Selection/#correlation-filter","title":"Correlation Filter","text":"<p>Remove features with low target correlation: - only apply to linear relationship - simple but less effective compared to univariate selection - reducing features helps explainability. But dropping them via correlations will just lead to loss of potentially useful features <pre><code># Calculate correlations\ncorr = df.corr()['target'].sort_values(ascending=False)\nprint(corr)\n</code></pre></p> <p>Remove highly correlated features (&gt;0.95): <pre><code>import pandas as pd\ncorr_matrix = X.corr().abs()\nupper = corr_matrix.where(\n    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n)\nto_drop = [column for column in upper.columns if any(upper[column] &gt; 0.95)]\nX_filtered = X.drop(to_drop, axis=1)\n</code></pre></p>"},{"location":"ML/Feature/Selection/#vif-for-numeric-features","title":"vif for numeric features","text":"<ul> <li>Measure multicollinearity: how much a feature is linearly correlated with other features</li> <li>Useful for linear models (e.g., linear regression), where multicollinearity can harm interpretability or inflate variance</li> <li>For tree-based models (like LightGBM, XGBoost), multicollinearity is not usually a problem, since trees can handle correlated features</li> <li>In real world, the very best variable could be extremely highly correlated with all the other variables -- you might drop it based on vif scores</li> </ul>"},{"location":"ML/Feature/Selection/#pearson-corr-vs-vif","title":"pearson corr vs vif","text":"<ul> <li>corr().abs(), using pearson correlation, is faster and simpler, great for quick pairwise filtering</li> <li>vif is more robust for detecting multicollinearity, especially in linear models.</li> <li>both are not good for tree based models like xgboost</li> </ul>"},{"location":"ML/Feature/Selection/#filter-methods","title":"Filter methods","text":"<ul> <li><code>f_classif</code> for numerical features and a categorical target</li> <li><code>chi-squared</code> for categorical features and a categorical target</li> </ul>"},{"location":"ML/Feature/Selection/#univariate-selection","title":"Univariate Selection","text":"<ul> <li>for regression problems: <code>f_regression</code></li> <li>for classification problems: <code>f_classif</code></li> <li>for catagrical features: ``</li> <li>doesn't consider feature interactions</li> </ul> <p>select the best features: <pre><code>from sklearn.feature_selection import SelectKBest, f_classif\n# select top 500 features for classification using ANOVA F-value between each feature and target\nselector = SelectKBest(score_func=f_classif, k=500)\nselected = selector.fit_transform(X_train, y_train)\n</code></pre></p> <p>calculate scores for all features: <pre><code>selector = SelectKBest(score_func=chi2, k=10)\nfit = selector.fit(X, y)\nf_scores = pd.DataFrame({'feature': X.columns, 'score': fit.scores_})\nprint(f_scores.nlargest(10, 'score'))\n</code></pre></p>"},{"location":"ML/Feature/Selection/#nonlinear-univariate","title":"Nonlinear Univariate","text":"<ul> <li>It's slow: https://github.com/scikit-learn/scikit-learn/issues/6904</li> <li>It has an inherent n.log(n) cost per feature as long as it's using exact nearest neighbors. <pre><code>from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\nmi_scores = mutual_info_regression(X, y, njob=-1, discrete_features=False, random_state=42)\nmi_scores = mutual_info_classif(X, y, njob=-1, discrete_features=False, random_state=42)\n</code></pre></li> </ul>"},{"location":"ML/Feature/Selection/#wrapper-methods","title":"Wrapper methods","text":"<ul> <li>use a machine learning model to evaluate the performance of different subsets of features</li> </ul>"},{"location":"ML/Feature/Selection/#permutation-importance-feature-shuffling","title":"Permutation Importance (Feature Shuffling)","text":"<p>Randomly shuffle the values of a specific variable and determine how that permutation affects the performance metric (Measures drop in model performance when each feature is shuffled) - Permute the values of each feature, one at the time - If a variable is important, a random permutation of its values will decrease dramatically any of these metrics - Non-important variables should have little to no effect on the model performance metric - Can be slow</p> <pre><code># Permutation Importance example\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import permutation_importance\n\n# Train a model, Random Forest is a great choice as it's a powerful tree-based model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X.to_pandas(), y.to_pandas())\n\n# Calculate Permutation Importance on the test set to get a more reliable estimate\nperm = permutation_importance(model, X.to_pandas(), y.to_pandas(), n_repeats=10, random_state=13)\n\n# Get and sort the `perm.importances_mean` attribute that gives the average importance score\ndf = pl.DataFrame({\n    'feature': X.columns,\n    'perm_score': perm.importances_mean,\n    'perm_score_std': perm.importances_std,\n}).sort('perm_score', descending=True)\nprint(df)\n</code></pre>"},{"location":"ML/Feature/Selection/#recursive-feature-elimination-rfe","title":"Recursive Feature Elimination (RFE)","text":"<p>https://machinelearningmastery.com/rfe-feature-selection-in-python/</p> <p>RFE works by searching for a subset of features by starting with all features in the training dataset and successfully removing features until the desired number of features reached. - Too slow for a huge amount of features</p> <pre><code>from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\n# Create a sample dataset\nX, y = make_classification(\n  n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=42\n)\n# Initialize the model and RFE selector\nmodel = LogisticRegression()\nrfe_selector = RFE(estimator=model, n_features_to_select=5, step=1)\n# Fit RFE\nrfe_selector = rfe_selector.fit(X, y)\n# Get the selected features\nselected_features = rfe_selector.support_\nprint(f'Selected features: {selected_features}')\n</code></pre>"},{"location":"ML/Feature/Selection/#embedded-methods","title":"Embedded methods","text":"<ul> <li>force the coefficients of less important features to become exactly zero</li> <li>larger <code>alpha</code> leads to heavier penalty. 0-no regularization, 0.01-light, 1-strong, 100-very strong</li> </ul> <p>Lasso (L1) regularization: - A linear model with linear relationships between features and target. - Shrinks some coefficients to zero, performing explicit feature selection based on linear correlations. - Sensitive to multicollinearity \u2014 often picks one feature among correlated groups.</p> <pre><code>from sklearn.linear_model import Lasso\nfrom sklearn.datasets import make_regression\n# Create a sample dataset with some irrelevant features\nX, y = make_regression(n_samples=100, n_features=20, n_informative=5, random_state=42)\n# Initialize and train the Lasso model\nlasso = Lasso(alpha=0.01)\nlasso.fit(X, y)\n# Features with a coefficient of 0 are not important\nprint(lasso.coef_)\n</code></pre> <p>Auto feature selection: <code>BorutaPy</code>: A Robust Feature Selection Algorithm - can be computationally expensive</p> <p>Time series feature generation: - <code>TSFresh</code> can automatically extract and filter useful time series features based on statistical tests - <code>Featuretools</code> can help with automated feature generation + selection (more for relational data, but flexible)</p>"},{"location":"ML/Feature/Selection/#create-shap-explainer-for-tree-base-models","title":"create SHAP explainer (for tree base models)","text":"<p>explainer = shap.TreeExplainer(model.regressor)</p>"},{"location":"ML/Feature/Selection/#sample-50-of-the-data-to-speed-up-the-calculation","title":"sample 50% of the data to speed up the calculation","text":"<p>rng = np.random.default_rng(seed=785412) sample = rng.choice(X_train.index, size=int(len(X_train)*0.5), replace=False) X_train_sample = X_train.loc[sample, :] shap_values = explainer.shap_values(X_train_sample)</p>"},{"location":"ML/Feature/Selection/#shap-summary-report-top-10-features-only","title":"shap summary report (top 10 features only)","text":"<p>shap.initjs() shap.summary_plot(shap_values, X_train_sample, max_display=10, show=False) fig, ax = plt.gcf(), plt.gca() ax.set_title('SHAP Summary plot') ax.tick_params(labelsize=8) fig.set_size(10, 4.5) ```</p>"},{"location":"ML/Feature/SeqBootstrap/","title":"Sequential Bootstrap","text":"<p>A sequential bootstrap is a variant of the bootstrap resampling method. It is used primarily in time series analysis or situations where data points are dependent (i.e., there's some temporal or sequential structure in the data).</p> <p>How Does Sequential Bootstrap Work? - Divide the data into blocks: The data is first divided into smaller, non-overlapping blocks or chunks (often called blocks of observations). - Resample blocks: These blocks are then sampled with replacement to create new sequences of data, preserving the temporal structure. - Reconstruct a new series: The blocks are stitched together to form a new time series, which can be used for further analysis or to estimate parameters (e.g., for model validation or hypothesis testing).</p> <p>Types of Sequential Bootstrap - Block Bootstrap (Fixed-Size Blocks) - Moving Block Bootstrap (Overlapping Blocks) - Circular Block Bootstrap</p> <p>example <pre><code>import numpy as np\n\n# Original time series data (example)\ndata = np.array([30, 32, 31, 29, 28, 30, 32, 33, 35, 34])\n\n# Parameters for the block bootstrap\nblock_size = 3  # size of each block\nn_bootstrap_samples = 5  # number of bootstrap samples\n\n# Create a function to perform block bootstrap\ndef block_bootstrap(data, block_size, n_samples):\n    n = len(data)\n    n_blocks = n // block_size\n    bootstrap_samples = []\n\n    # Create blocks of size `block_size`\n    blocks = [data[i:i + block_size] for i in range(0, n, block_size)]\n\n    # Generate bootstrap samples by sampling blocks with replacement\n    for _ in range(n_samples):\n        sampled_blocks = np.random.choice(blocks, size=n_blocks, replace=True)\n        bootstrap_sample = np.concatenate(sampled_blocks)\n        bootstrap_samples.append(bootstrap_sample)\n\n    return bootstrap_samples\n\n# Perform block bootstrap\nbootstrap_results = block_bootstrap(data, block_size, n_bootstrap_samples)\n\n# Print the bootstrap samples\nfor i, sample in enumerate(bootstrap_results):\n    print(f\"Bootstrap Sample {i+1}: {sample}\")\n</code></pre></p>"},{"location":"ML/Feature/Target/","title":"Target","text":""},{"location":"ML/Feature/Target/#target-engineering","title":"Target engineering","text":""},{"location":"ML/Feature/Temperature/","title":"temperature","text":"<p>temperature on electricity demand is not linear (Foreseeing the Worst: Forecasting Electricity DART Spikes): - heating degree day (hdd) = max (BP \u2212 T_t, 0) - cooling degree day (cdd) = max (T_t \u2212 BP, 0) - break point (BP) = 18.3 dC</p>"},{"location":"ML/Feature/Transform/","title":"Transform","text":""},{"location":"ML/Feature/Transform/#random-noise","title":"random noise","text":"<ul> <li>For small datasets, in order to avoid overfitting, random noise can be added to the values</li> </ul>"},{"location":"ML/Feature/Transform/#log-transformation","title":"log transformation","text":"<p>In regression problems, when the target values have a large scale or skewed distribution, gradient descent optimization can be less stable or slower to converge due to large gradient magnitudes. To address this, taking the logarithm of the target values can help normalize the scale, reduce skewness, and improve optimization stability and speed. - Reduces Skewness: It compresses the large values and spreads out the smaller values, making the distribution more symmetrical and closer to a normal distribution. - Stabilizes Variance: It can help address heteroscedasticity, a situation where the variance of the errors is not constant across all levels of the independent variables. - Taking the logarithm of targets is common when the target is strictly positive and has a heavy-tailed distribution (e.g., predicting prices, incomes). - Gradient boosting algorithms cannot extrapolate</p>"},{"location":"ML/Feature/Variance/","title":"Variance threshold","text":""},{"location":"ML/Feature/Variance/#sklearn-slow","title":"sklearn slow","text":"<pre><code>import polars as pl\nsklearn.feature_selection import VarianceThreshold\ndef sklearn_variance_threshold(\n    X: pl.DataFrame,\n    threshold: float = 0.0,\n) -&gt; pl.DataFrame:\n    selector = VarianceThreshold(threshold=0.0)\n    _ = selector.fit_transform(X)\n    df = X[selector.get_support()]\n    return df\n</code></pre>"},{"location":"ML/Feature/Variance/#polars-faster","title":"polars faster","text":"<p>about 20x faster <pre><code>import polars as pl\ndef polars_variance_threshold(\n    X: pl.DataFrame,\n    threshold: float = 0.0,\n) -&gt; pl.DataFrame:\n    stats = X.select([\n        pl.var(col).alias(col) for col in X.columns\n    ])\n    variances = stats.row(0)  # get variances as a list\n    df = X.select([\n        col for col, var in zip(X.columns, variances) \n        if var &gt; threshold\n    ])\n    return df\n</code></pre></p>"},{"location":"ML/Hyperparameter/Bayesian/","title":"bayesian tuning","text":"<ul> <li>Simplicity and Focus: https://github.com/bayesian-optimization/BayesianOptimization</li> <li>Flexibility and Complexity: https://github.com/hyperopt/hyperopt</li> <li>Scalability and Integration: https://github.com/optuna/optuna</li> </ul>"},{"location":"ML/Hyperparameter/Hyperparameter/","title":"Hyperparameter tuning","text":"<ul> <li>grid search</li> <li>random search</li> <li>Bayesian optimization</li> </ul>"},{"location":"ML/Hyperparameter/Hyperparameter/#tree-models-hyper-parameter-tuning","title":"tree models hyper parameter tuning","text":"<p>https://github.com/arnaudvl/ml-parameter-optimization - lgb and xgb cannot efficiently be tuned using gridsearch given the large amount of hyperparameters - the parameters for them are tuned iteritavely</p>"},{"location":"ML/Hyperparameter/Optuna/","title":"Optuna","text":"<p>Optuna is a popular open-source Python library designed for hyperparameter optimization.  It provides a flexible and efficient framework for automating the process of finding the best parameter values for machine learning models.</p>"},{"location":"ML/Hyperparameter/Optuna/#key-features","title":"Key Features","text":"<ul> <li>Hyperparameter Optimization: Optuna automates the search for optimal hyperparameters, saving time and effort.</li> <li>Various Optimization Algorithms: It supports a range of optimization algorithms, including TPE (Tree-structured Parzen Estimator), CMA-ES (Covariance Matrix Adaptation Evolution Strategy), and Random Search.</li> <li>Trial Management: Optuna manages individual optimization trials, allowing you to track and compare different parameter combinations.</li> <li>Visualization: It provides tools for visualizing the optimization process, helping you understand the search space and identify trends.</li> <li>Extensibility: Optuna is highly customizable and can be integrated with various machine learning frameworks and libraries.</li> </ul>"},{"location":"ML/Hyperparameter/Optuna/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Hyperparameter Tuning: Finding the best hyperparameters for models like neural networks, random forests, and support vector machines.</li> <li>Feature Engineering: Optimizing feature selection and preprocessing parameters.</li> <li>Model Selection: Choosing the most suitable model architecture or algorithm for a given problem.</li> </ul>"},{"location":"ML/Hyperparameter/Optuna/#example","title":"Example","text":"<pre><code>import optuna\n\ndef objective(trial):\n    # Define hyperparameters to optimize\n    x = trial.suggest_float('x', -10, 10)\n    y = trial.suggest_categorical('y', ['a', 'b', 'c'])\n\n    # Your machine learning model and evaluation logic here\n    model = ...\n    score = ...\n\n    return score\n\n# Create a study and run the optimization\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\n# Get the best trial and its parameters\nbest_trial = study.best_trial\nprint('Best value:', best_trial.value)\nprint('Best params:', best_trial.params)\n</code></pre>"},{"location":"ML/LightGBM/Example/","title":"Example","text":""},{"location":"ML/LightGBM/Example/#lightgbm-vs-xgboost","title":"LightGBM vs XGBoost","text":"<ul> <li>LightGBM generally faster than XGBoost, especially for large datasets, does not support polars (<code>df.to_arrow</code> and <code>sr.to_arrow</code> should work)</li> <li>XGBoost might have slightly lower memory requirements compared to LightGBM</li> </ul>"},{"location":"ML/LightGBM/Example/#parameters","title":"parameters","text":"<p>https://lightgbm.readthedocs.io/en/latest/Parameters.html</p> <p>The hyperparameters that have the greatest effect on optimizing the LightGBM evaluation metrics are ref: - <code>num_iterations</code>: [50, 1000]. How many trees will be built - <code>learning_rate</code>: [0.001, 0.01]. default=0.1. - <code>num_leaves</code>: [10, 100], (1, 131072], default=31. Maximum number of leaves in one tree - <code>max_depth</code>: [15, 100]. Maximum depth of the tree - <code>min_data_in_leaf</code>: [10, 200], default=20. Minimal number of data in one leaf - <code>feature_fraction</code>: [0.1, 1], default=1. alias: colsample_bytree. randomly select a subset of features on each iteration (tree) - <code>bagging_fraction</code>: [0.1, 1]. alias: <code>subsample</code>. randomly select part of data without resampling - <code>bagging_freq</code>: [0, 10]. frequency for bagging - <code>lambda_l1</code>: default=0. alias: reg_alpha - <code>lambda_l2</code>: default=0. alias: reg_lambda.</p>"},{"location":"ML/LightGBM/Example/#example_1","title":"example","text":"<p>https://github.com/microsoft/LightGBM/issues/2930</p>"},{"location":"ML/LightGBM/Example/#feature-selection","title":"feature selection","text":"<p>https://www.kaggle.com/code/pnprabakaran/feature-selection-before-hand-lightgbm</p>"},{"location":"ML/LightGBM/Example/#lightgbm-example","title":"LightGBM example","text":"<p><code>lgb.train</code> is the core training API for lightgbm. <pre><code>params = {\n    'verbose': -1,\n    'objective': 'regression_l1',\n    'seed': 13,\n    'device_type': 'gpu',\n    'num_threads': 0,\n    'learning_rate': 0.02,\n    'num_leaves': 96,\n    'min_data_in_leaf': 1500,\n    'feature_fraction': 0.6,\n    'feature_fraction_bynode': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n}\n\nnum_iterations = 1200 # number of iterations\n# train model\ntrain_set = lgb.Dataset(df[xcols], df['target'])\nmodel = lgb.train(\n    params=params,\n    train_set=train_set,\n    num_boost_round=num_iterations,\n    valid_sets=[train_set, test_set],\n    init_model=None,\n    callbacks=[lgb.log_evaluation(50), lgb.early_stopping(stopping_rounds=250)],\n)\nmodel.save_model('lgb_model.txt')\n# forecast\npreds = m.predict(df[xcols])\n\n# feature importance\nax = lgb.plot_importance(model, max_num_features=40, figsize=(15,15))\nplt.show()\n\n# feature importance to csv\ndf_importance = pd.DataFrame({\n    'feature': model.feature_name(),\n    'importance': model.feature_importance(),\n})\ndf_importance.to_csv('feature_importance.csv')\n</code></pre></p>"},{"location":"ML/LightGBM/Example/#lgbmregressor","title":"LGBMRegressor","text":"<p><code>LGBMRegressor</code> is the sklearn interface, a wrapper for LightGBM. The <code>fit(X, y)</code> call is standard sklearn syntax for model training. <pre><code>params = {\n    'verbose': -1,\n    'objective': 'mae',\n    'random_state': seed,\n    'device': 'gpu',\n    'n_jobs': 4,\n    'n_estimators': 1200,\n    'learning_rate': 0.03,\n    'max_depth': 10,\n    'num_leaves': 256,\n    'subsample': 0.6,\n    'colsample_bynode': 0.6,\n    'colsample_bytree': 0.9,\n    'importance_type': 'gain',\n}\nmodel = lgb.LGBMRegressor(**params)\n\nmodel.fit(\n    df_train.drop(columns=['target']),\n    df_train['target'] - df_train['target_lag_7d'].fillna(0),\n    eval_set=[(\n        df_valid.drop(columns=['target']),\n        df_valid['target'] - df_valid['target_lag_7d'].fillna(0),\n    )],\n    categorical_feature=cat_features,\n    callbacks=[\n        lgb.callback.early_stopping(stopping_rounds=100),\n        lgb.callback.log_evaluation(period=100)\n    ],\n)\nmodel.booster_.save_model(model_filename)\n\n# feature importance\nlgb.booster_.feature_importance()\nfea_imp_ = pd.DataFrame({'cols': train.columns, 'fea_imp': lgb.feature_importances_})\nfea_imp_.loc[fea_imp_.fea_imp &gt; 0].sort_values(by=['fea_imp'], ascending=False)\n\n# plot feature importance\nplt.barh(X_train.columns, xgb.feature_importances_)\nplt.show()\n</code></pre></p>"},{"location":"ML/LightGBM/Forecast/","title":"Forecast","text":""},{"location":"ML/LightGBM/Forecast/#train-model","title":"Train model","text":"<pre><code># Train the LightGBM model\nparams = {\n    'objective':'regression', \n    'metric':'mae',\n    'seed': 13, \n    'verbosity': -1,  # suppress all output [-1, 0, 1, 2]\n    'force_col_wise': True, # only for cpu\n    'learning_rate': 0.05, \n    'num_leaves': 40,\n    'max_depth': 20, \n    'min_data_in_leaf': 100, \n    'feature_fraction': 0.8, \n    'bagging_fraction': 0.9, \n    'bagging_freq': 2, \n    'lambda_l1': 0.5, \n    'lambda_l2': 2.5,\n}\n\n# Create dataset for LightGBM\ntrain_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n\n# Train the model\nt0 = time()\nmodel = lgb.train(\n    params=params, \n    train_set=train_data, \n    valid_sets=[test_data], \n    num_boost_round=10000, \n    callbacks=[\n        lgb.early_stopping(stopping_rounds=100, verbose=False), # verbose: log message with early stopping information\n        lgb.log_evaluation(500), # output every 500 boosts\n    ],\n)\nprint(f'lgb model training time: {time() - t0:.3f} seconds')\n</code></pre>"},{"location":"ML/LightGBM/Forecast/#predict","title":"Predict","text":"<pre><code># Make predictions\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n# Evaluate the model\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f'RMSE: {rmse:.3f}')\n\n# Plot the actual vs predicted values\nplt.figure(figsize=(14, 4))\nplt.plot(y_test.index, y_test.values, label='Actual', color='black')\nplt.plot(y_test.index, y_pred, label='Predicted', color='blue')\nplt.plot(y_test.index, y_pred - y_test.values + 1000, label='Error', color='red')\nplt.legend()\nplt.title('Forecasting with LightGBM')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"ML/LightGBM/Forecast/#plotly","title":"plotly","text":"<pre><code>fig = go.Figure()\n\n# Add traces\nx_val = y_test.index\ny_val = y_test.values\nfig.add_trace(go.Scatter(x=[x_val[0], x_val[-1]], y=[-100, -100], mode='lines', name='', line=dict(color='white'), showlegend=False))\nfig.add_trace(go.Scatter(x=x_val, y=y_val, mode='lines', name='Actual', line=dict(color='black')))\nfig.add_trace(go.Scatter(x=x_val, y=y_pred, mode='lines', name='Predicted', line=dict(color='blue')))\nfig.add_trace(go.Scatter(x=x_val, y=y_pred - y_val + 1000.0, mode='lines', name='Error', line=dict(color='red')))\n\n# Customize the layout\nfig.update_layout(\n    title='Forecasting with LightGBM',\n    xaxis_title='Time',\n    yaxis_title='Forecast',\n    showlegend=True,\n    xaxis_gridcolor='lightgray',\n    yaxis_gridcolor='lightgray',\n    height=600,\n    margin=dict(t=0, b=0, l=0, r=0),\n)\nfig.show()\n</code></pre>"},{"location":"ML/LightGBM/Hyperparam/","title":"Hyper Parameters","text":""},{"location":"ML/LightGBM/Hyperparam/#bayesian-optimization","title":"Bayesian Optimization","text":"<ul> <li>https://xgboosting.com/bayesian-optimization-of-xgboost-hyperparameters-with-bayes_opt/</li> <li>https://www.kaggle.com/code/somang1418/tuning-hyperparameters-under-10-minutes-lgbm</li> <li>https://www.analyticsvidhya.com/blog/2021/05/bayesian-optimization-bayes_opt-or-hyperopt/</li> </ul> <p>packages: - bayes_opt [bayesian-optimization] - hyperopt - skopt [scikit-optimize]</p>"},{"location":"ML/LightGBM/Hyperparam/#example-code","title":"example code","text":"<pre><code>t0 = time()\nfrom bayes_opt import BayesianOptimization\ndef lgb_hyper_opt_bayes(\n    X_train, y_train, init_round=5, opt_round=10, n_folds=5, n_estimators=10000, random_seed=13\n):\n    # prepare data\n    train_set = lgb.Dataset(data=X_train, label=y_train, free_raw_data=False)\n\n    # parameters\n    def lgb_eval(\n        learning_rate, \n        num_leaves, \n        max_depth, \n        min_data_in_leaf, \n        feature_fraction, \n        bagging_fraction, \n        bagging_freq, \n        lambda_l1,\n        lambda_l2,\n    ):\n        params = {\n            'objective':'regression', \n            'metric':'mae',\n            'verbosity': -1,  # suppress all output [-1, 0, 1, 2]\n            'force_col_wise': True, # only for cpu\n            'learning_rate': max(min(learning_rate, 1), 0),\n            'num_leaves': int(round(num_leaves)),\n            'max_depth': int(round(max_depth)),\n            'min_data_in_leaf': int(round(min_data_in_leaf)),\n            'feature_fraction': max(min(feature_fraction, 1), 0),\n            'bagging_fraction': max(min(bagging_fraction, 1), 0),\n            'bagging_freq': int(round(bagging_freq)),\n            'lambda_l1': max(lambda_l1, 0),\n            'lambda_l2': max(lambda_l2, 0),\n        }\n\n        lgb_cv = lgb.cv(\n            params=params, \n            train_set=train_set, \n            metrics=['l1'], \n            num_boost_round=n_estimators,\n            nfold=n_folds, \n            stratified=False, \n            shuffle=False, \n            seed=random_seed, \n            callbacks=[\n                lgb.early_stopping(stopping_rounds=100, verbose=False), # verbose: log message with early stopping information\n                lgb.log_evaluation(500), # output every 500 boosts\n            ],\n        )\n        # negative because we are minimizing\n        return -min(lgb_cv['valid l1-mean'])\n\n    pbounds = {\n        'learning_rate': (0.001, 0.2),\n        'num_leaves': (10, 100),\n        'max_depth': (15, 100),  \n        'min_data_in_leaf': (10, 200),\n        'feature_fraction': (0.1, 1),\n        'bagging_fraction': (0.8, 1),\n        'bagging_freq': (0, 10),\n        'lambda_l1': (0, 10),\n        'lambda_l2': (0, 10),\n    }   \n    lgb_bo = BayesianOptimization(\n        f=lgb_eval, \n        pbounds=pbounds,\n        random_state=200,\n    )\n\n    #init_points (5-20): How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    #n_iter (20-100): How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    lgb_bo.maximize(init_points=init_round, n_iter=opt_round)\n\n    # extracting the best result (highest -MAE is the best)\n    best_params = lgb_bo.max['params']\n    best_score = lgb_bo.max['target']\n    return best_params, best_score\n\nopt_params, opt_score = lgb_hyper_opt_bayes(X_train, y_train, init_round=5, opt_round=10, n_folds=5, n_estimators=10000, random_seed=13)\nprint(f'hyper opt time: {time() - t0:.3f} seconds')\nprint(opt_score)\nprint(opt_params)\n</code></pre>"},{"location":"ML/LightGBM/LightGBM/","title":"LightGBM","text":""},{"location":"ML/LightGBM/LightGBM/#lgb-python-docs","title":"lgb python docs","text":"<p>https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.cv.html#lightgbm.cv</p>"},{"location":"ML/LightGBM/LightGBM/#lgb-best-practices","title":"lgb best practices","text":"<p>https://www.linkedin.com/advice/0/what-best-practices-using-lightgbm-gradient-boosting-g0lye</p>"},{"location":"ML/LightGBM/LightGBM/#lgb-dataset","title":"lgb dataset","text":"<p>when we call <code>dataset</code>, it will: - bundle sparse features - bin continuous features into histograms, prevent creation of histogram bins that are too small (<code>min_data_in_bin</code>) - dropping features which are guaranteed to be uninformative (for example, features which are constant)</p>"},{"location":"ML/LightGBM/LightGBM/#missing-values","title":"missing values","text":"<ul> <li>LightGBM offers in-built options for handling missing values</li> <li>set the parameter accordingly <code>use_missing=True</code></li> </ul>"},{"location":"ML/LightGBM/LightGBM/#categorical-features","title":"categorical features","text":"<ul> <li>LightGBM offers in-built options for encoding categorical features</li> <li>specify which features are categorical by using the <code>categorical_feature</code> parameter in <code>dataset</code></li> <li>convert categorical columns to a categorical data type in pandas before splitting data into training and test sets</li> <li>take advantage of LightGBM's internal optimizations for categorical features rather than manually encoding these features</li> </ul>"},{"location":"ML/LightGBM/LightGBM/#early_stopping_round","title":"early_stopping_round","text":"<p>Use early stopping to prevent overfitting</p> <p>https://github.com/microsoft/LightGBM/issues/5196 - option 1: set <code>early_stopping_round</code> in <code>params</code> argument - option 2: passing <code>early_stopping()</code> callback via <code>callbacks</code> argument <pre><code>model = lgb.train(\n    params={**params, 'early_stopping_round': 3},\n    train_set=train_set,\n    num_boost_round=10,\n    valid_sets=[valid_set],\n    ...\n)\n\nbst_1 = lgb.train(\n    params=params,\n    train_set=train_set,\n    num_boost_round=10,\n    valid_sets=[valid_set],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=3),\n        lgb.log_evaluation(1)\n    ]\n)\n</code></pre></p>"},{"location":"ML/LightGBM/LossProfile/","title":"lightgbm loss profiles (Train &amp; Valid)","text":"<p>Plotting the training and validation loss (or other metrics) over boosting rounds is useful to understand: - When overfitting starts - Whether early stopping kicked in at the right point - Whether training should have run longer</p>"},{"location":"ML/LightGBM/LossProfile/#use-evals_result-in-lgbtrain","title":"Use <code>evals_result</code> in <code>lgb.train()</code>","text":"<p>The <code>evals_result</code> parameter lets you collect the training history.</p> <pre><code>import lightgbm as lgb\nimport matplotlib.pyplot as plt\n\nevals_result = {}\nmodel = lgb.train(\n    params,\n    train_set,\n    num_boost_round=1000,\n    valid_sets=[train_set, valid_set],\n    valid_names=['train', 'valid'],\n    evals_result=evals_result,\n    early_stopping_rounds=10,\n    verbose_eval=10\n)\n\ntrain_loss = evals_result['train']['multi_logloss']\nvalid_loss = evals_result['valid']['multi_logloss']\n\n# Plot the Loss Profiles\nplt.figure(figsize=(10, 6))\nplt.plot(train_loss, label='Train Loss')\nplt.plot(valid_loss, label='Valid Loss')\nplt.xlabel('Boosting Round')\nplt.ylabel('Log Loss')\nplt.title('Training vs Validation Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"ML/LightGBM/LossProfile/#use-lgbcv","title":"Use <code>lgb.cv()</code>","text":"<p>LightGBM's <code>cv()</code> function also returns loss per iteration:</p> <pre><code>cv_results = lgb.cv(\n    params,\n    train_set,\n    num_boost_round=1000,\n    nfold=5,\n    metrics='multi_logloss',\n    early_stopping_rounds=10\n)\n\n# plot the validation loss\nplt.plot(cv_results['multi_logloss-mean'])\nplt.title('CV Validation Loss')\nplt.xlabel('Boosting Round')\nplt.ylabel('Log Loss')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"ML/LightGBM/Memory/","title":"Memory","text":""},{"location":"ML/LightGBM/Memory/#lightgbm-memory-factors","title":"LightGBM memory factors","text":"<ul> <li>https://github.com/microsoft/LightGBM/issues/562</li> <li>https://lightgbm.readthedocs.io/en/latest/FAQ.html</li> </ul> <p>Adjusting these parameters can reduce memory usage: - <code>histogram_pool_size</code>: 1024? histogram cache ~ <code>20bytes * num_leaves * num_features * num_bins</code> - <code>num_leaves</code>: 255? When <code>num_leaves</code> decreases, the RAM required decreases exponentially - <code>max_bin</code>:</p> <p>Solution to lower RAM usage in LightGBM: - set <code>histogram_pool_size</code> parameter to the MB you want to use - approximately RAM used = histogram_pool_size + dataset size - lower <code>num_leaves</code>, lower <code>max_depth</code>, or lower <code>max_bin</code></p>"},{"location":"ML/LightGBM/Memory/#lightgbm-reduce-memory","title":"lightgbm reduce memory","text":"<ul> <li>https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html</li> <li>https://github.com/microsoft/LightGBM/issues/6319</li> </ul> <p>lightgbm training-time memory usage: - raw data - dataset - model - other</p> <p>Avoid the memory usage for the <code>raw data</code>: - construct a Dataset directly from a file (either a CSV/TSV/LibSVM file or a LightGBM Dataset binary file).</p> <p>Reduce the memory usage of the <code>dataset</code>: - use smaller <code>max_bin</code> [default = 255] or high <code>min_data_in_bin</code> - remove irrelevant features before construction - In Python, construct a dataset and perform training in the same process to avoid storing a copy of the raw data by passing <code>free_raw_data=True</code></p> <p>Reduce the size of the <code>model</code>: - use <code>early stopping</code> - set <code>max_depth</code> [default = -1] (good values of this will depend on num_leaves). - reduce <code>num_leaves</code>. default = 31 - reduce <code>n_estimators</code> (<code>num_boost_round</code>). - increase <code>min_gain_to_split</code>. default = 0.0 - increase <code>min_data_in_leaf</code>. default = 20</p>"},{"location":"ML/MLOps/Framework/","title":"Framework","text":"<p>http://www.tharwan.de/mlops-gridsearch.html</p> <p>commercial: - AzureML - Neptune - Weighs - Biases</p> <p>open source: - MLflow - CLearML - OpenMLOps</p>"},{"location":"ML/MLOps/Metric/","title":"Metric","text":""},{"location":"ML/Neural/LSTM/","title":"LSTM","text":"<p>LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit)  - are both types of recurrent neural network (RNN) layers designed to handle sequential data - gating mechanisms that allow them to capture long-term dependencies more effectively</p>"},{"location":"ML/Neural/LSTM/#lstm-vs-gru","title":"LSTM vs GRU","text":"<p>https://medium.com/@prudhviraju.srivatsavaya/lstm-vs-gru-c1209b8ecb5a</p> <p>LSTM:  - LSTM has more parameters, - which can result in slightly slower training times compared to GRU, - especially on larger datasets. GRU: - With fewer parameters, - GRU may have faster training times, - making it more efficient for larger datasets.</p>"},{"location":"ML/Neural/NBEATS/","title":"NBEATS","text":"<p>NBEATS (Neural Basis Expansion Analysis for Time Series) is a neural network-based model specifically designed for time series forecasting. - Focuses on: Capturing non-linear relationships and complex patterns in time series data. - Architecture: It typically consists of a stack of fully connected layers and/or convolutional layers, allowing it to capture complex patterns in time series data. - Training: NBEATS is trained using gradient descent-based optimization methods, where the model parameters are updated iteratively to minimize a predefined loss function. - Interpretability: While neural networks can be challenging to interpret due to their complex architecture, techniques like attention mechanisms in NBEATS can help understand which parts of the input sequence are important for the predictions.</p>"},{"location":"ML/Neural/NBEATS/#strengths","title":"Strengths","text":"<ul> <li>Can handle complex seasonality and trends.</li> <li>Offers good accuracy for short-term forecasting.</li> <li>Can incorporate past covariates (additional data points that might influence the forecast).</li> <li>Can be extended to handle multivariate forecasting (multiple time series at once).</li> </ul>"},{"location":"ML/Neural/NBEATS/#weaknesses","title":"Weaknesses","text":"<ul> <li>Requires more data for training compared to XGBoost.</li> <li>Can be computationally expensive to train, especially for large datasets.</li> <li>Interpretability of the model can be challenging.</li> <li>NBEATS does not support future exogenous variables???</li> </ul>"},{"location":"ML/Neural/NHITS/","title":"NHITS","text":"<p>NHITS (Neural Hierarchical Interpolation for Time Series) Forecasting. </p> <p>N-HITS is particularly suitable for <code>hierarchical time series</code> forecasting tasks,  - where the data can be naturally decomposed into multiple levels of aggregation - such as sales data at different geographical levels or product hierarchies</p>"},{"location":"ML/Neural/NHITS/#how-it-works","title":"how it works","text":"<ul> <li>It utilizes a special technique called hierarchical interpolation to efficiently forecast future values. This technique breaks down the forecasting process into smaller components focusing on different frequencies within the data.</li> <li>Additionally, NHITS employs multi-rate input processing, meaning it can analyze data at various granularities to capture both short-term and long-term patterns.</li> </ul>"},{"location":"ML/Neural/NHITS/#strengths","title":"Strengths","text":"<ul> <li>Accuracy: NHITS boasts high accuracy, especially for short-term forecasting, often outperforming established models like LSTMs.</li> <li>Complex seasonality:  It excels at capturing intricate seasonal patterns and trends in time series data.</li> <li>Efficiency: Compared to other high-accuracy models, NHITS can be computationally efficient, requiring less training time in some cases. </li> <li>Flexibility: It can handle multivariate forecasting (multiple time series at once) and incorporate past covariates (additional data points that might influence the forecast).</li> </ul>"},{"location":"ML/Neural/NHITS/#weaknesses","title":"Weaknesses","text":"<ul> <li>Data requirements: While generally efficient, NHITS might still require more data for training compared to simpler models.  </li> <li>Interpretability: Similar to other neural networks, understanding how NHITS arrives at predictions can be challenging.</li> </ul>"},{"location":"ML/Neural/NHITS/#help","title":"help","text":"<ul> <li>A research paper on NHITS: https://arxiv.org/abs/2201.12886</li> <li>Documentation on NHITS implementation: https://www.nixtla.io/</li> </ul>"},{"location":"ML/Neural/Package/","title":"Package","text":""},{"location":"ML/Neural/Package/#neuralforecast","title":"NeuralForecast","text":"<p>https://github.com/Nixtla/neuralforecast</p> <p>NeuralForecast offers a large collection of neural forecasting models - AutoRNN - AutoLSTM - AutoGRU - AutoMLP - AutoNBEATS - AutoNHITS</p> <p>Example: https://www.kaggle.com/code/guslovesmath/amazing-neuralforecast-nvda-forecasting <pre><code>from neuralforecast.models import NBEATS, NHITS, LSTM\n</code></pre></p>"},{"location":"ML/Neural/RNN/","title":"RNN","text":""},{"location":"ML/Neural/RNN/#feature-engineering-with-rnns-and-lightgbm","title":"Feature Engineering with RNNs and LightGBM","text":"<ul> <li>Train an RNN Model: In this approach, you first train an RNN model on your time series data. The RNN can be an LSTM or any other suitable variant.</li> <li>Extract Features from RNN Outputs: Use the hidden state or output vector from the final layer of the trained RNN as additional features for your LightGBM model. These features capture the sequential patterns learned by the RNN.</li> <li>Train LightGBM: Train a LightGBM model using your original time series features along with the extracted RNN features. This allows LightGBM to leverage both the sequential insights from the RNN and the non-linear relationships it can capture from the original features.</li> </ul>"},{"location":"ML/Neural/RNN/#ensemble-learning-with-rnns-and-lightgbm","title":"Ensemble Learning with RNNs and LightGBM","text":"<ul> <li>Train Separate Models: Train an independent RNN model and a LightGBM model on your time series data.</li> <li>Combine Predictions: During prediction, make predictions with both the RNN and LightGBM models. You can then combine these predictions using techniques like averaging, weighted averaging, or stacking. Averaging gives equal weight to both models, while weighted averaging allows you to assign higher weightage to the model performing better on a validation set. Stacking involves training a meta-model on the predictions from the individual models to make the final forecast.</li> </ul> <p>Here are some additional considerations when combining RNNs and LightGBM: * Choosing the Right Combination Strategy: The best approach depends on your specific problem and the characteristics of your data. Experiment with both feature engineering and ensemble learning to see which yields better results. * Data Preprocessing: Ensure consistent data preprocessing for both models. This might involve scaling or normalization if necessary. * Hyperparameter Tuning: Both RNNs and LightGBM have numerous hyperparameters that can significantly impact performance. Tune the hyperparameters of each model independently to optimize their contributions to the combined model.</p> <p>By combining the strengths of RNNs and LightGBM, you can potentially achieve more accurate and robust time series forecasts compared to using either model alone. However, remember that this approach can also increase model complexity and require more computational resources for training.</p>"},{"location":"ML/Neural/TCN/","title":"TCN","text":"<p>TCN (Temporal Convolutional Network) is a convolutional neural network (CNN) architecture used for time series forecasting.</p>"},{"location":"ML/Neural/TCN/#strengths","title":"Strengths","text":"<ul> <li>Focuses on long-range dependencies: TCNs excel at capturing these dependencies in time series data due to their convolutional layers, making them well-suited for tasks where long-term patterns are important (e.g., stock price trends).</li> <li>Potentially faster training: TCNs might be computationally less expensive to train than LSTMs for some datasets, especially for very long sequences.</li> </ul>"},{"location":"ML/Neural/TCN/#weaknesses","title":"Weaknesses","text":"<ul> <li>May struggle with complex seasonality: While TCNs can handle some seasonality, they might not be as adept at capturing intricate seasonal patterns compared to LSTMs.</li> <li>Hyperparameter tuning: TCNs can still require careful hyperparameter tuning to achieve optimal performance.</li> <li>Less established: Compared to LSTMs, TCNs are a relatively newer architecture, potentially leading to fewer resources and established practices.</li> </ul>"},{"location":"ML/Prophet/ConditionalSeasonality/","title":"conditional seasonalities","text":"<p>https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html#seasonalities-that-depend-on-other-factors</p>"},{"location":"ML/Prophet/ConditionalSeasonality/#peak-values-not-captured-in-forecast","title":"peak values not captured in forecast","text":"<p>https://github.com/facebook/prophet/issues/1791#issue-792284890</p> <ul> <li>using conditional seasonalities</li> <li>using log transform</li> </ul> <pre><code>def is_nfl_season(ds):\n    date = pd.to_datetime(ds)\n    return (date.month &gt; 8 or date.month &lt; 2)\n\ndf['on_season'] = df['ds'].apply(is_nfl_season)\ndf['off_season'] = ~df['ds'].apply(is_nfl_season)\n\n# disable deafult weekly_seasonality\nm = Prophet(weekly_seasonality=False)\n# add two new weekly_seasonalities\nm.add_seasonality(name='weekly_on_season', period=7, fourier_order=3, condition_name='on_season')\nm.add_seasonality(name='weekly_off_season', period=7, fourier_order=3, condition_name='off_season')\n\nfuture['on_season'] = future['ds'].apply(is_nfl_season)\nfuture['off_season'] = ~future['ds'].apply(is_nfl_season)\nforecast = m.fit(df).predict(future)\nfig = m.plot_components(forecast)\n</code></pre>"},{"location":"ML/Prophet/Issue/","title":"Issue","text":""},{"location":"ML/Prophet/Issue/#program-stalls","title":"Program stalls","text":"<p>https://github.com/facebook/prophet/issues/842</p> <p>Too few data points lead to numeric singularity??? then use newton not L-BFGS. Other reasons???</p>"},{"location":"ML/Prophet/Issue/#trend-issue","title":"Trend issue","text":"<p>The trend can be completely wrong for long term forecast: https://towardsdatascience.com/fixing-prophets-forecasting-issue-b473afe2cc70</p>"},{"location":"ML/Prophet/Issue/#optimization-terminated-abnormally-falling-back-to-newton","title":"Optimization terminated abnormally. Falling back to Newton.","text":"<p>https://github.com/facebook/prophet/issues/1678</p> <p>by disabling the Newton fallback, will will see all the errors from stanpy <pre><code>m = Prophet()\nm.stan_backend.set_options(newton_fallback=False)\nm.fit(df)\n</code></pre> then catch the error: <pre><code>except RuntimeError as e: \n</code></pre></p>"},{"location":"ML/Prophet/Issue/#line-search-failed-to-achieve-a-sufficient-decrease-no-more-progress-can-be-made","title":"Line search failed to achieve a sufficient decrease, no more progress can be made","text":"<p>https://discourse.mc-stan.org/t/line-search-failed-error/10163 <pre><code>Very small alpha (related to step size in line search) supports\nthe assumption that the search direction is not pointing downhill.\nYou can also test running with smaller ODE solver tolerances.\n</code></pre></p> <p>lbfgs (CmdStan User's Guide: https://github.com/stan-dev/stan-dev.github.io/blob/master/users/documentation/index.md) <pre><code>init_alpha = 0.001 (Default) The alpha value is for step size used\ntol_obj = 1e-12 (Default)\ntol_rel_obj = 10000 (Default)\ntol_grad = 1e-08 (Default)\ntol_rel_grad = 10000000 (Default)\ntol_param = 1e-08 (Default)\nhistory_size = 5 (Default)\n</code></pre></p>"},{"location":"ML/Prophet/Logging/","title":"Logging","text":""},{"location":"ML/Prophet/Logging/#disable-prophet-log","title":"disable prophet log","text":"<pre><code>import logging\nlogging.getLogger('prophet').setLevel(logging.ERROR)\n</code></pre>"},{"location":"ML/Prophet/Logging/#diable-cmdstanpy-log","title":"diable cmdstanpy log","text":"<pre><code>cmdstanpy_logger = logging.getLogger(\"cmdstanpy\")\ncmdstanpy_logger.disabled = True\n</code></pre>"},{"location":"ML/Prophet/Logging/#redirect-cmdstanpy-log","title":"redirect cmdstanpy log","text":"<pre><code>cmdstanpy_logger.disabled = False\ncmdstanpy_logger.handlers = [] #remove all existing handlers\ncmdstanpy_logger.setLevel(logging.DEBUG)\n\nhandler = logging.FileHandler('cmdstanpy.log') #logging.StreamHandler()\nhandler.setLevel(logging.DEBUG)\nhandler.setFormatter(\n    logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        \"%H:%M:%S\",\n    )\n)\ncmdstanpy_logger.addHandler(handler)\n</code></pre>"},{"location":"ML/Prophet/Logging/#suppress-log","title":"suppress log","text":"<p>https://github.com/facebook/prophet/issues/223 <pre><code>class suppress_stdout_stderr(object):\n    '''\n    A context manager for doing a \"deep suppression\" of stdout and stderr in\n    Python, i.e. will suppress all print, even if the print originates in a\n    compiled C/Fortran sub-function.\n\n    This will not suppress raised exceptions, since exceptions are printed\n    to stderr just before a script exits, and after the context manager has\n    exited (at least, I think that is why it lets exceptions through).\n    '''\n    def __init__(self):\n        # Open a pair of null files\n        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n        # Save the actual stdout (1) and stderr (2) file descriptors.\n        self.save_fds = [os.dup(1), os.dup(2)]\n\n    def __enter__(self):\n        # Assign the null pointers to stdout and stderr.\n        os.dup2(self.null_fds[0], 1)\n        os.dup2(self.null_fds[1], 2)\n\n    def __exit__(self, *_):\n        # Re-assign the real stdout/stderr back to (1) and (2)\n        os.dup2(self.save_fds[0], 1)\n        os.dup2(self.save_fds[1], 2)\n        # Close the null files\n        for fd in self.null_fds + self.save_fds:\n            os.close(fd)\n</code></pre></p>"},{"location":"ML/Prophet/Performance/","title":"Performane","text":"<p>https://towardsdatascience.com/how-to-run-facebook-prophet-predict-x100-faster-cce0282ca77d</p> <p>https://towardsdatascience.com/need-for-speed-optimizing-facebook-prophet-fit-method-to-run-20x-faster-166cd258f456</p>"},{"location":"ML/Prophet/Prophet/","title":"Prophet","text":"<p>Prophet will internally scale <code>y</code> before fitting the model so <code>y</code> does not need to be normalized.</p> <p>For data without any seasonality or holiday effects, Prophet is just a piecewise linear regression, so it's not appropriate for these time series.</p> <p>quick start:\\ https://facebook.github.io/prophet/docs/quick_start.html#python-api</p> <p>guide:\\ https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-prophet-in-python-3</p> <p>Implementing Facebook Prophet efficiently:\\ https://towardsdatascience.com/implementing-facebook-prophet-efficiently-c241305405a3</p> <p>\"Prophet uses a decomposable time series model (Harvey &amp; Peters 1990) with three main model components: trend, seasonality, and holidays.  They are combined in the following equation:</p> <p><code>y(t) = g(t) + s(t) + h(t) + e(t)</code></p> <p>Here g(t) is the trend function which models non-periodic changes in the value of the time series, s(t) represents periodic changes (e.g., weekly and yearly seasonality), and h(t) represents the effects of holidays which occur on potentially irregular schedules over one or more days. The error term e(t) represents any idiosyncratic changes which are not accommodated by the model; later we will make the parametric assumption that e(t) is normally distributed.\"</p> <p>\"For model fitting we use Stan\u2019s L-BFGS to find a maximum a posteriori estimate.\"</p>"},{"location":"ML/Prophet/Prophet/#seasonality","title":"seasonality","text":"<pre><code>m = Prophet(yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False)\nm.add_seasonality(name='yearly', period=365.25, fourier_order=10)\nm.add_seasonality(name='quarterly', period=365.25/4, fourier_order=10)\nm.add_seasonality(name='monthly', period=30.5, fourier_order=10)\nm.add_seasonality(name='weekly', period=7, fourier_order=10)\nm.add_seasonality(name='daily', period=1, fourier_order=10)\n</code></pre>"},{"location":"ML/Prophet/Prophet/#parameters","title":"parameters","text":"<pre><code>yearly_seasonality='auto',\nweekly_seasonality='auto',\ndaily_seasonality='auto',\nholidays=None,\nseasonality_mode='additive',\nseasonality_prior_scale=10.0,\nholidays_prior_scale=10.0,\nchangepoint_prior_scale=0.05,\nmcmc_samples=0,\ninterval_width=0.80,\nstan_backend=None\n</code></pre>"},{"location":"ML/Prophet/Prophet/#can-be-tuned","title":"can be tuned","text":"<ul> <li>changepoint_prior_scale: [0.001, 0.05, 0.5]<ul> <li>Parameter modulating the flexibility of the automatic changepoint selection.</li> <li>Large values will allow many changepoints, possibly overfit, will produce wide uncertainty intervals</li> <li>Small values will allow few changepoints, possibly underfit</li> </ul> </li> <li>seasonality_prior_scale: [0.01, 10, 10]<ul> <li>Parameter modulating the strength of the seasonality model.</li> <li>Larger values allow the model to fit larger seasonal fluctuations</li> <li>Smaller values dampen the seasonality</li> <li>Can be specified for individual seasonalities using add_seasonality.</li> </ul> </li> <li>holidays_prior_scale: [0.01, 10, 10]<ul> <li>Parameter modulating the strength of the holiday     components model, unless overridden in the holidays input.</li> </ul> </li> <li>seasonality_mode: 'additive' (default) or 'multiplicative'.</li> </ul>"},{"location":"ML/Prophet/Prophet/#may-be-tuned","title":"may be tuned","text":"<ul> <li>changepoint_range: 0.8<ul> <li>Proportion of history in which trend changepoints will be estimated.</li> <li>Defaults to 0.8 for the first 80%.</li> <li>Not used if<code>changepoints</code> is specified.</li> </ul> </li> </ul>"},{"location":"ML/Prophet/Prophet/#not-be-tuned","title":"not be tuned","text":"<ul> <li>growth: linear<ul> <li>String 'linear', 'logistic' or 'flat'</li> <li>to specify a linear, logistic or flat trend.</li> </ul> </li> <li>changepoints: None<ul> <li>List of dates at which to include potential changepoints.</li> <li>If not specified, potential changepoints are selected automatically.</li> </ul> </li> <li>n_changepoints: 25<ul> <li>Number of potential changepoints to include.</li> <li>Not used if input <code>changepoints</code> is supplied.</li> <li>If <code>changepoints</code> is not supplied, then n_changepoints potential changepoints     are selected uniformly from the first <code>changepoint_range</code> proportion of the history.</li> </ul> </li> <li>yearly_seasonality: auto<ul> <li>Fit yearly seasonality.</li> <li>Can be 'auto', True, False, or a number of Fourier terms to generate.</li> </ul> </li> <li>weekly_seasonality: auto<ul> <li>Fit weekly seasonality.</li> <li>Can be 'auto', True, False, or a number of Fourier terms to generate.</li> </ul> </li> <li>daily_seasonality: auto<ul> <li>Fit daily seasonality.</li> <li>Can be 'auto', True, False, or a number of Fourier terms to generate.</li> </ul> </li> <li>holidays: None<ul> <li>pd.DataFrame with columns holiday (string) and ds (date type)     and optionally columns lower_window and upper_window which specify a     range of days around the date to be included as holidays.     lower_window=-2 will include 2 days prior to the date as holidays. Also     optionally can have a column prior_scale specifying the prior scale for     that holiday.</li> </ul> </li> <li>mcmc_samples: Integer, if greater than 0, will do full Bayesian inference         with the specified number of MCMC samples. If 0, will do MAP         estimation.</li> <li>interval_width: Float, width of the uncertainty intervals provided         for the forecast. If mcmc_samples=0, this will be only the uncertainty         in the trend using the MAP estimate of the extrapolated generative         model. If mcmc.samples&gt;0, this will be integrated over all model         parameters, which will include uncertainty in seasonality.</li> <li>uncertainty_samples: 1000<ul> <li>Number of simulated draws used to estimate uncertainty intervals.</li> <li>Settings this value to 0 or False will disable uncertainty estimation and speed up the calculation.</li> </ul> </li> <li>stan_backend: str as defined in StanBackendEnum default: None - will try to         iterate over all available backends and find the working one</li> </ul>"},{"location":"ML/Prophet/Prophet/#neural-prophet","title":"Neural Prophet","text":"<p>https://arxiv.org/pdf/2111.15397</p>"},{"location":"ML/Prophet/Trend/","title":"Trend","text":"<p>There are three built-in trend functions  - piecewise linear - piecewise logistic growth, and - flat</p>"},{"location":"ML/Prophet/Trend/#force-trend-growth-rate-to-be-flat","title":"force trend growth rate to be flat","text":"<pre><code>m = Prophet(growth='flat')\n</code></pre>"},{"location":"ML/Prophet/Trend/#saturating-forecast-using-logistic-growth-trend-model","title":"Saturating Forecast using <code>logistic growth</code> trend model","text":"<p>https://facebook.github.io/prophet/docs/saturating_forecasts.html#forecasting-growth</p> <p>By default, Prophet uses a linear model for its forecast. Prophet allows you to make forecasts using a <code>logistic growth</code> trend model: - <code>cap</code> and <code>floor</code> columns can be used to bound the trend. - <code>cap</code> or <code>floor</code> must be specified for every row in the dataframe, and that it does not have to be constant.</p> <p>example <pre><code># fit model\ndf['cap'] = 8.5\nm = Prophet(growth='logistic')\nm.fit(df)\n\n# predict\nfuture = m.make_future_dataframe(periods=1826)\nfuture['cap'] = 8.5\nforecast = m.predict(future)\nfig = m.plot(forecast)\n</code></pre></p>"},{"location":"ML/Prophet/UncertaintyInterval/","title":"Uncertainty (Prediction) Interval","text":"<p>Not Confident Interval: https://stats.stackexchange.com/questions/619860/are-prophets-uncertainty-intervals-confidence-intervals-or-prediction-interva</p> <p>The default <code>uncertainty interval</code> is 80%.</p> <p>https://www.mikulskibartosz.name/understanding-uncertainty-intervals-generated-by-prophet/ <pre><code>m = Prophet(uncertainty_samples=1000, interval_width=0.95)\n</code></pre></p> <p>By default, Interval_width parameter is related to uncertainty in <code>trend</code> and <code>observation noise</code> only. The uncertainty interval of the calculated seasonality is not taken into account. But Prophet calculates it when we set the <code>mcmc_samples</code> parameter.  <pre><code>m = Prophet(uncertainty_samples=1000, mcmc_samples=1000, interval_width=0.95)\n</code></pre></p> <p>To see the uncertainty in seasonality we must do <code>full Bayesian sampling</code> by settting the <code>mcmc.samples</code>.  The default value of 0 for <code>mcmc.samples</code> mans that it uses <code>maximum of posterior estimation</code>.</p>"},{"location":"ML/Prophet/WeeklyComponent/","title":"Weekly component","text":"<p>Weekly component is also modeled with a truncated Fourier series. <pre><code>prophet = Prophet(weekly_seasonality=50)  # Use 50 for the series order instead of the default 3\n</code></pre></p>"},{"location":"ML/Quantile/PinballLoss/","title":"Pinball loss","text":""},{"location":"ML/Quantile/PinballLoss/#pinball-loss-in-quantile-regression","title":"pinball loss in Quantile Regression","text":"<ul> <li>https://mindfulmodeler.substack.com/p/how-i-made-peace-with-quantile-regression <pre><code>diff = y_true - y_pred\npinball_loss = np.maximum(quantile * diff, (quantile - 1) * diff).mean()\n</code></pre></li> <li>assume <code>quantile = 0.1</code> and <code>y_true = 100</code></li> <li>if <code>y_pred = 97</code> (y_pred &lt; y_true, <code>underpredict</code>): loss = 0.1 * (100 - 97) = 3 * 0.1</li> <li>if <code>y_pred = 103</code> (y_pred &gt; y_true, <code>overpredict</code>): loss = (1 - 0.1) * (103 - 100) = 3 * 0.9</li> <li>loss is asymmetric, more penalty for overpredict</li> <li>expect the model to underpredict 90% of the time and overpredict 10% of the time</li> <li>the predict mean is below the true - equivalent to the 10% quantile</li> </ul>"},{"location":"ML/Quantile/QuantileCrossing/","title":"Quantile crossing using lightgbm","text":"<p>Where lower quantile prediction (e.g., $\\alpha=0.05$) is higher than central quantile prediction (e.g., $\\alpha=0.50$ or the median) in quantile regression is called <code>Quantile Crossing</code> or <code>Quantile Non-Monotonicity</code>.</p> <p>That's because we train separate models for each quantile independently.</p> <p>Solutions:</p>"},{"location":"ML/Quantile/QuantileCrossing/#post-processing-with-sorting","title":"post processing with <code>sorting</code>","text":"<pre><code>sorted_preds = np.sort([q05_preds, q50_preds, q95_preds], axis=0)\nq05_corrected = sorted_preds[0]\nq50_corrected = sorted_preds[1]\nq95_corrected = sorted_preds[2]\n</code></pre>"},{"location":"ML/Quantile/QuantileCrossing/#post-processing-with-isotonic-regression","title":"post processing with <code>isotonic regression</code>","text":"<pre><code>import numpy as np\nfrom sklearn.isotonic import IsotonicRegression\n\n# Example quantile predictions from LightGBM model\nq05_preds = model.predict(X, num_iteration=model.best_iteration, quantile=0.05)\nq50_preds = model.predict(X, num_iteration=model.best_iteration, quantile=0.50)\nq95_preds = model.predict(X, num_iteration=model.best_iteration, quantile=0.95)\n\n# Quantile levels corresponding to the predictions\nquantiles = np.array([0.05, 0.50, 0.95])\n\n# Stack the predictions for isotonic regression\nvals = np.array([q05_preds, q50_preds, q95_preds])\n\n# Apply isotonic regression to enforce monotonicity\niso = IsotonicRegression(increasing=True)\ncorrected_vals = np.array([iso.fit_transform(quantiles, preds) for preds in vals])\n\n# Corrected quantiles\nq05_corrected = corrected_vals[0]\nq50_corrected = corrected_vals[1]\nq95_corrected = corrected_vals[2]\n</code></pre>"},{"location":"ML/Quantile/QuantileCrossing/#feature-selection-regularization-preventive","title":"Feature Selection / Regularization (Preventive)","text":"<p>Quantile crossing is often a sign of overfitting or instability in the model's structure.</p> <p>Action: Before training, try these steps to stabilize the individual models: - Reduce Overfitting: Increase regularization (lambda_l1, lambda_l2), decrease learning_rate, and increase min_child_samples. - Feature Selection: Crossing often happens when the model latches onto noisy features. Try reducing the number of input features (X) to see if the crossing disappears. - Bigger Dataset: If possible, train on more data. More data usually leads to more stable quantile estimates.</p>"},{"location":"ML/Quantile/QuantileCrossing/#constraint-enforcement-advancedrobust","title":"Constraint Enforcement (Advanced/Robust)","text":"<p>For a robust, production-ready solution, you should enforce the constraint during the modeling process.</p> <p>Recursive/Sequential Modeling (The Best Approach). Instead of training $K$ independent models, you train them sequentially and use the output of the previous model as a feature in the next. - Train $M_{0.05}$: Train the model for $\\tau=0.05$. - Train $M_{0.50}$: Train the model for $\\tau=0.50$, but add the predictions from $M_{0.05}$ as a new input feature (X). - Train $M_{0.95}$: Train the model for $\\tau=0.95$, and add the predictions from $M_{0.50}$ as a new input feature (X).</p>"},{"location":"ML/Sklearn/Preprocessing/StandardScalar/","title":"StandardScalar","text":""},{"location":"ML/Sklearn/Preprocessing/StandardScalar/#fit_transform-issue","title":"<code>fit_transform</code> issue","text":"<p>will convert <code>Float32</code>, <code>float32</code> to <code>float64</code> <pre><code>from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# will convert to float64\ntrain_eng[numerical_features] = scaler.fit_transform(train_eng[numerical_features])\n</code></pre></p>"},{"location":"ML/TensorFlow/TensorFlow/","title":"TensorFlow","text":""},{"location":"ML/TensorFlow/TensorFlow/#tensorflow-non-deterministic","title":"TensorFlow non-deterministic","text":"<ul> <li>Repeatability is not guaranteed in <code>TensorFlow</code></li> <li>https://www.twosigma.com/articles/a-workaround-for-non-determinism-in-tensorflow</li> </ul>"},{"location":"ML/XGBoost/Example/","title":"example","text":"<p>https://medium.com/mlearning-ai/forecast-with-xgboost-model-in-python-87d4b6cada98 <pre><code>import xgboost as xgb\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n\n#create example data\nX, y = make_classification(\n    n_samples=100,\n    n_informative=5,\n    n_classes=2,\n)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\n#training\nmodel = xgb.XGBRegressor(\n    n_estimators=100,\n    learning_rate=0.01,\n    subsample=0.75,\n    colsample_bytree=1,\n    max_depth=3,\n    gamma=0,\n)\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_train, y_train), (X_test, y_test)],\n    early_stopping_rounds=20,\n)\n\npred = model.predict(X_test)\n\n#display forecast\nplt.figure(figsize=(10, 5), dpi=80)\nx_ax = range(len(y_test))\nplt.plot(x_ax, y_test, label=\"test\")\nplt.plot(x_ax, pred, label=\"predicted\")\nplt.title('XGB prediction')\nplt.legend()\nplt.show()\n\n#save model\nmodel.save_model('model_sklearn.json')\n\n#load model\nmodel2 = xgb.XGBRegressor()\nmodel2.load_model('model_sklearn.json')\n</code></pre></p>"},{"location":"ML/XGBoost/Issue/","title":"Issue","text":""},{"location":"ML/XGBoost/Issue/#regression-num_class","title":"regression <code>num_class</code>","text":"<p>For regression problem if set <code>num_class = 5</code>, will get the error <code>Check failed: info.labels.Size() == preds.Size() (2880 vs. 14400)</code>.</p>"},{"location":"ML/XGBoost/Learn/","title":"Learn","text":""},{"location":"ML/XGBoost/Learn/#xgb-and-lgb","title":"xgb and lgb","text":"<ul> <li>basic: https://medium.com/@geokam/time-series-forecasting-with-xgboost-and-lightgbm-predicting-energy-consumption-460b675a9cee</li> <li>lag feature: https://medium.com/@geokam/time-series-forecasting-with-xgboost-and-lightgbm-predicting-energy-consumption-with-lag-features-dbf69970a90f</li> </ul>"},{"location":"ML/XGBoost/LossProfile/","title":"xgboost loss profiles (Train &amp; Valid)","text":""},{"location":"ML/XGBoost/LossProfile/#use-xgboosts-python-api-xgboosttrain","title":"Use XGBoost's Python API (<code>xgboost.train</code>)","text":"<pre><code>import xgboost as xgb\nimport matplotlib.pyplot as plt\n\n# DMatrix datasets\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndvalid = xgb.DMatrix(X_valid, label=y_valid)\n\nparams = {\n    'objective': 'multi:softprob',\n    'eval_metric': 'mlogloss',\n    'num_class': num_classes,\n    'tree_method': 'hist',\n    # Add other params...\n}\n\nevals_result = {}\nmodel = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=1000,\n    evals=[(dtrain, 'train'), (dvalid, 'valid')],\n    early_stopping_rounds=10,\n    evals_result=evals_result,\n    verbose_eval=10\n)\n\ntrain_loss = evals_result['train']['mlogloss']\nvalid_loss = evals_result['valid']['mlogloss']\n\n# Plot Loss Curves\nplt.figure(figsize=(10, 6))\nplt.plot(train_loss, label='Train Loss')\nplt.plot(valid_loss, label='Valid Loss')\nplt.xlabel('Boosting Round')\nplt.ylabel('Log Loss')\nplt.title('XGBoost: Training vs Validation Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"ML/XGBoost/LossProfile/#use-xgbcv","title":"Use <code>xgb.cv</code>","text":"<pre><code>cv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=1000,\n    nfold=5,\n    metrics='mlogloss',\n    early_stopping_rounds=10,\n    verbose_eval=10\n)\n\n# Plot mean validation loss\nplt.plot(cv_results['test-mlogloss-mean'], label='CV Valid Loss')\nplt.plot(cv_results['train-mlogloss-mean'], label='CV Train Loss')\nplt.xlabel('Boosting Round')\nplt.ylabel('Log Loss')\nplt.title('XGBoost CV Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"ML/XGBoost/Memory/","title":"Memory","text":""},{"location":"ML/XGBoost/Memory/#xgboost-reduce-memory","title":"XGBoost reduce memory","text":"<p>https://xgboosting.com/xgboost-use-less-memory/ - set <code>tree_method = 'hist'</code> - reduce <code>max_bin</code>. default = 256 - reduce <code>max_depth</code>. default = 6 - increase <code>min_child_weight</code>. default = 1 - <code>colsample_bytree</code>, <code>colsample_bylevel</code>, and <code>colsample_bynode</code>. default = 1</p>"},{"location":"ML/XGBoost/Parameters/","title":"XGB Parameters","text":""},{"location":"ML/XGBoost/Parameters/#search-grid-parameters","title":"Search Grid Parameters","text":"<p><pre><code>XGB_Grid_Params = {\n    'max_depth': [3, 4, 5, 6, 8, 10],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'n_estimators': [100, 250, 500, 750, 1000],\n    'subsample': np.arange(0.5, 1.0, 0.1),\n    'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n    'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n    'reg_alpha': [0, 0.1, 0.3],\n    'reg_lambda': [1, 2, 4],\n}\n</code></pre> - max_depth: [3, 10], default=6. The maximum depth of each tree - learning_rate: [0.01, 0.3], default=0.3. Controls the contribution of each tree to the final prediction - n_estimators: [50, 1000], default=100. The number of boosting rounds (trees) to fit. Higher values improve model accuracy but increase the risk of overfitting - subsample: [0.5, 1], default=1. Proportion of training data to randomly sample for each tree. Used to prevent overfitting - colsample_bytree: [0.5, 1], default=1. Proportion of features to randomly sample for each tree - colsample_bylevel: [0.5, 1], default=1. Proportion of features to sample for each level of each tree</p> <p>Regularization: - both <code>reg_alpha</code> and <code>reg_lambda</code> are regularization parameters used to control overfitting by penalizing the complexity of the model - <code>reg_alpha</code>: [0, inf], defsult=0. L1 regularization on the weights. Push some weights to zero, effectively performing feature selection - <code>reg_lambda</code>: [0, inf], defsult=1. L2 regularization on the weights. Keep the weights small and evenly distributed across features, which prevents the model from fitting too closely to the training data</p>"},{"location":"ML/XGBoost/Parameters/#subsample","title":"subsample","text":"<ul> <li><code>subsample</code>: the fraction of the training samples (randomly selected rows) that will be used to train each tree</li> </ul>"},{"location":"ML/XGBoost/Parameters/#colsample_by","title":"colsample_by*","text":"<p>https://medium.com/analytics-vidhya/xgboost-colsample-by-hyperparameters-explained-6c0bac1bdc1 - <code>colsample_bytree</code>: the fraction of features (randomly selected columns) when new tree is created - <code>colsample_bylevel</code>: random subsample of columns when every new level is reached - <code>colsample_bynode</code>: random subsample of columns based on every split (left or right swerve)</p>"},{"location":"ML/XGBoost/Parameters/#grid-search","title":"grid search","text":"<p>GridSearchCV: <pre><code>from xgboost import XGBRegressor\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n\nmodel = XGBRegressor()\nparameters = {\n    'max_depth': [3, 4, 6, 8, 10],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n    'n_estimators': [100, 300, 500, 700, 900, 1000],\n    'colsample_bytree': [0.3, 0.5, 0.7]\n}\ncv_split = TimeSeriesSplit(n_splits=4, test_size=100) # Coefficient of Variation\n\ngrid_search = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters)\ngrid_search.fit(X_train, y_train)\n</code></pre></p> <p>Note that: - GridSearchCV: Iterates over all combinations of hyperparameters in the grid. Can be computationally expensive, especially for large parameter grids and complex models. \u00a0  - RandomizedSearchCV: For a more efficient approach, consider using RandomizedSearchCV, which samples a fixed number of hyperparameter combinations randomly. \u00a0  - Bayesian Optimization: More advanced techniques like Bayesian optimization can further optimize the search process by intelligently selecting the next hyperparameter combination to evaluate.</p> <p>RandomizedSearchCV: <pre><code>import numpy as np\nimport pandas as pd\nfrom scipy.stats import randint\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\n# Load the Iris dataset\ndata = load_iris()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the model\nmodel = RandomForestClassifier(random_state=42)\n\n# Define the hyperparameter search space\nparam_dist = {\n    'n_estimators': randint(10, 200),\n    'max_depth': randint(1, 20),\n    'min_samples_split': randint(2, 10),\n    'min_samples_leaf': randint(1, 10),\n    'bootstrap': [True, False]\n}\n\n# Create the RandomizedSearchCV instance\nrandom_search = RandomizedSearchCV(\n    estimator=model, \n    param_distributions=param_dist, \n    n_iter=100,    # number of samples\n    cv=3,          # cross-validation folds\n    verbose=2, \n    random_state=42, \n    n_jobs=-1,\n)\n\n# Fit the model\nrandom_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\nprint(f\"Best hyperparameters: {random_search.best_params_}\")\n\n# Get the best model and evaluate it on the test set\nbest_model = random_search.best_estimator_\ntest_score = best_model.score(X_test, y_test)\nprint(f\"Test score: {test_score}\")\n</code></pre></p>"},{"location":"ML/XGBoost/PredictionInterval/","title":"Prediction Interval","text":"<p>https://cienciadedatos.net/documentos/py42-forecasting-prediction-intervals-machine-learning.html</p>"},{"location":"ML/XGBoost/PredictionInterval/#bootstrapping","title":"Bootstrapping","text":"<p>Prediction intervals are estimated based on bootstrapped residuals. <pre><code>from skforecast.model_selection import backtesting_forecaster\nmetric, predictions = backtesting_forecaster(\n    forecaster = forecaster,\n    y                  = data['Demand'],\n    initial_train_size = len(data_train) + len(data_val),\n    fixed_train_size   = False,\n    steps              = 7,\n    refit              = False,\n    interval           = [10, 90],\n    n_boot             = 1000,\n    metric             = 'mean_squared_error',\n    verbose            = False,\n    show_progress      = True,\n)\n</code></pre></p>"},{"location":"ML/XGBoost/PredictionInterval/#quantile-regresion-models","title":"Quantile regresion models","text":""},{"location":"ML/XGBoost/XGBoost/","title":"XGBoost","text":"<p>Machine Learning book: The Elements of Statistical Learning https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=380</p>"},{"location":"ML/XGBoost/XGBoost/#how-it-works","title":"how it works","text":"<p>https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-HowItWorks.html</p> <p>XGBoost: forecast the residuals on different levels and add the forecasts together.</p> <p>Builds a sequence of weak decision trees (models with low individual accuracy) that progressively improve on the predictions of previous trees. Each tree focuses on correcting the errors made by its predecessors.</p>"},{"location":"ML/XGBoost/XGBoost/#strength","title":"strength","text":"<p>good for tabular data</p>"},{"location":"ML/XGBoost/XGBoost/#multioutput","title":"multioutput","text":"<p>https://xgboost.readthedocs.io/en/stable/tutorials/multioutput.html</p>"},{"location":"Optimizaton/Convex/","title":"Convex","text":""},{"location":"Optimizaton/Convex/#bilinear-problem","title":"bilinear problem","text":"<ul> <li>x * y</li> <li>e^x y + y</li> <li>sin(x  + y) - x^2 <pre><code>obj: x * y\nvar: x, y\n</code></pre></li> </ul>"},{"location":"Optimizaton/Convex/#mccormick-envelopes","title":"McCormick envelopes","text":"<p>The McCormick Envelope is a type of convex relaxation used for the optimization of bilinear non-linear programming (NLP) problems: https://optimization.cbe.cornell.edu/index.php?title=McCormick_envelopes</p>"},{"location":"Optimizaton/Convex/#convex-concave-procedure-ccp","title":"Convex-Concave Procedure (CCP)","text":"<p>If objective is not convex but can be expressed as <code>f_convex - g_convex</code>: - Use first order Taylor approximation to replace g_convex - So objective will become convex - Run multiple iterations to converge to local minimum - No guarantee for global minimum (maybe a big issue for some problems)</p>"},{"location":"Optimizaton/Note/","title":"Note","text":""},{"location":"Optimizaton/Note/#solver-apis","title":"Solver APIs","text":"<p>This is the most easy part of the work but it is also very important for the user experience. Many solvers have limited API functions or they just do not provide users the flexibility to use their products.</p> <p>Usually their documentation is not good either, so you need to create simple examples to test each of the functions to confirm your understanding. If there are some examples being included in the help system it would be really helpful to the users.</p>"},{"location":"Optimizaton/Note/#numerical-issues","title":"Numerical Issues","text":"<p>One important topic about optimization is numerical issues. You will not notice these problems everyday but definitely you will encounter them occasionally and you will feel the pain.</p> <p>If you think the numerical issues are only for the solver developers then you are not correct. As a user, it is also very important to have some knowledge in this area. When you create a model, you may add some unnecessary large bounds for some rows and columns. This will not only make the solver unstable but also increase the optimization time and even lead to wrong solutions. Knowledge in this area will also help you do the performance tuning.</p>"},{"location":"Optimizaton/Note/#shadow-price","title":"Shadow Price","text":"<p>The dual of a constraint, in terms of economics, is also called the shadow price. It is the change of the objective value with per unit change of the constraint RHS, i.e., the potential value introduced when the constraint is tightened or relaxed with per unit value. Shadow prices are an important part of the optimization solution. However, we are frequently confused by some special cases.</p> <p>If the problem has some redundant constraints, the dual solution is not unique; the optimization solver might randomly choose one of the dual solution.  Theoretically, all dual solutions are correct while in some cases we understand that there is only one dual solution that is suitable for our problem. The other dual solution is just the artificial one that is generated by our inappropriate problem formulation. If there are redundant constraints, we need to relax one of the constraint or simply remove one to avoid the non-unique dual issue. Note that you might need to relax the bound of one of the variables in the redundant constraint instead.</p> <p>In some other cases, we might also be confused by the dual solution of MIP problems. When there are some integer variables in the problem, we can no longer explain the dual in the same way as we explain the dual of LP problems. Integer variables have discrete values, so there might be jumps in the change of the dual of some constraints. Under this circumstance, either dual is correct; one is associated with the direction of decreasing the constraint RHS and the other is for the increasing of the RHS.</p>"},{"location":"Optimizaton/Solver/","title":"Solvers","text":""},{"location":"Optimizaton/Solver/#commercial-solvers","title":"Commercial solvers","text":""},{"location":"Optimizaton/Solver/#open-source-solvers","title":"Open source solvers","text":""},{"location":"Optimizaton/Blog/grbpy/","title":"Using Gurobi Python matrix API to reduce problem creation time","text":"<p>When doing optimizations using Python, many people choose the popular <code>pyomo</code> package to create optimization problems. However, pyomo is known for its slow performance and some other issues. That's why <code>gurobipy</code> becomes a more attractive alternative.</p> <p>We definitely know that <code>gurobipy</code> is much faster for creating problems and easier to interact directly with the gurobi solver. But do you know that there is also a Python matrix API in gurobipy? Here I will demonstrate some gurobipy matrix API features that will make your code run even faster and easier to maintain.</p>"},{"location":"Optimizaton/Blog/grbpy/#how-to-use-gurobipy","title":"How to use gurobipy","text":"<p>An optimization problem basically contains an objective, some variables and some constraints. So the task to create a problem is creating decision variables, setting variable coefficients to the objective, and adding constraints based on the variables.</p> <p>Here I will use a basic example to show the whole process. Assume we have a variable demand for one year with 5 minutes intervals. The demand should be provided by two generators (g1: integer outputs with price 2, g2: continuous outputs with price 4). If there is not enough generation to meet the demand, there is a penalty proportional to the demand.</p> <p>The problem creation time is about 8.8 seconds for the following provided example.</p> <pre><code># gurobi version: v12.0.0rc1\nimport time\nimport numpy as np\nimport scipy.sparse as sp\nimport gurobipy as gp\nfrom gurobipy import GRB\n\nnp.random.seed(42)\n\n# Create a model\nmodel = gp.Model('gurobi_test')\n\n# Number of periods (1 year with 5min intervals)\nn_period = 365 * 288\nperiods = list(range(n_period))\ngen1 = np.random.randint(0, 6, n_period)\ngen2 = np.random.uniform(0, 9, n_period)\ndemand = np.random.uniform(0, 10, n_period)\npenalty = 3.5 * demand\n\n# Record time before creating the problem\nt0 = time.time()\n\n# Create variables\nv_g1 = {\n    i: model.addVar(vtype=GRB.INTEGER, lb=0, ub=gen1[i])\n    for i in periods\n}\nv_g2 = {\n    i: model.addVar(vtype=GRB.CONTINUOUS, lb=0, ub=gen2[i])\n    for i in periods\n}\nv_on = {\n    i: model.addVar(vtype=GRB.BINARY)\n    for i in periods\n}\n\n# Set objective\nobj_expr = gp.quicksum(\n    2 * v_g1[i] + 4 * v_g2[i] + penalty[i] * v_on[i]\n    for i in periods\n)\nmodel.setObjective(obj_expr, GRB.MINIMIZE)\n\n# Add constraints\nr1 = {\n    i: model.addConstr(\n        v_g1[i] + v_g2[i] + demand[i] * v_on[i] == demand[i]\n    ) for i in periods\n}\n\n# Update model and print time for creating the problem\nmodel.update()\nprint(f'Time for creating problem: {time.time() - t0:.3f} seconds')\n\n# Write problem to file in LP format\nmodel.write('c:/test/gurobi_test.lp')\n\n# Optimize the model\nmodel.optimize()\n\n# Check if the optimization was successful\nif model.status == GRB.OPTIMAL:\n    model.write('c:/test/gurobi_test.sol')\n    print('Model is optimal.')\n    print(f'Objective value: {model.objVal}')\nelif model.status == GRB.INFEASIBLE:\n    print('Model is infeasible.')\nelif model.status == GRB.UNBOUNDED:\n    print('Model is unbounded.')\nelse:\n    print(f'Optimization ended with status {model.status}')\n</code></pre>"},{"location":"Optimizaton/Blog/grbpy/#how-to-use-gurobi-python-matrix-api","title":"How to use Gurobi Python matrix API","text":"<p>Gurobi matrix API has a function called <code>model.addMVar</code>, which has a <code>shape</code> parameter. We can use this function to add multiple variables with the same type. Then we can use matrix operations similar to the ones in <code>numpy</code> to add objective terms and constraints.</p> <p>Note that the bounds of the variables and the constraint right-hand sides all can be inputs in the numpy array format. The variable bounds can also be updated using a numpy array. The variable solutions can be extracted into a numpy array as well.</p> <p><pre><code>...\n# Create variables\nv_g1 = model.addMVar(n_period, vtype=GRB.INTEGER, lb=0, ub=gen1)\nv_g2 = model.addMVar(n_period, vtype=GRB.CONTINUOUS, lb=0, ub=gen2)\nv_on = model.addMVar(n_period, vtype=GRB.BINARY)\n\n# Set objective\nobj_expr = (2 * v_g1 + 4 * v_g2 + penalty * v_on).sum()\nmodel.setObjective(obj_expr, GRB.MINIMIZE)\n\n# Add constraints\nr1 =  model.addConstr(\n    v_g1 + v_g2 + demand * v_on == demand\n)\n...\n</code></pre> After using matrix API functions, now the problem creation time is about 1.6 seconds - that's about 5x to 6x faster. At the same time, the code is shorter and cleaner.</p> <p>If the coefficients of the MVar elements are different in the constraints, we can use an overloaded decorator <code>@</code> to do that: <pre><code># c[0]:  x[0] + y[0] &gt;= 10\n# c[1]: 2x[1] + y[1] &gt;= 11\nx = model.addMVar(2, name='x')\ny = model.addMVar(2, name='y')\nM = sp.diags([1, 2]) # a sparse diagnostic matrix\nc = model.addConstr(M@x + y &gt;= np.array([10, 11]), name='c')\n</code></pre></p> <p>If we need to add a constraint only using one element of the MVar, this can be easily done: <pre><code>x = model.addMVar(2, name='x')\ny = model.addVar(name='y')\nc = model.addConstr(x[0] + y &gt;= 99)\n</code></pre></p>"},{"location":"Optimizaton/Blog/grbpy/#example-of-2d-mvars","title":"Example of 2D MVars","text":"<p>Assume we have some constraints like: <pre><code>c[0]: x[0,0] + x[0,1] + x[0,2] &gt;= 11\nc[1]: x[1,0] + x[1,1] + x[1,2] &gt;= 11\n</code></pre></p> <p>To add these constraints to the optimization problem, this can be easily done with 2D MVars: <pre><code>x = model.addMVar((2,3), name='x')\nc = model.addConstr(x.sum(axis=1) &gt;= 11, name='c')\n\nmodel.setObjective(10 * x.sum(), GRB.MINIMIZE)\n</code></pre></p> <p>By default, <code>MVar.sum()</code> will add all elements on all directions. By setting <code>axis=1</code> it will add all elements in the row direction.</p> <p>If the objective coefficients are not a constant, we can add the objective like this: <pre><code>coeffs = np.array([1,2])\nmodel.setObjective(coeffs @ x.sum(axis=1), GRB.MINIMIZE)\n</code></pre></p> <p>If the objective coefficients for each elements in the MVar are not the same, we can still use a matrix operation to add the objective: <pre><code>coeffs = np.array([[1,2,3], [4,5,6]])\nmodel.setObjective((coeffs * x).sum(), GRB.MINIMIZE)\n</code></pre></p>"},{"location":"Optimizaton/Blog/grbpy/#example-of-shifted-mvars","title":"Example of shifted MVars","text":"<p>In many cases we need to add constraints related to the difference of variables between two consecutive time points. This can also be done nicely with MVars.</p> <pre><code># x[0] - x[-1] &gt;= 0  # x[-1] = 9 is the initial value of x\n# x[1] - x[0]  &gt;= 1\n# x[2] - x[1]  &gt;= 2\nS = sp.diags(np.ones(3 - 1), -1, shape=(3, 3), format='csr')\n# S = [[0, 0, 0], [1, 0, 0], [0, 1, 0]]\nx0 = np.array([9, 0, 0])\nrhs = np.array([0, 1, 2])\nc = model.addConstr(x - S@x - x0 &gt;= rhs)\n</code></pre>"},{"location":"Optimizaton/Blog/grbpy/#changing-constraint-coefficients","title":"Changing constraint coefficients","text":"<p>If we need to update some constraints, for example, changing some variable coefficients, we can use the method <code>model.chgCoeff</code>: <pre><code>model.chgCoeff(c[0], x[0], 10.0)\n</code></pre></p> <p>Compared with other language APIs, there is not a method called <code>model.chgCoeffs()</code> in the Python API. Therefore, we can only change one constraint coefficient at a time. Hopefully the method <code>model.chgCoeffs()</code> will be added in the future.</p>"},{"location":"Optimizaton/Blog/grbpy/#updating-objective","title":"Updating Objective","text":"<p>Think about creating a project using object-oriented programming. In this case, ideally we need to set the objective terms related to each object separately. However, there is not a Python API method that can be used to add the objective terms gradually.</p> <p>There are two workarounds: - get the objective using <code>getObjective()</code> then add additional terms and set the objective again using <code>setObjective()</code>, or - add objective terms from different objects together and then set the objective using <code>setObjective()</code></p> <p>Here is an example showing how to do it using the second option: <pre><code>obj_expr = 0\nobj_expr += x.sum()\nobj_expr += (coeffs * y).sum()\nmodel.setObjective(obj_expr, GRB.MINIMIZE)\n</code></pre></p> <p>Note that setting the variable and constraint names will increase the problem creation time. Thus it's best to do so only for debugging purpose - you can use a flag to enable or disable variable and constraint name setting.</p> <p>More details about the Gurobi Python matrix API can be found in the manual: https://docs.gurobi.com/projects/optimizer/en/current/index.html</p>"},{"location":"Optimizaton/Gurobi/Expression/","title":"Expression","text":""},{"location":"Optimizaton/Gurobi/Expression/#gurobipy-expression","title":"gurobipy expression","text":"<ul> <li>avoid the <code>+=</code> operator when building many possibly large linear expressions</li> <li>instead, use the <code>addTerms</code> method</li> </ul>"},{"location":"Optimizaton/Gurobi/Install/","title":"Install","text":""},{"location":"Optimizaton/Gurobi/Install/#install-gurobipy","title":"install gurobipy","text":"<pre><code>conda search -c https://conda.anaconda.org/gurobi gurobi\nconda install -c https://conda.anaconda.org/gurobi gurobi=12.0.1\n</code></pre>"},{"location":"Optimizaton/Gurobi/Install/#gurobi-lp-format-using-notepad-regexp","title":"gurobi lp format using notepad++ regexp","text":"<p>requirements: - add <code>\\r\\n</code> after <code>:</code> - add <code>\\r\\n</code> before <code>+,-,&gt;=,&lt;=,=</code> - ignore <code>+,-</code> etc.</p> <p>solution: - find: <code>(:\\s)|(?&lt;!\\s{3})(\\s[+\\-&lt;&gt;=]=?)</code> - repl: <code>(?1\\1\\r\\n   )(?2\\r\\n  \\2)</code></p>"},{"location":"Optimizaton/Gurobi/Learn/","title":"Learn","text":""},{"location":"Optimizaton/Gurobi/Learn/#gurobi-webinars","title":"gurobi webinars","text":"<ul> <li>Non-Convex Quadratic Optimization: https://www.youtube.com/watch?v=0g5cMvOV7KY</li> <li>Models with Products of Binary Variables: https://www.gurobi.com/resource/models-with-products-of-binary-variables/</li> <li>parameter tunning tool: https://www.gurobi.com/documentation/9.1/refman/parameter_tuning_tool.html</li> </ul>"},{"location":"Optimizaton/Gurobi/MatrixAPI/","title":"Matrix API","text":""},{"location":"Optimizaton/Gurobi/MatrixAPI/#gurobi-matrix-example","title":"gurobi matrix example","text":"<p>https://docs.gurobi.com/projects/examples/en/current/examples/python/matrix1.html</p>"},{"location":"Optimizaton/Gurobi/MatrixAPI/#brief-example","title":"brief example","text":"<pre><code>import time\nimport numpy as np\nimport scipy.sparse as sp\nimport gurobipy as gp\nfrom gurobipy import GRB\n\nnp.random.seed(42)\n\ntry:\n    # Create a model\n    model = gp.Model('gurobi_test')\n\n    # constraints using 2d vars\n    m = 2\n    n = 3\n    # cv[0]: v[0,0] + v[0,1] + v[0,2] &gt;= 11\n    # cv[1]: v[1,0] + v[1,1] + v[1,2] &gt;= 11\n    V = model.addMVar((m, n), name='v')\n    M = np.full(n, 1)\n    H = np.full(m, 11)\n    cv = model.addConstr(V@M &gt;= H, name='cv')\n\n    # objective \n    N = np.array([[1,2,3], [4,5,6]])\n    # N = np.random.rand(m, n)\n    obj_expr += (N*V).sum()\n    # obj_expr += gp.quicksum(N[i, j] * V[i, j] for i in range(m) for j in range(n)) # very slow\n\n    # other constraints using mvar\n    A = sp.diags([1,2,3]) # can also be a numpy 2d array\n    X = model.addMVar(3)\n    C = np.array([1,2,3])\n    model.addConstr(A@X &gt;= C)\n\n    # constraints with shifted vars\n    # x[1] - x[0] &gt;= 1 # -&gt; x[1] &gt;= C[1] + x[0]\n    # x[2] - x[1] &gt;= 2\n    S = sp.diags(np.ones(n - 1), -1, shape=(n, n), format='csr')\n    cs = model.addConstr(X - S@X &gt;= C)\n\n    # m = 1024; n = 8192; d = 0.2\n    # B = sp.random(m, n, d, format='csr')\n    # b = np.random.rand(m)\n    # C = np.random.rand(n)\n    # t0 = time.time()\n    # X = model.addMVar(n)\n    # model.addConstr(B@X &gt;= b) # a little slower\n    # # model.addMConstr(B, X, '&gt;', b)\n    # print(f'time: {time.time() - t0:.3f}')\n\n    Y = model.addMVar(3, name='y')\n    D = np.array([3,4,6])\n    model.addConstr(A@Y &gt;= D)\n\n    # # Create variables\n    # x = model.addVar(vtype=GRB.CONTINUOUS, name='x', lb=0, ub=1)\n    # y = model.addVar(vtype=GRB.BINARY, name='y')\n\n    # Set objective function (minimize cx + dxy)\n    model.setObjective(C@X + X@sp.diags(D)@Y, GRB.MINIMIZE)\n\n    # # Add constraint: x + 2y &gt;= 1\n    # model.addConstr(x + 2 * y &gt;= 1, 'c0')\n\n    X.setAttr('VarName', [f'x[{i}]' for i in range(3)])\n    model.write('c:/test/gurobi_test.lp')\n\n    # Optimize the model\n    model.optimize()\n\n    # Check if the optimization was successful\n    if model.status == GRB.OPTIMAL:\n        model.write('c:/test/gurobi_test.sol')\n        print('Model is optimal.')\n        print(f'Objvalue: {model.objVal}')\n        print(f'Solution: x = {X.x}, y = {Y.x}')\n    elif model.status == GRB.INFEASIBLE:\n        print('Model is infeasible.')\n    elif model.status == GRB.UNBOUNDED:\n        print('Model is unbounded.')\n    else:\n        print(f'Optimization ended with status {model.status}')\n\nexcept gp.GurobiError as e:\n    print(f'Gurobi error: {e}')\nexcept Exception as e:\n    print(f'An unexpected error occurred: {e}')\n</code></pre>"},{"location":"Optimizaton/Gurobi/Modify/","title":"Modify","text":""},{"location":"Optimizaton/Gurobi/Modify/#modify-a-model","title":"modify a model","text":"<p>https://docs.gurobi.com/projects/examples/en/current/overview/modification.html</p>"},{"location":"Optimizaton/Gurobi/Parameter/","title":"Parameter","text":"<p>https://www.gurobi.com/documentation/9.5/refman/parameters.html</p>"},{"location":"Optimizaton/Gurobi/Parameter/#mipgap","title":"MIPGap","text":"<p>Relative MIP optimality gap, default 1e-4</p>"},{"location":"Optimizaton/Gurobi/Performance/","title":"Performance","text":""},{"location":"Optimizaton/Gurobi/Performance/#gurobi-performance","title":"gurobi performance","text":"<p>https://support.gurobi.com/hc/en-us/articles/360013420111-How-do-I-improve-the-time-to-build-my-model - try using <code>Model.addVars()</code> to create a sparse tupledict of variables, then use the select(), sum(), and prod() methods to iterate over only the matching variables - it is more efficient to build a linear or quadratic expression by modifying an existing expression rather than repeatedly creating new expressions - the <code>Model.addLConstr()</code> method is a slightly faster alternative to the more general <code>Model.addConstr()</code>. It can be up to 50% faster, particularly for very sparse constraints - the Python Matrix API can be significantly faster than the traditional <code>Model.addConstr()</code> process</p>"},{"location":"Optimizaton/Gurobi/Performance/#gurobi-python-matrix","title":"Gurobi python matrix:","text":"<ul> <li>https://cdn.gurobi.com/wp-content/uploads/gurobipy_matrixfriendly_webinar-slides.pdf</li> <li>python matrix api: https://support.gurobi.com/hc/en-us/sections/360009927292-Python-Matrix-API</li> </ul>"},{"location":"Optimizaton/Gurobi/PythonAPI/","title":"Python API","text":""},{"location":"Optimizaton/Gurobi/PythonAPI/#python-api-overview","title":"python api overview","text":"<p>https://docs.gurobi.com/projects/optimizer/en/current/reference/python/overview.html - Most modifications to an existing model are done through the attribute interface (e.g., changes to variable bounds, constraint right-hand sides, etc.). - The main exceptions are modifications to the constraint matrix (Model.chgCoeff) and to the objective function (build an expression and call Model.setObjective).</p>"},{"location":"Optimizaton/Gurobi/Solution/","title":"Solution","text":""},{"location":"Optimizaton/Gurobi/Solution/#get-solution","title":"get solution","text":"<pre><code># Print the values of all variables\nfor v in model.getVars():\n    print(f'{v.VarName} = {v.X}')\n\n# Retrieve the values of multiple variables at once\nvars = model.getVars()\nvalues = model.getAttr('X', vars)\nnames = model.getAttr('VarName', vars)\n\nfor name, val in zip(names, values):\n    print(f'{name:&lt;10} {val}')\n</code></pre>"},{"location":"Optimizaton/Gurobi/Solution/#example","title":"example","text":"<pre><code>def solve(self):\n    self.model.optimize()\n\n    # Check if the optimization was successful\n    if self.model.status == GRB.OPTIMAL:\n        print('Model is optimal. Objective value: {self.model.objVal}')\n    elif self.model.status == GRB.INFEASIBLE:\n        print('Model is infeasible.')\n    elif self.model.status == GRB.UNBOUNDED:\n        print('Model is unbounded.')\n    else:\n        print(f'Optimization ended with status {self.model.status}')\n\ndef get_solution(self):\n    # Retrieve the values of multiple variables at once\n    vars = self.model.getVars()\n    names = self.model.getAttr('VarName', vars)\n    values = self.model.getAttr('X', vars)\n    solution = pd.DataFrame({'col': names, 'val': values})\n    return solution\n</code></pre>"},{"location":"Optimizaton/Gurobi/Terminate/","title":"Terminate optimization","text":""},{"location":"Optimizaton/Gurobi/Terminate/#callback-code","title":"callback code","text":"<p>https://www.gurobi.com/documentation/current/refman/cb_codes.html</p>"},{"location":"Optimizaton/Gurobi/Terminate/#terminate-optimization-using-callback","title":"terminate optimization using callback","text":"<ul> <li>https://support.gurobi.com/hc/en-us/community/posts/360071928312-Stopping-the-program-if-best-objective-has-not-changed-after-a-while</li> <li>https://support.gurobi.com/hc/en-us/articles/360047717291-How-do-I-use-callbacks-to-terminate-the-solver <pre><code>import time\nimport gurobipy as gp\nfrom gurobipy import GRB\n\ndef cb(model, where):\n    if where == GRB.Callback.MIPNODE:\n        # Get model objective\n        obj = model.cbGet(GRB.Callback.MIPNODE_OBJBST)\n\n        # Has objective changed?\n        if abs(obj - model._cur_obj) &gt; 1e-8:\n            # If so, update incumbent and time\n            model._cur_obj = obj\n            model._time = time.time()\n\n    # Terminate if objective has not improved in 20s\n    if time.time() - model._time &gt; 20:\n        model.terminate()\n\nm = gp.Model()\n</code></pre></li> </ul>"},{"location":"Optimizaton/Gurobi/Terminate/#callback-using-pyomo","title":"callback using pyomo","text":"<p>https://stackoverflow.com/questions/58200552/pyomo-and-gurobi-does-pyomo-support-solver-callbacks-to-gurobi <pre><code>def my_callback(cb_m, cb_opt, cb_where):\n    if cb_where == GRB.Callback.MIPSOL:\n        cb_opt.cbGetSolution(vars = [cb_m.x, cb_m.y])\n        if cb_m.y.value &lt; (cb_m.x.value - 2) ** 2 - 1e-6:\n            print('adding cut')\n            cb_opt.cbLazy(_add_cut(m.x.value))\n\ndef my_callback(cb_m, cb_opt, cb_where):\n    if cb_where == GRB.Callback.MIP:\n        # General MIP callback\n        objbst = cb_opt.cbGet(GRB.Callback.MIP_OBJBST)\n        objbnd = cb_opt.cbGet(GRB.Callback.MIP_OBJBND)\n        if abs(objbst - objbnd) &lt; cb_m.gaprel * (1.0 + abs(objbst)):\n            print(f'Stop early - gap achieved: {cb_m.gaprel*100}%'))\n            # cb_opt.terminate() # AttributeError: 'GurobiPersistent' object has no attribute 'terminate'\n            cb_opt._solver_model.terminate()\n\nopt = pe.SolverFactory('gurobi_persistent')\nopt.set_instance(m)\nopt.set_callback(my_callback)\nsolver.solve(tee=True)\n</code></pre></p>"},{"location":"Optimizaton/Gurobi/VarCon/","title":"Variable and constraint","text":""},{"location":"Optimizaton/Gurobi/VarCon/#gurobipy-varcon-name","title":"gurobipy var/con name","text":"<p>if use default variable/constraint names, use default value <code>None</code> not empty string<code>''</code>.</p>"},{"location":"Optimizaton/Pyomo/Constraint/","title":"Constraint","text":"<p>https://pyomo.readthedocs.io/en/stable/library_reference/kernel/constraint.html <pre><code>classpyomo.core.kernel.constraint.constraint(expr=None, body=None, lb=None, ub=None, rhs=None)\n</code></pre></p>"},{"location":"Optimizaton/Pyomo/Constraint/#expression","title":"expression","text":"<pre><code>model.con1 = Constraint(expr=2.0 &lt;= model.x + model.y[0,2]) \nmodel.con2 = Constraint(expr=2*model.x + model.y[1,1] == 1.0)\n</code></pre>"},{"location":"Optimizaton/Pyomo/Constraint/#func","title":"func","text":"<pre><code>model.par_mat = Param(model.set_i, model.set_j, default=1) \ndef con_rule(model):\n    return summation(model.par_mat, model.x) &lt;= 1\nmodel.con = Constraint(rule=con_rule)\n</code></pre>"},{"location":"Optimizaton/Pyomo/Constraint/#using-decoration-func","title":"using decoration func","text":"<pre><code>def con_eq_rule(model, i):\n    if i == 0:\n        return Constraint.Skip\n    return model.x[i] - model.y[i] == model.val\nmodel.con_eq = Constraint(model.I, rule = con_eq_rule)\n\n@model.Constraint(model.set_i)\ndef con_eq(model, i):\n    if i == 0:\n        return Constraint.Skip\n    return model.x[i] - model.y[i] == model.val\n</code></pre>"},{"location":"Optimizaton/Pyomo/Constraint/#add-a-new-term-to-existing-con","title":"add a new term to existing con","text":"<p>https://groups.google.com/g/pyomo-forum/c/Abc3GuUSe88 <pre><code>import pyomo.environ as pyo\n\n# create model and var\nm = pyo.ConcreteModel()\nm.x = pyo.Var(within=pe.NonNegativeReals)\n\n# create a con\n@m.Constraint()\ndef con(m):\n    return 2 &lt;= m.x    \n# m.con = pyo.Constraint(expr=m.x &gt;= 2)\nm.con.pprint()\n\n# add a new term to con\nm.con._body += m.x**2\nm.con.pprint()\n</code></pre></p>"},{"location":"Optimizaton/Pyomo/Constraint/#create-an-empty-constraint-and-then-add-terms","title":"create an empty constraint and then add terms","text":"<pre><code>import pyomo.environ as pyo\n\n# create model and var\nm = pyo.ConcreteModel()\nm.x = pyo.Var(within=pe.NonNegativeReals)\n\n# create a con: must use tuple not expr\n@m.Constraint()\ndef con(m):\n    return (0, 0, 10)     \n# m.con = pyo.Constraint(expr=(0, 0, 10))\nm.con.pprint()\n\n# add a new term to con\nm.con._body += m.x**2\nm.con.pprint()\n</code></pre>"},{"location":"Optimizaton/Pyomo/Constraint/#add-constraint-name","title":"add constraint name","text":"<p>https://groups.google.com/g/pyomo-forum/c/5DgnivI1JRY</p> <p>Not good. <pre><code>m.c = pyo.Constraint(pyo.Any)\nm.c[name1] = m.x + m.y == 1\nm.c[name2] = \u2026\n\nsolver.solve(m, symbolic_solver_labels=True)\n</code></pre></p> <p>Good solution <pre><code>#setattr(m, 'con_1', pyo.Constraint(expr=(0, 0, 10)))\nm.add_component('con_1', pyo.Constraint(expr=(0, 0, 10)))\ncon = getattr(m, 'con_1')\ncon._body += m.x\ncon.pprint()\n</code></pre></p>"},{"location":"Optimizaton/Pyomo/Constraint/#change-constraint-lhsrhs","title":"change constraint lhs/rhs","text":"<p>https://stackoverflow.com/questions/74065872/pyomo-change-bounds-of-existing-contraints</p> <p>use of a mutable Param to leave a placeholder in the expression tree <pre><code>m.con1_rhs = Param(mutable=True, initialize=3)\nm.con1 = pyo.Constraint(m.x &lt;= model.con1_rhs)\n\n# then to change the RHS\nm.con1.rhs.set_value(3.1)\n</code></pre></p>"},{"location":"Optimizaton/Pyomo/Expression/","title":"Expression","text":""},{"location":"Optimizaton/Pyomo/Expression/#examples","title":"examples","text":"<pre><code>model.expr = Expression(expr=model.x + 2 * model.y[1])\n\ndef expr_rule(model, i):\n    if i == 1:\n        return Expression.Skip\n    return model.y[i]\nmodel.expr = Expression(model.I, rule=expr_rule)\n</code></pre>"},{"location":"Optimizaton/Pyomo/Function/","title":"Function","text":"<p>https://pyomo.readthedocs.io/en/stable/developer_reference/expressions/performance.html</p>"},{"location":"Optimizaton/Pyomo/Function/#summation","title":"summation","text":"<ul> <li>summation = sum_product</li> <li>dot_product = sum_product <pre><code>m.x = pyo.Param(m.I, within=pyo.Reals)\nm.y = pyo.Var(m.I, bounds=(0.0, 10.0), domain=pyo.NonNegativeReals, initialize=0.001)\npyo.summation(m.x, m.y, index=m.I)\n</code></pre></li> </ul>"},{"location":"Optimizaton/Pyomo/Learn/","title":"Learn","text":"<ul> <li>https://github.com/brentertainer/pyomo-tutorials</li> <li>https://github.com/WenYuZhi/PyomoTutorial/blob/master/Chapter2%20ToyCase.ipynb</li> </ul>"},{"location":"Optimizaton/Pyomo/Learn/#style-guide","title":"style guide","text":"<p>https://mobook.github.io/MO-book/notebooks/appendix/pyomo-style-guide.html</p>"},{"location":"Optimizaton/Pyomo/Learn/#example","title":"example","text":"<p>https://github.com/tum-ens/urbs/blob/master/teaching/01_Electricity_supply_of_an_island.ipynb</p>"},{"location":"Optimizaton/Pyomo/Model/","title":"Model","text":""},{"location":"Optimizaton/Pyomo/Model/#write-lp","title":"write lp","text":"<pre><code>instance.write('model.lp', io_options={'symbolic_solver_labels': True})\n</code></pre>"},{"location":"Optimizaton/Pyomo/Model/#warm-start","title":"warm start","text":"<ul> <li><code>tee</code> Print the progress of the solver during optimizing.</li> <li><code>logfile</code>: We can also write the log to a file.</li> <li><code>timelimit</code>: The solver will stop after 600 seconds and keep the best solution. <pre><code>instance = model.create()\ninstance.y[0] = 1\ninstance.y[1] = 0\n\nwith pyo.SolverFactory(\n    'cplex',\n    solver_io='python',\n    manage_env=True,\n    options=connection_params,\n) as cpx:\n    cpx.options['LogFile'] = 'model.log'\n    opt.options['TimeLimit'] = 600\n    cpx.options['MIGap'] = 0.000001\n    results = cpx.solve(instance, tee=True, warmstart=True)\n</code></pre></li> </ul>"},{"location":"Optimizaton/Pyomo/Objective/","title":"Objective","text":""},{"location":"Optimizaton/Pyomo/Objective/#using-summation","title":"using <code>summation</code>","text":"<pre><code>model.coeffs = Param(model.set_i, model.set_j, default=1) \ndef obj_rule(model):\n    return summation(model.coeffs, model.x)\nmodel.obj = Objective(rule=obj_rule, sense=maximize)\n</code></pre>"},{"location":"Optimizaton/Pyomo/Objective/#using-expression","title":"using expression","text":"<pre><code>def obj_rule(model):\n    expr = 0\n    for i in model.I:\n        for j in model.J:\n            expr += log(model.x[i,j]) + model.x[i,j]**2\n    return expr\n</code></pre>"},{"location":"Optimizaton/Pyomo/Objective/#using-index","title":"using index","text":"<pre><code>def obj_rule(model, i, j):\n    if i == j:\n        return Objective.Skip\n    else:\n      return 2 * model.x[i,j]\nmodel.obj = Objective(model.set_i, model.set_j, rule=obj_rule, sense=minimize)\n</code></pre>"},{"location":"Optimizaton/Pyomo/Objective/#update-objective","title":"update objective","text":"<pre><code>m.obj = Objective(expr=m.x)\nm.obj.set_value(expr=m.o.expr + m.e)\n\nm.obj_base = m.x1 + 2 * m.x2, \nm.obj_changing = m.x1 * m.x2, \nm.obj.set_value(expr=m.base_expr + m.changing_expr)\n</code></pre>"},{"location":"Optimizaton/Pyomo/Objective/#add-a-term-to-objective","title":"add a term to objective","text":"<pre><code>import pyomo.environ as pyo\n\n# create model and var\nm = pyo.ConcreteModel()\nm.x = pyo.Var(within=pyo.NonNegativeReals)\n\n# create obj\n@m.Objective()\ndef obj(m):\n    return 0  # must add a placeholder via expr. will not work `model.obj = pyo.Objective(sense=pyo.minimize)` without `expr=0`\nm.obj.pprint()\n\n# add a term\nm.obj += m.x**2\nm.obj.pprint()\n</code></pre>"},{"location":"Optimizaton/Pyomo/Param/","title":"Param","text":""},{"location":"Optimizaton/Pyomo/Param/#single-value","title":"single value","text":"<pre><code>model.p = Param(initialize=1, mutable=True)\nmodel.p = 2\n</code></pre>"},{"location":"Optimizaton/Pyomo/Param/#default","title":"default","text":"<pre><code>model.set_i = Set(initialize = [i for i in range(3)])  \nmodel.set_j = Set(initialize = [i for i in range(3)])\nmodel.par_mat = Param(model.I, model.J, default=1) # a 4x4 matrix with initial values of 1\n</code></pre>"},{"location":"Optimizaton/Pyomo/Param/#init-using-dict","title":"init using dict","text":"<pre><code># initialize value must be a dict or function\ndata = {(0,0): 1, (1,2): 33, (2,1): 20}\nmodel.par_mat = Param(model.set_i, model.set_j, initialize=data, default=0) # others are zeros\n</code></pre>"},{"location":"Optimizaton/Pyomo/Param/#init-using-func","title":"init using func","text":"<pre><code>def init(model, i, j):\n    return i*i if i == j else 0\nmodel.par_mat = Param(model.set_i, model.set_j, initialize=init)\n</code></pre>"},{"location":"Optimizaton/Pyomo/Param/#update-values-in-mutable-param","title":"update values in mutable Param","text":"<pre><code>ind = ['x', 'y', 'z']\nval = [100, 200, 300]\nm.I = Set(initiliaze=ind)\nm.p = Param(m.I, initialize=dict(zip(ind, val)), mutable=True)\n\n# update one value\nm.p['x'] = 0\n\n# update mutiple values\nm.p.store_values({'y': 2, 'z': 3})\n</code></pre>"},{"location":"Optimizaton/Pyomo/Performance/","title":"Performance","text":"<p>https://pascua.iit.comillas.edu/aramos/simio/transpa/s_GoodOptimizationModelingPracticesPyomo.pdf</p>"},{"location":"Optimizaton/Pyomo/Performance/#threads","title":"Threads","text":"<pre><code>solver.options['Threads'] = int(\n    (psutil.cpu_count(logical=True) + psutil.cpu_count(logical=False))/2\n)\n</code></pre>"},{"location":"Optimizaton/Pyomo/Performance/#sensitivity-analysis-with-persistent-solvers","title":"Sensitivity analysis with persistent solvers","text":"<p>Sequential resolution of similar problems in memory <pre><code>solver.remove_constraint(model.ConstraintName)\nmodel.del_component(model.SetName)\nsolver.add_constraint(model.ConstraintName)\n</code></pre></p>"},{"location":"Optimizaton/Pyomo/Performance/#distributed-computing","title":"Distributed computing","text":"<p>Create the problems and send them to be solved in parallel, and retrieve the solution once solved <pre><code>model.ConstraintName.deactivate()\nmodel.del_component(model.SetName)\nmodel.ConstraintName.activate()\n</code></pre></p>"},{"location":"Optimizaton/Pyomo/Performance/#problem-formulation-performance","title":"problem formulation performance","text":"<p>https://pianshen.com/ask/437410350304/</p> <p>slow <pre><code>def flows(model, et, t):\n    return pyo.quicksum(\n        model.in_flow[:, et, t], \n        linear=True,\n        start=pyo.quicksum(model.out_flow[:, et, t], linear=True)\n    ) == 0\n\nmodel.add_component(\n    'flows', \n    pyo.Constraint(model.energy_type, model.t, rule=flows)\n)\n</code></pre></p> <p>It turns out the problem was with the slices <code>[:, et, t]</code>: <pre><code>def flows(model, et, t):\n    vars = [model.in_flow[obj, et, t] for obj in model.objects_index]\n    vars.extend([model.out_flow[obj, et, t] for obj in model.objects_index])\n    return pyo.quicksum(vars) == 0\n</code></pre></p>"},{"location":"Optimizaton/Pyomo/RangeSet/","title":"RangeSet","text":"<p>inmutable</p>"},{"location":"Optimizaton/Pyomo/RangeSet/#basic","title":"Basic","text":"<pre><code>model = pyo.ConcreteModel()\nmodel.r1 = pyo.RangeSet(1, 3)      # {1,2,3}\nmodel.r2 = pyo.RangeSet(3)         # {1,2,3}\nmodel.r3 = pyo.RangeSet(1, 5, 2)   # {1,3,5}\nmodel.r4 = pyo.RangeSet(5, 1, -2)  # {1,3,5}\nmodel.r5 = pyo.RangeSet(0, 1, 0.5) # {0, .5, 1}\n</code></pre>"},{"location":"Optimizaton/Pyomo/RangeSet/#rangeset-with-conditional-filtering","title":"RangeSet with Conditional Filtering","text":"<pre><code>def filter_rule(model, i):\n    return i % 3 == 1\nmodel.r6 = pyo.RangeSet(1, 5, filter=filter_rule) # {1,4}\n</code></pre>"},{"location":"Optimizaton/Pyomo/Set/","title":"Sets","text":"<p>https://pascua.iit.comillas.edu/aramos/simio/transpa/s_GoodOptimizationModelingPracticesPyomo.pdf</p>"},{"location":"Optimizaton/Pyomo/Set/#basic","title":"Basic","text":"<pre><code>model.set_1 = Set(initialize=[i for i in range(6)], mutable=True)    # init with a list\nmodel.set_2 = Set(initialize=('red', 'green', 'blue'), doc='colors') # init with a tuple\n</code></pre>"},{"location":"Optimizaton/Pyomo/Set/#subsets","title":"Subsets","text":""},{"location":"Optimizaton/Pyomo/Set/#lag-and-lead-operators-firstlast-prevnext","title":"Lag and lead operators: first/last, prev/next","text":""},{"location":"Optimizaton/Pyomo/Set/#circular-indexes-prewnextw","title":"Circular indexes (prew/nextw)","text":""},{"location":"Optimizaton/Pyomo/Set/#union-intersection-of-sets","title":"Union, intersection of sets","text":""},{"location":"Optimizaton/Pyomo/Setup/","title":"Setup","text":""},{"location":"Optimizaton/Pyomo/Setup/#track-component-build-time","title":"track component build time","text":"<p>use <code>report_timing=True</code> in <code>create()</code> method to track the time it takes to build each component of the model.</p>"},{"location":"Optimizaton/Pyomo/Setup/#tips","title":"tips","text":"<p>https://groups.google.com/g/pyomo-forum/c/YTPByhZzboA - try to avoid innecessary loops - try to get all your \"if\" comparisons out of the loops - try to use pythonic way to iterate, instead of raw \"for\"s</p>"},{"location":"Optimizaton/Pyomo/Solve/","title":"Solve","text":""},{"location":"Optimizaton/Pyomo/Solve/#solve_1","title":"solve","text":"<pre><code>model.dual = Suffix(direction=Suffix.IMPORT) # dual vars\ngrb = SolverFactory('gurobi')\nsol = opt.solve(model)\n</code></pre>"},{"location":"Optimizaton/Pyomo/Solve/#solution","title":"solution","text":"<pre><code>ind = []\nvar = []\nval = []\nfor v in instance.component_objects(pyo.Var, active=True):\n    name = v.getname()\n    for index in v:\n        ind.append(index if v.dim() &gt; 0 else None)\n        var.append(name)\n        val.append(v[index].value)\nsol = {'ind': ind, 'var': var, 'val': val}\n</code></pre>"},{"location":"Optimizaton/Pyomo/Solve/#objective-and-dual","title":"objective and dual","text":"<pre><code>obj_val = value(model.obj)\nfor con in model.cons:\n    print(model.dual[model.cons[con]])\n</code></pre>"},{"location":"Optimizaton/Pyomo/Solve/#solution-status","title":"solution status","text":"<pre><code>solution.solver.termination_condition # optimal, feasible, infeasible etc.\nsolution.solver.termination_message # str\nsolution.solver.status  # stattus: ok, warning, error, aborted, or unknown \n</code></pre>"},{"location":"Optimizaton/Pyomo/Var/","title":"Var","text":""},{"location":"Optimizaton/Pyomo/Var/#dynamic-name","title":"dynamic name","text":"<pre><code>setattr(m, 'x', pyo.Var(domain=pyo.Reals))\nm.add_component('x', pyo.Var(domain=pyo.Reals))\n</code></pre>"},{"location":"Optimizaton/Pyomo/Var/#single-var","title":"single var","text":"<pre><code>m.x = Var(within=Reals, bounds=(0,6), initialize=1.5)\n</code></pre>"},{"location":"Optimizaton/Pyomo/Var/#multi-var","title":"multi-var","text":"<pre><code>#m.x[0], m.x[1]\nm.x = Var(range(2), initialize=(0,1), bounds=[(0,1), (2,3)])\n\ndef bounds(model, i, j):\n   return (0, None)\nm.set_i = Set(initialize = [i for i in range(3)])  \nm.set_j = Set(initialize = [i for i in range(3)])  \nm.y = Var(m.set_i, m.set_j, within=PositiveIntegers, bounds=bounds)\n</code></pre>"},{"location":"Optimizaton/Pyomo/Var/#init","title":"init","text":"<pre><code>def init(m, i, j):\n    return i + j\nm.x = Var(m.set_i, m.set_j, initialize=2.0)\nm.y = Var(m.set_i, m.set_j, initialize={(0,0):1.0, (2,2): 1.1})\nm.z = Var(m.set_i, m.set_j, initialize=init)\n</code></pre>"},{"location":"Optimizaton/Pyomo/Var/#fix-val","title":"fix val","text":"<pre><code>m.x.fix(1) # bounds will ignored by the solver\n</code></pre>"},{"location":"Optimizaton/Pyomo/Var/#lbub","title":"lb/ub","text":"<pre><code>m.x.lb = 0\nm.x.ub = None\n</code></pre>"},{"location":"Optimizaton/Pyomo/sos/","title":"SOS","text":"<p>A special ordered sets (SOS) of type N cannot have more than N of its members taking non-zero values, and those that do must be adjacent in the set.</p>"},{"location":"PowerBI/Basic/","title":"Basic","text":""},{"location":"PowerBI/Basic/#directquery-vs-import","title":"DirectQuery vs Import","text":"<p>DirectQuery will run queries directly in the DB server, and all of the normal Power Query transformation features can be used.</p> <p>Import will cache the data so the file size is larger, and has limitations on the dataset size and cannot switch back to the Directquery method.</p>"},{"location":"PowerBI/Basic/#rest-api-with-dynamic-parameter","title":"REST API with dynamic parameter","text":"<p>https://blog.crossjoin.co.uk/2016/08/23/web-contents-m-functions-and-dataset-refresh-errors-in-power-bi/</p> <p>\"The problem is that when a published dataset is refreshed, Power BI does some static analysis on the code to determine what the data sources for the dataset are and whether the supplied credentials are correct. Unfortunately in some cases, such as when the definition of a data source depends on the parameters from a custom M function, that static analysis fails and therefore the dataset does not refresh.\"</p> <p>Query parameter can be overwritten: <pre><code>Web.Contents(\n \"https://data.gov.uk/api/3/action/package_search?q=apples\", \n [Query=[q=\"oranges\"]]\n)\n</code></pre></p>"},{"location":"PowerBI/CalculatedTable/","title":"Calculated Table","text":"<p>duplicate column use DAX RELATED???</p> <p>Calculated tables are tables in the Data Model whose data source is a DAX expression that returns a table. </p>"},{"location":"PowerBI/CalculatedTable/#power-query-or-dax","title":"Power query or DAX","text":"<p>https://www.reddit.com/r/PowerBI/comments/axkydz/power_query_or_calculated_table/</p> <p>The rule of thumb is to push your Calculated Column/Table (basically anything that is not Measure) as close to your database/query/data source/data warehouse as possible, before loading them into your data model. </p> <p>In short, it's because the PowerBI's Vertipaq engine compresses the data (before loading into the model) better than Calculated Column/Table, reducing memory usage and improving performance.</p> <p>But others disagree.</p>"},{"location":"PowerBI/CalculatedTable/#power-query-vs-dax-calculated-columns","title":"Power query vs DAX calculated columns","text":"<p>https://www.sqlbi.com/articles/comparing-dax-calculated-columns-with-power-query-computed-columns/</p>"},{"location":"PowerBI/CalculatedTable/#issue-calculated-table-slow-down-incremental-refresh","title":"issue: calculated table slow down incremental refresh","text":"<p>Power Query works bottom up, not top down, if: - Query 1 to SQL Server - Query 2 to transform Query 1 to a good FACT table - Query 3 based on Query 2 to filter your FACT table. Both query 3 and query 2 will go all the way back to Query 1 independently. Power query doesn't process Query 2 and then use hold the results of that for Query 3. It will redo Query 2, and consequently Query 1.</p> <p>It's better to directly get the data from SQL server (not use calculated table). </p> <p>Also, some calculated table not updated after incremental refresh.</p>"},{"location":"PowerBI/CalculatedTable/#issue-calculated-columns-bsed-on-calculated-table-will-not-update-after-refresh","title":"issue: calculated columns bsed on calculated table will not update after refresh","text":"<p>Better to avoid calculated tables, either using DAX or Power Query</p>"},{"location":"PowerBI/CalculatedTable/#pivot-columns","title":"Pivot columns","text":"<pre><code>P = SUMMARIZE(T,\n    T[A],\n    T[B],\n    \"C\",CALCULATE(SUM(T[V]),T[I]=\"C\"),\n    \"D\",CALCULATE(SUM(T[V]),T[I]=\"D\")\n)\n</code></pre>"},{"location":"PowerBI/CalculatedTable/#group-tables-based-on-relationships","title":"group tables based on relationships","text":"<pre><code>CalculatedTable = ADDCOLUMNS (\n    SUMMARIZE (\n        Sales,\n        'Date'[Year],\n        Product[Size]\n    ),\n    \"Total Quantity\", CALCULATE ( SUM ( Sales[Quantity] ) )\n)\n</code></pre>"},{"location":"PowerBI/CalculatedTable/#new-table-basd-on-relatioships","title":"New table basd on relatioships","text":"<pre><code>T = ADDCOLUMNS (\n    SUMMARIZE (\n        T1,\n        T1[A],\n        T2[U],\n        T3[X],\n        T1[B],\n        T1[C]\n    ),\n    \"V\", CALCULATE ( SUM ( T1[V] ) )\n)\n</code></pre>"},{"location":"PowerBI/CalculatedTable/#constant-table","title":"Constant table","text":"<pre><code>NameQuarter = DATATABLE(\n    \"Name\",STRING,\n    \"Quarter\",INTEGER,\n    {\n        {\"A\", 1},\n        {\"B\", 2},\n        {\"C\", 3},\n        {\"D\", 4}\n    }\n)\n</code></pre>"},{"location":"PowerBI/DataType/","title":"Data type","text":""},{"location":"PowerBI/DataType/#types","title":"types","text":"<pre><code>type text, type date, type time, type number\nText.Type, Date.Type, Int64.Type, Number.Type  \n</code></pre>"},{"location":"PowerBI/DataType/#transform-columns","title":"transform columns","text":"<p>The third parameter <code>type datetime</code> is for null values. If not set the returned column type is Any (<code>ABC123</code>). <pre><code>Table.TransformColumns(Source, {\"MyDate\", each fn_UtcToLocal(_), type datetime})\n</code></pre></p>"},{"location":"PowerBI/Datetime/","title":"Datetime","text":""},{"location":"PowerBI/Datetime/#datetime-feature","title":"Datetime feature","text":"<p>If you enable the auto date/time feature, Power BI Desktop creates private Date tables in the background.  Power BI creates a Date table per column with either Date or DateTime data type regardless of whether we need to analyse them or not. </p> <p>This can cause serious performance and storage issues. To disable this feature, just reverse back the auto date/time feature settings.</p>"},{"location":"PowerBI/Debug/","title":"Debug","text":""},{"location":"PowerBI/Debug/#check-filters","title":"check filters","text":"<p>If a table results are not correct, output the table and  all the filters and slicers (include hideden ones) will be in the results sheet.</p>"},{"location":"PowerBI/Debug/#show-intermediate-measure-results-to-a-table","title":"show intermediate measure results to a table","text":"<p>If a measure is not correct you can change the mreasure to output the intermediate reults and output the table for viewing.</p>"},{"location":"PowerBI/Format/","title":"Format","text":""},{"location":"PowerBI/Format/#column-dynamic-format","title":"column dynamic format","text":"<p>Format column value to text: https://radacad.com/dynamically-change-the-format-of-values-in-power-bi</p>"},{"location":"PowerBI/Format/#table-conditional-format","title":"table conditional format","text":"<p>Visulizations &gt; Column (down arrow) &gt; Conditional formatting</p>"},{"location":"PowerBI/Format/#insert-blank-columns","title":"insert blank columns","text":"<p>https://wmfexcel.com/2020/05/31/insert-blank-columns-to-table-in-powerbi-desktop/</p>"},{"location":"PowerBI/IncrementalRefresh/","title":"Incremental refresh","text":""},{"location":"PowerBI/IncrementalRefresh/#time-zone","title":"time zone","text":"<p>Time zone used in Power BI incremental refresh is UTC.</p>"},{"location":"PowerBI/IncrementalRefresh/#steps","title":"steps","text":"<p>https://docs.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-overview</p> <ul> <li>Add two parameters <code>RangeStart</code> (data start date) and <code>RangeEnd</code></li> <li>Apply customer filter to the data column: right click the column -&gt;  Date/Time Filters -&gt; Custom Filter</li> <li>Define incremental refresh policy on the table: right click -&gt; incremental refresh</li> </ul>"},{"location":"PowerBI/IncrementalRefresh/#limitation","title":"limitation","text":"<ul> <li>cannot work for data with last_updated timestamp (upsert not supported)</li> <li>table references to the refresh table will not get all the data (not refreshed).\\   https://community.powerbi.com/t5/Desktop/incremental-refresh-on-table-by-reference/td-p/940721</li> </ul>"},{"location":"PowerBI/Json/","title":"Json","text":""},{"location":"PowerBI/Json/#example1","title":"example1","text":""},{"location":"PowerBI/Json/#file","title":"file","text":"<pre><code>{ \"name\":\"John\",\n  \"country\":[\n    {\"id\":\"US\",\"prob\":0.09},\n    {\"id\":\"AU\",\"prob\":0.06}\n  ]\n}\n</code></pre>"},{"location":"PowerBI/Json/#mlang","title":"mlang","text":"<pre><code>let\n    Source = Json.Document(File.Contents(\"C:\\tmp\\name.json\")),\n    #\"Converted to Table\" = Table.FromRecords({Source}),\n    #\"Expanded country\" = Table.ExpandListColumn(#\"Converted to Table\", \"country\"),\n    #\"Expanded country1\" = Table.ExpandRecordColumn(#\"Expanded country\", \"country\", {\"id\", \"prob\"}, {\"country.id\", \"country.prob\"}),\n    #\"Changed Type\" = Table.TransformColumnTypes(#\"Expanded country1\",{ {\"country.id\", Text.Type}, {\"country.prob\", Number.Type} })\nin\n    #\"Changed Type\"\n</code></pre>"},{"location":"PowerBI/Json/#table","title":"table","text":"name country.id country.prob John US 0.09 John AU 0.06"},{"location":"PowerBI/Learn/","title":"Learning","text":""},{"location":"PowerBI/Learn/#microsoft-power-bi-guided-learning","title":"Microsoft Power BI Guided Learning","text":"<p>https://docs.microsoft.com/en-us/power-bi/guided-learning/</p>"},{"location":"PowerBI/Model/","title":"Model","text":""},{"location":"PowerBI/Model/#n-n-relationship","title":"n-n relationship","text":"<p>https://docs.microsoft.com/en-us/power-bi/guidance/relationships-many-to-many</p>"},{"location":"PowerBI/PaginatedReport/","title":"Paginated Report","text":"<p>https://docs.microsoft.com/en-us/power-bi/paginated-reports/report-builder-shared-datasets</p>"},{"location":"PowerBI/PowerBI/","title":"PowerBI","text":""},{"location":"PowerBI/PowerBI/#powerbi","title":"PowerBI","text":""},{"location":"PowerBI/PowerBI/#table","title":"table","text":"<pre><code>Table.Combine({tbl1, tbl2})\nTable.PromoteHeaders(Source, [PromoteAllScalars=true])\nTable.RemoveColumns(Source,{\"Col1\", \"Col2\", \"Col3\"})\nTable.AddColumn(Source, \"ColNew\", each Date.AddDays([LASTUPDATE],3))\nTable.RenameColumns(Source,{{\"Col1\", \"Region\"}, {\"Col2\", \"Zone\"}})\nTable.TransformColumnTypes(Source, {{\"Col1\", type datetime}, {\"Col2\", date},\n  {\"Col3\", Int64.Type}, {\"Col4\", type number}, {\"Col5\", type text}})\nTable.SelectColumns(Source,{\"Col1\"})\n\nTable.Distinct(SelectedCols)\nTable.SplitColumn(Source, \"Col\", Splitter.SplitTextByDelimiter(\"_\",\n  QuoteStyle.Csv), {\"C1\", \"C2\", \"C3\"})\n</code></pre>"},{"location":"PowerBI/PowerBI/#load-csv","title":"load csv","text":"<pre><code>Source = Csv.Document(File.Contents(data_path &amp; \"file.csv\"),\n  [Delimiter=\",\", Columns=11, Encoding=1252, QuoteStyle=QuoteStyle.None]),\nPromotedHeaders = Table.PromoteHeaders(Source, [PromoteAllScalars=true]),\nChangedType = Table.TransformColumnTypes(PromotedHeaders,\n  {{\"C1\", type datetime}, {\"C2\", Int64.Type}}),\nChangedType2 = Table.TransformColumnTypes(ChangedType, {{\"Period\", type datetime}}, \"en-AU\")\n</code></pre>"},{"location":"PowerBI/PowerBI/#load-sql","title":"load sql","text":"<pre><code>src = MySQL.Database(\"host:port\", \"db\", [ReturnSingleDatabase=true,\n   Query=\"select c1,c2 from tb limit 3;\", CommandTimeout=#duration(0, 1, 40, 0)])\n\nsrc = MySQL.Database(\"host:port\", \"db\", [ReturnSingleDatabase=true]),\ndbt = src{[Schema=\"db\",Item=\"tb\"]}[Data]\n</code></pre>"},{"location":"PowerBI/PowerBI/#run-python","title":"run python","text":"<pre><code>PySource = Python.Execute(\"from pathlib import Path#(lf)import pandas as pd#(lf)dataset = pd.DataFrame([[str(Path.home())]], columns = [1])\"),\nhomeDir = Text.Trim(Lines.ToText(PySource{[Name=\"dataset\"]}[Value][1])),\n</code></pre>"},{"location":"PowerBI/PowerBI/#named-range","title":"named range","text":"<pre><code>FolderPath= Excel.CurrentWorkbook(){[Name=\"NamedRange\"]}[Content]{0}[Column1],\n</code></pre>"},{"location":"PowerBI/PowerBI/#power-query-errors-please-rebuild-this-data-combination","title":"Power Query Errors: Please Rebuild This Data Combination","text":"<p>https://www.excelguru.ca/blog/2015/03/11/power-query-errors-please-rebuild-this-data-combination/</p>"},{"location":"PowerBI/Blog/Summarize/","title":"Summarize","text":""},{"location":"PowerBI/Blog/Summarize/#how-it-works","title":"how it works","text":""},{"location":"PowerBI/Blog/Summarize/#virtual-table","title":"virtual table","text":""},{"location":"PowerBI/Blog/Summarize/#why-not-use-it-to-create-new-columns","title":"why not use it to create new columns","text":""},{"location":"PowerBI/Blog/Summarize/#example-with-issue","title":"example with issue","text":""},{"location":"PowerBI/DAX/CONCATENATEX/","title":"CONCATENATEX","text":""},{"location":"PowerBI/DAX/CONCATENATEX/#join-string","title":"join string","text":"<p>Shops table: Id, Shop, Country, Region Products table: ProductType, ProductQuantity, ShopId</p> <p>Create a Table to show the Total ProductQuantity by Country and Region, and all the product types. <pre><code>ConcatenatedProductType = CALCULATE(\n    CONCATENATEX(\n        VALUES(Products[ProductType]),\n        Products[ProductType], \", \"\n    ),\n    FILTER(\n        Products,\n        RELATED(Shops[Id]) = Products[ShopId]\n    )\n)\n</code></pre></p>"},{"location":"PowerBI/DAX/Calculate/","title":"Calculate","text":""},{"location":"PowerBI/DAX/Calculate/#column-with-fixed-value-not-affected-by-filters","title":"column with fixed value (not affected by filters)","text":"<pre><code>VAR __val = CALCULATE(\n    AVERAGE(ValTable[Value]), \n    ALL(AttributeTypes),\n    AttributeTypes[Attribute] = \"New\"\n)\n</code></pre>"},{"location":"PowerBI/DAX/Calculate/#multiple-conditions","title":"multiple conditions","text":"<pre><code>VAR __val = CALCULATE(\n    SUM(ValTable[Value]),\n    ALL(ValTable[Year]), \n    AttributeTypes[Attribute] in {\"New\", \"Used\"},\n    ValTable[Year] &lt;= SELECTEDVALUE(ValTable[Year])\n)\n</code></pre>"},{"location":"PowerBI/DAX/Calculate/#calculate-with-treatas","title":"calculate with treatas","text":"<pre><code>VAR __val = Calculate(\n    AVERAGE(Tab1[Quantity]),\n    TREATAS(\n        SUMMARIZE(Tab2, Tab2[RegionId], Tab2[CountryId], Tab2[SateId]),\n        Tab1[RegionId],\n        Tab1[CountryId],\n        Tab1[StateId]\n    )\n)\n</code></pre>"},{"location":"PowerBI/DAX/Calculate/#divid-with-sumx","title":"divid with sumx","text":"<pre><code>VAR __val = DIVIDE( \n    CALCULATE(\n        SUMX(\n            NATURALLEFTOUTERJOIN(Tab1, Tab2),  \n            Tab1[Quantity] * Tab2[Price]\n        ),\n        TREATAS(\n            SUMMARIZE(Tab1, Tab1[RegionId], Tab1[CountryId], Tab1[StateId]),\n            Tab2[RegionId],\n            Tab2[CountryId],\n            Tab2[StateId]\n        )  \n    ),\n    SUM( Tab1[Quantity] )\n)\n</code></pre>"},{"location":"PowerBI/DAX/Calculate/#calculated-column-in-records-with-filter","title":"calculated column in Records with filter","text":"<p>The Id in Records table has at least one row in Tbl table <pre><code>HasRecords = CALCULATE(\n    COUNTROWS(Tbl), \n    FILTER(\n        ALL(Tbl[RecordId], Tbl[Year]), \n        Tbl[RecordId]=Records[Id] &amp;&amp; Tbl[Year] &lt;= YEAR(TODAY())\n    )\n) &gt; 0\n</code></pre></p>"},{"location":"PowerBI/DAX/Contains/","title":"Contains","text":""},{"location":"PowerBI/DAX/Contains/#val-of-tab1-in-tab2","title":"val of tab1 in tab2","text":"<p>https://community.powerbi.com/t5/Desktop/DAX-query-to-compare-a-value-in-one-table-to-see-if-it-exists-in/m-p/57029#M23247</p> <p>If just use COUNTROWS(Tab2) by itself, you'd get the total number of rows for that table showing up for every row in Tab1. <pre><code>InTable2 = CALCULATE(\n    COUNTROWS(Tab2), \n    FILTER(\n        ALL(Tab2[Id], Tab2[Type]),\n        Tab2[Id]=Tab1[Id] &amp;&amp; Tab2[Type]=\"Dev\"\n    )\n) &gt; 0\n</code></pre></p>"},{"location":"PowerBI/DAX/DAX/","title":"DAX","text":"<p>https://docs.microsoft.com/en-us/dax/dax-overview</p> <p>definitions of measures, calculated columns, calculated tables, row-level security etc.</p>"},{"location":"PowerBI/DAX/DAX/#measures","title":"Measures","text":"<p>Measures are formula calculations depending on the context such as filter, slicer etc.</p>"},{"location":"PowerBI/DAX/DAX/#calculated-columns","title":"Calculated columns","text":"<p>A calculated column's values are calculated using DAX row by row.</p>"},{"location":"PowerBI/DAX/DAX/#calculated-tables","title":"Calculated tables","text":"<p>A calculated table is calculated using DAX based on other tables.</p>"},{"location":"PowerBI/DAX/DAX/#filter","title":"filter","text":"<pre><code>TableNew =\n    VAR __IDsWihFlag = FILTER(__Table,[Flag])\nRETURN\n    FILTER('Table',[id] IN SELECTCOLUMNS(__IDsWithFlag,\"id\",[id]))\n</code></pre>"},{"location":"PowerBI/DAX/DAX/#diff-between-values-of-two-dates","title":"diff between values of two dates","text":"<p>https://www.fourmoo.com/2018/11/06/dax-measure-getting-difference-between-2-values-in-a-table/</p> <p>https://www.sqlservercentral.com/articles/month-over-month-calculation-using-dax-in-power-bi</p> <p>Steps: - create two dates tables including the dates in that table - the first table has a relationship with the original table while the second is isolated - calculate the measure values for the first date as usual - calculate the measure values for the seonds date by ignore the slicer one - so the page has two single selcetion slicers  <pre><code>NewDateValue = CALCULATE (\n    Sum(MyTable[Value])\n)\nOldDateValue = CALCULATE (\n    SUM(MyTable[Value]),\n    ALL(Dates[Date]),\n    MyTable[Date] = SELECTEDVALUE(Dates2[Date])\n)\n</code></pre></p>"},{"location":"PowerBI/DAX/DAX/#change-values-dynamicly-based-on-slicer","title":"change values dynamicly based on slicer","text":"<p>https://community.powerbi.com/t5/Desktop/Change-calculated-table-dynamically-on-slicer-selection/m-p/1195458#M535454</p>"},{"location":"PowerBI/DAX/DAX/#distinct-column-values-to-table","title":"distinct column values to table","text":"<pre><code>NewTable = DISTINCT(OldTable[ColName])\nNewTable = DISTINCT(SELECTCOLUMNS(OldTable, \"NewColName\", [OldColName]))\n</code></pre>"},{"location":"PowerBI/DAX/DAX/#measure-conditional-on-another-column","title":"measure conditional on another column","text":"<p>If: <code>If(Dates[Date]=MaxDate, \"MaxDate\", FORMAT(Dates[Date], \"yyyy-MM-dd hh:mm:ss\"))</code> <pre><code>DateStr = \n    VAR MaxDate = MAX(Dates[Date])\n    VAR SecondMaxDate = MAXX(FILTER(Dates,[Date]&lt;MaxDate), Dates[Date])\nReturn     \n    SWITCH(Dates[Date], MaxDate, \"MaxDate\", SecondMaxDate, \"SecondMaxDate\", FORMAT(Dates[Date], \"yyyy-MM-dd hh:mm:ss\"))\n</code></pre></p>"},{"location":"PowerBI/DAX/DAX/#measure-by-filtering-column","title":"measure by filtering column","text":"<pre><code>NewMeasure = CALCULATE (\n    SUM(MyTable[Value]),\n    MyTable[ValueType] = \"Good\"\n)\n</code></pre>"},{"location":"PowerBI/DAX/DAX/#pivot-table","title":"pivot table","text":"<pre><code>PvtTable = SUMMARIZE(\n    MyTable,\n    MyTable[Date],\n    MyTable[Type],\n    \"CodeA\", CALCULATE(SUM(MyTable[Value]), MyTable[Code]=\"CodeA\"),\n    \"CodeB\", CALCULATE(SUM(MyTable[Value]), MyTable[Code]=\"CodeB\")\n) \n</code></pre>"},{"location":"PowerBI/DAX/DAX/#group-table","title":"group table","text":"<pre><code>GrpTable = ADDCOLUMNS(\n    SUMMARIZE (\n        MyTable,\n        MyTable[Date],\n        TypeTable[Type]\n    ),\n    \"Value\", CALCULATE(SUM(MyTable[Value]))\n)\n</code></pre>"},{"location":"PowerBI/DAX/Filter/","title":"Filter","text":""},{"location":"PowerBI/DAX/Filter/#select-rows-as-new-table","title":"select rows as new table","text":"<pre><code>FILTER (\n    Customer,\n    Customer[Continent] = \"Europe\" &amp;&amp; blabla\n)\n</code></pre>"},{"location":"PowerBI/DAX/Func/","title":"Function","text":"<p>https://vincent-tseng.medium.com/dax-hasonevalue-vs-hasonefilter-724fab64f9aa https://www.sumproduct.com/blog/article/power-pivot-principles/ppp-hasonefilter-vs-hasonevalue-vs-isfiltered</p>"},{"location":"PowerBI/DAX/Func/#hasonefiltercolumnname","title":"HASONEFILTER(columnName)","text":"<p>Returns TRUE when the number of directly filtered values on columnName is one. - one value - direct filter</p>"},{"location":"PowerBI/DAX/Func/#hasonevaluecolumnname","title":"HASONEVALUE(columnName)","text":"<p>Returns TRUE when the context of a specific column has been filtered down to one distinct value only. - one value - direct/cross-filter</p>"},{"location":"PowerBI/DAX/Func/#isfilteredcolumnname","title":"ISFILTERED(columnName)","text":"<p>Returns TRUE when the columnName parameter is filtered in the PivotTable (all simple, non-total rows and columns in a PivotTable are filtered by a given context). - can have any number of values - direct filter</p>"},{"location":"PowerBI/DAX/Func/#iscrossfilteredcolumnname","title":"ISCROSSFILTERED(columnName)","text":"<p>Returns TRUE when ColumnName or a column of TableName is being cross-filtered.  - can have any number of values - direct/cross filter</p>"},{"location":"PowerBI/DAX/Func/#summerize","title":"SUMMERIZE","text":"<p>Returns a summary table for the requested totals over a set of groups. Similar to SQL <code>Group By</code>.</p>"},{"location":"PowerBI/DAX/Func/#sumx","title":"SUMX","text":"<p>SUMX works on virtual table while SUM does not.  We can use SUMMERIZE and SUMX together to get sum of all groups  <pre><code>SUMX(SUMMERIZE(table, groupby_col_name, val_col_name, val_expr), [val_col_name])\n</code></pre></p> <p>Sales example <pre><code>VAR __tbl = SUMMARIZE(Sales, SalesCountries[CountryName], \"@val\", [Profit_Measure]) \nVAR __tot = SUMX(__tbl, [@val])\n</code></pre></p> <p>Demand weighted average price example. In this example, <code>SUMX</code> will first calculate the values record by record then sum them together. <pre><code>VAR __dwa_price = DIVIDE( \n    SUMX( \n        Sales,\n        Sales[Quantity] * Sales[Price]\n    ),\n    SUM( Sales[Quantity] )\n)\n</code></pre></p>"},{"location":"PowerBI/DAX/Func/#allexcept","title":"ALLEXCEPT","text":"<p>Removes all context filters in the table except filters that have been applied to the specified columns. <pre><code>AverageTotal = \n  VAR __val = CALCULATE([AverageSales], ALLEXCEPT(Sales, Dates[Year]))\nRETURN\n  __val\n</code></pre></p>"},{"location":"PowerBI/DAX/Func/#selectedvalue","title":"SELECTEDVALUE","text":"<p>returns the value when the context for columnName has been filtered down to one distinct value <code>only</code>.</p>"},{"location":"PowerBI/DAX/Func/#concatenatex","title":"CONCATENATEX","text":"<p>For multiple selected values in a slicer, we should check the count and possibly using <code>CONCATENATEX</code> <pre><code>VAR __quarter = IF(\n    COUNTROWS(VALUES(Sales[Quarter])) = 1,\n    SELECTEDVALUE(Sales[Quarter]),\n    CONCATENATEX(VALUES(Sales[Quarter]), Sales[Quarter], \",\", Sales[Quarter], ASC)\n)\n</code></pre></p>"},{"location":"PowerBI/DAX/Math/","title":"Math","text":"<p>https://dax.guide/quotient/</p>"},{"location":"PowerBI/DAX/Math/#mod","title":"MOD","text":"<p>Returns the remainder after a number is divided by a divisor. <code>MOD(5, 2) = 1</code></p>"},{"location":"PowerBI/DAX/Math/#quotient","title":"QUOTIENT","text":"<p>Returns the integer portion of a division. <code>QUOTIENT(5, 2) = 2</code></p>"},{"location":"PowerBI/DAX/RELATED/","title":"RELATED","text":"<p>https://learn.microsoft.com/en-us/dax/related-function-dax</p> <p>RELATED(Tbl[Col]) must be in the head (from) side of the relatioship.</p> <p>The RELATED function is used to retrieve values from a related table in a data model.  It allows you to navigate from one table to another based on a defined relationship between them.</p> <p>The RELATED function takes a column reference as an argument and returns the value of that column from the related table.  It is typically used within calculated columns or measures to perform calculations or filter data based on related tables.</p> <p>Here's an example to illustrate the usage of the RELATED function: - Consider two tables: \"Orders\" and \"Customers\". - The \"Orders\" table has a foreign key column \"CustomerID\" that relates to the \"CustomerID\" column in the \"Customers\" table.</p> <p>Suppose you want to calculate the total sales made by each customer in the \"Customers\" table.  You can create a calculated column in the \"Customers\" table using the RELATED function to retrieve the corresponding sales values from the \"Orders\" table: <pre><code>TotalSales =\n    CALCULATE (\n        SUM ( Orders[Sales] ),\n        RELATED ( Orders[CustomerID] )\n    )\n</code></pre> In this example, the RELATED function is used to retrieve the \"CustomerID\" from the \"Orders\" table related to each row in the \"Customers\" table.  The CALCULATE function is then used to sum the \"Sales\" values from the \"Orders\" table for the related customer.  The result is stored in the \"TotalSales\" calculated column in the \"Customers\" table.</p> <p>The RELATED function is a powerful tool for performing calculations across related tables and leveraging the data model's defined relationships in Power BI.</p>"},{"location":"PowerBI/DAX/Switch/","title":"Switch","text":"<pre><code>VAR __att = SWITCH(\n    SELECTEDVALUE(InputTypes[Input]),\n    \"Avg\", \n        AVERAGE(ValTable[Value]),\n    \"Max\", \n        MAX(ValTable[Value]),        \n    BLANK()\n)\n</code></pre>"},{"location":"PowerBI/DAX/TREATAS/","title":"DAX TREATAS","text":"<p>https://docs.microsoft.com/en-us/dax/treatas-function</p> <p><code>TREATAS(table expression, column[, other optional columns])</code></p> <p>Applies the result of a table expression as filters to columns from an unrelated table.</p>"},{"location":"PowerBI/DAX/TREATAS/#detailed-explanation","title":"detailed explanation","text":"<p>https://www.sqlbi.com/articles/understanding-data-lineage-in-dax/</p>"},{"location":"PowerBI/DAX/TREATAS/#special-case","title":"special case","text":"<p>https://community.powerbi.com/t5/Desktop/Unexpected-return-from-TREATAS-function/m-p/2068473#M771436</p> <p>It gets the mapped column values in the expression and then get the <code>rows</code> with mapped column values (only the ones inboth). After that, it check other selected columns in the selected rows.</p> <p>In this case, it first selects the rows with managers <code>John and Sam</code> in Sales, and then for each rows check the columns <code>Date</code> and <code>ID</code> - do the calculation if there is a match.  </p>"},{"location":"PowerBI/DAX/TREATAS/#use-case-join-two-fact-tables","title":"Use case (join two fact tables)","text":""},{"location":"PowerBI/DAX/TREATAS/#problem","title":"Problem","text":"<pre><code>Dim tables:\nDm1: Year\nDm2: Company\n\nFact tables:\nTb1: Year, Company, TaxRate\nTb2: Year, Company, Department, Profit\n\nDm1/2 linked to both Tb1/2 with 1-n relationships\nBut there are no direct relationships between Tb1/2.\n\nView Table:\nTbv: Year, Company, Department, Profit, TaxRate, Tax\n</code></pre>"},{"location":"PowerBI/DAX/TREATAS/#solution","title":"Solution","text":"<p>The <code>TaxRate</code> and <code>Tax</code> must be calculated using measure. And we have to use <code>TREATAS</code> to link Tb1 to Tb2. <pre><code>VAR __tax_rate = CALCULATE(\n    AVERAGE(Tb1[TaxRate]),\n    TREATAS(VALUES(Tb2[Year]), Tb1[Year]),\n    TREATAS(VALUES(Tb2[Company]), Tb1[Company])\n)\nVAR __tax = __tax_rate * SUM(Tb2[Profit])\n</code></pre></p>"},{"location":"PowerBI/DAX/TREATAS/#treatas-with-multiple-table-columns","title":"treatas with multiple table columns","text":"<pre><code>VAR __val = Calculate(\n    AVERAGE(CertificateFees[CertificateFee]),\n    TREATAS(\n        SUMMARIZE(Transactions, Transactions[CertificateType], Transactions[TransactionType], Transactions[SaleId]),\n        CertificateFees[CertificateType],\n        CertificateFees[TransactionType],\n        CertificateFees[SaleId]\n    )\n)\n</code></pre>"},{"location":"PowerBI/DAX/Table/","title":"Dax table","text":""},{"location":"PowerBI/DAX/Table/#datatable","title":"Datatable","text":"<pre><code>NameQuarter = DATATABLE(\n    \"Name\", STRING,\n    \"Quarter\", INTEGER, \n    {\n        {\"Q1\", 1}, {\"Q2\", 2},\n    }\n)\n</code></pre>"},{"location":"PowerBI/DAX/Table/#distinct-cols","title":"Distinct Cols","text":"<pre><code>NewTable = DISTINCT(SELECTCOLUMNS(\n    Filter(\n        OldTable,  \n        OldTable[CodeId] = 1\n        &amp;&amp; OldTable[Year] &gt;= YEAR(TODAY())-1 &amp;&amp; OldTable[Date] &gt;= TODAY() - 90\n    ),\n    \"NewCol1\", [OldCol1], \n    \"NewCol2\", [OldCol2]\n))\n</code></pre>"},{"location":"PowerBI/DAX/Total/","title":"Total","text":"<p>https://community.powerbi.com/t5/Quick-Measures-Gallery/Measure-Totals-The-Final-Word/m-p/547907</p> <p>https://community.powerbi.com/t5/DAX-Commands-and-Tips/Dealing-with-Measure-Totals/td-p/63376</p>"},{"location":"PowerBI/DAX/Total/#calculate-total-use-a-different-formula","title":"calculate total use a different formula","text":"<p>The total is the sum of the values in each row for a value of <code>AnotherTbl[AnotherCol]</code> - <code>HASONEVALUE</code> check if a specific column <code>has only one distinct value</code> within the current evaluation context - <code>HASONEFILTER</code> check if a specific column is being <code>filtered by a single value</code> within the current evaluation context <pre><code>MyCalculatedCol = \n    VAR __row_val = MyMeasure    \n    VAR __is_row = HASONEVALUE(MyTable[MyColInTbl])\nRETURN\n    if(__is_row, \n        __row_val, \n            VAR __tbl = SUMMARIZE(MyTable, AnotherTbl[AnotherCol], \"@val\", [MyMeasure]) \n            VAR __tot = SUMX(__tbl, [@val])\n        RETURN __tot\n    )\n</code></pre></p>"},{"location":"PowerBI/DAX/Var/","title":"Var","text":"<p>Variables are calculated within the scope in which they are written, and then the result of them is stored and used in the rest of the expression.</p>"},{"location":"PowerBI/DAX/Var/#var-vs-measure","title":"Var vs Measure","text":"<ul> <li>Var will calculate the value in advance and store it</li> <li>Measure can be considered a function and will be calculated in place with current context</li> </ul>"},{"location":"PowerBI/DAX/Var/#wrong-place-to-use-var","title":"Wrong place to use var","text":"<p>https://radacad.com/caution-when-using-variables-in-dax-and-power-bi</p> <p>Using var defined outside of the calculation context will lead to wrong results.</p> <p>Wrong <pre><code>Cost =     \n    VAR __val = [Amount] * [Price]\n    VAR __tbl = SUMMARIZE(tbl, tbl[yr], \"@val\", __val) //__val calculated for all years\n    VAR __tot = SUMX(__tbl, [@val]))\n    VAR __is_val = HASONEVALUE(tbl[yr])\n    VAR __ret = IF(__is_val, __val, __tot)\nRETURN\n    __ret\n</code></pre></p> <p>Correct <pre><code>Cost__ = [Amount] * [Price]\nCost = \n    VAR __is_val = HASONEVALUE(tbl[yr])\n    VAR __ret = IF(\n        __is_val,\n        [Cost__],\n        (\n            VAR __tbl = SUMMARIZE(tbl, tbl[yr], \"@val\", [Cost__])\n            VAR __tot = SUMX(__tbl, [@val])\n        RETURN \n            __tot\n        )\n    )     \nRETURN\n    __ret\n</code></pre></p>"},{"location":"PowerBI/Data/DataSource/","title":"Data Source","text":""},{"location":"PowerBI/Data/DataSource/#parquet","title":"parquet","text":"<p>https://blog.crossjoin.co.uk/2021/03/21/parquet-file-performance-in-power-bi-power-query/</p> <p>read parquet files in power bi</p>"},{"location":"PowerBI/Data/Dataflow/","title":"Dataflow","text":"<ul> <li>https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-create</li> <li>https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-develop-solutions</li> </ul>"},{"location":"PowerBI/Data/Dataflow/#example","title":"example","text":"<p>https://data-marc.com/2019/04/16/pimp-the-dataflows-connector-in-power-bi/</p> <p><pre><code>let\n    Source = PowerBI.Dataflows(null),\n    #\"My WorkspaceId\" = Source{[workspaceId=\"1123-4566-897\"]}[Data],\n    #\"My WdataflowId\" = #\"My WorkspaceId\"{[dataflowId=\"123-456\"]}[Data],\n    #\"My First Table\" = #\"My WdataflowId\"{[entity=\"my-table-name\"]}[Data],\n    #\"My Sorted Rows\" = Table.Sort(#\"My First Table\",{{\"SALESDATE\", Order.Descending}})\nin\n    #\"My Sorted Rows\"\n</code></pre> - <code>PowerBI.Dataflows(null)</code> returns all dataflows in all namespaces where the logged-in user has access to on tenant level. - <code>PowerBI.Dataflows()</code> similar to <code>PowerBI.Dataflows(null)</code>, its exact behavior might differ depending on the context and Power BI version - <code>PowerBI.Dataflows([])</code>, not recommended. why?</p>"},{"location":"PowerBI/Data/Dataflow/#directquery","title":"DirectQuery","text":"<ul> <li>https://community.fabric.microsoft.com/t5/Service/Report-not-updating-using-Data-Flow/m-p/2977125</li> <li>https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-directquery</li> </ul> <p>For connection to DataFlow, if using <code>import</code> instaed of <code>directquery</code> the report will not be updated automatically. - For visuals using DirectQuery tables the visual will query to get the latest data from the data source. - For visuals using imported tables the visual will only query data already imported to the dataset on the last data refresh.</p> <p>To use <code>directquery</code> we should do - subscription must be <code>Premium</code> - Data -&gt; Table -&gt; right click: enable <code>\"Include in report refresh</code> - Navigate to the Premium dataflow, and set <code>enhanced compute engine</code> to <code>On</code>, and refresh the dataflow</p>"},{"location":"PowerBI/Data/ReferenceTable/","title":"Reference Table","text":"<p>The term \"reference table\" in Power BI can have two different meanings depending on the context:</p> <p>1. Referencing a Query (Power Query):</p> <p>In Power Query, a reference table is not actually a separate table itself, but rather a way to reuse the steps of another query within your current query. Imagine you have two queries:</p> <ul> <li>Query1: Extracts data from a specific source and performs some transformations.</li> <li>Query2: Needs the transformed data from Query1, but also wants to do some additional calculations.</li> </ul> <p>Here, you can create a reference to Query1 within Query2. This means Query2 will execute the steps of Query1 first, then apply its own additional steps to the resulting data. This eliminates the need to duplicate the steps of Query1, making your queries more efficient and organized.</p> <p>2. Dimension Table in Data Model:</p> <p>In the Power BI data model, a reference table can refer to a dimension table that serves as a source of information for one or more fact tables. Dimension tables typically hold static data like customer names, product categories, or dates. Instead of storing the same information repeatedly in each fact table, you can create a reference to the dimension table to avoid data redundancy and improve performance.</p> <p>Therefore, understanding the context of which type of reference table you're encountering is crucial to interpret its function accurately.</p> <p>Here are some additional points to consider:</p> <ul> <li>Performance: Referencing queries can improve performance if used strategically, as it eliminates redundant data retrieval. However, overuse can also backfire if the referenced query has slow execution times.</li> <li>Maintainability: Using reference tables in the data model promotes better data organization and maintainability, as changes to the dimension table will automatically be reflected in all connected fact tables.</li> <li>Clarity: Always name your reference tables clearly to avoid confusion and ensure easy understanding of your data model.</li> </ul> <p>I hope this explanation clarifies the concept of reference tables in Power BI. If you have any further questions about specific scenarios or need more details, feel free to ask!</p>"},{"location":"PowerBI/IO/Parquet/","title":"Parquet","text":""},{"location":"PowerBI/IO/Parquet/#parquet","title":"parquet","text":""},{"location":"PowerBI/IO/Parquet/#get-parquet-files-from-web-api","title":"get parquet files from web api","text":"<p>https://community.fabric.microsoft.com/t5/Power-Query/Using-the-function-Web-Contents-with-more-than-2-options/td-p/1566974 <pre><code>let\n  // URL of the Parquet file\n  Source = Binary.Buffer(Web.Contents(\n    \"YOUR URL\", [\n      Headers = [#\"Authorization\"=\"Static Token Example\",#\"Content-Type\"=\"application/json\"]\n    ]\n  )),\n  // Load the Parquet file into a table\n  Table = Parquet.Document(Source)\nin\n  Table\n</code></pre></p>"},{"location":"PowerBI/MLang/CondOnMax/","title":"New column conditional on max value","text":""},{"location":"PowerBI/MLang/CondOnMax/#max-and-second-max","title":"max and second max","text":"<pre><code>let\n    Source = Sql.Database(p_SqlServer, p_SqlDbName),\n    Data = Source{[Schema=\"sch\",Item=\"tbl\"]}[Data],\n    #\"Selected Columns\" = Table.SelectColumns(Data,{\"Date\"}),\n    #\"Selected Rows\" = Table.SelectRows(#\"Selected Columns\", each [Date] &gt; RangeStart and [Date] &lt;= RangeEnd),\n    #\"Removed Duplicates\" = Table.Distinct(#\"Selected Rows\"),\n    #\"Local Date Source\" = Table.TransformColumns(#\"Removed Duplicates\", {\"Date\", each fn_UtcToLocal(Date.AddDays(_,1)), type datetime}),\n    #\"Added YearMonth\" = Table.AddColumn(#\"Local Date Source\", \"YearMonth\", each Date.Year([Date])*100+Date.Month([Date]), Int64.Type),\n    #\"Calculated Max Date\" = List.Max(#\"Added YearMonth\"[Date]),\n    #\"Calculated Second Max Date\" = List.Max(Table.SelectRows(#\"Added YearMonth\", each [Date] &lt; #\"Calculated Max Date\")[Date]),\n    #\"Added Date String\" = Table.AddColumn(#\"Added YearMonth\", \"DateStr\", each if [Date] = #\"Calculated Max Date\" then \"1st\" else if [Date] = #\"Calculated Second Max Date\" then \"2th\" else DateTime.ToText([Date], \"yyyy-MM-dd hh:mm:ss\"), type text)\nin\n    #\"Added Date String\"  \n</code></pre>"},{"location":"PowerBI/MLang/Distinct/","title":"Distinct","text":""},{"location":"PowerBI/MLang/Distinct/#distinct-col-as-new-table","title":"Distinct col as new table","text":"<p>Better <pre><code>Source = Table.SelectColumnd(Tbl, {\"Year\"}),\nDistinctRows = Table.Distinct(Source)\n</code></pre></p> <p>Also works <pre><code>Source = List.Distinct(Tbl[Year]),\nRows = List.Transform(Source, each {_}),\nResult = Table.FromRows(Rows, type table [Year = Int64.Type])\n</code></pre></p>"},{"location":"PowerBI/MLang/IncrementalRefresh/","title":"IncrementalRefresh","text":""},{"location":"PowerBI/MLang/IncrementalRefresh/#incremental-refresh","title":"Incremental refresh","text":""},{"location":"PowerBI/MLang/IncrementalRefresh/#web-api","title":"web api","text":"<pre><code>let\n    Source = Json. Document (Web.Contents(\"http://localhost:8000/api/data\",\n    [\n        Query=[apikey=\"key\", tablename=\"sales\", datestart=DateTime.ToText(RangeStart), dateend=DateTime.ToText(RangeEnd)]\n    ])),\n\n    #\"Converted to Table\" = Table.FromList(Source, Splitter. SplitByNothing(), null, null, ExtraValues. Error),\n    #\"Expanded Column1\" = Table.ExpandRecordColumn(#\"Converted to Table\", \"Column1\", {\"code\", \"quantity\", \"from_date\", \"to_date\"}, {\"code\", \"quantity\", \"from_date\", \"to_date\"}), #\"Changed Type\" = Table.TransformColumnTypes(#\"Expanded Column1\",{{\"code\", Int64. Type}, {\"quantity\", Int64. Type}, {\"from_date\", type datetime}, {\"to_date\", type datetime}}), #\"Filtered Rows\" = Table.SelectRows(#\"Changed Type\", each [from_date] &gt; RangeStart and [from_date] &lt;= RangeEnd)\nin\n    #\"Filtered Rows\"\n</code></pre>"},{"location":"PowerBI/MLang/Join/","title":"Join","text":"<p>https://learn.microsoft.com/en-us/powerquery-m/table-join</p>"},{"location":"PowerBI/MLang/Join/#inner-join","title":"Inner join","text":"<pre><code>Table.Join(\n    Tbl1, \"CustomerID\",\n    Tbl2, \"CustomerID\",\n    JoinKind.Inner\n)\n</code></pre>"},{"location":"PowerBI/MLang/List/","title":"List","text":""},{"location":"PowerBI/MLang/List/#listcontains-in-cluase-in-select-rows","title":"List.Contains, In cluase in select rows","text":"<p>Buffer will save the list values in memory <pre><code>Filters = Table.SelectRows(Source, each List.Contains(List.Buffer(Tbl[Val]), [QtYr]))\n</code></pre></p>"},{"location":"PowerBI/MLang/List/#listgenerator","title":"List.Generator","text":"<p>Get the list of quarters (4) not old than 5 years <pre><code>let\n    Source = List.Generate(\n        () =&gt; [n = 1, d = c_Today],\n        each [n] &lt;= 4 and Date.Year([d]) &gt; Date.Year(c_Today) - 5,\n        each [n = [n] + 1, d = Date.AddMonths([d], -3)],\n        each Text.End(Number.ToText(Date.Year([d])),2) &amp; \"Q\" &amp; Number.ToText(Date.QuarterOfYear([d]))\n    ),\n    #\"List To Column\" = Table.FromList(Source, null, {\"YQ\"})  # List must be text\nin\n    #\"List To Column\"\n</code></pre></p>"},{"location":"PowerBI/MLang/List/#list-to-table-change-to-rows","title":"List to table (change to rows)","text":"<pre><code>let\n    Source = List.Distinct(Tbl[Year]),\n    Rows = List.Transform(Source, each {_}),\n    Result = Table.FromRows(Rows, type table [Year = Int64.Type])\nin\n    Result\n</code></pre>"},{"location":"PowerBI/MLang/List/#add-list-as-new-table-column","title":"Add list as new table column","text":"<pre><code>let\n    Source = Table,\n    MyList = List.Generator(),\n    Columns = Table.ToColumns(Source) &amp; {MyList}, \n    Headers = Table.ColumnNames(Source) &amp; {\"Col_New\"},\n    MergedTable = Table.FromColumns(Columns, Headers)\nin\n    MergedTable\n</code></pre>"},{"location":"PowerBI/MLang/MLang/","title":"m language","text":""},{"location":"PowerBI/MLang/MLang/#text-function","title":"text function","text":"<p>https://docs.microsoft.com/en-us/powerquery-m/text-functions</p> <p>https://docs.microsoft.com/en-us/powerquery-m/power-query-m-function-reference</p>"},{"location":"PowerBI/MLang/MLang/#constant","title":"constant","text":"<pre><code>//c_LocalTimeAdjust\nlet\n    Source = #duration(0,10,0,0)\nin\n    Source\n</code></pre>"},{"location":"PowerBI/MLang/MLang/#function","title":"function","text":"<pre><code>//fn_UtcToLocal\nlet\n    Source = (InputDateTime as datetime)=&gt;\nlet\n    OutputDateTime = InputDateTime + c_LocalTimeAdjust\nin\n    OutputDateTime\nin\n    Source \n</code></pre>"},{"location":"PowerBI/MLang/MLang/#filter-rows","title":"filter rows","text":"<pre><code>Table.SelectRows(\n    MyTable, each [CustomerID] &gt; 2\n)\n</code></pre>"},{"location":"PowerBI/MLang/MLang/#get-one-col-and-sort","title":"get one col and sort","text":"<pre><code>let\n    Source = List.Distinct(T[A]),\n    SourceTable = Table.FromColumns({Source}, {\"A\"}),\n    ColumnATable = Table.Sort(SourceTable, {\"A\", Order.Descending})\nin\n    ColumnATable\n</code></pre>"},{"location":"PowerBI/MLang/MLang/#get-data-from-sql-incremental-refresh","title":"get data from sql (incremental refresh)","text":"<pre><code>let\n    Source = Sql.Database(SQLHostName, \"DatabaseName\"),\n    MyTable = Source{[Schema=\"SchemaName\",Item=\"TableName\"]}[Data],\n    #\"Select Columns\" = Table.SelectColumns(MyTable,{\"Date\"}),\n    #\"Filter Rows\" = Table.SelectRows(#\"Select Columns\", each [Date] &gt; RangeStart and [Date] &lt;= RangeEnd),\n    #\"Remove Duplicates\" = Table.Distinct(#\"Filter Rows\"),\n    #\"Convert Date To Local\" = Table.TransformColumns(#\"Remove Duplicates\", {\"Date\", each fn_UtcToLocal(_), type datetime})\nin\n    #\"Convert Date To Local\"\n</code></pre>"},{"location":"PowerBI/MLang/MLang/#group-and-unpivot-table","title":"group and unpivot table","text":"<pre><code>let\n    Source = Sql.Database(SQLHostName, \"DatabaseName\"),\n    MyTable = Source{[Schema=\"SchemaName\",Item=\"TableName\"]}[Data],\n    #\"Expand Linked Table\" = Table.ExpandRecordColumn(MyTable, \"SchemaName.LinkedTable\", {\"LinkedId\"}, {\"LinkedId\"}),\n    #\"Remove Columns\" = Table.RemoveColumns(#\"Expand Linked Table\",{\"ColA\", \"ColB\"}),        \n    #\"Group Rows\" = Table.Group(#\"Remove Columns\", {\"Date\", \"LinkedId\"}, {\n            {\"ValA\", each List.Sum([ValA]), type nullable number}, {\"ValB\", each List.Sum([ValB]), type nullable number}\n    }),\n    #\"Unpivot Columns\" = Table.UnpivotOtherColumns(#\"Group Rows\", {\"Date\", \"LinkedId\"}, \"Attribute\", \"Value\")\nin\n    #\"Unpivot Columns\"\n</code></pre>"},{"location":"PowerBI/MLang/Math/","title":"Math","text":""},{"location":"PowerBI/MLang/Math/#mod-and-integer-division","title":"mod and integer division","text":"<pre><code>#\"Added Year\" = Table.AddColumn(Source, \"Year\", each Number.IntegerDivide([YearMonth], 100), Int64.Type),\n#\"Added Month\" = Table.AddColumn(#\"Added Year\", \"Month\", each Number.Mod([YearMonth], 100), Int64.Type)\n</code></pre>"},{"location":"PowerBI/MLang/QueryGroup/","title":"Query group","text":""},{"location":"PowerBI/MLang/QueryGroup/#create-a-query-group","title":"create a query group","text":"<p>right click and new group</p>"},{"location":"PowerBI/MLang/SelectCols/","title":"Select Columns","text":""},{"location":"PowerBI/MLang/SelectCols/#select-unique-cols","title":"select unique cols","text":"<pre><code>let\n    Source = Sql.Database(p_SqlServer, p_SqlDbName),\n    Data = Source{[Schema=\"sch\",Item=\"tbl\"]}[Data],\n    #\"Selected Columns\" = Table.SelectColumns(Data,{\"Date\"}),\n    #\"Selected Rows\" = Table.SelectRows(#\"Selected Columns\", each [Date] &gt; RangeStart and [Date] &lt;= RangeEnd),\n    #\"Removed Duplicates\" = Table.Distinct(#\"Selected Rows\"),\n    #\"Local Date Source\" = Table.TransformColumns(#\"Removed Duplicates\", {\"Date\", each fn_UtcToLocal(Date.AddDays(_,1)), type datetime}),\n    #\"Added YearMonth\" = Table.AddColumn(#\"Local Date Source\", \"YearMonth\", each Date.Year([Date])*100+Date.Month([Date]), Int64.Type)\nin\n    #\"Added YearMonth\"\n</code></pre>"},{"location":"PowerBI/MLang/Table/","title":"MTable","text":""},{"location":"PowerBI/MLang/Table/#const-table","title":"const table","text":"<pre><code>let\n    Source = Table.FromRows(\n        {\n            {1, \"A\"},\n            {2, \"B\"}\n        },\n        type table [Id = Int64.Type, Code = text]\n    )\nin\n    Source\n</code></pre>"},{"location":"PowerBI/MLang/Table/#replace-value","title":"replace value","text":"<pre><code>#\"Replaced Value\" = Table.ReplaceValue(\n  #\"Source\",\n  each [ColA], each if Text.Contains([ColB],\"AA\") then \"AA\" else\n    if Text.Contains([ColB],\"BB\") then \"BB\" else [ColA],\n  Replacer.ReplaceValue,\n  {\"ColA\"}\n),\n</code></pre>"},{"location":"PowerBI/MLang/Table/#lookup-value","title":"lookup value","text":"<pre><code>#\"Lookup Column\" = Table.AddColumn(\n  #\"Source\",\n  \"NameId\",\n  each #\"NameTypes\"[Id]{List.PositionOf(#\"NameTypes\"[Name], [Name])}\n),\n</code></pre>"},{"location":"PowerBI/MLang/TableFunc/","title":"TableFunc","text":""},{"location":"PowerBI/MLang/TableFunc/#table-functions","title":"Table Functions","text":"<p>data types: INt64.Type, type text, type logical, type datetime</p>"},{"location":"PowerBI/MLang/TableFunc/#selectcolumns","title":"SelectColumns","text":"<p>Table.SelectColumns(Data, {\"Date\", \"Value\"})</p>"},{"location":"PowerBI/MLang/TableFunc/#addcolumn","title":"AddColumn","text":"<p>Table.AddColumn(Data, \"YearMonth\", each Date.Year([Date])*100+Date.Month([Date]), Int64.Type)</p> <p>Table.AddColumn(Data, \"FinYear\", each   if Date.Month([Date]) &gt;= 7 then Date.Year([Date]) + \"-\" + Text.End(Text.From(Date.Year([Date]) + 1), 2)   else (Date.Year([Date]) - 1) + \"-\" + Text.End(Text.From(Date.Year([Date])), 2), type text )</p>"},{"location":"PowerBI/MLang/TableFunc/#removecolumns","title":"RemoveColumns","text":"<p>Table.RemoveColumns(Data, {\"Id\", \"Period\", \"Version\"})</p>"},{"location":"PowerBI/MLang/TableFunc/#renamecolumns","title":"RenameColumns","text":"<p>Table.RenameColumns(Data, {{\"TypeId\",\"Id\"}, {\"CountryName\", \"Country\"}})</p>"},{"location":"PowerBI/MLang/TableFunc/#transformcolumns","title":"TransformColumns","text":"<p>Table.TransformColumns(Data, {\"Date\", each Date.AddDays(_,1), type datetime})</p>"},{"location":"PowerBI/MLang/TableFunc/#splitcolumn","title":"SplitColumn","text":"<p>Table.SplitColumn(Data, \"YearMonth\", Splitter.SplitTextByPositions({0, 4}, false), {\"Year\", \"Month\"})</p>"},{"location":"PowerBI/MLang/TableFunc/#expandrecordcolumn","title":"ExpandRecordColumn","text":"<p>Table.ExpandRecordColumn(Data, \"dbo.Sales\", {\"Name\", \"Quantity\"}, {\"Sales.Name\", \"Sales.Quantity\"})</p>"},{"location":"PowerBI/MLang/TableFunc/#unpivotothercolumns","title":"UnpivotOtherColumns","text":"<p>Table.UnpivotOtherColumns(Data, {\"Date\", \"Year\", \"Quarter\", \"Type\"}, \"Attribute\", \"Value\")</p>"},{"location":"PowerBI/MLang/TableFunc/#selectrows","title":"SelectRows","text":"<p>Table.SelectRows(Data, each [Date] &gt; RangeStart and [Date] &lt;= RangeEnd)</p>"},{"location":"PowerBI/MLang/TableFunc/#replacevalue","title":"ReplaceValue","text":"<p>Table.ReplaceValue(Data, each [Type], each if Text.Contains([Make],\"Toyota\") then \"Car\" else if Text.Contains([Make],\"Apple\") then \"Mobile\" else [Type], Replacer.ReplaceValue, {\"Type\"})</p>"},{"location":"PowerBI/MLang/TableFunc/#distinct","title":"Distinct","text":"<p>Table.Distinct(Data)</p>"},{"location":"PowerBI/MLang/TableFunc/#join","title":"Join","text":"<p>Table.Join(Data, \"DataId\", SalesTable, \"SaleId\")</p>"},{"location":"PowerBI/Model/CrossFilter/","title":"Cross-filtering","text":"<ul> <li>Cross-filtering is used when we want to apply filters from one visual or data point to affect the values or appearance of other visuals or data points in the report. </li> <li>Cross-filtering helps us establish relationships between different visualizations and allows us to analyze and explore data from multiple perspectives simultaneously.</li> </ul> <p>Scenarios to use Power BI cross-filtering: - Exploring related data:    When have multiple visuals in the report that are based on related data, we can use cross-filtering to interactively explore and analyze the data across those visuals.    For example, if you have a bar chart showing sales by product category and a line chart showing sales over time,    you can use cross-filtering to select a specific product category in the bar chart and see how it impacts the sales trend in the line chart.</p> <ul> <li> <p>Filtering hierarchies:    Cross-filtering is particularly useful when working with hierarchical data.    You can use it to filter data at different levels of a hierarchy and see how it affects other visuals.    For instance, if you have a tree map displaying sales by country, you can drill down to a specific country,    and the other visuals in your report will adjust accordingly to show data related to that country.</p> </li> <li> <p>Slicing and dicing data:    Cross-filtering enables you to slice and dice your data dynamically.    You can choose one or more values in a visual, and those selections will automatically filter the data displayed in other visuals.    This interactive filtering capability allows you to analyze different subsets of your data quickly and easily.</p> </li> <li> <p>Filtering across pages or report sections:    If you have a Power BI report with multiple pages or report sections, you can utilize cross-filtering to maintain the selected filters across those different areas.    This ensures consistent filtering and analysis experience as you navigate through the report.</p> </li> </ul>"},{"location":"PowerBI/Model/Relationship/","title":"Relationship","text":""},{"location":"PowerBI/Model/Relationship/#dotted-relationship-line","title":"dotted relationship line","text":"<p>inactive relationship</p>"},{"location":"PowerBI/Model/Relationship/#ambiguous-relationship","title":"ambiguous relationship","text":"<p>https://cloudbi.com.au/how-to-manage-ambiguous-relationship-in-power-bi/</p> <p>three tables: Tab-Customer-Country, Tab-Customer, Tab-Sales</p> <p>circular table: by creating a reference dim table (Tab-Sales-Country) and duplicate the slicer  so we have two synchronized clicers:  - one for customer (customer-country) - another for sales (sales country)</p>"},{"location":"PowerBI/Online/App/","title":"App","text":""},{"location":"PowerBI/Online/App/#publish-app","title":"publish app","text":"<p>Workspace -&gt; create app -&gt; setup</p>"},{"location":"PowerBI/Python/Learn/","title":"Learn","text":"<p>https://www.linkedin.com/pulse/learning-use-python-power-bi-experiment-infuse-data-solutions/</p> <p>https://community.fabric.microsoft.com/t5/Desktop/Using-Python-Instead-of-Measures/m-p/759238</p>"},{"location":"PowerBI/Task/Task/","title":"Task","text":""},{"location":"PowerBI/Task/Task/#slicer-latest-date","title":"Slicer latest date","text":"<p>Make sure slicer always has the default latest value, such as current year, quater, day etc.</p>"},{"location":"PowerBI/Task/Task/#n-n-relationship","title":"n-n relationship","text":"<p>Create a bridge table so replace the n-n relationship by n-1 and 1-n relationships.</p>"},{"location":"PowerBI/Task/Task/#total","title":"Total","text":"<p>Create measures to show employee salary and bonus with department and total summary (assume each department has the same bonus rate).</p> <p>Functions to determine if the measure is in employee, department or all: HASONEVALUE, HASONEFILTER</p> <p>Functions to get the summary of the bonus: SUMMERIZE, SUMX</p> <p>Caveat: do not use variables in place of measure. Understand the differences between variables (temp value) and measures (function)</p>"},{"location":"PowerBI/Tool/FindViewsUsedColumn/","title":"Find which views used the table column","text":""},{"location":"PowerBI/Tool/FindViewsUsedColumn/#layout","title":"Layout","text":"<p>https://github.com/Askrr/PBI_tableLookUp/tree/main</p> <p>After extract (unzip) the pbi file to a folder, the layout details are in this file: /PowerBIFileZip/Report/Layout - Search for the Table column name to find which view used it.</p>"},{"location":"PowerBI/View/Chart/","title":"Chart","text":""},{"location":"PowerBI/View/Chart/#combo-chart","title":"Combo chart","text":"<p>https://learn.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-combo-chart?tabs=powerbi-desktop</p> <p>put multiple charts with different types in the same figure</p>"},{"location":"PowerBI/View/Filter/","title":"Filter","text":""},{"location":"PowerBI/View/Filter/#tretas","title":"TRETAS","text":"<p>TRETAS WILL transfers a filter context from a table to another, simulating the behavior of a physical relationship defined in the data model. </p> <p>Filter in lookup_column will be applied to target_column <pre><code>[Filtered Measure] :=\nCALCULATE (\n    &lt;target_measure&gt;,\n    TREATAS (\n        VALUES ( &lt;lookup_column&gt; ),\n        &lt;target_column&gt; \n    )\n)\n\n[Total Amount] :=\nCALCULATE (\n    SUM ( Sale[Amount] ),\n    TREATAS (\n        SUMMARIZE ( 'Date', 'Date'[Year], 'Date'[Month] ),\n        Sale[Year],\n        Sale[Month]\n    )\n)\n</code></pre></p>"},{"location":"PowerBI/View/Gantt/","title":"Gantt","text":""},{"location":"PowerBI/View/Gantt/#features","title":"features","text":"<p>https://xviz.com/blogs/efficient-project-management-power-bi-latest-gantt-chart/</p>"},{"location":"PowerBI/View/HierarchyAxis/","title":"hierarchy axis","text":"<p>step 1: - select chart type - set axis data: year and quarter - set legend data: region - set value data: Sales step 2: - set x-axis as category - turn off concatenate labels: format pane -&gt; X axis -&gt; turn off concatenate labels - make sure sort order should be ascending, click on ellipsis icon (three dots in the figure up-right corner) -&gt; Sort ascending</p>"},{"location":"PowerBI/View/HierarchySlicer/","title":"Hierarchy Slicer","text":"<p>https://appsource.microsoft.com/en-us/product/power-bi-visuals/WA104380820?tab=Overview</p>"},{"location":"PowerBI/View/Matrix/","title":"Matrix","text":""},{"location":"PowerBI/View/Matrix/#example","title":"example","text":"<p>create a matrix with <code>rows</code> country and city, and <code>values</code> year, sale_quantity, price and revenue - collape the rows to country level, the values should show the sum, weighted average, and sum - if expanded we can see the city level values with cities as a dented list - the total should show the correct summary values as well</p>"},{"location":"PowerBI/View/Matrix/#turning-off-stepped-layout","title":"Turning off stepped layout","text":"<p>Turn off stepped layout on the row headers card, to put each row label in a separate column not the default stepped (indented) layout.</p>"},{"location":"PowerBI/View/Matrix/#subtotal","title":"subtotal","text":"<p>Will use the same measure to calculate value and total so sometimes the total can be incorrect. In this case, we need to use different measures for value and total calculation. <pre><code>Measure = \n    VAR __is_val = HASONEFILTER(Tb2[Col2])\n    VAR __tmp_val = CALCULATE(\n        SUM(Tb1[Value]), \n        ALL(Tb2),\n        ALL(Tb1[Colx]),        \n        Tb2[Col2] = \"CC\",\n        Tb1[Colx] &lt;= SELECTEDVALUE(Tb1[Colx])\n    )    \nRETURN    \n    IF(\n        __is_val,\n        SUMX(\n            Tb1,\n            SWITCH(\n                RELATED(Tb3[Col3]),\n                \"AA\", __val,\n                \"BB\", __val - __tmp_val,\n                BLANK()\n            )\n        ),\n        SUMX(\n            FILTER(\n                Tb1, \n                RELATED(Tb2[Col2]) in {\"AA\", \"CC\"}\n                    &amp;&amp; Tb1[Colx] &lt;= SELECTEDVALUE(Tb1[Colx])\n            ),\n            Tb1[Value]\n        )\n    )\n</code></pre></p> <p>customize subtotals and totals: https://community.powerbi.com/t5/Desktop/Different-DAX-calculations-for-different-row-hierarchies-in-a/m-p/2136701#M788896 <pre><code>Switch = \n    var _territoty=1\n    var _region=2\n    var _Total=3\nreturn \n    IF(\n        ISINSCOPE('Table'[Region]),\n        _region,\n        IF(\n            ISINSCOPE('Table'[Territory]), \n            _territoty, \n            _Total\n        )\n    )\n</code></pre></p> <p>Seems <code>ISINSCOPE</code> does not work. We need to use <code>HASONEFILTER(Tbl(Col))</code> to determine which row level the measure is.</p>"},{"location":"PowerBI/View/Matrix/#subtotal-to-sum-values-city-price-on-cities","title":"subtotal to sum values (city price) on cities","text":"<pre><code>AvgPrice = SUMX(\n    SUMMARIZE('Table', 'Table'[city], \"Average\", divide(sum('Table'[price]),SUM('Table'[quantity]))),\n    [Average]\n)\n</code></pre>"},{"location":"PowerBI/View/Matrix/#sumx-vs-switch-calculate","title":"SUMX vs SWITCH CALCULATE","text":"<p><code>SUMX</code> will calculate the value for all row levels. If use <code>SWITCH</code> and CALCULATE based on a specific row level, the subtotal will not show up. <pre><code>Measure = SUMX(\n    Tb1,\n    SWITCH(\n        RELATED(Tb2[Col1]),\n        \"AA\", Tb1[Value],\n        \"BB\", Tb1[Value],\n        BLANK()\n    )\n)\n\nMeasure = SWITCH(\n    SELECTEDVALUE(Tb2[Col1]),\n    \"AA\", CALCULATE(SUM(Tbl[Value])),\n    \"BB\", CALCULATE(SUM(Tbl[Value])),\n    BLANK()\n)\n</code></pre></p>"},{"location":"PowerBI/View/Matrix/#multiple-row-headers-in-matrix-visual","title":"Multiple row headers in Matrix visual","text":"<p>similar to pandas df with multiindex columns</p> <p>https://community.fabric.microsoft.com/t5/Desktop/Multiple-row-headers-in-Matrix-visual/td-p/372306</p>"},{"location":"PowerBI/View/Matrix/#conditional-formatting","title":"Conditional formatting","text":"<p>Right click the column value in Visualization -&gt; Conditional Formatting</p>"},{"location":"PowerBI/View/Measure/","title":"Measure","text":"<p>Note: - We can't reference the column as part of a measure without using one of the aggregation functions in DAX</p>"},{"location":"PowerBI/View/Measure/#show-one-of-measure-based-on-slicer","title":"show one of measure based on slicer","text":"<p>https://towardsdatascience.com/dynamic-filtering-in-power-bi-5a3e2d2c1856</p> <ul> <li>measure 1: measure1 = SUM('tbl'[amount])</li> <li>measure 2: measure2 = CALCULATE('tbl-measure'[measure1],DATESYTD(Dates[Date])), year to day</li> <li>create a slicer key-val table <code>tbl-keyval</code>: SelectedKey = MIN('tbl-keyval'[key])</li> <li>SelectedMeasure = SWITCH([SelectedKey], 1,'tbl-measure'[measure1], 2,'tbl-measure'[measure2])</li> </ul>"},{"location":"PowerBI/View/Measure/#use-slicer-selected-value","title":"use slicer selected value","text":"<p>Measure can use slicer selected value to calculate the measure values. <pre><code>Sales for Selected Category in 2024 = CALCULATE(\n    SUM(Sales[Sales Amount]),\n    SaleYears[Year] = 2024,\n    Sales[Category] = SELECTEDVALUE(Category[Category Name])\n)\n</code></pre></p> <p>or <pre><code>Sales for Selected Category in 2024 = CALCULATE(\n    SUM(Sales[Sales Amount]),\n    FILTER(\n        Sales,\n        SaleYears[Year] = 2024\n        &amp;&amp; Sales[Category] = SELECTEDVALUE(Category[Category Name])\n    )\n)\n</code></pre> The purpose of the FILTER function in the examples is to explicitly control which data is included in the calculation based on the slicer selection.   While context filters often work implicitly, using FILTER within measures offers these benefits: - Explicit Control: Ensures the calculation precisely follows the slicer selection, even in complex scenarios with multiple filters or relationships. - Handling Multiple Selections: FILTER, unlike SELECTEDVALUE, can manage scenarios where slicers allow multiple values to be chosen. - Conditional Logic: You can create more intricate logic within measures using FILTER alongside IF or other conditional functions. - Custom Calculations: FILTER enables calculations that might not be possible with context filters alone, such as filtering on a measure or calculated column.</p> <p>Here's a breakdown of how the FILTER function works within a measure: 1. Retrieves the selected value: The <code>SELECTEDVALUE</code> function extracts the value chosen in the slicer. 2. Applies the filter: The <code>FILTER</code> function creates a virtual table containing only rows from the specified table where the specified condition is met. In this case, the condition ensures the <code>Category</code> or <code>Country</code> column matches the selected value. 3. Calculates using the filtered data: The calculation (e.g., SUM or AVERAGEX) is performed on this filtered subset of data, ensuring alignment with the slicer selection.</p>"},{"location":"PowerBI/View/Slicer/","title":"Slicer","text":"<p>home table https://forum.enterprisedna.co/t/applying-two-slicers-from-the-same-field-on-table-visual/7454/10</p>"},{"location":"PowerBI/View/Slicer/#order","title":"order","text":"<p>click 3-dots and sort and change the sort order</p>"},{"location":"PowerBI/View/Slicer/#use-fact-or-dim-col-for-slicer","title":"use fact or dim col for slicer","text":"<p>Generally we should use col in dim table for slicer - dim col can control another dim table so will only show values related to the slicer - will not affect another slicer - avoid deadlock</p>"},{"location":"PowerBI/View/Slicer/#restrict-slicer-values-to-filtered-values-from-another-slicer","title":"restrict slicer values to filtered values from another slicer","text":"<p>https://www.sqlbi.com/articles/syncing-slicers-in-power-bi/</p> <p>Filter slicers without using bidirectional filters in Power BI. - each customer can only have a few colors - the color slicer should only show the colors for that customer - use a filter to exclude blank values - or use <code>visual interactions</code></p>"},{"location":"PowerBI/View/Slicer/#remove-values-not-in-fact-table-from-slicer","title":"remove values not in fact table from slicer","text":"<p>Problem: slicer uses dim table column and some values are not in fact table 2 - do not want to show the non-existent values in the slicer for table 2</p> <p>Solution: create a measure based on the values in fact table 2 and use it to filter the slicer.</p> <p>Limit the slicer to show values present in a fact table: <pre><code>SlicerFilter =\n    VAR _val_in_fact2 =\n        VALUES(fact_tbl2[col])\nRETURN\n    IF(MAX(dim_tbl[col]) IN _val_in_fact2, 1, 0)\n</code></pre></p>"},{"location":"PowerBI/View/Slicer/#blank-value-issue","title":"<code>blank</code> value issue","text":"<p>Sometimes the slicer will show blank() even all tables do not have blank values.</p> <p>This happens when a table on the n-side of a 1-to-n relationship contains values not present in the corresponding column of the table on the 1-side (with the slicer on a column from the table on the 1-side). Just like a Left Outer Join on the 1-side table.</p>"},{"location":"PowerBI/View/Slicer/#forced-value","title":"Forced value","text":"<p><code>Chiclet Slicer</code> can force select one of the existing values.</p> <p>For the default slicer, when source data changed, the last selected missing value will still in the slicer. </p>"},{"location":"PowerBI/View/Slicer/#slicers-stuck-on-non-existing-values","title":"Slicers stuck on non-existing values","text":"<p>Problem: - Slicer-year points to years - Slicer-quarter points to quarters - In the table there are two records: (A, 2000, 1) and (B, 2001, 3) - When the values for the sclicers are 2000 and 1, it's not possible to show the second record</p> <p>Workaround - Let one of the slicer to be a multi-select and un-select all, so the other slicer will show all values</p> <p>Solution: - see <code>Visual interactions</code></p>"},{"location":"PowerBI/View/Slicer/#visual-interactions","title":"Visual interactions","text":"<ul> <li>slicer A -&gt; column A, slicer B -&gt; column B</li> <li>value selected in slicer B should not affect slicer A (still show all values)</li> <li>Select slicer B &gt; Format &gt; Edit interactions &gt; click <code>no impact</code> icon near slicer A</li> </ul>"},{"location":"PowerBI/View/Slicer/#selected-value","title":"Selected value","text":"<pre><code>VAR __val = CALCULATE(\n    SUM(Tbl[Value]), \n    ALL(Tbl[Quarter]),\n    Tbl[Quarter] &lt;= SELECTEDVALUE(Tbl[Quarter])\n)\n</code></pre>"},{"location":"PowerBI/View/Slicer/#default-max-value","title":"Default max value","text":"<p>PowerBI will remember a constant value so we need to replace the max with a constant, e.g. <pre><code>DateStr = \n    VAR MaxDate = MAX(Dates[Date])\n    VAR SecondMaxDate = MAXX(FILTER(Dates,[Date]&lt;MaxDate), Dates[Date])\nReturn \n    SWITCH(Dates[Date], MaxDate, \"Today\", SecondMaxDate, \"Yesterday\", FORMAT(Dates[Date], \"yyyy-MM-dd hh:mm:ss\"))\n</code></pre></p>"},{"location":"PowerBI/View/Slicer/#slicer-not-showing-all-values-in-a-table","title":"Slicer not showing all values in a table","text":"<p>https://community.fabric.microsoft.com/t5/Desktop/Slicer-not-showing-all-values-that-are-shown-in-visual/m-p/2580206#M910972</p> <p>Scenario: both the slicer and view table column refer to the same table column</p> <p>Solution: one of the relationships in the dataset had the cross-filter direction set to both which resulted in the date filter affecting the product group dimension.</p> <p>Need to filter out empty records based on another column.</p>"},{"location":"PowerBI/View/SlicerDate/","title":"Date slicer","text":""},{"location":"PowerBI/View/SlicerDate/#include-all-dates-in-a-date-range-slicer","title":"include all dates in a date range slicer","text":"<p>Measure can affect slicers, but can only be used in visual level filter.</p> <p>https://medium.com/microsoft-power-bi/automatically-adjust-power-bi-date-range-slicer-to-show-current-data-1b21e4cc6cc2 - create a measure: <code>is_max_date = IF(MAX(DimDate[Date]) &lt;= TODAY(),1,0)</code> - add the measure to the visual filter</p> <p>Before solution does not work!</p> <p>Try to clear the selection (on the top right)!</p>"},{"location":"PowerBI/View/SlicerSync/","title":"Sync Slicers","text":"<p>https://docs.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-slicers?tabs=powerbi-desktop#sync-and-use-slicers-on-other-pages</p> <p>View -&gt; Sync slicers.</p>"},{"location":"PowerBI/View/SlicerSync/#syncing-slicers-on-the-same-page","title":"Syncing Slicers on the Same Page","text":"<p>See: https://learn.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-slicers?tabs=powerbi-desktop</p> <p>There are two main methods:</p> <ul> <li>Using the \"Sync Slicers\" Pane:</li> <li>Select the first slicer you want to sync.</li> <li>Go to the \"Format\" pane in the right sidebar.</li> <li>Under \"General,\" enable the \"Sync slicers\" option.</li> <li>Repeat steps 1-3 for the other slicer you want to sync.</li> <li>Go to the \"View\" menu and select \"Sync Slicers.\"</li> <li>In the \"Sync Slicers\" pane, enter the same group name for both slicers.</li> <li>Under \"Synchronization options,\" choose whether to sync selection changes, field changes, or both.</li> <li> <p>Click \"Apply.\"</p> </li> <li> <p>Using Interaction Settings:</p> </li> <li>Select the first slicer you want to sync.</li> <li>Go to the \"Format\" pane in the right sidebar.</li> <li>Click on \"Edit interactions.\"</li> <li>In the \"Visual interactions\" pane, uncheck the other visuals you don't want the slicer to interact with.</li> <li>Repeat steps 1-4 for the other slicer you want to sync.</li> </ul>"},{"location":"PowerBI/View/SlicerSync/#syncing-slicers-on-different-pages","title":"Syncing Slicers on Different Pages","text":"<ol> <li>Go to the \"View\" menu and select \"Sync Slicers.\"</li> <li>In the \"Sync Slicers\" pane, click on the page icon next to the desired slicer.</li> <li>Select the slicer on the other page you want to sync with.</li> <li>Enter the same group name for both slicers.</li> <li>Under \"Synchronization options,\" choose whether to sync selection changes, field changes, or both.</li> <li>Click \"Apply.\"</li> </ol>"},{"location":"PowerBI/View/SlicerSync/#additional-tips","title":"Additional Tips","text":"<ul> <li>You can sync multiple slicers to the same group.</li> <li>You can control the direction of the synchronization by checking \"Apply selection from one slicer to the other\" in the \"Sync Slicers\" pane.</li> <li>You can use bookmarks to store different slicer selections and easily switch between them.</li> </ul>"},{"location":"PowerBI/View/StackedColumn/","title":"Stacked column chart","text":""},{"location":"PowerBI/View/StackedColumn/#remove-gaps","title":"remove gaps","text":"<ul> <li>Visualizations -&gt; X axis and set the type to Categorical</li> <li>Columns -&gt; Spacing and set Inner padding to 0%</li> <li>Change X axis back to continuous (not work)</li> </ul>"},{"location":"PowerBI/View/Table/","title":"Table","text":""},{"location":"PowerBI/View/Table/#conditional-formating","title":"conditional formating","text":"<p>Right click the value column and select conditional formating.</p>"},{"location":"PowerBI/View/Table/#change-column-name","title":"change column name","text":"<p>Right click the value column and select change names -&gt; enter.</p>"},{"location":"PowerBI/View/Table/#group-rows-into-sections","title":"group rows into sections","text":"<p>https://inforiver.com/blog/general/group-rows-columns-matrix-powerbi/</p>"},{"location":"PowerBI/VirtualRelationship/TableJoin/","title":"Table Join","text":""},{"location":"PowerBI/VirtualRelationship/TableJoin/#naturalleftouterjoin","title":"NATURALLEFTOUTERJOIN","text":"<p>https://dax.guide/naturalleftouterjoin/</p> <p>can be used to join two tables without any relationships. The join will use the columns with the same name. T he join result is a temporal table but the table must be used in a DAX context (measure).</p>"},{"location":"PowerBI/VirtualRelationship/TableJoin/#calculate-the-sumproduct-of-columns-from-two-tables","title":"Calculate the sumproduct of columns from two tables","text":"<pre><code>CalculatedTaxRate =\n    VAR __avg_rate = Calculate(\n        AVERAGE(TblTaxRate[Rate]),\n        TREATAS(\n            SUMMARIZE(TblProfit, TblProfit[Year], TblProfit[Company], TblProfit[Department]),\n            TblTaxRate[Year],\n            TblTaxRate[Company],\n            TblTaxRate[Department]\n        )\n    )\n    VAR __tmp = NATURALLEFTOUTERJOIN( \n        TblProfit, \n        TblTaxRate\n    )    \n    VAR __weighted_rate = DIVIDE( \n        CALCULATE(\n            SUMX(__tmp,  TblProfit[Profit] * TblTaxRate[Rate]),\n            TREATAS(\n                SUMMARIZE(TblProfit, TblProfit[Year], TblProfit[Company], TblProfit[Department]),\n                TblTaxRate[Year],\n                TblTaxRate[Company],\n                TblTaxRate[Department]\n            )  \n        ),\n        SUM( TblProfit[Profit] )\n    )\n    VAR __is_row = HASONEFILTER(TblProfit[ProfitType])\nRETURN\n    if(__is_row, __avg_rate, __weighted_rate)\n</code></pre>"},{"location":"PowerBI/VirtualRelationship/UseRelationship/","title":"USERELATIONSHIP","text":"<p>Will activate the n-n relationship. Best to set the filter direction.</p> <pre><code>TotalFee = CALCULATE(\n  SUM(TblFee[Fee]),\n  USERELATIONSHIP(TblDept[Dept],TblFee[Dept]),\n  USERELATIONSHIP(TblDept[Year],TblFee[Year])\n)\n</code></pre>"},{"location":"Python/NAN/","title":"NaN","text":"<p>df groupby mean does not support \"skipna=False\"</p>"},{"location":"Python/NAN/#drop-nan-from-array","title":"drop nan from array","text":"<pre><code>arr[~pd.isnull(arr)]\n</code></pre>"},{"location":"Python/NAN/#check-df-nan-values","title":"check df nan values","text":"<pre><code>sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n</code></pre>"},{"location":"Python/Python/","title":"Python","text":""},{"location":"Python/Python/#python-tutorial","title":"python tutorial","text":"<p>https://docs.python.org/3/tutorial/</p>"},{"location":"Python/Python/#python-tips","title":"python tips","text":"<p>https://www.qtrac.eu/pytips.html</p>"},{"location":"Python/Python/#python-lecture-notes","title":"python lecture notes","text":"<p>https://codefellows.github.io/sea-python-401d4/lectures/</p>"},{"location":"Python/Python/#python-package-list","title":"python package list","text":"<p>https://github.com/ml-tooling/best-of-python</p>"},{"location":"Python/Python/#idioms-and-anti-idioms-in-python","title":"Idioms and Anti-Idioms in Python","text":"<p>https://docs.python.org/3.1/howto/doanddont.html</p>"},{"location":"Python/Python/#python-essentials","title":"python essentials","text":"<ul> <li>env: install, editor etc</li> <li>basic data structure: list, set, dict and associated functions</li> <li>advanced data structure: numpy, pandas and associated functions</li> <li>datetime (python, numpy, pandas datatime formats)</li> <li>fileio</li> <li>typing</li> <li>class: method, property, abstract class</li> <li>generator</li> <li>decorator (can be used in debugging, and more)</li> <li>how to deal with exceptions</li> <li>how to log info and error messages including stacktrace</li> <li>parallel</li> <li>async</li> <li>excel</li> <li>sql (sqlalchemy)</li> </ul>"},{"location":"Python/Python/#testing","title":"testing","text":"<ul> <li>pytest (ignore)</li> <li>unittest (mock)</li> </ul>"},{"location":"Python/Python/#documentation","title":"documentation","text":"<ul> <li>how to write document in code (function, class, module)</li> <li>how to write document associated with code (mkdocs)</li> </ul>"},{"location":"Python/Python/#package-management","title":"package management","text":"<ul> <li>how to manage python modules (pip, conda, mamba)</li> <li>how to build and release packages using pip and conda</li> </ul>"},{"location":"Python/Python/#containerization","title":"containerization","text":"<ul> <li>how to containerize your apps (dockerfile)</li> </ul>"},{"location":"Python/Python/#special-topics","title":"special topics","text":"<ul> <li>jupyter notebook </li> <li>web scraping</li> <li>web api (fastapi)</li> <li>dashboard (dash)</li> <li>useful packages: click, dask, pyarrow</li> <li>performance (timeit, caching)</li> </ul>"},{"location":"Python/basic/","title":"basic","text":""},{"location":"Python/basic/#find-the-mpl_tools-directory","title":"find the mpl_tools directory","text":"<pre><code>import importlib\nimportlib.import_module('mpl_toolkits').__path__\n\n#check version\npython -m pip freeze | findstr matplotlib\n\n#check type\nisinstance(i, int)\nisinstance(s, str)\n\n#install module\npython -m pip install numpy\n</code></pre>"},{"location":"Python/basic/#pyinstaller","title":"pyinstaller","text":"<p>It is possible to compile python scripts to an executable using the pyinstaller module. Helps with version control and not having to rely on module dependencies.</p> <p>Pyinstaller won't install nested dependencies that it can't see and it isn't obvious that it hasn't worked until runtime. You can work through package at a time and either add them to the hidden_imports list in the spec_file or in the command line or in hook files. All doable, just a little iterative.</p>"},{"location":"Python/basic/#install-mysqldb-on-windows-for-python-3","title":"Install MySQLdb on Windows for Python 3","text":"<p>To install MySQLdb on Windows go to this link \\ https://www.lfd.uci.edu/~gohlke/pythonlibs/#mysqlclient\\ Download the appropriate .whl for your Python version. <pre><code>python -m pip install mysqlclient-1.3.13-cp36-cp36m-win_amd64.whl\n</code></pre></p>"},{"location":"Python/API/API/","title":"API","text":"<p>https://www.infoq.com/podcasts/api-showdown-rest-graphql-grpc/</p>"},{"location":"Python/API/API/#http-error-422-unprocessable-entity","title":"HTTP error 422: Unprocessable Entity","text":"<p>https://progressivecoder.com/a-guide-to-fastapi-request-body-using-pydantic-basemodel/</p> <p>https://fastapi.tiangolo.com/tutorial/body-multiple-params/</p> <p>When there is only one Pydantic BaseModel parameter, the name of the parameter should not be included in the data/json parameter.</p>"},{"location":"Python/API/API/#rest-api","title":"REST API","text":"<p>Most standard (json) for web-based APIs.</p>"},{"location":"Python/API/API/#graphql","title":"GraphQL","text":"<p>Client can specify just the information they need, which can greatly reduce duplicate or unnecessary data being transmitted.</p> <p>https://python.plainenglish.io/fastapi-microservice-patterns-graphql-api-b09ccb1de37f</p> <p>https://github.com/grpc-up-and-running/samples/tree/master/ch02/productinfo</p>"},{"location":"Python/API/API/#grpc","title":"gRPC","text":"<p>for fast transport, leveraging HTTP/2.</p> <p>https://www.velotio.com/engineering-blog/grpc-implementation-using-python</p>"},{"location":"Python/API/Authorization/","title":"Authorization","text":"<ul> <li>https://learn.microsoft.com/en-us/entra/identity-platform/index-web-app?pivots=devlang-python&amp;tabs=windows</li> <li>https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth2-auth-code-flow</li> <li>https://intility.github.io/fastapi-azure-auth/usage-and-faq/calling_your_apis_from_python/</li> </ul> <p>If someone wants to use your API, they should create their own app registration and their own secret.</p>"},{"location":"Python/API/Authorization/#get-token","title":"get token","text":"<pre><code>import requests\ndef get_token(\n    auth_url, client_id, client_secret, scope, grant_type = 'client_credentials'\n):\n    \"\"\"\n    Return: tuple[status_code, access_token]\n    (200, {\n        'token_type': 'Bearer',\n        'expires_in': 3599,\n        'ext_expires_in': 3599,\n        'access_token': 'xxxx'\n     })\n    \"\"\"    \n    data = { \n        'client_id': client_id,        \n        'client_secret': client_secret,\n        'scope': scope,\n        'grant_type': grant_type,\n    }\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    resp = requests.post(url=auth_url, data=data, headers=headers)\n    return  resp.status_code, resp.json()\n</code></pre>"},{"location":"Python/API/Authorization/#call-api-with-token","title":"call api with token","text":"<pre><code># test\ntenant_id = '567f1234-c7de-4321-987a-dd4e54321c98'\nclient_id = '1234de76-54a3-21ae-97bc-6eba3456789e'\nclient_secret = 'xxx'\nscope = f'{client_id}/.default'\n\nauth_url = f'https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token'\nurl = 'https://test.data.example.com'\naccess_token = get_token(auth_url, client_id, client_secret, scope)[1]['access_token']\nheaders = {'Authorization': f'Bearer {access_token}'}\napi_resp = requests.get(url=url, headers=headers)\nprint(api_resp.json())\n</code></pre>"},{"location":"Python/API/Example/","title":"Example","text":""},{"location":"Python/API/Example/#test-api-function","title":"test api function","text":"<pre><code>from fastapi import FastAPI, APIRouter, status\nfrom fastapi.testclient import TestClient\nfrom pydantic import BaseModel\nfrom datetime import date, datetime\nfrom typing import Union, Optional\nfrom json import dumps\nimport pydantic_core\n\napp = FastAPI()\nrouter = APIRouter()\nclient = TestClient(app)\n\nclass SHA256(str):\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v: Union[bytes, str], values: Optional[dict] = None) -&gt; str:\n        if isinstance(v, bytes):\n            v = v.hex()\n\n        if not isinstance(v, str):\n            raise TypeError('Must be string or bytes')\n\n        if len(v) != 64:\n            raise TypeError('String must be 64 characters')\n\n        return v\n\nclass MyModel_i(BaseModel):\n    sha256_hash: str \n\nclass MyModel_o(BaseModel):\n    sha256_hash: SHA256\n    class Config:\n        from_attributes = True\n\n@router.post(\"/myapp\", response_model=dict, status_code=status.HTTP_201_CREATED)\ndef myapp_test(\n    params: MyModel_o,\n) -&gt; dict:\n    return params.model_dump()\napp.include_router(router)\n\ndata = dumps(MyModel_i(\n    sha256_hash=hashlib.sha256('xyz'.encode('utf-8')).hexdigest()\n).model_dump())\nresponse = client.post(\"/myapp\", data=data)\n\n# Assert status code is 201 (created)\nprint(response.status_code)\n\n# Check that the returned task is correct\nresponse_data = response.json()\nprint(response_data)\n</code></pre>"},{"location":"Python/API/Exception/","title":"Error handling","text":"<p>https://fastapi.tiangolo.com/tutorial/handling-errors/</p>"},{"location":"Python/API/Exception/#httpexception","title":"HTTPException","text":"<p>Use <code>HTTPException</code> to return HTTP responses with errors to the client.</p> <p>The <code>detail</code> parameter can be a dict, a list, etc. They are handled automatically by FastAPI and converted to JSON.</p> <pre><code>from fastapi import FastAPI, HTTPException\n\napp = FastAPI()\nitems = {'foo': 'The Foo Wrestlers'}\n\n@app.get('/items/{item_id}')\nasync def read_item(item_id: str):\n    if item_id not in items:\n        raise HTTPException(status_code=404, detail='Item not found')\n    return {'item': items[item_id]}\n</code></pre> <p>The detail can be get using <code>resp.json()</code>. Note that if there are no errors, <code>resp.json()</code> will return <code>[]</code>.  If there are sever side errors, <code>resp.json()</code> will crash.</p>"},{"location":"Python/API/FastAPI/","title":"FastAPI","text":"<p>https://fastapi.tiangolo.com/tutorial</p> <p>FastAPI is an ASGI framework - the resulting app object doesn't talk HTTP directly,  - it just receives dictionaries and stuff passed by the ASGI server,  - and that's how it handles the rest.</p> <p>You need something to pass those arguments to the FastAPI app. It could be Uvicorn, Hypercorn, Daphne... etc.</p> <p>https://github.com/tiangolo/fastapi/issues/2062 - FastAPI is based on Starlette, Starlette is an ASGI micro-framework/toolkit.  - Uvicorn is made/maintained by the same developer(s) as Starlette.  - They are not tightly coupled or depend on each other in any way, but are the natural closest match.</p> <p>By default, if you install with pip install <code>uvicorn[standard]</code> that will include and use Uvloop, a drop-in replacement for the asyncio loop, with very high performance. You don't have to do anything in your code to use it, just install <code>uvicorn[standard]</code>.</p> <p>That <code>Uvloop</code> thing is what gives all the performance to anything (or almost anything) in async Python that has high performance. So, chances are Uvloop would give the best performance.</p>"},{"location":"Python/API/FastAPI/#run","title":"run","text":"<p><pre><code>if __name__ == '__main__':\n    uvicorn.run('api.main:app', host='127.0.0.1', port=8000)\n</code></pre> or run in terminal <code>uvicorn 'api.main:app' --host '127.0.0.1' --port 8000 --reload</code></p>"},{"location":"Python/API/FastAPI/#bigger-application-structure","title":"bigger application structure","text":"<p>https://fastapi.tiangolo.com/tutorial/bigger-applications/</p>"},{"location":"Python/API/FastAPI/#model","title":"model","text":"<p>https://fastapi.tiangolo.com/tutorial/body/</p> <p>https://realpython.com/fastapi-python-web-apis/</p> <ul> <li><code>request body</code> is the data sent by client to server. </li> <li><code>response body</code> is the data the API sends back to the client.</li> </ul> <p>If a parameter is not present in the path and it also uses Pydantic BaseModel, FastAPI automatically considers it as a request body. </p> <p>FastAPI will read the incoming request payload as JSON and convert the corresponding data types if needed. Also, it will perform validation and return an appropriate error response.</p>"},{"location":"Python/API/FastAPI/#postgresql","title":"postgresql","text":"<p>https://dev.to/jbrocher/fastapi-testing-a-database-5ao5</p> <p>https://www.educative.io/answers/how-to-use-postgresql-database-in-fastapi</p> <p>https://ahmed-nafies.medium.com/fastapi-with-sqlalchemy-postgresql-and-alembic-and-of-course-docker-f2b7411ee396</p>"},{"location":"Python/API/FastAPI/#test","title":"test","text":"<p>https://fastapi.tiangolo.com/tutorial/testing/ <pre><code>from fastapi import FastAPI\nfrom fastapi.testclient import TestClient\n\napp = FastAPI()\n@app.get(\"/\")\nasync def read_main():\n    return {\"msg\": \"Hello World\"}\nclient = TestClient(app)\n\ndef test_read_main():\n    response = client.get(\"/\")\n    assert response.status_code == 200\n    assert response.json() == {\"msg\": \"Hello World\"}\n</code></pre></p>"},{"location":"Python/API/IO/","title":"IO","text":""},{"location":"Python/API/IO/#convert-sync-to-async","title":"Convert sync to async","text":"<pre><code>async def to_async(iterator):\n    for i in iterator:\n        yield i\n</code></pre>"},{"location":"Python/API/IO/#convert-async-generator-stream-to-file-like-object","title":"Convert async generator stream to file like object","text":"<p>https://stackoverflow.com/questions/59413796/how-to-convert-async-generator-stream-into-a-file-like-object-in-python3</p> <p>Maybe can use <code>io.BytesIO</code> as well instead of <code>SpooledTemporaryFile</code>. <pre><code>data_file = SpooledTemporaryFile(\n    mode='w+b',\n    max_size=MAX_RECEIVED_DATA_MEMORY_SIZE,\n)\nasync for chunk in request.stream():\n    data_file.write(chunk)\ndata_file.seek(0)\n</code></pre></p>"},{"location":"Python/API/Issue/","title":"Issue","text":""},{"location":"Python/API/Issue/#slow-api","title":"slow api","text":"<p>https://github.com/tiangolo/fastapi/issues/1379</p> <p>https://www.reddit.com/r/kubernetes/comments/u7gc2u/troubleshooting_slow_ready_pod/</p>"},{"location":"Python/API/Issue/#debug-http-400-error","title":"debug http 400 error","text":"<p>https://stackoverflow.com/questions/55724157/debugging-a-python-requests-module-400-error <code>requests.json()</code> will return all the info about the error.</p>"},{"location":"Python/API/Middleware/","title":"Middleware","text":"<p>https://fastapi.tiangolo.com/tutorial/middleware/</p> <p>A middleware is a function that works with every <code>request</code> before it is processed by any specific path operation. And also with every <code>response</code> before returning it.</p>"},{"location":"Python/API/Middleware/#gzipmiddleware","title":"GZipMiddleware","text":"<p>https://docs.djangoproject.com/en/4.1/ref/middleware/</p> <p>Handles GZip responses for any request that includes \"gzip\" in the Accept-Encoding header.</p> <p>This middleware should be placed before any other middleware that need to read or write the response body so that compression happens afterward.</p> <p>Must add header to work correctly: <pre><code>app = FastAPI()\napp.add_middleware(GZipMiddleware, minimum_size=1000) #only compress when content body &gt;= 1000 bytes\n\n@router.get(\"/test\")\ndef test_handler():\n    content = {\"test1\": 1}\n    headers = {\"Accept-Encoding\": \"gzip\"}\n    return JSONResponse(content=content, headers=headers)\n</code></pre></p>"},{"location":"Python/API/Param/","title":"Parameter","text":""},{"location":"Python/API/Param/#header-parameter","title":"Header parameter","text":"<p>https://fastapi.tiangolo.com/tutorial/header-params/</p> <p>Get the header as an input parameter <pre><code>from typing import Optional\nfrom fastapi import FastAPI, Header\n\napp = FastAPI()\n\n@app.get(\"/items/\")\nasync def read_items(user_agent: Optional[str] = Header(None)):\n    return {\"User-Agent\": user_agent}\n</code></pre></p>"},{"location":"Python/API/Pydantic/","title":"Pydantic","text":"<p>Pydantic offers build-in data validation and type checking, suitable for web APIs and external data handling.</p>"},{"location":"Python/API/Pydantic/#20-update-list","title":"2.0 update list","text":"<p>https://2pointers.medium.com/an-introduction-to-pydantic-v2-alpha-pre-release-a-massive-improvement-over-previous-version-748a1f1118ba</p>"},{"location":"Python/API/Pydantic/#model-field-name","title":"<code>model</code> field name","text":"<p>have reserved every field that starts with <code>model_</code> for themselves.</p> <p>option 1: using <code>alias</code> <pre><code>from pydantic import BaseModel, Field\n\nclass MyModel(BaseModel):\n    model_: str = Field(..., alias='model')\n</code></pre></p> <p>option 2: disable reserved words <pre><code>from pydantic import BaseModel, ConfigDict\n\nclass MyModel(BaseModel):\n    model_config = ConfigDict(protected_namespaces=())\n    model: str = Field(..., title='The name of the model to use')\n</code></pre></p>"},{"location":"Python/API/Pydantic/#from_attributes","title":"from_attributes","text":"<p>In Pydantic, the <code>from_attributes</code> setting in the <code>Config</code> class is a relatively new feature (introduced in Pydantic v2)  that allows you to create a Pydantic model instance directly from attributes (i.e., keyword arguments) instead of the usual dictionary-based initialization.</p> <p>By default, Pydantic models expect input data to be passed as a dictionary (or something that can be converted to one),  but with <code>from_attributes = True</code>, you can initialize the model using individual attributes directly as you would with a regular class.</p>"},{"location":"Python/API/Pydantic/#example","title":"Example","text":"<pre><code>from pydantic import BaseModel\n\nclass MyPydanticModel(BaseModel):\n    id: int\n    name: str\n\n    class Config:\n        from_attributes = True\n</code></pre>"},{"location":"Python/API/Pydantic/#without-from_attributes-default-behavior","title":"Without <code>from_attributes</code> (default behavior)","text":"<p>Normally, you would instantiate the model like this:</p> <pre><code>data = {\"id\": 1, \"name\": \"Alice\"}\nmodel_instance = MyPydanticModel(**data)\n</code></pre> <p>This works because Pydantic automatically parses the dictionary and assigns the values to the model's fields.</p>"},{"location":"Python/API/Pydantic/#with-from_attributes-true","title":"With <code>from_attributes = True</code>","text":"<p>With <code>from_attributes</code> enabled, you can instantiate the model directly with attributes:</p> <pre><code>model_instance = MyPydanticModel(id=1, name=\"Alice\")\n</code></pre> <p>This works as if <code>id</code> and <code>name</code> were regular class attributes, but Pydantic will still validate them according to the model's type annotations and run any other model validation logic defined in the model.</p>"},{"location":"Python/API/Pydantic/#why-would-you-use-from_attributes","title":"Why would you use <code>from_attributes</code>?","text":"<p>This feature can be helpful in scenarios where: - You have objects or data sources that already provide values as individual attributes, and you want to avoid packing them into a dictionary first. - You want to simplify the instantiation process when creating models programmatically.</p> <p>In this case, the <code>from_attributes</code> setting makes the instantiation of <code>MyPydanticModel</code> feel like working with a normal Python class, where attributes can be passed directly instead of a dictionary.</p>"},{"location":"Python/API/Pydantic/#summary","title":"Summary","text":"<ul> <li><code>from_attributes = True</code> allows Pydantic models to be initialized using keyword arguments (like a regular Python class) rather than needing to pass a dictionary.</li> <li>It's useful when you already have data in the form of attributes, and it can simplify your model instantiation code.</li> </ul>"},{"location":"Python/API/Response/","title":"Response Types","text":""},{"location":"Python/API/Response/#custom-response","title":"custom response","text":"<p>https://fastapi.tiangolo.com/advanced/custom-response/</p>"},{"location":"Python/API/Response/#fileresponse","title":"FileResponse","text":"<p>will save the file on disk and return a path.</p>"},{"location":"Python/API/Response/#jsonresponse","title":"JSONResponse","text":"<p><code>**caveat**</code>: For file larger than 250MB, when using <code>pd.read_json</code> will get error: <code>Could not reserve memory block</code></p>"},{"location":"Python/API/Response/#option-1-default","title":"Option 1: default","text":"<p>The default one useing <code>jsonable_encoder</code> (return list of jsons) is at least 5x slower than <code>df.to_json</code>. <pre><code>resp = df.fillna('').to_dict(orient='records')                        #default response\nresp = JSONResponse(jsonable_encoder(df.fillna('').to_dict(orient='records'))) #default equvalent\n</code></pre></p>"},{"location":"Python/API/Response/#option-2-dfto_json","title":"Option 2: df.to_json","text":"<p>5 - 10x faster than default <pre><code>content = df.fillna('').to_json(orient='records', date_format='iso', date_unit='s') #df.to_json\nresp = Response(content, media_type=\"application/json\")\n\nresp.headers['Accept-Encoding'] = 'gzip'  #seems not required for gzip compression!\n</code></pre></p> <p>Performance for <code>json</code> <pre><code>content = df.fillna('').to_json(orient='records', date_format='iso', date_unit='s')\nresp = Response(content, media_type=\"application/json\")\n\nheaders = {\"Accept\":\"application/json\"}\ndf = pd.read_json(url, storage_options=headers)                                # 8.996\ndf = pd.DataFrame.from_dict(requests.get(url, headers=headers).json())         #12.830\ndf = pd.read_json(io.BytesIO(requests.get(url, headers=headers).content))      #13.907\ndf = pd.read_json(requests.get(url, headers=headers).content.decode('utf-8'))  #15.657\n</code></pre></p>"},{"location":"Python/API/Response/#response","title":"Response","text":"<p>Response is designed to handle complete responses that are generated in one go. Therefore, Response doesn't have a built-in way to directly accept an <code>io.BytesIO</code> object.</p> <p>Use <code>Response</code> if cache is required (much faster than StreamingResponse) - Response only supports <code>string</code>, <code>json</code> data or <code>bytes</code> data - <code>io.BytesIO.getvalue()</code> gets the file like object (in memory) content as bytes</p> <p>Performance for <code>parquet</code> <pre><code>bio = io.BytesIO(df.to_parquet(compression='brotli')).getvalue()\nbio = io.BytesIO(df.to_parquet(compression=None)).getvalue() #compared to compression, faster but slower for cached data\nresp = Response(bio, media_type=\"bytes/parquet\")             #Response only support string or bytes\nresp.headers[\"Content-Disposition\"] = 'attachment; filename=data.parquet'\n\nheaders = {\"Accept\": \"bytes/parquet\"}\ndf = pd.read_parquet(url, storage_options=headers)                                #1.120\ndf = pq.read_pandas(io.BytesIO(requests.get(url, headers=headers).content))       #2.125\ndf = pd.read_parquet(pa.BufferReader(requests.get(url, headers=headers).content)) #2.273\ndf = pd.read_parquet(io.BytesIO(requests.get(url, headers=headers).content))      #2.550\n</code></pre></p>"},{"location":"Python/API/Response/#streamingresponse","title":"StreamingResponse","text":"<p>caveat cannot directly work with <code>aiocache import cached</code></p> <p>When send a large amount of data, e.g., 50 MB, through API, we might get timeout, other network issues for downloading such a data from the server.</p> <p>Streaming response will ensure the data being downloaded chunk by chunk to avoid these issues.</p> <p><code>io.BytesIO</code> is a class that allows you to create an in-memory buffer for binary data, which can be used to read or write data in chunks. This makes it a natural fit for use with <code>StreamingResponse</code>.</p> <p>requires an iterator object to send the results in chunks.</p> <p>https://cloudbytes.dev/snippets/received-return-a-file-from-in-memory-buffer-using-fastapi</p>"},{"location":"Python/API/Response/#return-a-parquet-file-similar-performance-to-json","title":"return a parquet file (similar performance to json)","text":"<p>Note: StreamingResponse is very slow compared to Response.</p> <p>Due to the content BytesIO not <code>typing.AsyncIterable</code>? https://github.com/tiangolo/fastapi/issues/2302 <pre><code>#bio = io.BytesIO()\n#df.to_parquet(bio)\n#bio.seek(0) #can use .getvalue() .getbuffer().nbytes\nbio = io.BytesIO(df.to_parquet(compression='brotli'))\nresp = StreamingResponse(bio, media_type=\"bytes/parquet\")\nresp.headers[\"Content-Disposition\"] = 'attachment; filename=data.parquet'\ndf = pd.read_parquet(io.BytesIO(req.content))\n</code></pre></p> <p>Solution: Convert the content to async now ony a little slower than using Response. <pre><code>async def iterfile():\n    with bio as f:\n        while True:\n            chunk = f.read(1024) #chunk_size is 1024 bytes (1 KB to 1 MB good enough)\n            if not chunk:\n                break\n            yield chunk\nresp = StreamingResponse(iterfile(), media_type=\"bytes/parquet\")\n</code></pre></p>"},{"location":"Python/API/Response/#return-an-image","title":"return an image","text":"<p>https://stackoverflow.com/questions/55873174/how-do-i-return-an-image-in-fastapi <pre><code>byte_im = BytesIO()\nimage.save(byte_im, 'JPEG')\nbyte_im.seek(0)   #get byte_im size: byte_im.getbuffer().nbytes\n\nreturn StreamingResponse(byte_im, media_type='image/jpeg')\n</code></pre></p> <p>Another solution: faster?</p> <p>https://stackoverflow.com/questions/66223811/how-to-increase-transfer-speed-when-posting-an-image-to-a-rest-api <pre><code>data = {'shape': image.shape, 'img': base64.b64encode(image.tobytes())}\nimage = np.frombuffer(base64.b64decode(data.img)).reshape(data.shape)\n</code></pre></p>"},{"location":"Python/API/Response/#api-get","title":"api get","text":"<p>https://stackoverflow.com/questions/73564771/fastapi-is-very-slow-in-returning-a-large-amount-of-json-data</p> <p>To prevent browser show large amount of data - set <code>Content-Disposition</code> header to Response using the <code>attachment</code> parameter and passing a filename</p> <p>Performance - parquet (Response) is 2-4x faster than json (Response) - json (Response) is 2x (5MB, 1x 50MB) faster than parquet (StreamingResponse, defaul chunk size), will return bytes - json is 8x faster than csv. Why csv is slow - not well compressed compared to json and require more time to parse? - another test indicates that json has similar or slower performance than csv</p> <pre><code>import io\nimport requests\nimport pandas as pd\n\n@router.get(\"/get_df\",\n    status_code=status.HTTP_200_OK,\n    description='Test api get',\n)\nasync def get_df(\n    request: Request,\n):\n    df = pd.DataFrame([['i',1],['j', 2]], columns=['k', 'v'])\n\n    if request.headers.get('Accept') == 'text/csv':\n        content = io.StringIO(df.to_csv(index=False))\n        resp = StreamingResponse(content, media_type='text/csv')\n        resp.headers['Content-Disposition'] = 'attachment; filename=data.csv'\n    elif request.headers.get('Accept') == 'bytes/parquet':\n        content = io.BytesIO(df.to_parquet(compression='brotli'))\n        resp = StreamingResponse(content, media_type='bytes/parquet')\n        resp.headers['Content-Disposition'] = 'attachment; filename=data.parquet'\n    else:\n        content = df.fillna('').to_json(orient='records', date_format='iso', date_unit='s')\n        resp = Response(content, media_type='application/json')\n    return resp\n\ndef url_to_df(url, header_type):\n    if header_type == 'csv':\n        return pd.read_csv(url, storage_options={'Accept':'text/csv'})\n    elif header_type == 'parquet':\n        return pd.read_parquet(url, storage_options={'Accept':'bytes/parquets'})\n    else: #json\n        return pd.read_json(url, storage_options={'Accept':'application/json'})\n\ndef req_to_df(url, header_type):\n    if header_type == 'csv':\n        req = requests.get(url, headers={\"Accept\":\"text/csv\"})\n        return pd.read_csv(io.StringIO(req.content.decode('utf-8')))\n    elif header_type == 'parquet':\n        req = requests.get(url, headers={\"Accept\":\"bytes/parquet\"})\n        return pd.read_parquet(io.BytesIO(req.content))\n    else: #json\n        req = requests.get(url, headers={'Accept':'application/json'})\n        return pd.DataFrame.from_dict(req.json()))\n\ndef api_get(url, header_type='json', faster=True):\n    if faster: #30% faster\n        df = url_to_df(url, header_type)\n    else:\n        df = req_to_df(url, header_type)\n    return df\n</code></pre>"},{"location":"Python/API/ShareData/","title":"Share data between workers","text":"<p>https://stackoverflow.com/questions/65686318/sharing-python-objects-across-multiple-workers</p> <p>https://stackoverflow.com/questions/71085983/modern-apis-with-fastapi-redis-caching</p> <p>https://ubaydah.hashnode.dev/building-a-blog-service-with-fastapi-and-redis-om</p>"},{"location":"Python/API/ShareData/#using-database","title":"using database","text":"<p>PostgreSQL, MariaDB, MongoDB</p>"},{"location":"Python/API/ShareData/#using-cache","title":"using cache","text":"<p>Redis, Memcached</p>"},{"location":"Python/API/WebServer/","title":"Web Server","text":"<p>https://github.com/tiangolo/fastapi/issues/2062</p> <p>if using docker with kuberntes or some load balancer, i'll recommend: <code>CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]</code></p> <p>After trying - <code>hypercorn</code> and <code>gunicorn</code> with 1 or 4 workers, - <code>hypercorn</code> with trio or default, - <code>gunicorn</code> with <code>uvicorn</code> worker I came to the conclusion that with the above configuration I got the best results when receiving many requests on endpoints that make several requests.</p>"},{"location":"Python/API/WebServer/#uvicorn","title":"uvicorn","text":"<p><code>uvicorn</code> is an asynchronous ASGI web server. It uses <code>uvloop</code> if it can. Uvicorn supports HTTP/1.1 and WebSockets, but does not (yet) support HTTP/2.</p> <p>cli <pre><code>uvicorn 'my_fastapi.fileapi:app' --host '127.0.0.1' --port 7000 --loop 'uvloop' --workers 1 --root-path '/api' --reload\n</code></pre></p> <pre><code>import uvicorn\nuvicorn.run(\n    'my.api.main:app',\n    host='127.0.0.1',\n    port=7000,\n    reload=False,\n)\n</code></pre> <p>Settings: https://www.uvicorn.org/settings/ - <code>--loop</code>: The <code>uvloop</code> implementation provides greater performance, but is not compatible with Windows or PyPy. - Underwindows windows can use loop <code>asyncio</code>.</p>"},{"location":"Python/API/WebServer/#hypercorn","title":"hypercorn","text":"<p><code>Hypercorn</code> is an asynchronous ASGI web server. It can use <code>uvloop</code> as Uvicorn does, or use other event loops. It supports HTTP/2.</p> <p>Config: https://pgjones.gitlab.io/hypercorn/how_to_guides/configuring.html</p> <p>https://levelup.gitconnected.com/deploy-fastapi-with-hypercorn-http-2-asgi-8cfc304e9e7a <pre><code>hypercorn 'my_fastapi.fileapi:app' --bind 127.0.0.1:5000 --worker-class 'uvloop' --workers 2 --root-path '/api' --reload --debug\n</code></pre></p> <pre><code>#import uvloop\nimport asyncio\nfrom hypercorn.config import Config\nfrom hypercorn.asyncio import serve\n\nimport os\nos.environ['HYPERCORN_WORKERS'] = '2'\nos.environ['HYPERCORN_DEBUG'] = 'False'\nos.environ['HYPERCORN_ROOT_PATH'] = 'c:/api'\n\nconfig = Config()\nconfig.bind = ['127.0.01:8000']\n#config.worker_class = uvloop.Loop # uvloop only for linux\n#config.use_reload = auto_reload\nconfig.accesslog = '-'\nasyncio.run(serve(app, config))\n</code></pre> <p>more example <pre><code>import click\n#import uvloop\nimport asyncio\nfrom hypercorn.config import Config\nfrom hypercorn.asyncio import serve\n\nfrom fastapi import FastAPI\nfrom fastapi.middleware.gzip import GZipMiddleware\n\nfrom . import __version__\nfrom .routers import (\n    dev,\n    tst,\n)\n\napp = FastAPI(\n    title='Dev REST API',\n    version=__version__,\n    docs_url='/',\n)\n\napp.add_middleware(GZipMiddleware, minimum_size=100000)\n\napp.include_router(sales.router, prefix='/sales', tags=['Sales'])\napp.include_router(marketing.router, prefix='/marketing', tags=['Marketing'])\n\nimport os\nos.environ['HYPERCORN_WORKERS'] = '2'\nos.environ['HYPERCORN_DEBUG'] = 'False'\nos.environ['HYPERCORN_ROOT_PATH'] = 'c:/test/api'\n\n@click.command()\n@click.option('--host', type=click.STRING, default='127.0.0.1')\n@click.option('--port', type=click.INT, default=7000)\n@click.option('--auto-reload/--no-auto-reload', default=True)\ndef cli(**kwargs):\n    host = kwargs['host']\n    port = kwargs['port']\n    auto_reload = kwargs['auto_reload']\n    config = Config()\n    config.bind = [f'{host}:{port}']\n    #config.worker_class = uvloop.Loop # uvloop only for linux\n    #config.use_reload = auto_reload\n    #config.accesslog = '-'\n    asyncio.run(serve(app, config))\n</code></pre></p>"},{"location":"Python/API/WebServer/#gunicorn","title":"gunicorn","text":"<p><code>gunicorn</code> is a synchronous web server.</p>"},{"location":"Python/API/gRPC/","title":"gRCP","text":"<p>https://grpc.io/docs/languages/python/basics/</p> <p>install packages <pre><code>pip install grpcio grpcio-tools googleapis-common-protos\n</code></pre></p> <p>compile protocols <pre><code>python -m grpc_tools.protoc -I definitions/ --python_out=definitions/builds/ --grpc_python_out=definitions/builds/ definitions/service.proto\n</code></pre></p>"},{"location":"Python/API/gRPC/#comparison-between-grcp-and-fastapi","title":"comparison between gRCP and FastAPI","text":"<p>https://alek-cora-glez.medium.com/my-next-api-grpc-restful-5d888289acd</p> <p>https://betterprogramming.pub/grpc-file-upload-and-download-in-python-910cc645bcf0</p>"},{"location":"Python/API/gRPC/#fastapi-and-grpc","title":"FastAPI and gRPC","text":"<p>https://dev.to/ankitbrijwasi/connect-fastapi-golang-services-using-grpc-4k3d</p>"},{"location":"Python/API/gRPC/#grpc-api","title":"gRPC api","text":"<p>https://medium.com/google-cloud/building-apis-with-grpc-continued-f53b5a5ab850</p>"},{"location":"Python/App/Module/","title":"Module","text":""},{"location":"Python/App/Module/#win32com","title":"win32com","text":"<p>install pywin32 for win32com: <pre><code>python -m pip install pywin32\n</code></pre></p>"},{"location":"Python/App/Module/#plotly-and-cufflinks","title":"Plotly and Cufflinks","text":"<p>cufflinks is the link between plotly and pandas. <pre><code>import pandas as pd\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n</code></pre></p>"},{"location":"Python/App/Module/#geographical-plotting","title":"Geographical Plotting","text":"<pre><code>import plotly.plotly as py\nimport plotly.grath_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\nlayout = dict(geo={'scope':'usa'})\nchoromap = go.Figure(data=[data], layout=layout)\niplot(choromap)\n</code></pre>"},{"location":"Python/App/image/","title":"Tool","text":""},{"location":"Python/App/image/#remove-image-background","title":"Remove image background","text":"<pre><code>from PIL import Image\nfrom rembg import remove\n\nimg_in = Image.open('test.jpg')\nremove(img_in).save('new.png')\n</code></pre>"},{"location":"Python/App/pdf2tbl/","title":"pdf2tbl","text":""},{"location":"Python/App/pdf2tbl/#pdfplumber","title":"pdfplumber","text":"<p>pdfplumber: extract text and tables from pdf files</p> <p>settings\\ https://github.com/jsvine/pdfplumber#extracting-tables</p> <p>examples\\ https://github.com/jsvine/pdfplumber/blob/stable/examples/notebooks/extract-table-nics.ipynb</p> <pre><code>pdf = pdfplumber.open(file_input)\npg = pdf.pages[34]\n#pg.to_image()\n\ntable_settings = {\n    \"vertical_strategy\": \"lines\",\n    \"horizontal_strategy\": \"lines\",\n    \"intersection_x_tolerance\": 15,\n    \"intersection_y_tolerance\": 15,\n    \"snap_tolerance\": 10,\n    \"edge_min_length\":3\n}\ntbls = pg.extract_tables(table_settings)\n\nlines = pg.extract_text().splitlines()\n\n#seems the settings only works for first table\ntbs = page.find_tables(table_settings)\nheight = min(page.height, tb.bbox[3])\ntp = page.within_bbox((0, tb.bbox[1], page.width, height))\nts = tp.find_tables(table_settings)\nif len(ts) == 0:\n    continue\ntl = ts[0].extract(x_tolerance=5, y_tolerance=5)\nth = list(filter(None, tl[0])) #remove empty items\n</code></pre>"},{"location":"Python/App/pdf2tbl/#camelot","title":"camelot","text":"<p>camelot: extract tables from pdf files. also works for scanned files. cannot get page text</p>"},{"location":"Python/App/pdf2tbl/#install-ghostscript-from-httpswwwghostscriptcomdownloadgsdnldhtml","title":"install Ghostscript from: https://www.ghostscript.com/download/gsdnld.html\\","text":""},{"location":"Python/App/pdf2tbl/#pip-install-camelot-pycv","title":"pip install \"camelot-py[cv]\"","text":"<pre><code>tbls = camelot.read_pdf(file_input, pages='all', process_background=True)\nfor tbl in tbls:\n    df = tbl.df\n    print(f'Page: {tbl.page}')\n    print(df.head(3))\n    print('\\n')\n</code></pre>"},{"location":"Python/App/util/","title":"util","text":""},{"location":"Python/App/util/#argparser","title":"argparser","text":"<p>https://docs.python.org/3/library/argparse.html</p>"},{"location":"Python/App/util/#progress","title":"progress","text":"<pre><code>def progress(width, msg):\n    print(f'{msg: &lt;{width}}\\r', end='', flush=True)\n    return max(len(msg), width)\n\nw = 75\nfor i in range(7):\n    time.sleep(2)\n    w = progress(w, f'Progress: [{i}/100] {i:.1f}%')\nprogress(w, ' ')\n</code></pre>"},{"location":"Python/App/util/#send-email","title":"send email","text":"<pre><code>import smtplib #actual sending function\nfrom email.mime.text import MIMEText #email modules we'll need\n\ndef send_email(subject, str_msg, joinlines = False):\n    email_from = 'my.name@example.com'\n    email_to = 'my.name@gmail.com' #;example2@gmail.com'\n    #create a text/plain message\n    msg = MIMEText('\\n'.join(str_msg) if joinlines else str_msg)\n    msg['Subject'] = subject\n    msg['From'] = email_from\n    msg['To'] = email_to\n\n    #send the message via our own SMTP server, but don't include the envelope header\n    s = smtplib.SMTP('mail.xx.yy')\n    s.sendmail(email_from, [email_to], msg.as_string())\n    s.quit()\n</code></pre>"},{"location":"Python/App/xml/","title":"xml","text":"<p>https://towardsdatascience.com/processing-xml-in-python-elementtree-c8992941efd2</p> <pre><code>import xml.etree.ElementTree as et\n\ndef parse_con(dir, file, duid):\n    \"\"\"Parse the input XML file and store the result in a pandas\n    DataFrame with the given columns.\n\n    #&lt;NEMSPDCaseFile&gt;\n    #    &lt;NemSpdInputs&gt;\n    #        &lt;RegionCollection&gt;\n    #        &lt;TraderCollection&gt;\n    #        &lt;InterconnectorCollection&gt;\n    #        &lt;ConstraintScadaDataCollection&gt;\n    #        &lt;GenericEquationCollection&gt;\n    #        &lt;GenericConstraintCollection&gt;\n    \"\"\"\n\n    print(f'    {file}')\n    tree = et.parse(os.path.join(dir,file))\n    root = tree.getroot()\n\n    pth1 = './NemSpdInputs/GenericConstraintCollection/GenericConstraint'\n    pth2 = f'/LHSFactorCollection/TraderFactor[@TraderID=\"{duid}\"]'\n\n    #get periodending, UIGF_ATime=\"20201204104000\"\n    pe = root.find('NemSpdInputs').find('Case').attrib.get('UIGF_ATime')\n    pe = pd.to_datetime(pe, format='%Y%m%d%H%M%S')\n\n    #get generic constraint info\n    rows = []\n    cols = ['SettlementDate', 'ConstraintID','Version','EffectiveDate',\n            'VersionNo','Type','ViolationPrice','RHS','Force_SCADA', f'LHS_{duid}']\n    for gc in root.findall(f'{pth1}{pth2}/../..'):\n        row = [pe] + [gc.attrib.get(col) for col in cols[1:-1]]\n        lhs = gc.findall(f'.{pth2}')\n        row.append(lhs[0].attrib.get('Factor'))\n        rows.append(row)\n    df = pd.DataFrame(rows, columns=cols)\n\n    return df\n\ndef parse_sol(dir, file, duid):\n    \"\"\"Parse the input XML file and store the result in a pandas\n    DataFrame with the given columns.\n\n    #&lt;NEMSPDCaseFile&gt;\n    #    &lt;NemSpdInputs&gt;\n    #        &lt;RegionCollection&gt;\n    #        &lt;TraderCollection&gt;\n    #        &lt;InterconnectorCollection&gt;\n    #        &lt;ConstraintScadaDataCollection&gt;\n    #        &lt;GenericEquationCollection&gt;\n    #        &lt;GenericConstraintCollection&gt;\n    \"\"\"\n\n    print(f'    {file}')\n    tree = et.parse(os.path.join(dir,file))\n    root = tree.getroot()\n\n    pth3 = f'./NemSpdOutputs/TraderSolution[@TraderID=\"{duid}\"]'\n\n    #get gen solution info\n    sol = root.find(pth3)\n    srw = [sol.attrib.get(col) for col in scls]\n\n    return srw\n</code></pre>"},{"location":"Python/Architect/Decorator/","title":"Decorator","text":"<p>https://peps.python.org/pep-0318/</p> <p>https://www.datacamp.com/tutorial/decorators-python</p> <p>Decorators dynamically alter the functionality of a <code>function</code>, <code>method</code>, or <code>class</code> without having to directly use subclasses or change the source code of the function being decorated. - allow adding new functionality to an existing object without modifying its structure - decorators are usually called before the definition of a function that is decorated - application of decorators for a function is from the bottom up - can do preprocessing and after processing of the function call</p>"},{"location":"Python/Architect/Decorator/#function-decorator","title":"Function decorator","text":"<pre><code>import inspect\nimport functools\ndef decorator_maker(dp: str):\n    \"Decorator maker function\"\n    def decorator(func):\n        \"Decorator function\"\n        sig = inspect.signature(func)\n        @functools.wraps(func)        \n        def wrapper(*args, **kwargs):\n            \"Wrapper function\"\n            print(f'decorator maker args[0]: {dp}')\n            bound = sig.bind(*args, **kwargs)\n            kw = bound.arguments.get('kw')\n            print(f'function call kwargs[kw]: {kw}')                                   \n            kw = f'{dp}_{kw.upper()}'   #pre-processing kwargs\n            kwargs['kw'] = kw \n            re = func(*args, **kwargs)  #function call\n            rt = f'{re}_appendix'       #post-processing results\n            kwargs['kw'] = kw \n            return rt\n        return wrapper\n    return decorator\n\n@decorator_maker('prefix')\ndef decorated_func(*, kw: str):\n    print(f'decorated func preprocessed kwargs[kw]: {kw}')\n    return f'decorated func postprocessed result: {kw}'\n\nprint(decorated_func(kw='ok'))\n</code></pre>"},{"location":"Python/Architect/Decorator/#class-decorator","title":"class decorator","text":"<p>https://builtin.com/software-engineering-perspectives/python-class-decorator</p> <p>Class must have  - <code>__init__</code> - <code>__call__</code></p>"},{"location":"Python/Architect/Function/","title":"Function","text":"<p>Functions can passed as an argument, returned from a function, modified, and assigned to a variable. </p>"},{"location":"Python/Architect/Function/#closure","title":"Closure","text":"<p>A nested function (function defined in another function) can access the outer scope of the enclosing function. This pattern is known as a Closure. It is a critical concept in decorators.</p>"},{"location":"Python/Architect/Function/#functoolspartial","title":"functools.partial","text":"<p><code>functools.partial</code> is a powerful function in Python's <code>functools</code> module. It allows you to create a new function by partially applying arguments to an existing function, effectively creating a new function with some of the arguments already set.</p> <p>The <code>functools.partial</code> function returns a new function that can be called with the remaining arguments to complete the call to the original function. <code>functools.partial</code> is particularly useful when you have a function that requires many arguments, and you want to create a simplified version of it by fixing some of the arguments in advance. It's commonly used in functional programming and can make code more concise and easier to read.</p> <p>Here's an example to illustrate its usage: <pre><code>import functools\n\n# Original function\ndef add(a, b):\n    return a + b\n\n# Create a new function with 'a' fixed to 5\nadd_five = functools.partial(add, 5)\n\n# Now, 'add_five' works like a function that takes only one argument 'b'\nresult = add_five(3)  # This is equivalent to calling add(5, 3)\nprint(result)  # Output: 8\n</code></pre></p>"},{"location":"Python/Async/Asyncio/","title":"Asyncio","text":"<p>https://docs.python.org/3/library/asyncio-eventloop.html</p>"},{"location":"Python/Async/Asyncio/#call-async-function","title":"call async function","text":"<pre><code>import asyncio\nasync def async_func():\n    df = pd.read_parquet('data.parquet')\n    return df\n\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    try:\n        df = loop.run_until_complete(async_func())\n    finally:\n        loop.close()\n</code></pre>"},{"location":"Python/Async/Asyncio/#async-to-sync","title":"async to sync","text":"<pre><code>def sync(coro):\n    loop = asyncio.new_event_loop()\n    future = asyncio.run_coroutine_threadsafe(coro, loop)\n    return future.result()\n</code></pre>"},{"location":"Python/Async/Error/","title":"Error","text":""},{"location":"Python/Async/Error/#runtimewarning-coroutine-defaultazurecredentialget_token-was-never-awaited","title":"RuntimeWarning: coroutine 'DefaultAzureCredential.get_token' was never awaited","text":"<p>The coroutine function has never been called correctly. <pre><code>import asyncio\nasync def my_coro():\n    print('Hello there')\ncoro = my_coro() #will throw the warning: only creates a coroutine object not run the coroutine\n\n#correct way to call the coroutine\nasyncio.run(my_coro())\n#or\nawait my_coro()\n</code></pre></p>"},{"location":"Python/Async/Trio/","title":"trio","text":"<p>Trio is a Python library designed for async concurrency,  and it provides a simpler alternative to <code>asyncio</code> for managing concurrent tasks. - https://trio.readthedocs.io/en/stable/ - a friendly Python library for async concurrency and I/O</p>"},{"location":"Python/Async/Trio/#simple-example-running-two-async-tasks-simultaneously","title":"simple example: Running Two Async Tasks Simultaneously","text":"<p>Run two async tasks simultaneously using Trio.  Each task will simulate some work with a <code>sleep</code> and print a message before and after the work.</p> <pre><code>import trio\n\nasync def task_1():\n    print(\"Task 1 starting\")\n    await trio.sleep(2)  # Simulate some work (sleep for 2 seconds)\n    print(\"Task 1 done\")\n\nasync def task_2():\n    print(\"Task 2 starting\")\n    await trio.sleep(1)  # Simulate some work (sleep for 1 second)\n    print(\"Task 2 done\")\n\nasync def main():\n    # Run both tasks concurrently\n    async with trio.open_nursery() as nursery:\n        nursery.start_soon(task_1)\n        nursery.start_soon(task_2)\n\n# Start the Trio event loop and run the main function\ntrio.run(main)\n</code></pre> <p>Explanation: - <code>task_1()</code> and <code>task_2()</code>: These are asynchronous functions that simulate some work by using <code>trio.sleep()</code>. The first task sleeps for 2 seconds, while the second one sleeps for 1 second. - <code>main()</code>: This is the main asynchronous function where we create a nursery (a concept in Trio for managing concurrent tasks). We use <code>nursery.start_soon()</code> to start the two tasks concurrently. - <code>trio.run(main)</code>: This starts the Trio event loop and runs the <code>main</code> function.</p>"},{"location":"Python/Async/Trio/#triolowlevelwait_readable","title":"trio.lowlevel.wait_readable","text":"<p>In Trio, <code>trio.lowlevel.wait_readable</code> is a low-level function used for waiting for a file descriptor  (or any object that behaves like a file descriptor) to become readable.  It's part of the Trio's low-level API, which provides more control over concurrency but requires a deeper understanding of async programming.</p> <p>This function is primarily used when you need to wait for data to become available for reading from a stream or file descriptor,  and it integrates with the Trio event loop to ensure that your program doesn't block while waiting.</p>"},{"location":"Python/Async/Trio/#how-it-works","title":"How it Works","text":"<p><code>trio.lowlevel.wait_readable</code> allows your program to wait until a specified file descriptor (like a socket or a pipe) is ready to be read.  This is useful in scenarios where you're working with custom file descriptors or streams and want to avoid blocking your async code while  waiting for data to be available.</p> <p>The general signature is: <pre><code>trio.lowlevel.wait_readable(fd: int, timeout: Optional[float] = None)\n</code></pre></p> <ul> <li><code>fd</code>: The file descriptor that you want to monitor for readability. It could be any object that supports the <code>fileno()</code> method (e.g., a socket, pipe, or file).</li> <li><code>timeout</code>: (Optional) The number of seconds to wait before the operation times out. If not provided, it will block indefinitely until the file descriptor becomes readable.</li> </ul>"},{"location":"Python/Async/Trio/#example-usage","title":"Example Usage","text":"<p>we'll use <code>trio.lowlevel.wait_readable</code> to wait for data to be available on a socket:</p> <pre><code>import trio\nimport socket\n\nasync def handle_client(client_socket):\n    print(\"Waiting for data...\")\n    # Wait until the client socket is readable\n    await trio.lowlevel.wait_readable(client_socket)\n    data = client_socket.recv(1024)\n    print(f\"Received data: {data.decode()}\")\n    client_socket.close()\n\nasync def server():\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.bind(('localhost', 12345))\n    server_socket.listen(5)\n    print(\"Server listening on port 12345...\")\n\n    while True:\n        client_socket, client_address = await trio.to_thread.run_sync(server_socket.accept)\n        print(f\"Accepted connection from {client_address}\")\n        # Handle each client in a separate task\n        trio.spawn(handle_client, client_socket)\n\nasync def main():\n    await server()\n\ntrio.run(main)\n</code></pre> <p>Explanation: - Server Setup: A server socket is created that listens on port <code>12345</code>. When a client connects, the server accepts the connection and hands it off to a new task for processing. - <code>handle_client</code>: This is the function that handles the client connection. It first waits until the client socket is readable using <code>trio.lowlevel.wait_readable</code>. - Waiting for Readability: <code>await trio.lowlevel.wait_readable(client_socket)</code> will block until there is data available on the <code>client_socket</code> to read. - Receiving Data: Once the socket is readable, it reads the incoming data with <code>recv(1024)</code> and prints it.</p>"},{"location":"Python/Async/Trio/#when-to-use-wait_readable","title":"When to Use <code>wait_readable</code>:","text":"<ul> <li>Custom Streams: If you are working with custom streams or file descriptors (like sockets, pipes, or file objects) and need more control over when they are ready to be read or written.</li> <li>Low-Level Control: When you need low-level control over file descriptors or sockets beyond the higher-level Trio abstractions like <code>trio.open_nursery()</code> or <code>trio.Stream</code>.</li> <li>Non-blocking I/O: To avoid blocking the event loop while waiting for external resources (like a socket) to be ready for reading.</li> </ul>"},{"location":"Python/Async/Trio/#considerations","title":"Considerations:","text":"<ul> <li><code>wait_readable</code> is low-level: For most high-level use cases, you'll be better off using Trio's <code>trio.open_nursery()</code> with async stream abstractions (like <code>trio.SocketStream</code>), which manage readability and writability for you.</li> <li>Timeouts: If you want to add a timeout, you can pass the <code>timeout</code> argument (in seconds), and it will raise a <code>TimeoutError</code> if the file descriptor isn't ready within that time.</li> </ul> <p>In summary, <code>trio.lowlevel.wait_readable</code> is a powerful tool for handling readiness notifications of file descriptors or streams, but it is typically used when you need low-level control or need to interact with legacy code that uses raw file descriptors.</p>"},{"location":"Python/Blog/BestPractices/","title":"Python Programing Bets Practice","text":""},{"location":"Python/Blog/BestPractices/#make-action-on-your-goal","title":"make action on your goal","text":"<p>Don't just read -- read, create, then think about it and improve it.</p> <p>learning pandas? practice things like dataframe creation, filtering, replacing and adding columns, groupby-agg, join/merge, pivot, chained method.</p> <p>learn1 to build a conda package? create a mini example with configurations and run it.</p>"},{"location":"Python/Blog/CachesinPython/","title":"Caches used in Python","text":""},{"location":"Python/Blog/CachesinPython/#different-cache-packages","title":"different cache packages","text":"<ul> <li>when to use which</li> <li>which is better</li> </ul>"},{"location":"Python/Blog/GroupbyApplyPerf/","title":"How I reduced a Python app run time from two hours to 20 seconds?","text":"<p>Pandas <code>df.groupby.apply</code> is too slow for two Dataframes</p> <p>We have a Python app that was too slow. It took about two hours to extract product forecast data from a database and to merge it with actual records. After some refactorization and optimization, I managed to reduce the run time to less than 20 seconds.</p> <p>Assume we have some products and each has some daily sales revenue, the actual records. We also have daily forecast revenue. The task is to merge the actual and forecast data together.</p> <p>Once there are missing records in the actual data, we believe that the actual revenue from that day are not reliable and they should be replaced with forecast data.</p> <p>To finish this task, for each product, we need to first find the last consecutive date in the actual data and then get the forecast data after that date so we can merge the actual and forecast data together.</p>"},{"location":"Python/Blog/GroupbyApplyPerf/#dummy-data-for-testing","title":"Dummy data for testing","text":"<p>The performance of different implementations has been tested using some dummy data. I created dummy data using a function <code>gen_rand_df</code> that is described in my previous post. I also used a function <code>explode_date_range</code> in my another post to explode date ranges.</p> <p>Firstly, we create some product info with <code>product_id</code>, <code>start_date</code> and <code>end_date</code> for the actual sales records and expand the date ranges to daily records. <pre><code>nrow = 5000\nd1 = gen_rand_df(\n    nrow=nrow,\n    str_cols={\n        'count': 1,\n        'name': 'product_id',\n        'str_len': 10,\n        'str_cnt': nrow,\n    },\n    ts_cols={\n        'count': 2,\n        'name': ['start_date', 'end_date'],\n        'start_date': '2025-01-01',\n        'end_date': '2035-01-01',\n        'freq': 'D',\n        'random': True,\n    },\n)\ndf1 = explode_date_range(\n    df=d1.query('start_date &lt; end_date').drop_duplicates(subset=['product_id']),\n    start_date_col='start_date',\n    end_date_col='end_date',\n    freq='D',\n)\n</code></pre></p> <p>Secondly, we create some sales forecast info for products that has actual sales data. <pre><code>d2 = gen_rand_df(\n    nrow=nrow,\n    str_cols={\n        'count': 1,\n        'name': 'product_id',\n        'col_strs': d1['product_id'].unique(),\n    },\n    ts_cols={\n        'count': 2,\n        'name': ['start_date', 'end_date'],\n        'start_date': '2025-01-01',\n        'end_date': '2035-01-01',\n        'freq': 'D',\n        'random': True,\n    },\n)\ndf2 = explode_date_range(\n    df=d2.query('start_date &lt; end_date').drop_duplicates(subset=['product_id']),\n    start_date_col='start_date',\n    end_date_col='end_date',\n    freq='D',\n)\n</code></pre></p> <p>Then, we create some dummy product sales revenue for both the actual and forecast records. <pre><code>d3 = gen_rand_df(\n    nrow=max(df1.shape[0], df2.shape[0]),\n    float_cols={\n        'count': 2,\n        'name': ['daily_revenue1', 'daily_revenue2'],\n        'low': 0,\n        'high': 1e3,\n        'missing_pct': [0.1, 0],\n    },\n)\n</code></pre></p> <p>Finally, we add the sales revenue to the actual and forecast data. <pre><code>df_actual = (\n    df1\n    .assign(daily_revenue=d3['daily_revenue1'].values[:df1.shape[0]])\n    .set_index(['product_id', 'daily_revenue'])\n)\ndf_forecast = (\n    df2\n    .assign(daily_revenue=d3['daily_revenue2'].values[:df2.shape[0]])\n    .set_index(['product_id', 'daily_revenue'])\n)\n</code></pre></p> <p>Here are the first few lines of the actual sales data: <pre><code>                      daily_revenue\nproduct_id date\nP3hLcLj43u 2025-01-26    128.570203\n           2025-01-27    499.277862\n           2025-01-28    601.498358\n</code></pre></p>"},{"location":"Python/Blog/GroupbyApplyPerf/#getting-last-consecutive-date","title":"Getting last consecutive date","text":"<p>The function used to get the last consecutive date from a date series has been implemented as follows: <pre><code>def get_last_consecutive_date(dates: pd.Series) -&gt; np.datetime64 | None:\n    # Empty input\n    if dates.empty:\n        return None\n\n    dates = np.unique(dates)\n\n    # Only one unique element in the list\n    if len(dates) == 1:\n        return dates[0]\n\n    diffs = np.diff(dates).astype('timedelta64[D]').astype(int)\n    last_consecutive_day_index = np.where(diffs &gt; 1)[0]\n    if len(last_consecutive_day_index) == 0:\n        return dates[-1] # all dates are consecutive\n    else:\n        return dates[last_consecutive_day_index[0]]\n</code></pre></p>"},{"location":"Python/Blog/GroupbyApplyPerf/#using-pandas-dfgroupbyapply","title":"Using Pandas <code>df.groupby.apply</code>","text":"<p>As we have to perform the same task for each group of products, naturally we can use Pandas <code>df.groupby.apply</code>. But this function generally only works for one DataFrame. Here we have two DataFrames and one option is passing the second DataFrame as a parameter.</p> <p>Here is the implementation: <pre><code>def keep_records_after_consecutive_dates_v1(\n    df_forecast: pd.DataFrame,\n    df_actual: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    product_id = df_forecast.index.values[0][df_forecast.index.names.index('product_id')]\n    dates = df_actual.query('product_id == @product_id &amp; daily_revenue.notna()').index.unique('date')\n    # Get last consecutive date and filter df_forecast\n    if len(dates) &gt; 0:\n        last_consecutive_date = get_last_consecutive_date(dates)\n        df_forecast = df_forecast.query('date &gt; @last_consecutive_date')\n    return df_forecast\n\ndf_v1 = (\n    df_forecast\n    .groupby('product_id', group_keys=False)\n    .apply(keep_records_after_consecutive_dates_v1, df_actual)\n)\n</code></pre> The run time is 327 seconds.</p>"},{"location":"Python/Blog/GroupbyApplyPerf/#avoiding-repeated-query-and-filtering","title":"Avoiding repeated query and filtering","text":"<p>By checking the previous implementation, we can observe that we have a repeated query and filtering for each product on the actual sales DataFrame. That is likely to slow down the process.</p> <p>Now we do the query for all products and group the product records in advance. Hopefully this will make it much faster.</p> <p>Here is the updated version: <pre><code>def keep_records_after_consecutive_dates_v2(\n    df_forecast: pd.DataFrame,\n    df_actual: DataFrameGroupBy,\n) -&gt; pd.DataFrame:\n    product_id = df_forecast.index.values[0][df_forecast.index.names.index('product_id')]\n    if product_id in df_actual.groups:\n        dates = df_actual.get_group(product_id).index.unique('date')\n        # Get last consecutive date and filter df_forecast\n        if len(dates) &gt; 0:\n            last_consecutive_date = get_last_consecutive_datex(dates)\n            df_forecast = df_forecast.query('date &gt; @last_consecutive_date')\n    return df_forecast\n\ngrp_actual = df_actual.query('daily_revenue.notna()').groupby('product_id')\ndf_v2 = (\n    df_forecast\n    .groupby('product_id', group_keys=False)\n    .apply(keep_records_after_consecutive_dates_v2, grp_actual)\n)\n</code></pre> Now the run time is 17.6 seconds --- that's about 18x faster.</p>"},{"location":"Python/Blog/GroupbyApplyPerf/#using-a-python-for-loop","title":"Using a Python for-loop","text":"<p>The <code>.apply()</code> often has some overhead compared to a pure Python for-loop. We now replace the <code>.apply()</code> with a for-loop. At the same time we can remove the index parsing for all product groups. <pre><code>def keep_records_after_consecutive_dates_v3(\n    df_forecast: pd.DataFrame,\n    df_actual: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    dates = df_actual.index.unique('date')\n    # Get last consecutive date and filter df_forecast\n    if len(dates) &gt; 0:\n        last_consecutive_date = get_last_consecutive_datex(dates)\n        df_forecast = df_forecast.query('date &gt; @last_consecutive_date')\n    return df_forecast\n\ndfs = []\ngrp_actual = df_actual.query('daily_revenue.notna()').groupby('product_id')\ngrp_forecast = df_forecast.groupby('product_id')\nproduct_ids = df_forecast.index.unique('product_id')\nfor product_id in product_ids:\n    if product_id in grp_actual.groups:\n        df = keep_records_after_consecutive_dates_v3(\n            grp_forecast.get_group(product_id),\n            grp_actual.get_group(product_id),\n        )\n    else:\n        df = grp_forecast.get_group(product_id)\n    dfs.append(df)\ndf_v3 = pd.concat(dfs, axis=0)\n</code></pre> The run time is 9.8 seconds --- that's about 1.8x faster than version #2.</p>"},{"location":"Python/Blog/GroupbyApplyPerf/#vectorized-process-without-for-loop","title":"Vectorized process without for-loop","text":"<p>It's obvious that we can vectorize the calculation of the last consecutive date for all products. Pandas <code>groupby().apply()</code> on a Series can be very efficient, as it often operates on NumPy arrays internally.</p> <p>We can also avoid the for-loop by using vectorized join and filtering operations. By doing that we don't need to join small DataFrames for all products using <code>pd.concat</code>.</p> <p>The final optimized version is showing as follows: <pre><code>def keep_records_after_consecutive_dates_v4(\n    df_forecast: pd.DataFrame,\n    df_actual: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    # Get last consecutive date for each product\n    last_consecutive_dates = (\n        df_actual\n        .query('daily_revenue.notna()')\n        .reset_index('date')\n        .groupby('product_id')['date']\n        .apply(get_last_consecutive_date)\n        .to_frame()\n        .rename(columns={'date': 'last_consecutive_date'})\n    )\n    # Filter df_forecast\n    df_forecast = (\n        df_forecast\n        .join(last_consecutive_dates, on='product_id', how='left')\n        .fillna({'last_consecutive_date': pd.Timestamp.min})\n        .query('date &gt; last_consecutive_date')\n        .drop(columns='last_consecutive_date')\n    )\n    return df_forecast\n\ndf_v4 = keep_records_after_consecutive_dates_v4(df_forecast, df_actual)\n</code></pre> The final run time is 2.4 seconds - that's about 4x faster than version #3 and about 130x faster than the original version #1 (327 seconds).</p>"},{"location":"Python/Blog/GroupbyApplyPerf/#summary","title":"summary","text":"<p>By avoiding repeated query and filtering operations and vectorization of some other operations, I successfully made a Python process running 130x faster. I also did some similar optimization for extracting forecast data from a database. Ultimately the application total execution time was reduced from over two hours to less than 20 seconds.</p> <p>Want to know more tips about coding and other things, please visit: https://github.com/seanslma/maki</p>"},{"location":"Python/Blog/NumbaPerf/","title":"Make Python Loops 5x to 10x Faster Using Numba","text":"<p>Numba is a just-in-time (JIT) compiler for python that translates python code into highly optimized machine code at runtime. It can significantly improve the performance of numerical computations by enabling high-performance execution of functions, particularly those that make heavy use of numpy arrays.</p> <p>Here we will first briefly explain key features of numba and when to use it, and then provide an example demonstrating how to accelerate code performance by leveraging various numba features. If you are already familiar with numba, go directly to the third section about the demonstration.</p>"},{"location":"Python/Blog/NumbaPerf/#key-features-of-numba","title":"Key features of numba","text":"<ul> <li>JIT compilation: Numba compiles python functions into machine code, allowing for efficient code generation tailored to specific hardware and data types.</li> <li>Numerical acceleration: Numba is particularly well-suited for numerical computations involving arrays and mathematical operations. It can often achieve performance comparable to compiled languages like C or Fortran.</li> <li>Compatibility with numpy: Numba seamlessly integrates with numpy, to accelerate numpy functions and operations, making them much faster.</li> <li>Parallel computing: Numba supports parallel execution on multi-core CPUs and GPUs, enabling us to leverage the power of parallel hardware to speed up computations.</li> <li>Custom UDFs: We can create custom user-defined functions (UDFs) in numba and use them within our python code. These UDFs can be compiled and optimized for performance.</li> </ul>"},{"location":"Python/Blog/NumbaPerf/#when-to-use-and-to-avoid-numba","title":"When to use and to avoid numba","text":"<p>Numba is particularly well-suited for numerical computations involving arrays and mathematical operations. Here are some specific cases where we should consider using numba:</p> <ul> <li>Array operations: If our code heavily involves operations on numpy arrays, such as element-wise arithmetic, matrix multiplication, or reductions, numba can significantly accelerate these computations.</li> <li>Mathematical functions: Numba can optimize calls to mathematical functions like <code>sin</code>, <code>cos</code>, <code>exp</code>, and <code>log</code>, providing a performance boost compared to their python counterparts.</li> <li>Custom functions: If we have custom functions that perform numerical calculations, numba can compile them into machine code for improved efficiency.</li> <li>Loops: Numba can often optimize loops that iterate over arrays or perform numerical calculations within the loop body.</li> </ul> <p>However, not all python code can be optimized using numba and thus improve the performance. There are some limitations to consider before using numba:</p> <ul> <li>I/O bound operations: Numba will not help much with operations that are I/O bound, such as reading/writing files or network operations.</li> <li>Dynamic python features: If our code relies heavily on python's dynamic features (like modifying functions at runtime), numba may not be suitable, as it works best with statically typed, straightforward code.</li> <li>Non-numerical code: For code that does not involve numerical calculations or array manipulations, other optimization techniques may be more appropriate.</li> <li>Numba can introduce overhead: If we are working with small datasets or functions that run very quickly, the overhead of JIT compilation might outweigh the performance benefits.</li> </ul> <p>To determine whether numba is appropriate for our use case, we can:</p> <ul> <li>Profile our code: Use profiling tools to identify the bottlenecks in our code and see if they involve numerical computations.</li> <li>Try numba and measure the performance: Experiment with numba and compare the performance of our code with and without numba.</li> <li>Consider the trade-offs: Weigh the potential performance benefits against the overhead and limitations of using numba.</li> </ul> <p>Overall, if we have numerical or scientific computations that need to be optimized, numba is a powerful tool that can lead to significant performance improvements with minimal code changes.</p>"},{"location":"Python/Blog/NumbaPerf/#data-for-testing-demonstration","title":"Data for testing demonstration","text":"<p>Let's create a 2D numpy array filled with randomly generated data. Each row represents a scenario and here we will calculate the distance between any two scenarios. <pre><code>import numpy as np\nnp.random.seed(11)\narr = np.random.rand(100, 1000)\n</code></pre></p>"},{"location":"Python/Blog/NumbaPerf/#initial-version","title":"Initial version","text":"<p>We calculate the distance between two scenarios using the 1-norm, which measures the sum of the absolute differences between corresponding elements. <pre><code>def calculate_distances1(arr):\n    m = arr.shape[0]\n    n = arr.shape[1]\n    dist_arr = np.zeros((m, m))\n    for i in range(m):\n        for j in range(i):\n            v = 0.0\n            for k in range(n):\n                 v += abs(arr[i, k] - arr[j, k])\n            dist_arr[i, j] = v\n            dist_arr[j, i] = v\n    return dist_arr\n# 2.68 s \u00b1 16.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre> The run time is about 2.68 seconds, for 100 scenarios. If we have 1000 scenarios the time would be 268 seconds. That is too slow and we must improve the performance.</p>"},{"location":"Python/Blog/NumbaPerf/#using-numpy-function","title":"Using numpy function","text":"<p>Here we update the code to calculate the 1-norm using the numpy function <code>np.linalg.norm()</code>. <pre><code>def calculate_distances2(arr):\n    m = arr.shape[0]\n    dist_arr = np.zeros((m, m))\n    for i in range(m):\n        for j in range(i):\n            dist_arr[i, j] = np.linalg.norm(arr[i] - arr[j], 1)\n            dist_arr[j, i] = dist_arr[i, j]\n    return dist_arr\n# 40.7 ms \u00b1 1.50 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre> Now the run time is 40.7 ms - about <code>65x</code> faster! As the numpy function is implemented in C, there is no surprise that the performance has been improved significantly.</p>"},{"location":"Python/Blog/NumbaPerf/#using-numbanjit","title":"Using numba.njit","text":"<p>Can we improve the performance further? Yes, by using numba, definitely we can. <pre><code>from numba import njit\n@njit(cache=True)\ndef calculate_distances3(arr):\n    ...\n# 10.9 ms \u00b1 507 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre> It is great that there is a <code>4x</code> performance improvement with numba on the numpy function.</p> <p>The numba <code>njit</code> decorator is used to compile the python function to optimized machine code in nopython mode. We can also use the <code>jit</code> decorator, which allows the function to fall back to the original python implementation if numba cannot compile it.</p> <p>When we set <code>cache=True</code>, numba stores the compiled function in a cache on disk. So the next time we execute the script, it can load the precompiled function, avoiding the overhead of recompilation.</p>"},{"location":"Python/Blog/NumbaPerf/#using-numbanjit-with-data-types","title":"Using numba.njit with data types","text":"<p>Can we do it better? Yes, we need to use numba data type signature. <pre><code>@njit('float64[:,::1](float64[:,::1])', cache=True)\ndef calculate_distances4(arr):\n    ...\n# 10.5 ms \u00b1 208 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre> We explicitly set the data types of the input parameters and the output. In this case, there is only minor performance improvement, most likely that numba can infer data types even without data type signature. More details about the numba data type signature can be found in the numba documents (see References section).</p> <p>In generall, by specifying data types, numba can generate more efficient machine code. Knowing the exact types allows it to optimize the generated code for those types, leading to faster execution and improved memory management.</p>"},{"location":"Python/Blog/NumbaPerf/#replacing-numpy-function-with-a-python-loop","title":"Replacing numpy function with a python loop","text":"<p>As numba is good for loops, here we will replace the numpy function by a <code>python loop</code> to further boost performance. <pre><code>@njit('float64[:,::1](float64[:,::1])', cache=True)\ndef calculate_distances5(arr):\n    m = arr.shape[0]\n    n = arr.shape[1]\n    dist_arr = np.zeros((m, m))\n    for i in range(m):\n        for j in range(i):\n            v = 0.0\n            for k in range(n):\n                 v += abs(arr[i, k] - arr[j, k])\n            dist_arr[i, j] = v\n            dist_arr[j, i] = v\n    return dist_arr\n# 8.20 ms \u00b1 163 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre> Numba is indeed good for loops. There is a <code>1.2x</code> performance improvement now, and it's about <code>5x</code> faster than the numpy version.</p>"},{"location":"Python/Blog/NumbaPerf/#using-numbanjit-parallel-mode","title":"Using numba.njit parallel mode","text":"<p>Modern computers often have multiple cores. By leveraging parallel computing, we can significantly reduce execution time. <pre><code>from numba import njit, prange\n@njit('float64[:,::1](float64[:,::1])', cache=True, parallel=True, nogil=True)\ndef calculate_distances6(arr):\n    m = arr.shape[0]\n    n = arr.shape[1]\n    dist_arr = np.zeros((m, m))\n    for i in prange(m):\n        for j in range(i):\n            v = 0.0\n            for k in range(n):\n                 v += abs(arr[i, k] - arr[j, k])\n            dist_arr[i, j] = v\n            dist_arr[j, i] = v\n    return dist_arr\n# 3.68 ms \u00b1 157 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre> Here we update the code to use numba <code>parallel mode</code> with the help of the <code>prange</code> function.</p> <p>By setting <code>parallel=True</code>, numba's JIT compiler will analyze the function's code and automatically identify opportunities for parallelization, especially within loops. However, using <code>prange</code> provides more explicit control over parallelization and can be more effective in certain cases.</p> <p>Finally the run time is 3.68 ms (4 cpu cores). It is about <code>10x</code> faster compared to the numpy function version without using numba.njit (40.7 ms). It is about <code>700x</code> faster compared to the raw python code (2.68 seconds).</p>"},{"location":"Python/Blog/NumbaPerf/#references","title":"References","text":"<ul> <li>Numba data type signature</li> <li>Numba data type signature caveats</li> <li>Optimizing python loops using numba</li> </ul>"},{"location":"Python/Blog/PackageManagement/","title":"How to manage python package dependencies","text":"<p>The package management is actually one of the most challenging part of Python programming.</p>"},{"location":"Python/Blog/PackageManagement/#pin-package-version","title":"pin package version","text":"<p>due to bugs or significant changes in new versions</p>"},{"location":"Python/Blog/PackageManagement/#one-pinned-version-might-block-many-packages-to-be-updated","title":"one pinned version might block many packages to be updated","text":""},{"location":"Python/Blog/PackageManagement/#worse-there-are-dependency-conflicts","title":"worse - there are dependency conflicts","text":""},{"location":"Python/Blog/PackageManagement/#solution","title":"solution","text":"<ul> <li>temporally pin a package version till the system is broken</li> <li>find a workaround for the bug so we can still use the new version</li> </ul>"},{"location":"Python/Blog/PackageManagement/#best-practices-for-pinning-python-dependencies","title":"Best Practices for Pinning Python Dependencies","text":"<p>When building Python applications into docker images, ideally we should always update to the latest version of the packages that we use in the application. However, in reality there are many issues related to the packages and we are forced to pin some of the packages to a specific version. Here based on my work experience I will discuss the best practices for pinning your Python dependencies - what you should and should not do and how to do it correctly.</p>"},{"location":"Python/Blog/PackageManagement/#record-all-package-versions","title":"Record all package versions","text":"<p>Consider using a lock file even with pip: Generate a requirements.txt file with specific versions for more control.</p>"},{"location":"Python/Blog/PackageManagement/#dont-over-pin","title":"Don't over-pin","text":"<p>Allow some flexibility for minor or patch updates to benefit from bug fixes and security patches.</p> <p>Specify version ranges (e.g., &gt;=1.2.3,&lt;2.0.0) to allow updates within compatible ranges. </p>"},{"location":"Python/Blog/PackageManagement/#regularly-review-and-update-pinned-versions","title":"Regularly review and update pinned versions","text":"<p>Stay current with security patches and bug fixes.</p>"},{"location":"Python/Blog/PackageManagement/#avoid-major-changes-if-there-are-issues","title":"Avoid major changes if there are issues","text":"<p>New major release usually has many new features and changes - will break your apps test locally using the new version first</p>"},{"location":"Python/Blog/PackageManagement/#only-pinning-package-versions-as-a-temporal-solution","title":"Only pinning package versions as a temporal solution","text":"<p>if no workaround, pin it major versions due to large changes - update the changes later need to check the latest version</p>"},{"location":"Python/Blog/PandasExplodeDateRange/","title":"How to explode date ranges in a Pandas DataFrame 30x faster","text":"<p>During data analysis, it is very common that we need to convert our data to the interval resolution from a lower time resolution such as quarterly or monthly to half hourly data.</p> <p>We can do the conversion easily in Python using pandas. However, we know that the pandas <code>df.explode</code> function is very slow. Here I will show how we can make this process 30x faster without using another Python package.</p>"},{"location":"Python/Blog/PandasExplodeDateRange/#polars-version","title":"polars version","text":"<p>https://stackoverflow.com/questions/73161185/repeating-a-date-in-polars-and-exploding-it</p> <p>is it faster?</p>"},{"location":"Python/Blog/PandasExplodeDateRange/#pandas-dataframe-for-testing","title":"Pandas DataFrame for testing","text":"<p>For testing the code performance, I used the <code>gen_rand_df</code> function in my previous post to create a dummy pandas DataFrame: <pre><code>df = gen_rand_df(\n    nrow=100,\n    str_cols={\n        'count': 2,\n        'name': ['id', 'category'],\n        'str_len': [8, (5,20)],\n        'str_count': [100, 30],\n    },\n    ts_cols={\n        'count': 2,\n        'name': ['start_date', 'end_date'],\n        'start_date': ['2020-01-01', '2023-01-01'],\n        'end_date': ['2023-01-01', '2025-01-01'],\n        'freq': 'MS',\n        'random': True,\n    },\n    float_cols={\n        'count': 2,\n        'low': 0.0,\n        'high': 100.0,\n        'missing_pct': 0.1,\n    },\n)\n</code></pre> Here are the first two rows of the 100 rows of the created DataFrame: <pre><code>         id    category start_date   end_date        f1         f2\n0  8v5KSoKX       jIMki 2020-01-01 2023-07-01  35.20661  76.041564\n1  ihXEKLSb  bws6TOEr06 2020-05-01 2023-02-01       NaN  26.725758\n</code></pre></p>"},{"location":"Python/Blog/PandasExplodeDateRange/#initial-solution-from-chatgpt-and-google-gemini","title":"Initial solution from ChatGPT and Google Gemini","text":"<p>We need to explode the date range (from start_date to end_date) of each row in the DataFrame to half hourly and keep all other columns.</p> <p>To do that, I got solutions from ChatGPT and Google Gemini after a few iterations (they are basically the same): <pre><code>df['ts'] = df.apply(lambda row:\n    pd.date_range(row['start_date'], row['end_date'], freq='30min'), axis=1\n)\ndf = df.explode('ts')\n</code></pre> And the first two rows of the result DataFrame are: <pre><code>         id category start_date   end_date        f1         f2                  ts\n0  8v5KSoKX    jIMki 2020-01-01 2023-07-01  35.20661  76.041564 2020-01-01 00:00:00\n0  8v5KSoKX    jIMki 2020-01-01 2023-07-01  35.20661  76.041564 2020-01-01 00:30:00\n</code></pre></p> <p>The solution works but it is very slow. The time for creating the <code>ts</code> column is <code>703 ms \u00b1 6.57 ms</code> and exploding is <code>691 ms \u00b1 7.47 ms</code>, for a DataFrame with only 100 rows.</p> <p>I tried different prompts to get a faster solution from the AI applications but failed; the solution either is wrong or has errors. My suggestion would be that only use the AI applications to give you some ideas or a draft solution. The best solution can only be created by a person with some knowledge in that domain.</p>"},{"location":"Python/Blog/PandasExplodeDateRange/#using-a-for-loop-instead-of-the-dfapply-function","title":"Using a <code>for-loop</code> instead of the <code>df.apply</code> function","text":"<p>We know that the <code>df.apply</code> is slow so I will replace it by a <code>for-loop</code>. There are a couple of ways to iterate over the DataFrame rows. Let us check them: <pre><code># 351 \u00b5s \u00b1 57.8 \u00b5s\nfor (_, row) in df.iterrows(): pass\n# 271 \u00b5s \u00b1 65.7 \u00b5s\nfor row in df.to_records(index=False): pass\n# 26.5 \u00b5s \u00b1 12.9 \u00b5s\nfor start, end in zip(df['start_date'], df['end_date']): pass\n# 10.4 \u00b5s \u00b1 2.55 \u00b5s\nfor start, end in zip(df['start_date'].values, df['end_date'].values): pass\n</code></pre> The last version is 34x faster than <code>df.iterrows()</code>. The improvement will be even larger for a DataFrame with many more rows.</p> <p>The improved version for creating the <code>ts</code> column is: <pre><code>df['ts'] = [\n    pd.date_range(start, end, freq='30min')\n    for start, end in zip(df['start_date'].values, df['end_date'].values)\n]\n</code></pre> Now the time for the improved version is <code>684 ms \u00b1 12.6 ms</code>; it is still too slow.</p>"},{"location":"Python/Blog/PandasExplodeDateRange/#implementing-a-custom-dfexplode-function","title":"Implementing a custom <code>df.explode</code> function","text":"<p>Seems there is not much we can do for creating the <code>ts</code> column much faster.</p> <p>Now let us be focusing on the <code>df.explode</code> part. We will implement our own version for exploding the lists in the <code>ts</code> column.</p> <p>We know that the <code>df.reindex</code> function can be used to resample rows of a DataFrame based on provided new index. Here we will use this function to implement a new <code>explode</code> function.</p> <p>First we can create the new index and <code>ts</code> column, using <code>pd.concat</code> to merge the DataFrames created from each row: <pre><code>d = (\n    df\n    .get(['ts'])\n    .reset_index(drop=True)\n    .rename_axis('i', axis=0)\n    .reset_index()\n)\ndt = pd.concat([\n    pd.DataFrame({'i': i, 'ts': ts})\n    for (i, ts) in zip(d['i'].values, d['ts'].values)\n]).set_index('i').rename_axis(None, axis=0)\n</code></pre></p> <p>Then we use the <code>df.reindex</code> function to sample the other columns in the original DataFrame and add the exploded <code>ts</code> column: <pre><code>df = df.drop(columns='ts').reindex(dt.index)\ndf['ts'] = dt.ts\n</code></pre></p> <p>Putting the two parts together, here is the custom <code>explode</code> function: <pre><code>def explode_df_column(df):\n    dt = pd.concat([\n        pd.DataFrame({'i': i, 'ts': ts})\n        for (i, ts) in enumerate(df['ts'].values)\n    ]).set_index('i').rename_axis(None, axis=0)\n    df = df.drop(columns='ts').reindex(dt.index)\n    df['ts'] = dt.ts\n    return df\n</code></pre></p> <p>The time for this function is <code>22 ms \u00b1 285 \u00b5s</code>, 30x faster compared to the <code>df.explode</code> function that has a time of <code>691 ms \u00b1 7.47 ms</code>.</p>"},{"location":"Python/Blog/PandasExplodeDateRange/#creating-a-ts-column-with-value-of-lists-not-required","title":"Creating a <code>ts</code> column with value of lists not required","text":"<p>For our use case, creating a intermediate column with a list of timestamps for each value is not required. We can merge this step into the step for creating the <code>dt</code> DataFrame.</p> <p>Here is the final solution based on this idea: <pre><code># Create a DataFrame with new index and the 30min ts column\ndt = pd.concat([\n    pd.DataFrame({'i': i, 'ts': pd.date_range(start, end, freq='30min')})\n    for i, (start, end) in enumerate(zip(df['start_date'], df['end_date']))\n]).set_index('i').rename_axis(None, axis=0)\n\n# Resample original df based on new index and add the exploded ts column\ndf = df.reindex(dt.index).assign(ts=dt.ts)\n</code></pre></p> <p>Great! The time for this solution is <code>49.8 ms \u00b1 932 \u00b5s</code>, about 30x faster than the initial solution that has a time of <code>1.394s</code> (<code>703 ms \u00b1 6.57 ms</code> + <code>691 ms \u00b1 7.47 ms</code>).</p> <p>We can wrap the method into a function and add other parameters used for limiting the min/max datetime and keeping the original DataFrame index or not. It is up to you to do the remaining work.</p> <p>In summary, we improved a method 30x faster, used to explode datetime ranges in a DataFrame to a new timestamp column and copy other columns. At the same time, we created a new function that is also about 30x faster than the pandas <code>df.explode</code> function.</p>"},{"location":"Python/Blog/PandasTestDataFrame/","title":"How to create dummy Pandas DataFrames for testing","text":"<p>When working with Pandas in Python, constantly we need some DataFrames for different purposes, such as testing Python code to check errors and benchmarking code performance.</p> <p>Here I will explain how to create dummy Pandas DataFrames with the help of AI. At the end, I will put the code for generating different data type columns together so we can use it everywhere to create DataFrames when needed.</p>"},{"location":"Python/Blog/PandasTestDataFrame/#dataframe-column-data-types","title":"DataFrame column data types","text":"<p>The data types commonly used in a DataFrame are <code>string</code>, <code>datetime</code>, <code>integer</code>, and <code>float</code>. All other data types can be converted from these types. For example we can easily convert the integer type to bool and convert the string type to categorical.</p> <p>We will use AI to help us generate the draft code for each of the data types and then test and modify the code to ensure it works as expected. The prompts and the replies will not be included here. You should always try it yourself if interested.</p>"},{"location":"Python/Blog/PandasTestDataFrame/#creating-a-string-list","title":"Creating a string list","text":"<p>To create a list of strings with random lengths, we should be able to control the number of strings and the minimum and maximum string lengths. We should also be able to restrict the character set from which the strings are created.</p> <p>To create a random string, we can first generate a random integer to represent the string length and then randomly select characters from the provided character set. <pre><code>characters = string.ascii_letters + string.digits\nstring_length = random.randint(min_length, max_length)\nrandom_string = ''.join(random.choices(characters, k=string_length))\n</code></pre></p> <p>We can then use a list comprehension or a for loop to create a list of random strings. An alternative way is using <code>numpy</code> for generating random numbers and selecting the characters.</p>"},{"location":"Python/Blog/PandasTestDataFrame/#creating-a-datetime-list","title":"Creating a datetime list","text":"<p>Assume we will create a datetime range for one year with a resolution unit of second.</p> <p>Using <code>pandas</code>, this is straightforward: <pre><code>timestamps = pd.date_range(\n    start='2020-01-01',\n    end='2021-01-01',\n    freq='s',\n    inclusive='left',\n    unit='s',\n)\n</code></pre></p> <p>To create a datetime range we can use <code>numpy</code> as well: <pre><code>start = np.datetime64('2020-01-01', 's').astype(int)\nstop = np.datetime64('2021-01-01', 's').astype(int)\ntimestamps = np.arange(\n    start=start,\n    stop=stop,\n    step=(stop - start)//(366*24*3600),\n).astype('datetime64[s]')\n</code></pre></p> <p>The <code>pandas</code> version will be used as it is much faster and more convenient to generate datetime ranges with different frequencies.</p> <p>If needed, random datetime values between a range can be generated by randomly selecting datetime values from the generated datetime list.</p>"},{"location":"Python/Blog/PandasTestDataFrame/#creating-an-integerfloat-list","title":"Creating an integer/float list","text":"<p>The <code>random.randint</code> and <code>random.uniform</code> functions can be used to generate random integer and float numbers as well.</p> <p>As <code>numpy.random</code> is built on top of <code>random</code> and offers a much richer set of features, we will use the <code>numpy.random</code> module to generate random numbers.</p> <pre><code>rng = np.random.default_rng(seed=rand_seed)\nintegers = rng.integers(low=low, high=high, size=size)\nfloats = rng.uniform(low=low, high=high, size=size)\n</code></pre>"},{"location":"Python/Blog/PandasTestDataFrame/#creating-a-list-with-missing-values","title":"Creating a list with missing values","text":"<p>Sometimes our data has missing values. To generate some missing values we can set the percentage of the missing values. We can then determine if a value should be missing or not based on the uniform value assigned to that value. <pre><code>mask = rng.uniform(size=len(vals)) &lt;= missing_pct\nvals[mask] = missing_val\n</code></pre></p> <p>It is worth noting that different types of data have different types of missing values. For a string type the missing value is <code>None</code>, and <code>np.datetime64('NaT')</code> for the datetime, <code>np.nan</code> for the float. We need to convert the integer data to float before assigning the missing values.</p>"},{"location":"Python/Blog/PandasTestDataFrame/#creating-a-pandas-dataframe","title":"Creating a Pandas DataFrame","text":"<p>To create a DataFrame, we need to specify the number of rows and columns and the data types as well as some properties associated with the columns.</p> <p>After putting all previous parts together and optimizing the code to reduce duplicates, the final version is here (comments are not included). <pre><code>from typing import Union\nimport string\nimport numpy as np\nimport pandas as pd\n\ndef gen_rand_strs(\n    rng: np.random._generator.Generator,\n    str_cnt: int,\n    str_len: tuple[int, int],\n    str_chars: list[str],\n) -&gt; list[str]:\n    str_lens = rng.integers(low=str_len[0], high=str_len[1], size=str_cnt, endpoint=True)\n    rand_strs = [''.join(rng.choice(str_chars, size=str_len)) for str_len in str_lens]\n    return rand_strs\n\ndef gen_str_vals(\n    size: int,\n    rng: np.random._generator.Generator,\n    str_cnt: int = None,\n    str_len: Union[int, tuple[int, int]] = None,\n    str_chars: list[str] = None,\n    col_strs: list[str] = None,\n) -&gt; np.ndarray:\n    if str_cnt is None:\n        str_cnt = 10\n    if str_len is None:\n        str_len = (5, 5)\n    elif type(str_len) == int:\n        str_len = (str_len, str_len)\n    if col_strs is None:\n        if str_chars is None:\n            str_chars = [c for c in string.ascii_letters + string.digits]\n        col_strs = gen_rand_strs(rng, str_cnt, str_len, str_chars)\n    val = rng.choice(col_strs, size=size)\n    return val\n\ndef gen_ts_vals(\n    size: int,\n    rng: np.random._generator.Generator,\n    start_date: str = None,\n    end_date: str = None,\n    freq: str = None,\n    random: bool = False,\n) -&gt; np.ndarray:\n    if start_date is None:\n        start_date = '2024-01-01'\n    if end_date is None:\n        end_date = '2025-01-01'\n    if freq is None:\n        freq = 'D'\n    if random is None:\n        random = False\n    val = pd.date_range(start_date, end_date, freq=freq, inclusive='left')[:size]\n    if random:\n        val = rng.choice(val, size=size)\n    return val\n\ndef gen_num_vals(\n    size: int,\n    rng: np.random._generator.Generator,\n    low: Union[int, float] = None,\n    high: Union[int, float] = None,\n    dtype: str = None,\n) -&gt; np.ndarray:\n    if low is None:\n        low = 0\n    if high is None:\n        high = 2\n    func = rng.integers if dtype[0] == 'i' else rng.uniform\n    vals = func(low=low, high=high, size=size)\n    return vals\n\ndef gen_missing_vals(\n    vals: np.ndarray,\n    rng: np.random._generator.Generator,\n    dtype: str,\n    missing_pct: float = None,\n) -&gt; np.ndarray:\n    if missing_pct is None or missing_pct &lt;= 0 or missing_pct &gt;= 1:\n        return vals\n    if dtype == 's':\n        missing_val = None\n    elif dtype == 't':\n        missing_val = np.datetime64('NaT')\n    else:\n        missing_val = np.nan\n    if dtype == 'i':\n        vals = vals.astype(np.float64)\n    mask = rng.uniform(size=len(vals)) &lt;= missing_pct\n    vals[mask] = missing_val\n    return vals\n\ndef sanitize_parameters(\n    name_prefix: str,\n    params: dict,\n    par_names: list[str],\n) -&gt; tuple[list[str], list[dict], list[float]]:\n    if type(params) == int:\n        cnt = params\n        par = {}\n    else:\n        cnt = params['count']\n        par = {\n            k: v + [None] * (cnt - len(v)) if type(v) == list else [v] * cnt\n            for k, v in params.items()\n        }\n    if name_prefix in ('i', 'f'):\n        par_names += ['dtype']\n        par['dtype'] = [name_prefix] * cnt\n    default_val = [None] * cnt\n    parameters = [\n        {key: par.get(key, default_val)[i] for key in par_names}\n        for i in range(cnt)\n    ]\n    col_names = par.get('name', [f'{name_prefix}{i}' for i in range(1, cnt+1)])\n    col_missing_pcts = par.get('missing_pct', default_val)\n    return col_names, parameters, col_missing_pcts\n\ndef gen_rand_df(\n    nrow: int,\n    str_cols: dict = None,\n    ts_cols: dict = None,\n    int_cols: dict = None,\n    float_cols: dict = None,\n    rand_seed: int = 11,\n) -&gt; pd.DataFrame:\n    col_types = ['s', 't', 'i', 'f']\n    inputs = [str_cols, ts_cols, int_cols, float_cols]\n    funcs = [gen_str_vals, gen_ts_vals, gen_num_vals, gen_num_vals]\n    par_names = [\n        ['str_cnt', 'str_len', 'str_chars', 'col_strs'],\n        ['start_date', 'end_date', 'freq', 'random'],\n        ['low', 'high'],\n        ['low', 'high'],\n    ]\n    df = pd.DataFrame()\n    rng = np.random.default_rng(seed=rand_seed)\n    for i, params in enumerate(inputs):\n        if params is not None:\n            col_names, col_params, col_missing_pcts = sanitize_parameters(\n                col_types[i], params, par_names[i]\n            )\n            df = pd.concat([df, pd.DataFrame({\n                col: gen_missing_vals(\n                    funcs[i](nrow, rng, **col_params[j]),\n                    rng,\n                    col_types[i],\n                    col_missing_pcts[j],\n                ) for j, col in enumerate(col_names)\n            })], axis=1)\n    return df\n</code></pre></p>"},{"location":"Python/Blog/PandasTestDataFrame/#how-to-use-the-gen_rand_df-function-with-examples","title":"How to use the <code>gen_rand_df</code> function with examples","text":"<p>Now let's show how to use the <code>gen_rand_df</code> function to create dummy DataFrames.</p> <p>In this example, we simply set the number of rows and column date types and leave all other properties as default. <pre><code>df = gen_rand_df(\n    nrow=365*24*60,\n    str_cols=1,\n    ts_cols=2,\n    int_cols=1,\n    float_cols=2,\n)\nprint(df[:2])\n</code></pre></p> <p>Output: <pre><code>      s1         t1         t2  i1        f1       f2\n0  IZD8v 2024-01-01 2024-01-01   0  1.593760  1.64884\n1  P9r1i 2024-01-02 2024-01-02   0  1.622772  1.94318\n</code></pre></p> <p>Here we provide another example to show how to set all the properties for different data type columns. Note that the <code>str_len</code> and <code>str_count</code> are not used as we provide the <code>col_strs</code> for the two string columns. <pre><code>d2 = gen_rand_df(\n    nrow=10,\n    str_cols={\n        'count': 2,\n        'name': ['country', 'color'],\n        'str_len': [3, (3,9)],\n        'str_count': [2, 5],\n        'col_strs': [['UK', 'US', 'AU'], ['blue', 'black', 'red']]\n    },\n    ts_cols={\n        'count': 2,\n        'name': ['start_date', 'end_date'],\n        'start_date': ['2020-01-01', '2024-01-01'],\n        'end_date': ['2021-01-01', '2025-01-01'],\n        'freq': 'QS',\n        'random': False,\n    },\n    int_cols={\n        'count': 1,\n        'name': ['quantity'],\n        'low': [0],\n        'high': [100],\n        'missing_pct': [0.3],\n    },\n    float_cols={\n        'count': 2,\n        'name': ['price', 'charge'],\n        'low': [1, 0.1],\n        'high': [100, 0.9],\n        'missing_pct': [0.3, 0.2],\n    },\n)\nprint(d2[:3])\n</code></pre></p> <p>Output <pre><code>  country  color start_date   end_date  quantity      price    charge\n0      UK  black 2020-01-01 2024-01-01      86.0        NaN  0.657889\n1      UK    red 2020-04-01 2024-04-01      36.0  24.294822  0.371457\n2      AU  black 2020-07-01 2024-07-01      14.0        NaN  0.113502\n</code></pre></p>"},{"location":"Python/Blog/ReadCSVPerf/","title":"Read CSV Files 10x to 40x Faster Using pyarrow and polars","text":"<p>CSV (comma-separated values) files have been widely used in different areas. They can be easily exported from almost all programming languages. They can also be loaded into all text editors and many other applications. However, the main disadvantage is that CSV files are usually larger than files with other formats and it is slow to load them into memory.</p> <p>Here we compare different options for reading CSV files by using the <code>pandas</code>, <code>polars</code> and <code>pyarrow</code> Python packages. We test the loading performance for CSV files each with a different data type. Based on the test results, we should be able to determine which option to use when we need reading CSV files faster.</p>"},{"location":"Python/Blog/ReadCSVPerf/#creating-test-data","title":"Creating test data","text":"<p>CSV files with three data types, <code>string</code>, <code>float</code>, and <code>datetime</code>, have been used to test the file reading performance. All the testing CSV files were created using the scripts in my previous post; each CSV file has 10 million rows and three columns with the same data type and a size of about 500 MB.</p> <p>The <code>string</code> type CSV file was created with: <pre><code>df_str = gen_rand_df(\n    nrow=10000000,\n    str_cols={\n        'count': 3,\n        'name': ['c1', 'c2', 'c3'],\n        'str_len': [10, (1,15), (1,50)],\n        'str_count': [1000, 500, 100],\n    },\n)\ndf_str.to_csv(filename, index=False)\n</code></pre></p> <p>The <code>float</code> type CSV file was created with: <pre><code>df_flt = gen_rand_df(\n    nrow=10000000,\n    float_cols={\n        'count': 3,\n        'name': ['c1', 'c2', 'c3'],\n        'low': [0, -100, 0],\n        'high': [1, 100, 1e5],\n    },\n)\ndf_flt.to_csv(filename, index=False)\n</code></pre></p> <p>The <code>datetime</code> type CSV file was created with: <pre><code>df_dts = gen_rand_df(\n    nrow=10000000,\n    ts_cols={\n        'count': 3,\n        'name': ['c1', 'c2', 'c3'],\n        'start_date': ['2020-01-01', '2021-01-01', '2022-01-01'],\n        'end_date': ['2021-01-01', '2022-01-01', '2023-01-01'],\n        'freq': 's',\n        'random': False,\n    },\n)\ndf_dts.to_csv(filename, index=False)\n</code></pre></p>"},{"location":"Python/Blog/ReadCSVPerf/#reading-csv-files-using-pandas","title":"Reading CSV files using <code>pandas</code>","text":"<p>In pandas, when reading CSV files, there are three types of parsers that are available (<code>python</code>, <code>c</code>, and <code>pyarrow</code>). The parser can be set via the parameter <code>engine</code>. There are also two backend data types (backend_dtype: <code>numpy_nullable</code> and <code>pyarrow</code>) for storing the data. We will check the performance of the combinations of different parsers and backend data types.</p> <p>The data types passed to the functions are a dictionary like this: <code>dtype = {'c1': type, 'c2': type, 'c3': type}</code>. - For <code>string</code> values the type is <code>str</code>. There are also two string data types available for pyarrow (dtype_pa): <code>pd.ArrowDtype(pa.string())</code> and <code>string[pyarrow]</code> (dtype_pa_str2); the latter supports NumPy-backed nullable types. - For <code>float</code> values the type is <code>float</code> and <code>float64[pyarrow]</code>, for <code>numpy_nullable</code> and <code>pyarrow</code> backends respectively. - For <code>datatime</code> values the type is <code>datetime64[s]</code> and <code>pd.ArrowDtype(pa.timestamp('s'))</code>. Notice that, when using the pandas datetime data types such as <code>datetime64[s]</code>, the datetime type columns must be passed to the function separately. While using the <code>pyarrow</code> data types, all types can be passed to the function in the same format.</p> <p>The following options are tested: - c + numpy_nullable + dtype_str + astype   <pre><code>import pandas as pd\npd.read_csv(\n    file, engine='c', dtype_backend='numpy_nullable', dtype=dtype_str\n).astype(dtype)\n</code></pre> - c + numpy_nullable + dtype</p> <p>For <code>string/float</code>:   <pre><code>pd.read_csv(\n    file, engine='c', dtype_backend='numpy_nullable', dtype=dtype\n)\n</code></pre>   For <code>datetime</code>:   <pre><code>pd.read_csv(\n    file, engine='c', dtype_backend='numpy_nullable',\n    parse_dates=['c1','c2','c3'],\n)\n</code></pre> - c + pyarrow + dtype</p> <p>For <code>string/float</code>:   <pre><code>pd.read_csv(\n    file, engine='c', dtype_backend='pyarrow', dtype=dtype\n)\n</code></pre>   For <code>datetime</code>:   <pre><code>pd.read_csv(\n    file, engine='c', dtype_backend='pyarrow',\n    parse_dates=['c1','c2','c3'],\n)\n</code></pre> - c + pyarrow + dtype_pa   <pre><code>pd.read_csv(\n    file, engine='c', dtype_backend='pyarrow', dtype=dtype_pa\n)\n</code></pre> - pyarrow + numpy_nullable + dtype</p> <p>For <code>string/float</code>:   <pre><code>pd.read_csv(\n    file, engine='pyarrow', dtype_backend='numpy_nullable', dtype=dtype\n)\n</code></pre>   For <code>datetime</code>:   <code>py   pd.read_csv(       file, engine='pyarrow', dtype_backend='numpy_nullable',       parse_dates=['c1','c2','c3'],   ) - pyarrow + pyarrow + dtype</code>py   pd.read_csv(       file, engine='pyarrow', dtype_backend='pyarrow', dtype=dtype   ) - pyarrow + pyarrow + string[pyarrow]   <pre><code>pd.read_csv(\n    file, engine='pyarrow', dtype_backend='pyarrow', dtype=dtype_pa_str2\n)\n</code></pre> - pyarrow + pyarrow + dtype_pa   <pre><code>pd.read_csv(\n    file, engine='pyarrow', dtype_backend='pyarrow', dtype=dtype_pa\n)\n</code></pre> - pyarrow + pyarrow + dtype_pa + to numpy_nullable   <pre><code>pd.read_csv(\n    file, engine='pyarrow', dtype_backend='pyarrow', dtype=dtype_pa\n).convert_dtypes(dtype_backend='numpy_nullable')\n</code></pre> - pyarrow + pyarrow   <pre><code>pd.read_csv(\n    file, engine='pyarrow', dtype_backend='pyarrow'\n)\n</code></pre></p> <p>The performance results for these options are as follows: <pre><code>                                                         str    float  datetime performance_order_for_float\nc       + numpy_nullable + dtype_str + astype            3.93s  18.2s  18.5s    10\nc       + numpy_nullable + dtype                         3.88s  3.29s  15.4s     6\nc       + pyarrow        + dtype                         3.27s  3.55s  16.6s     7\nc       + pyarrow        + dtype_pa                      5.17s  16.8s  53.2s     9\npyarrow + numpy_nullable + dtype                         3.50s  0.54s  1.15s     4\npyarrow + pyarrow        + dtype                         7.62s  0.50s  1.67s     3\npyarrow + pyarrow        + string[pyarrow]               4.05s  15.8s  11.1s     8\npyarrow + pyarrow        + dtype_pa                      0.39s  0.48s  0.44s     2\npyarrow + pyarrow        + dtype_pa + to numpy_nullable  2.74s  2.68s  1.64s     5\npyarrow + pyarrow                                        0.48s  0.47s  0.37s     1\n</code></pre></p> <p>Based on the test results, we can conclude that: - We can get the best performance when using <code>pyarrow</code> for the parser, backend and dtype (<code>pyarrow + pyarrow + dtype_pa</code>). - The <code>pyarrow + pyarrow + dtype_pa</code> option is about 10x, 7x, and 35x faster than the default option (<code>c + numpy_nullable + dtype</code>) for <code>string</code>, <code>float</code> and <code>datetime</code>, separately. - Compared to the <code>c</code> parser, the <code>pyarrow</code> parser is a little faster for <code>string</code>, 6x faster for <code>float</code>, and 10-14x faster for <code>datetime</code>. - Using the <code>pyarrow</code> backend with the <code>c</code> parser, there are no performance improvements; if also using the <code>pyarrow</code> dtype the performance is much worse. - The <code>pd.ArrowDtype(pa.string())</code> string data type is about 10x faster than the <code>string[pyarrow]</code> string data type. - The <code>pyarrow</code> parser can automatically determine the data types without any performance loss; this is especially useful when you do not know the data types in the CSV files.</p> <p>We should understand that the <code>pyarrow</code> parser works in parallel mode while the <code>c</code> parser is not. Also converting the data from the <code>numpy_nullable</code> to <code>pyarrow</code> dtype or vice versa might be time-consuming.</p>"},{"location":"Python/Blog/ReadCSVPerf/#reading-csv-files-using-polars","title":"Reading CSV files using <code>polars</code>","text":"<p>The <code>polars</code> package is relatively new. But it becomes popular recently due to its performance both in speed with vectorized execution and memory efficiency using <code>arrow</code>. Also it is designed with a clean and concise API for handling large datasets with lazy evaluation.</p> <p>The data types passed to the <code>polars</code> functions are a dictionary like this: <code>dtypes = {'c1': dtype, 'c2': dtype, 'c3': dtype}</code>. - For <code>string</code> values the dtype is <code>pl.Utf8</code>. - For <code>float</code> values the dtype is <code>pl.Float64</code>. - For <code>datatime</code> values the dtype is <code>pl.Datetime</code>.</p> <p>The following options are tested: - default: without providing the dtypes parameter. Note that if some columns with <code>float</code> type have empty values, the data type will be parsed as <code>string</code> - not smart enough compared to <code>pyarrow.csv</code>.   <pre><code>import polars as pl\npl.read_csv(file)\n</code></pre> - eager: the default mode, any operations are executed immediately   <pre><code>pl.read_csv(file, dtypes=dtypes)\n</code></pre> - lazy: operations are not executed until you explicitly call the <code>collect()</code> method   <pre><code>pl.scan_csv(file, dtypes=dtypes).collect()\n</code></pre> - streaming: it processes the data in batches instead of loading everything at once, good for handling large datasets that might exceed available memory   <pre><code>pl.scan_csv(file, dtypes=dtypes).collect(streaming=True)\n</code></pre> - sql api eager: interact with data using familiar SQL syntax   <pre><code>pl.SQLContext(\n    data=pl.scan_csv(file, dtypes=dtypes)\n).execute('select * from data', eager=True)\n</code></pre> - sql api eager + to pandas   <pre><code>pl.SQLContext(\n    data=pl.scan_csv(file, dtypes=dtypes)\n).execute(\n    'select * from data', eager=True\n).to_pandas(use_pyarrow_extension_array=False)\n</code></pre> - sql api eager + to pandas pyarrow   <pre><code>pl.SQLContext(\n    data=pl.scan_csv(file, dtypes=dtypes)\n).execute(\n    'select * from data', eager=True\n).to_pandas(use_pyarrow_extension_array=True)\n</code></pre></p> <p>The tested performance results are as follows: <pre><code>                                          str    float  datetime\ndefault                                   0.52s  0.38s  0.37s\neager                                     0.46s  0.40s  0.39s\nlazy                                      0.45s  0.38s  0.41s\nstreaming                                 0.42s  0.40s  0.42s\nsql api eager                             0.46s  0.38s  0.40s\nsql api eager + to pandas                 1.59s  0.47s  0.48s\nsql api eager + to pandas pyarrow         0.99s  0.43s  0.45s\n</code></pre></p> <p>It is obvious from the results that: - The performance is quite consistent for all the options using <code>polars</code>. - The <code>polars</code> CSV reading has a similar performance compared to <code>pandas</code> with <code>pyarrow</code>. - If we need a <code>numpy_nullable</code> pandas DataFrame, <code>polars</code> can still be a better option.</p>"},{"location":"Python/Blog/ReadCSVPerf/#reading-csv-files-using-pyarrowcsv","title":"Reading CSV files using <code>pyarrow.csv</code>","text":"<p>The module, <code>pyarrow.csv</code>, is one of the great modules within the <code>pyarrow</code> library that specifically deals with reading and writing CSV files. It offers robust functionalities to efficiently process CSV data with some great features, such as inferring data types during reading and supporting various file formats.</p> <p>Here we test the performance of the <code>pyarrow.csv</code> module with three data types in the format <code>convert_options = pv.ConvertOptions(column_types={'c1': dtype, 'c2': dtype, 'c3': dtype})</code>. - For <code>string</code> values the dtype is <code>pa.string()</code>. - For <code>float</code> values the dtype is <code>pa.float64()</code>. - For <code>datatime</code> values the dtype is <code>pa.timestamp('s')</code>.</p> <p>The following options are tested and compared: - default   <pre><code>import pyarrow.csv as pv\npv.read_csv(file)\n</code></pre> - default + to pandas   <pre><code>pv.read_csv(file).to_pandas()\n</code></pre> - default + to pandas pyarrow   <pre><code>pv.read_csv(file).to_pandas(types_mapper=pd.ArrowDtype)\n</code></pre> - dtype   <pre><code>pv.read_csv(file, convert_options=convert_options)\n</code></pre> - dtype + to pandas   <pre><code>pv.read_csv(file, convert_options=convert_options).to_pandas()\n</code></pre> - dtype + to pandas pyarrow   <pre><code>pv.read_csv(file, convert_options=convert_options).to_pandas(types_mapper=pd.ArrowDtype)\n</code></pre></p> <p>The performance results for the previous options are shown here: <pre><code>                               str    float  datetime\ndefault                        0.39s  0.44s  0.38s\ndefault + to pandas            1.07s  0.45s  0.42s\ndefault + to pandas pyarrow    0.48s  0.43s  0.33s\ndtype                          0.39s  0.40s  0.36s\ndtype   + to pandas            0.99s  0.45s  0.41s\ndtype   + to pandas pyarrow    0.39s  0.42s  0.37s\n</code></pre></p> <p>From these results we can conclude that: - The <code>pyarrow.csv</code> module has a similar performance compared to <code>polars</code>. - If we need to load CSV files into a <code>pandas</code> DataFrame, <code>pyarrow.csv</code> is the fastest option.</p>"},{"location":"Python/Blog/ReadCSVPerf/#best-options-from-pandas-polars-and-pyarrow","title":"Best options from <code>pandas</code>, <code>polars</code>, and <code>pyarrow</code>","text":"<p>There is no surprise that all options using <code>arrow</code> to store data have a similar performance for reading CSV files; <code>polars</code> also uses <code>arrow</code> to save the data in memory. The <code>arrow</code> package is not just faster by parallelizing the reading, it is also more memory efficient.</p> <p>The <code>polars</code> package is relatively new compared to <code>pandas</code>. It has some great new features but might not have the functions we need. It's entirely up to us to decide which package to use. If we use <code>polars</code> do all our data manipulations I would suggest we stick to <code>polars</code> for reading CSV files.</p> <p>If <code>pandas</code> is still our preference, to load CSV files efficiently, we should use the <code>pyarrow</code> parser, backend and dtype or <code>pyarrow.csv</code> to improve the performance further. If we also need to use the <code>numpy_nullable</code> backend, it is best to read CSV files using <code>pyarrow.csv</code> and then convert the backend to <code>numpy_nullable</code>.</p>"},{"location":"Python/Blog/ReadExcelPerf/","title":"Read Excel performance","text":""},{"location":"Python/Blog/ReadExcelPerf/#test-data","title":"test data","text":""},{"location":"Python/Blog/ReadExcelPerf/#duckbd","title":"duckbd","text":""},{"location":"Python/Blog/ReadExcelPerf/#pandas","title":"pandas","text":""},{"location":"Python/Blog/ReadExcelPerf/#polars","title":"polars","text":""},{"location":"Python/Blog/ReadParquetPerf/","title":"Read parquet performance","text":""},{"location":"Python/Blog/ReadParquetPerf/#pandas","title":"pandas","text":""},{"location":"Python/Blog/ReadParquetPerf/#pyarrow","title":"pyarrow","text":""},{"location":"Python/Blog/ReadParquetPerf/#polars","title":"polars","text":""},{"location":"Python/Blog/ReadParquetPerf/#test","title":"test","text":""},{"location":"Python/Blog/ReadSqlDataType/","title":"The Pandas Function <code>pd.read_sql</code> Returns An Empty DataFrame Without Correct Data Types","text":"<p>We provide a solution to the issue you might need</p> <p>When querying data from databases such as MS SQL Server via the Driver <code>pyodbc</code>, we can conveniently get the data as a pandas DataFrame by using <code>pd.read_sql</code>. Generally, the driver provides information about the column names, data types, and other metadata associated with the result set. However, when the query result set is empty, the data type information is not available and pandas returns an <code>empty</code> DataFrame with all column types as <code>object</code>.</p> <p>An empty DataFrame with wrong data types can cause issues in your Python code. If you do not check whether the returned DataFrame is empty or not your code will crash in many cases such as trying to extract the year from a datetime column and doing aggregations on float columns. Here we explain how to get the data type information in this situation when using <code>sqlalchemy</code> to create the query.</p>"},{"location":"Python/Blog/ReadSqlDataType/#get-the-data-types-from-the-query-statement","title":"Get the data types from the query statement","text":"<p>There are multiple approaches to get the data types but they are not always available in most situations.</p>"},{"location":"Python/Blog/ReadSqlDataType/#option-1-resultcursordescription","title":"Option 1: <code>result.cursor.description</code>","text":"<p>The first approach is using the <code>.description</code> attribute of the database <code>cursor</code> object: <pre><code>import sqlalchemy\n\nconn_string = f'mssql+pyodbc://{username}:{pwd}@{server_name}'\nconn_string += f'/{database_name}?driver=ODBC+Driver+17+for+SQL+Server'\nengine = sqlalchemy.create_engine(conn_string)\nconnection = engine.connect()\n\nquery = 'SELECT ID, Name, Price, StartDate FROM sales.Product;'\nresult = connection.execute(query)\nprint(result.cursor.description)\n</code></pre></p> <p>The output will be something like: <pre><code>(\n    ('ID', &lt;class 'int'&gt;, None, 10, 10, 0, False),\n    ('Name', &lt;class 'str'&gt;, None, 50, 50, 0, False),\n    ('Price', &lt;class 'decimal.Decimal'&gt;, None, 19, 19, 4, False),\n    ('StartDate', &lt;class 'datetime.datetime'&gt;, None, 23, 23, 3, False),\n)\n</code></pre></p>"},{"location":"Python/Blog/ReadSqlDataType/#option-2-querystatementselected_columns","title":"Option 2: <code>query.statement.selected_columns</code>","text":"<p>If the first approach does not work and you use <code>sqlalchemy</code> to create the query, you should still be able to get the data types.</p> <p>Assume we defined the <code>sales.Product</code> Table as: <pre><code>from sqlalchemy.orm import declarative_base\nfrom sqlalchemy import create_engine, Column, DateTime, DECIMAL, Integer, Unicode\n\n# Base classes and will hold the metadata about the tables\nBase = declarative_base()\n\n# A declarative class for Table `sales.Product` by inheriting from the Base class\nclass Product(Base):\n    __tablename__ = 'Product'\n    __table_args__ = {'schema': 'sales'}\n\n    ID = Column(Integer, primary_key=True)\n    Name = Name = Column(Unicode(50), nullable=False)\n    Price = Column(DECIMAL(19, 4))\n    StartDate = Column(DateTime)\n\n\n# Create a SQLAlchemy engine\nengine = create_engine(f'{database_connection_url}')\n\n# Create tables in the database\nBase.metadata.create_all(engine)\n</code></pre></p> <p>And we created the query in this way: <pre><code>from sqlalchemy import sql, types\nfrom sqlalchemy.orm import Session\n\nsession = Session(bind=engine)\nsp = Product\nquery = session.query(\n    sp.ID.label('id'),\n    sp.Name.label('name'),\n    sp.Price.label('price'),\n    sp.StartDate.label('start_date'),\n)\n</code></pre></p> <p>Finally we can get the data types from the query: <pre><code>if isinstance(query, sqlalchemy.orm.query.Query):\n    dtype = {\n        c.name: c.type.__class__.__name__\n        for c in query.statement.selected_columns\n    }\n</code></pre></p> <p>The <code>dtype</code> is a dictionary with column names as keys and data types as values: <pre><code>dtype = {\n    'id': 'Integer',\n    'Name': 'Unicode',\n    'price': 'DECIMAL',\n    'start_date': 'DateTime',\n}\n</code></pre></p> <p>Note that the items in <code>query.statement.selected_columns</code> can have different types, such as:</p> <ul> <li> <li> <li> <p>The <code>Cast</code> class is from the <code>sql.func.cast</code> function:</p> <pre><code>query = session.query(\n    sql.func.cast(sp.Price, types.Float),\n)\n</code></pre> <p>However, the <code>Cast</code> class does not have the <code>name</code> property. To fix the issue we have to convert the <code>Cast</code> column to a <code>Lable</code> column:</p> <pre><code>query = session.query(\n    sql.func.cast(sp.Price, types.Float).label('price'),\n)\n</code></pre>"},{"location":"Python/Blog/ReadSqlDataType/#pass-the-data-type-information-to-pdread_sql","title":"Pass the data type information to <code>pd.read_sql</code>","text":"<p>There is a parameter <code>dtype</code> in <code>pandas.read_sql(..., dtype=None)</code> that can be used to pass the data types for the query results.</p> <p>Note that in the previous section the extracted data types are the types defined in <code>sqlalchemy</code>. We need to convert them to the types that can be used in <code>pandas</code>. Here we provide a mapping for most of the data types: <pre><code>sa_to_pd_dtype = {\n    'BigInteger': 'int64',\n    'BIT': 'bool',\n    'Boolean': 'bool',\n    'Date': 'datetime64[ns]',\n    'DateTime': 'datetime64[ns]',\n    'DECIMAL': 'float',\n    'Enum': 'category',\n    'Float': 'float',\n    'Integer': 'int64',\n    'Interval': 'timedelta64',\n    'LargeBinary': 'str',\n    'Numeric': 'float',\n    'SmallInteger': 'int16',\n    'String': 'str',\n    'Time': 'datetime64[ns]',\n    'TIMESTAMP': 'datetime64[ns]',\n    'Unicode': 'str',\n}\n</code></pre></p> <p>And we set the data types when extracting the data using <code>pd.read_sql</code>: <pre><code>dtype = {\n    col: sa_to_pd_dtype[typ]\n    for col, typ in dtype.items()\n    if typ in sa_to_pd_dtype\n}\ndf = pd.read_sql(..., dtype=dtype)\n</code></pre></p>"},{"location":"Python/Blog/ReadSqlDataType/#why-did-i-get-nulltype-for-some-data-columns","title":"Why did I get <code>NullType</code> for some data columns?","text":"<p>Assume the previous query has been changed to: <pre><code>query = session.query(\n    sp.ID.label('id'),\n    sp.Name.label('name'),\n    sp.Price.label('price'),\n    sql.func.dateadd(\n        sql.text('day'), 1, sp.StartDate\n    ).label('actual_start_date'),\n)\n</code></pre> In this case, the data type for the column <code>actual_start_date</code> will be <code>NullType</code> instead of <code>DateTime</code>.</p> <p>By digging into the <code>sqlalchemy</code> documents we find out that this is caused by the <code>sql.func.dateadd</code>. Basically for functions that are not known, the type defaults to the <code>NullType</code>. There are also other functions such as <code>sql.func.rtrim</code>, <code>sql.func.replace</code>, <code>sql.func.year</code>, <code>sql.func.avg</code> and <code>sql.func.round</code> that might lead to the <code>NullType</code>.</p> <p>To fix the issue, we need to pass the data type directly to the function: <pre><code>sql.func.dateadd(\n    sql.text('day'), 1, sp.StartDate, type_=types.DateTime\n).label('actual_start_date')\n</code></pre></p>"},{"location":"Python/Blog/ReadSqlDataType/#reference","title":"Reference","text":"<ul> <li>SQLAlchemy accessing column types from query results</li> <li>SQLAlchemy getting column data types of query results</li> <li>SQL and Generic Functions</li> </ul>"},{"location":"Python/Blog/SqlSslSecurityLevel/","title":"Login Timeout From Ubuntu 22.04 To SQL Server Database","text":"<p>Recently I upgraded Ubuntu from 18.04 to 22.04 and Python from 3.9 to 3.12. Suddenly my connection to an SQL Server failed with <code>login timeout expired</code>.</p> <p>After some debugging and testing I fixed the issue. It's good to share here what I did and the solution. Hopefully, it will be helpful to you as well.</p>"},{"location":"Python/Blog/SqlSslSecurityLevel/#the-error-message","title":"The Error Message","text":"<p>When trying to connect to an SQL server database created a few years ago, I received the error: <pre><code>sqlalchemy.exc.OperationalError: (pyodbc.OperationalError) (\n'HYT00', '[HYT00] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0) (SQLDriverConnect)'\n)\n</code></pre> The error is actually caused by not using workload identity, and pod-iid-identity was not setup!</p>"},{"location":"Python/Blog/SqlSslSecurityLevel/#checking-the-issue","title":"Checking The Issue","text":"<p>Initially, I thought it was due to the OpenSSL version as Python 3.12 by default uses OpenSSL 3.0. However, we have another SQL server database that still works fine after the upgrading. This led me to check the SQL Server version and just found out that the old version <code>12.0.2000.8</code> does not support SSL/TLS 3.0. Based on the error message and the SQL Server version, I quickly worked out the solution with some google searches.</p>"},{"location":"Python/Blog/SqlSslSecurityLevel/#the-solution","title":"The Solution","text":"<p>There are two solutions or workarounds.</p>"},{"location":"Python/Blog/SqlSslSecurityLevel/#option-1-upgrading-sql-server-version","title":"Option 1: Upgrading SQL Server Version","text":"<p>Obviously the first option is upgrading the SQL Server version, as we already know that a newer version of SQL Server still works. However, if you are not the person with the permission to do the upgrading or you have other important things in your list, this option might not be good.</p>"},{"location":"Python/Blog/SqlSslSecurityLevel/#option-2-downgrading-ssltls-security-level","title":"Option 2: Downgrading SSL/TLS Security Level","text":"<p>Ubuntu 22.04 typically uses OpenSSL 3.x, so the <code>CipherString</code> for SSL/TLS connections in Ubuntu 22.04 defaults to a security level of 2. As the old version SQL Server does not support a security level of 2, we can downgrade the security level as a temporal workaround (note that doing this may compromise the security).</p> <p>Now for the workaround, in the openssl config file <code>/etc/ssl/openssl.cnf</code>, we only need to change the last <code>CipherString</code> line from <code>CipherString = DEFAULT:@SECLEVEL=2</code> to <code>CipherString = DEFAULT:@SECLEVEL=0</code>.</p> <p>If working with docker images, you can add the following to your Dockerfile: <pre><code>USER root\n\n# workaround for ubuntu 22.04 and odbc driver 17 with sql server 12.0\nRUN head -n -1 /etc/ssl/openssl.cnf &gt; openssl_cnf_temp_file &amp;&amp; \\\n    echo \"CipherString = DEFAULT:@SECLEVEL=0\" &gt;&gt; openssl_cnf_temp_file &amp;&amp; \\\n    mv openssl_cnf_temp_file /etc/ssl/openssl.cnf\n\nUSER &lt;your-user-name&gt;\n</code></pre></p>"},{"location":"Python/Blog/SqlSslSecurityLevel/#references","title":"References","text":"<ul> <li>https://github.com/microsoft/msphpsql/issues/1112</li> </ul>"},{"location":"Python/CLI/Click/","title":"Click","text":""},{"location":"Python/CLI/Click/#return-value-using-click","title":"return value using click","text":"<p>https://stackoverflow.com/questions/26246824/how-do-i-return-a-value-when-click-option-is-used-to-pass-a-command-line-argume</p> <p>Use <code>standalone_mode=False</code>: <pre><code>print(main(standalone_mode=False))\n</code></pre></p>"},{"location":"Python/CLI/Click/#multiple-values-from-environment-values","title":"Multiple Values from Environment Values","text":"<p>https://click.palletsprojects.com/en/7.x/options/</p> <ul> <li>The default implementation for all types is to split on <code>whitespace</code></li> <li>The exceptions to this rule are the <code>File</code> and <code>Path</code> types: Unix systems <code>colon</code> (:), and Windows <code>semicolon</code> (;)</li> </ul>"},{"location":"Python/CLI/Click/#functions","title":"functions","text":"<ul> <li><code>click.add_command</code> Registers another <code>class:Command</code> with this group</li> <li><code>click.make_pass_decorator</code> Given an object type this creates a decorator with innermost context of type</li> <li><code>click.argument</code> Attaches an argument to the command</li> <li><code>click.option</code> Attaches an option to the command</li> </ul>"},{"location":"Python/CLI/Click/#clickparamtype","title":"click.ParamType","text":"<p>Represents the type of a parameter. Validates and converts values from the command line or Python into the correct type.</p>"},{"location":"Python/CLI/Click/#subcommands","title":"subcommands","text":"<p>organize the commands as subcommands within the task group, providing a more modular and structured CLI application.</p> <p>main.py <pre><code>import click\nfrom task import task\n\n@click.group()\ndef cli():\n    \"\"\"A simple todo application.\"\"\"\n    pass\n\n# Add the 'task' subcommand group to the main CLI\ncli.add_command(task)\n\nif __name__ == '__main__':\n    cli()\n</code></pre></p> <p>task.py <pre><code>import click\nfrom functools import partial\n\n# Add some parameters to click.option\nclick.option = partial(click.option, show_default=True)\n\n@click.group()\ndef task():\n    \"\"\"Manage tasks.\"\"\"\n    pass\n\n@task.command()\n@click.argument('task')\ndef add(task):\n    \"\"\"Add a new task.\"\"\"\n    click.echo(f'Added task: {task}')\n\n@task.command('list')\n@click.option('--show/--no-show', default=True)\ndef list_tasks(show: bool):\n    \"\"\"List all tasks.\"\"\"\n    click.echo(\"List of tasks:\")\n    if show:\n        click.echo('Number of tasks: ')\n</code></pre></p> <p>Here - We create a subcommand group task using <code>@click.group()</code> in the <code>task.py</code> file. - Inside the <code>task.py</code> file, we use <code>@task.command()</code> to define subcommands within the task group for add and list_tasks. - In the <code>main.py</code> file, we add the task subcommand group to the main CLI using <code>cli.add_command(task)</code>.</p> <p>Now, you can use the task subcommand group with its subcommands: <pre><code>python main.py task add \"Buy groceries\"\npython main.py task list --show=False\n</code></pre></p>"},{"location":"Python/CLI/PyInvoke/","title":"PyInvoke","text":"<p>https://docs.pyinvoke.org/en/stable <pre><code>from invoke import task\n@task\ndef build(c):\n    print(\"Building!\")\n</code></pre> in shell run  <pre><code>invoke build\n</code></pre></p> <p>https://github.com/pyinvoke/invoke</p> <p>a Python library for managing shell-oriented subprocesses and organizing executable Python code into CLI-invokable tasks. </p> <p>Invoke is a Python library used for automating and managing tasks.  It provides a way to define and execute tasks, similar to build tools like Make, Ant, or Gradle,  but it's specifically designed for Python projects and is often used in conjunction with other build and automation tools.</p> <p>Here are some key features and use cases of Invoke: - Task Automation: Invoke allows you to define and execute tasks in Python scripts. These tasks can be used to automate common development and deployment processes. - Command-Line Interface (CLI): It provides a command-line interface for running tasks, making it easy to integrate into your development workflow. - Configuration: You can configure tasks using Python code, making it flexible and easy to adapt to your project's needs. - Dependency Management: Tasks can have dependencies, so you can ensure that certain tasks run before others. This is useful for building complex automation workflows. - Parallel Execution: Invoke supports parallel execution of tasks, which can significantly speed up the execution of tasks when appropriate. - Integration: It can be used alongside other Python libraries and tools, such as virtual environments, testing frameworks, and more.</p>"},{"location":"Python/CLI/PyInvoke/#env-variables","title":"env variables","text":"<p>When using ctx.run(), we can pass environment variables. The <code>env</code> parameter allows us to specify environment variables that should be available for the command we're running.</p> <pre><code>from invoke import task\n\n@task\ndef my_task(ctx):\n    # Define the environment variables\n    env_vars = {\n        'MY_VAR': 'some_value',\n        'ANOTHER_VAR': 'another_value',\n    }\n\n    # Run a shell command with the environment variables\n    result = ctx.run('echo $MY_VAR', env=env_vars)\n\n    # Print the output of the command\n    print(result.stdout)  \n</code></pre> <ul> <li>The environment variables you specify will only be available for the command being run.</li> <li>If want to keep existing environment variables and just add or override some, the values in the env dictionary will merge with the current environment automatically.</li> </ul>"},{"location":"Python/CLI/Terminal/","title":"Terminal","text":""},{"location":"Python/CLI/Terminal/#terminal-multiple-lines","title":"terminal multiple lines","text":"<p>avoid empty lines otherwise will get 'IndentationError: unexpected indent'</p>"},{"location":"Python/Cache/Aiocache/","title":"Aiocache","text":"<p>Asyncio cache supporting multiple backends (memory, redis and memcached).</p> <p>Aiocache is an asynchronous caching library that is based on asyncio. This means that it can be used to cache data in an asynchronous way, which can be useful for improving the performance of I/O-bound applications.</p>"},{"location":"Python/Cache/Aiocache/#cached","title":"cached","text":"<p>Do not use <code>aiocache.cached</code> for a sync function. It will return a coroutine object.</p>"},{"location":"Python/Cache/Aiocache/#config","title":"config","text":"<pre><code>from aiocache import caches, Cache\ncaches.set_config({\n    'default': {\n        'cache': 'aiocache.SimpleMemoryCache',\n        'serializer': {\n            'class': 'aiocache.serializers.StringSerializer'\n        }\n    },\n    'redis': {\n        'cache': 'aiocache.RedisCache',\n        'endpoint': '127.0.0.10',\n        'port': 6378,\n        'serializer': {\n            'class': 'aiocache.serializers.PickleSerializer'\n        }\n    }\n})\ncache = Cache(Cache.REDIS, endpoint='127.0.0.10', port=6379)\n</code></pre>"},{"location":"Python/Cache/Aiocache/#how-to-use-redis-to-cache-streamingresponse","title":"how to use redis to cache StreamingResponse","text":"<p>serialize the headers and contents of the StreamingResponse. See API Response for more details.</p>"},{"location":"Python/Cache/Cache/","title":"Cache","text":"<p>https://aiocache.readthedocs.io/en/latest/caches.html</p>"},{"location":"Python/Cache/Cache/#question","title":"question","text":"<p>if there are multiple servers and the next call might not be go to the same server- cache cannot be used!</p> <p>Solution? use a distributed cache, like Redis or Memcached, to share data across the pods: - deploy a memory cache service as a Kubernetes service - install cache client library in app - use the cache client to store and retrieve data</p>"},{"location":"Python/Cache/Cache/#cache-streamingresponse","title":"Cache StreamingResponse","text":"<p>How to cache StreamReponse or make a cache middleware\\ https://github.com/tiangolo/fastapi/issues/4751</p> <p>Solution:  - https://github.com/krukov/cashews/issues/107 - cache each chunk of the StreamingResponse content with a separate key - cache other StreamingResponse properties into a bytes object - then recreate the StreamingResponse based on the cached content and properties</p> <p>my solution using aiocache (borrowed code from the following link) https://github.com/Krukov/cashews/pull/123/files#diff-3df331569a7628a330e72e831d8f338342ef954c7cb3897c6195565a85c32b6fR1</p> <p>Note that the code can also be updated to fallback to a default memory cache if redis server (ping) is not available. <pre><code>class stream_cached(cached):\n    async def set_in_cache(self, key, value):\n        try:\n            if isinstance(value, StreamingResponse):\n                value = await stream_cached.encode_streaming_response(\n                    value=value,\n                    cache=self.cache,\n                    key=key,\n                    ttl=self.ttl,\n                )\n            await self.cache.set(key, value, ttl=self.ttl)\n        except Exception:\n            cached_logger.exception(\n                f\"Unexpected error. Couldn't set {value} in key {key}\"\n            )\n\n    async def get_from_cache(self, key):\n        try:\n            value = await self.cache.get(key)\n            is_streaming_response = (\n                value is not None and 'streamingresponse' in key\n            )\n            if is_streaming_response:\n                value = await stream_cached.decode_streaming_response(\n                    value=value,\n                    cache=self.cache,\n                    key=key,\n                )\n            return value\n        except Exception:\n            cached_logger.exception(\n                f\"Unexpected error. Couldn't retrieve key {key}\"\n            )\n\n    async def encode_streaming_response(\n        value: StreamingResponse,\n        cache: Cache,\n        key: str,\n        ttl: object,\n    ) -&gt; bytes:\n        value.body_iterator = stream_cached.set_iterator(\n            cache, key, value.body_iterator, ttl\n        )\n        serialized_value = b''\n        serialized_value += bytes(value.media_type, 'utf-8') + b':'\n        serialized_value += bytes(str(value.status_code), 'utf-8') + b':'\n        for header_name, header_value in value.raw_headers:\n            serialized_value += header_name + b'=' + header_value + b';'\n        return serialized_value\n\n    async def decode_streaming_response(\n        value: bytes,\n        cache: Cache,\n        key: str,\n    ) -&gt; StreamingResponse:\n        media_type, status_code, headers = value.split(b':')\n        media_type = str(media_type)\n        status_code = int(status_code)\n        raw_headers = []\n        for header in headers.split(b';'):\n            if not header:\n                continue\n            header_name, header_value = header.split(b'=')\n            raw_headers.append((header_name, header_value))\n        content = stream_cached.get_iterator(cache, key)\n        resp = StreamingResponse(\n            content=content,\n            media_type=media_type,\n            status_code=status_code,\n        )\n        resp.raw_headers = raw_headers\n        return resp\n\n    async def set_iterator(cache: Cache, key: str, iterator, ttl):\n        chunk_number = 0\n        async for chunk in iterator:\n            await cache.set(f'{key}:chunk:{chunk_number}', chunk, ttl=ttl)\n            yield chunk\n            chunk_number += 1\n\n    async def get_iterator(cache: Cache, key: str):\n        chunk_number = 0\n        while True:\n            chunk = await cache.get(f'{key}:chunk:{chunk_number}')\n            if not chunk:\n                return\n            yield chunk\n            chunk_number += 1\n</code></pre></p>"},{"location":"Python/Cache/Cache/#key_builder","title":"key_builder","text":"<pre><code>def key_builder(f, **kwargs):\n    return f.__name__ + json.dumps({\n        k: v for k, v in kwargs.items() if k != 'request'\n    })\n</code></pre>"},{"location":"Python/Cache/Cache/#cache-decorator-with-request-header","title":"Cache decorator with request header","text":"<p>https://aiocache.readthedocs.io/en/latest/decorators.html</p> <p>When use cache in api, we should also consider the header. Otherwise different headers will get the same response.</p> <p>Issue: StreamingResponse is not supported as data is sent chunk by chunk - only the last chunk (empty string) is cached. See <code>StreamingResponse</code> definition. <pre><code>from typing import Optional\nfrom aiocache import cached, Cache\nfrom fastapi import status, Request, Header\n\n@router.get(\n    \"/hello\",\n    status_code=status.HTTP_200_OK,\n    description='Get hello'\n)\n@vary_on_headers('Accept')\n@cached(ttl=3600, cache=Cache.MEMORY, namespace='dev', key_builder=key_builder)\nasync def get_hello(\n    request: Request,\n    accept: Optional[str] = Header(None), # header is `Accept`\n):\n    return 'hello, Accept header is `{accept}`'\n</code></pre></p>"},{"location":"Python/Cache/Cache/#cache-dataframe","title":"Cache dataframe","text":"<p>https://github.com/aio-libs/aiocache/issues/493 <pre><code>import zlib\nimport pickle\nimport pandas as pd\n\nimport asyncio\nfrom aiocache import Cache\nfrom aiocache.serializers import BaseSerializer\n\nclass CompressionSerializer(BaseSerializer):\n    DEFAULT_ENCODING = None #zlib works with bytes\n\n    def dumps(self, value):\n        compressed = zlib.compress(pickle.dumps(value))\n        return compressed\n\n    def loads(self, value):\n        #if value is too large to read into memory\n        #use zlib.decompressobj instead of zlib.decompress \n        decompressed = pickle.loads(zlib.decompress(value))\n        return decompressed\n\ncache = Cache(Cache.MEMORY, serializer=CompressionSerializer(), namespace='dev')\n\ndf = pd.DataFrame({'x':[1,2], 'y':[3,4]})\nloop = asyncio.get_event_loop()\nloop.run_until_complete(cache.set(\"key\", df))\nloop.run_until_complete(cache.get(\"key\"))\nloop.run_until_complete(cache.delete(\"key\"))\n</code></pre></p>"},{"location":"Python/Cache/Cachetools/","title":"Cachetools","text":"<p>Cachetools is a synchronous caching library that is based on collections. This means that it can only be used to cache data in a synchronous way.  However, it is a more lightweight library than Aiocache.</p>"},{"location":"Python/Cache/Cachetools/#example","title":"example","text":"<p><code>maxsize</code>: max number of items in the cache <code>ttl</code>: unit is second <pre><code>import time\nimport numpy as np\nimport pandas as pd\nfrom cachetools import cached, TTLCache\n\ndef d1(m, n):\n    data = np.random.randn(m, n)\n    cols = [f'col{j}' for j in range(1,n+1)]\n    df = pd.DataFrame(data, columns=cols)\n    return df\n\ncache=TTLCache(maxsize=float('inf'),ttl=60)\n@cached(cache)\ndef f1(m,n):\n    df=d1(m,n)\n    return df\n\nt0 = time.time()\nd1 = f1(100000,10)\nprint(f'time: {time.time() - t0:.3f}')\nt0 = time.time()\nd2 = f1(100000,10)\nprint(f'time: {time.time() - t0:.3f}')\nt0 = time.time()\nd3 = f1(100000,10)\nprint(f'time: {time.time() - t0:.3f}')\n</code></pre></p>"},{"location":"Python/Cache/Cachetools/#custom-key","title":"custom key","text":"<p>caveat: to be thread-safe, we must provide a <code>Lock</code> object to the cached decorator. <pre><code>import threading\nfrom cachetools import cached, TTLCache\n\nCACHE_TIME_LIMIT = 1 * 60 * 60 # one hour\ncachetools_cache = TTLCache(maxsize=float('inf'), ttl=CACHE_TIME_LIMIT)\n\ndef key_builder(f, namespace, exclude, *args, **kwargs):\n    params = {}\n    special_type = ''\n    params['args'] = args\n    if isinstance(exclude, str):\n        exclude = [exclude]\n    for k, v in kwargs.items():\n        if exclude is not None and k in exclude:\n            continue\n        if k == 'special' and isinstance(v, Request):\n            special_type = v.headers.get('Accept')\n        else:\n            if isinstance(v, AzureBlobFileSystem):\n                v = {\n                    'account_url': v.account_url,\n                    'account_name': v.account_name,\n                }\n            params[k] = v\n    return f'{f.__name__}:{namespace}{json.dumps(params)}`{special_type}'\n\ndef cachetools_cachedx(f):\n    return cached(\n        cachetools_cache,\n        key=lambda *args, **kwargs: (key_builder(f, *args, **kwargs)),\n    )(f)\n\ndef cachetools_cached(\n    namespace: str = '',\n    exclude: list[str] = None,\n    key_builder: Callable = key_builder,\n):\n    def decorator(f):\n        return cached(\n            cachetools_cache,\n            key=lambda *args, **kwargs:\n                (key_builder(f, namespace, exclude, *args, **kwargs)),\n            lock=threading.Lock(), #ensure thread-safe\n        )(f)\n    return decorator\n\n@cachetools_cached(namespace='dev', exclude='fs', key_builder=key_builder)\ndef read_parquet_cache(\n    *,\n    fs: AzureBlobFileSystem,\n    path: str,\n    columns: list[str],\n) -&gt; pd.DataFrame:\n    with fs.open(path) as f:\n        df = pd.read_parquet(path=f, columns=columns)\n    return df\n</code></pre></p>"},{"location":"Python/Cache/FlaskCaching/","title":"FlaskCaching","text":""},{"location":"Python/Cache/FlaskCaching/#example","title":"Example","text":"<pre><code>from flask_caching import Cache\ncache = Cache(app.server, config={\n    'CACHE_TYPE': 'SimpleCache' #'filesystem',\n    #'CACHE_DIR': 'cache-directory'\n})\n\n@cache.memoize(timeout=CACHE_TIMEOUT)\ndef my_func():\n    return df\n</code></pre>"},{"location":"Python/Cache/Redis/","title":"Redis","text":""},{"location":"Python/Cache/Redis/#check-redis-server-availability","title":"check redis server availability","text":"<pre><code>import redis\n\ndef check_redis_availability():\n    # Create a StrictRedis instance\n    redis_client = redis.StrictRedis(host='localhost', port=6379, decode_responses=True)\n\n    try:\n        # Ping the Redis server\n        response = redis_client.ping()\n    except Exception as e:\n        print(f'Error checking Redis availability: {e}')\n    else:\n        if response:\n            print('Redis server is available.')\n        else:\n            print('Redis server did not respond to PING.')\n\n# Check if Redis server is available\ncheck_redis_availability()\n</code></pre>"},{"location":"Python/Cache/Redis/#cache-df","title":"cache df","text":"<pre><code>import io\nimport time\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport redis\nr = redis.StrictRedis(host='my-redis', port=6379)\n\ndef paqdumps(df):\n    buf = io.BytesIO() \n    d = pa.Table.from_pandas(df)    \n    pq.write_table(d, buf, compression='zstd')      \n    b = buf.getvalue()\n    r.set('df', b)\n    return b\n\ndef paqloads():\n    b = r.get('df') \n    buf = pa.BufferReader(b)\n    return pq.read_table(buf).to_pandas()  \n\nt0 = time.time(); _ = paqdumps(df)  ; print(f'dump time: {time.time() - t0}')\nt0 = time.time(); dx = paqloads()  ; print(f'load time: {time.time() - t0}')\n</code></pre>"},{"location":"Python/Cache/Redis/#redis-cache-decorator","title":"redis cache decorator","text":"<p>https://gist.github.com/mminer/34d4746fa82b75182ee7 <pre><code>import json\nfrom functools import wraps\nfrom redis import StrictRedis\n\nredis = StrictRedis()\n\ndef cached(func):\n    \"\"\"\n    Decorator that caches the results of the function call.\n\n    We use Redis in this example, but any cache (e.g. memcached) will work.\n    We also assume that the result of the function can be seralized as JSON,\n    which obviously will be untrue in many situations. Tweak as needed.\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        # Generate the cache key from the function's arguments.\n        key_parts = [func.__name__] + list(args)\n        key = '-'.join(key_parts)\n        result = redis.get(key)\n\n        if result is None:\n            # Run the function and cache the result for next time.\n            value = func(*args, **kwargs)\n            value_json = json.dumps(value)\n            redis.set(key, value_json)\n        else:\n            # Skip the function entirely and use the cached value instead.\n            value_json = result.decode('utf-8')\n            value = json.loads(value_json)\n\n        return value\n    return wrapper\n</code></pre></p>"},{"location":"Python/Cache/Redis/#another-redis-decorator","title":"another redis decorator","text":"<p>https://raynix.info/archives/4440</p>"},{"location":"Python/Celery/Example/","title":"Example","text":"<pre><code>from celery import Task, registry, Celery\n\nclass Task01(Task):\n    _v: float\n    def __init__(self):\n        self._v = 0   \n    def execute(self, x):\n        self._v += x\n        return self._v\n\napp = Celery(\n    'tasks', \n    broker='redis://localhost:6379',\n    backend='redis://localhost:6379',\n)\n@app.task(base=Task01)\ndef task_01(x):\n    val = task_01.execute(x)\n    return val\n</code></pre>"},{"location":"Python/Class/ClassMethod/","title":"Class Method","text":"<ul> <li>Class methods are bound to the <code>class</code> and not the instance of the class. </li> <li>They can access and modify <code>class-level</code> variables but don't have access to instance-specific attributes.</li> </ul>"},{"location":"Python/Class/ClassMethod/#example","title":"example","text":"<pre><code>class MyClass:\n    class_variable = 0\n    def __init__(self, value):\n        self.value = value\n    def instance_method(self):\n        print('Instance method called with value:', self.value)\n    @classmethod\n    def class_method(cls):\n        print('Class method called with class variable:', cls.class_variable)\n\n# Create instance and call instance method\nobj1 = MyClass(10)\nobj1.instance_method()\n\n# Call class method\nMyClass.class_variable = 5\nMyClass.class_method()\n</code></pre>"},{"location":"Python/Class/Initialization/","title":"Initialization","text":""},{"location":"Python/Class/Initialization/#new-vs-init","title":"new vs init","text":"<p><code>__init__</code> will always called after <code>__new__</code></p> <p><code>__new__</code>:  - static class method - create a new instance - return a new class instance</p> <p><code>__init__</code>:  - instance method - initialize a new instance - return nothing</p>"},{"location":"Python/Class/Property/","title":"Property","text":""},{"location":"Python/Class/Property/#get-and-set-property","title":"get and set property","text":"<pre><code>class MyCls(object):\n    def __init__(self):\n        self._x = None\n\n    @property\n    def x(self):\n        return self._x\n\n    @x.setter\n    def x(self, value):\n        self._x = value\n</code></pre>"},{"location":"Python/Class/Underscore/","title":"Underscore","text":""},{"location":"Python/Class/Underscore/#_x","title":"_x","text":""},{"location":"Python/Class/Underscore/#__x","title":"__x","text":""},{"location":"Python/Class/Underscore/#x","title":"x","text":""},{"location":"Python/Conda/Bashrc/","title":"Bashrc","text":""},{"location":"Python/Conda/Bashrc/#auto-activate-env","title":"auto activate env","text":"<p>run <code>mamba shell init</code> to add code in <code>.bashrc</code>. after that add <code>mamba activate my-env</code></p>"},{"location":"Python/Conda/Bashrc/#conda-env-name-not-show-in-terminal","title":"conda env name not show in terminal","text":"<p>You have custom <code>PS1</code> definitions that are being applied after your <code>mamba activate my-env</code> command.</p> <p>This custom PS1 is being applied after <code>mamba activate my-env</code>, effectively overwriting the prompt modification that Mamba attempts to do.</p> <p>Move custom PS1 definitions before <code>conda/mamba initialize</code>.</p>"},{"location":"Python/Conda/Bashrc/#show-git-branch-name-in-color","title":"show git branch name in color","text":"<pre><code># Show git branch name\nforce_color_prompt=yes\ncolor_prompt=yes\nparse_git_branch() {\n   git branch 2&gt; /dev/null | sed -e '/^[^*]/d' -e 's/* \\(.*\\)/(\\1)/'\n}\nif [ \"$color_prompt\" = yes ]; then\n   PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[01;31m\\]$(parse_git_branch)\\[\\033[00m\\]\\$ '\nelse\n   PS1='${debian_chroot:+($debian_chroot)}\\u@\\h:\\w$(parse_git_branch)\\$ '\nfi\nunset color_prompt force_color_prompt\n</code></pre>"},{"location":"Python/Conda/Build/","title":"Build","text":""},{"location":"Python/Conda/Build/#build","title":"Build","text":"<p>https://docs.conda.io/projects/conda-build/en/latest/concepts/recipe.html</p>"},{"location":"Python/Conda/Build/#parameters","title":"parameters","text":"<p>https://docs.conda.io/projects/conda-build/en/stable/resources/commands/conda-build.html</p> <p>Building a conda package requires a recipe. A conda-build recipe is a flat directory that contains the following files: - meta.yaml: Only <code>package/name</code> and <code>package/version</code> are required - build.sh: It is executed using the bash command for macOS and Linux. - bld.bat: It is executed using cmd for Windows. - run_test.[py,pl,sh,bat]: a test script that runs automatically if it is part of the recipe. - Optional patches that are applied to the source. - Other resources that are not included in the source and cannot be generated by the build scripts.</p> <p>bld.bat <pre><code>\"%PYTHON%\" setup.py install\nif errorlevel 1 exit 1\n</code></pre></p> <p>build.sh <pre><code>$PYTHON setup.py install     # Python command to install the script\n</code></pre></p>"},{"location":"Python/Conda/Build/#buildsh","title":"build.sh","text":"<p>You can define the <code>build.sh</code> script inline in the meta.yaml file otherwise conda-build will look for a <code>build.sh</code> script in the recipe directory <pre><code>build:\n  number: 100\n  script: python -m pip install . --no-deps --ignore-installed #--no-cache-dir -vvv\n</code></pre></p>"},{"location":"Python/Conda/Build/#install-conda-build","title":"install conda-build","text":"<ul> <li>Must install cond-build in <code>base</code> env.</li> <li>https://docs.conda.io/projects/conda-build/en/stable/install-conda-build.html</li> <li>For proper functioning, it is strongly recommended to install conda-build in the conda base environment. Not doing so may lead to problems. <pre><code>conda activate base\nconda install conda-build\n</code></pre></li> </ul>"},{"location":"Python/Conda/Build/#conda-mambabuild","title":"conda mambabuild","text":"<p><code>boa</code> has been deprecated in favor of <code>rattler\u2011build</code>, a substantially faster rust-based alternative. - https://github.com/conda/conda-build/issues/5351 - <code>boa</code> appears to be a concluded project - latest conda &gt;= 23.10.0 by default use <code>libmamba</code> solver so there is no need to use <code>conda mambabuild</code> that requires <code>boa</code></p>"},{"location":"Python/Conda/Build/#debug","title":"debug","text":"<p>https://docs.conda.io/projects/conda-build/en/stable/user-guide/recipes/debugging.html</p> <p>debugging is a process of getting into or recreating the environment and set of shell environment variables that conda-build creates during its build or test processes. <pre><code>conda debug recipe\nconda debug recipe --python=3.12 --variants=\"{blas_impl: 'openblas'}\"\ncd /debug/path &amp;&amp; source /debug/path/build_env_setup.sh\nconda build purge-all # remove previously built packages\n</code></pre></p>"},{"location":"Python/Conda/Build/#build-conda-package","title":"build conda package","text":"<p>We run the command inside the project root folder: - <code>--output</code> will disable all output messages - <code>--croot</code> path cannot be a subfolder of the current project folder; must be outside of the project folder   <code>AssertionError: Can't merge/copy source into subdirectory of itself.  Please create separate spaces for these things.</code></p> <p>Example: <pre><code>export VERSION=0.15.1\nconda build recipe --no-anaconda-upload --python 3.12 --croot ../conda-build --no-test\n</code></pre> - We assume in the meta.yaml file there is a variable named VERSION. - In the first line we pass the version value to the VERSION variable. - The second line is used to build the conda package to a folder called conda-build.</p> <p>More options: <pre><code>conda build recipe --no-anaconda-upload --python 3.12 --croot c:/pkg/conda --no-test\nconda build recipe --no-anaconda-upload --python 3.12 --croot /build/path --no-test --channel ch1 --channel ch2\n</code></pre> Note that without set <code>--python</code>, will build a package compatible to the python version in the current env. We can also set the cli flags via env var <code>CONDA_PY</code> and the flag <code>--variants</code> which accepts JSON-formatted text. example: https://docs.conda.io/projects/conda-build/en/latest/resources/variants.html <pre><code>conda build recipe --variants \"{python: [2.7, 3.5], vc: [9, 14]}\"\n</code></pre></p>"},{"location":"Python/Conda/Build/#remove-source-and-build-intermediates","title":"remove source and build intermediates","text":"<pre><code>conda build purge\n</code></pre>"},{"location":"Python/Conda/Build/#pass-variable-value-to-metayaml","title":"pass variable value to meta.yaml","text":"<p>before running <code>conda build</code> <pre><code>export VERSION=1.0.0\n</code></pre></p> <p>Templating with Jinja: https://docs.conda.io/projects/conda-build/en/3.21.x/resources/define-metadata.html - Conda-build supports Jinja templating in the <code>meta.yaml</code> file - We can use Jinja templating inside the <code>meta.yaml</code> file to dynamically inject environment variables. <pre><code>package:\n  name: {{ NAME }}\n  version: {{ VERSION }}\n</code></pre> In this example, the values of env varables <code>NAME</code> and <code>VERSION</code> will be injected into the <code>meta.yaml</code> file.</p>"},{"location":"Python/Conda/Build/#metayaml","title":"meta.yaml","text":"<p>https://stackoverflow.com/questions/38919840/get-package-version-for-conda-meta-yaml-from-source-file</p> <p>https://www.underworldcode.org/articles/build-conda-packages/ <pre><code>{% set NAME = \"pkg\" %}\n{% set VERSION = load_setup_py_data().version %}\n{% set GITHUB_URL = load_setup_py_data().url %}\n{% set DESCRIPTION = load_setup_py_data().description %}\n\npackage:\n  name: {{ NAME }}\n  version: {{ VERSION }}\n\nsource:\n  path: ../\n\noutputs:\n  - name: {{ NAME }}\n    build:\n      number: 0\n      script: python -m pip install --no-deps --ignore-installed .\n      entry_points:\n        - pkg-run = pkg.main:cli\n    requirements:\n      host:\n        - pip\n        - python\n        - setuptools &gt;=41.0.0\n      run:\n        - click\n        - numba\n        - numpy\n        - pandas\n        - python\n    about:\n      home: {{ GITHUB_URL }}\n      summary: {{ DESCRIPTION }}\n\n  - name: {{ NAME }}.test\n    requirements:\n      run:\n        - pytest\n        - pytest-cov\n        - coverage\n    test:\n      source_files:\n        - tests\n    about:\n      home: {{ GITHUB_URL }}\n      summary: Test dependencies for {{ NAME }}\n\n  - name: {{ NAME }}.doc\n    requirements:\n      run:\n        - docstring_parser\n        - mkdocs\n        - mkdocstrings &lt;0.18\n        - mkdocs-click\n        - mkdocs-material\n        - mkdocs-material-extensions\n        - pytkdocs\n    about:\n      home: {{ GITHUB_URL }}\n      summary: Doc dependencies for {{ NAME }}\n</code></pre></p>"},{"location":"Python/Conda/Build/#issue","title":"issue","text":"<p>https://stackoverflow.com/questions/69030813/doing-a-conda-build-does-not-find-any-files/69044365#69044365</p>"},{"location":"Python/Conda/Build/#push-conda-package-to-conda-repo","title":"push conda package to conda repo","text":"<pre><code>import requests\ndef push_conda(\n    filepath,\n    *,\n    channel='example/dev',\n    platform='linux-64',\n    repo_url='https://conda.example.com',\n    force=False,\n):\n    url = f'{repo_url}/api/packages'\n    files = {'file': open(filepath, 'rb')}\n    params = {\n        'channel': channel,\n        'platform': platform,\n        'force': False,\n    }\n    r = requests.post(url, files=files, data=params)\n    if r.status_code != 200:\n        try:\n            msg = r.json().get('message')\n        except json.JSONDecodeError:\n            msg = r.text\n        finally:\n            raise Exception(msg)\n\nfilepath = '/path/to/conda-build/linux-64/my-build-0.1.0-py39_0.tar.bz2'\npush_conda(filepath)\n</code></pre>"},{"location":"Python/Conda/CLI/","title":"CLI","text":"<p>https://docs.conda.io/projects/conda/en/4.6.0/commands</p>"},{"location":"Python/Conda/CLI/#activate-anaconda","title":"activate anaconda","text":"<pre><code>conda create -n py38 python=3.8 anaconda\nset PATH=C:\\Anaconda\\envs\\py33\\Scripts;C:\\Anaconda\\envs\\py33;%PATH%\nconda env list\nconda activate py38\nconda env remove -n ENV_NAME\n</code></pre>"},{"location":"Python/Conda/Cache/","title":"Cache","text":""},{"location":"Python/Conda/Cache/#shared-cache","title":"shared cache","text":"<p>https://docs.anaconda.com/free/anaconda/packages/shared-pkg-cache/</p> <p>Create a shared cache for all pipeline build agents. This will also reduce the time to clean the cache after each build.</p>"},{"location":"Python/Conda/Cache/#cache-in-pipeline","title":"cache in pipeline","text":"<p>https://stackoverflow.com/questions/62420695/how-to-cache-pip-packages-within-azure-pipelines</p>"},{"location":"Python/Conda/Channel/","title":"Channel","text":"<p>https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-channels.html</p>"},{"location":"Python/Conda/Channel/#list-channels","title":"list channels","text":"<pre><code>conda config --show channels\n</code></pre>"},{"location":"Python/Conda/Channel/#delete-a-channel","title":"delete a channel","text":"<pre><code>conda config --remove channels https://anaconda.org\n</code></pre>"},{"location":"Python/Conda/Channel/#list-packages-available-in-channel","title":"list packages available in channel","text":"<p>https://stackoverflow.com/questions/43222407/how-to-list-package-versions-available-with-conda <pre><code>conda search -c conda-forge jupyterlab # will also search in the channels listed in .condarc\nconda search -c conda-forge \"jupyterlab&gt;=3.5\"\nconda search -c conda-forge --override-channels jupyterlab # search the provided channel only\nconda search -c conda-forge --override-channels --json \"jupyterlab&gt;=3.6\"\nconda search --platform linux-64 -c conda-forge --override-channels jupyterlab # default to current platform\n</code></pre></p>"},{"location":"Python/Conda/Channel/#custom-channel","title":"custom channel","text":"<p>https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-custom-conda-channel</p>"},{"location":"Python/Conda/Conda/","title":"Conda","text":"<p>Conda and Mamba Commands for Managing Virtual Environments\\ https://www.imranabdullah.com/2021-08-21/Conda-and-Mamba-Commands-for-Managing-Virtual-Environments</p>"},{"location":"Python/Conda/Conda/#conda-vs-pip","title":"conda vs pip","text":"<p>https://pythonspeed.com/articles/conda-vs-pip/#:~:text=The%20fundamental%20difference%20between%20pip,even%20the%20Python%20interpreter%20itself).</p> <ul> <li>conda is more powerful</li> <li>conda has more freedom to choose some tools like BLAS based on mkl</li> <li>pip wheel can lead larger package size</li> <li>pip wheel not include executables and tools</li> </ul>"},{"location":"Python/Conda/Conda/#install-conda","title":"install conda","text":"<p>don't install conda in a non-base env: - https://github.com/conda/conda/issues/11519 - once activated that env will be shown as base</p>"},{"location":"Python/Conda/Conda/#show-all-conda-info","title":"show all conda info","text":"<pre><code>conda info --all\n</code></pre>"},{"location":"Python/Conda/Conda/#update-conda","title":"update conda","text":"<pre><code>conda update -n base conda\n</code></pre>"},{"location":"Python/Conda/Conda/#create-an-env","title":"create an env","text":"<pre><code>conda create --name &lt;env-name&gt;\n</code></pre>"},{"location":"Python/Conda/Conda/#init","title":"init","text":"<p>Only need to run conda initialization when the CONDA_SHLVL variable is 0 - there are no activated envs.</p>"},{"location":"Python/Conda/Conda/#activate","title":"activate","text":"<p>Conda puts each environment on a stack as it is activated and deactivating merely pops one item off the stack. The depth of the conda environment stack is stored in the environment variable CONDA_SHLVL. Running conda deactivate that many times should deactivate all environments. <pre><code>for i in $(seq ${CONDA_SHLVL}); do\n    conda deactivate\ndone\n</code></pre></p>"},{"location":"Python/Conda/Conda/#activate-anacondaminicondaconda-forge","title":"activate anaconda/miniconda/conda-forge","text":"<pre><code>conda create -n env-py38 python=3.8 anaconda\nset PATH=C:\\Anaconda\\envs\\py33\\Scripts;C:\\Anaconda\\envs\\py33;%PATH%\nconda env list\nconda activate env-py38\nconda env remove -n ENV_NAME #if the folder still there, kill python and delete the folder\nconda clean -afy #remove cache\n\n#clone an existing env\nconda create --name ENV_NAME_new --clone ENV_NAME_from\n</code></pre>"},{"location":"Python/Conda/Conda/#create-a-notebook-kernel","title":"create a notebook kernel","text":"<p>The second command is for registering the kernel for the environment. Do we need to run the second command? <pre><code>conda install -c anaconda ipykernel\npython -m ipykernel install --user --name=test-env\n</code></pre></p>"},{"location":"Python/Conda/Conda/#install-package","title":"install package","text":"<pre><code>conda install -c conda-forge dash-bootstrap-components\n</code></pre>"},{"location":"Python/Conda/Conda/#from-github","title":"from github","text":"<pre><code>conda activate my-env\nconda install git pip\npip install git+git://github.com/plotly/Kaleido@master\n# mamba install python-kaleido\n</code></pre>"},{"location":"Python/Conda/Condarc/","title":"Condarc","text":""},{"location":"Python/Conda/Condarc/#config","title":"config","text":"<p>The <code>channel_alias</code> specifies a base URL to prepend to all channel names that do not start with <code>http://</code> or <code>https://</code>.  So, any channel name without an explicit URL will be prefixed with <code>https://conda.example.com/</code>.</p> <p>Therefore, <code>example/uat</code> will be interpreted as <code>https://conda.example.com/example/uat</code>, effectively linking it to <code>https://conda.example.com/</code>. <pre><code>auto_update_conda: false\nupdate_dependencies: false\nchannel_alias: https://conda.example.com/\nchannel_priority: false\nchannels:\n  - example/uat #!top\n  - https://conda.anaconda.org/conda-forge #!top\n  - https://repo.anaconda.com/pkgs/msys2\nssl_verify: False #/etc/ssl/certs/ca-certificates.crt\nenvs_dirs:\n  - ~/conda-envs/\npinned_packages:\n  - conda=4.9.2\n  - mamba=0.33.3\n</code></pre></p>"},{"location":"Python/Conda/Condarc/#use-tarbz2-instead-of-conda-format","title":"use <code>tar.bz2</code> instead of <code>.conda</code> format","text":"<p>in <code>.condarc</code> add: <pre><code>conda-build:\n  pkg_format: 'tar.bz2'\n</code></pre></p>"},{"location":"Python/Conda/Config/","title":"Config","text":"<p>https://docs.conda.io/projects/conda/en/latest/configuration.html</p> <p>https://conda.io/projects/conda/en/latest/user-guide/configuration/use-condarc.html</p>"},{"location":"Python/Conda/Config/#conda-config","title":"conda config","text":"<p>https://docs.conda.io/projects/conda/en/latest/commands/config.html</p> <p>Modify configuration values in .condarc. <code>--env</code> Write to the active conda environment .condarc file <pre><code>conda config --add channels conda-canary\nconda config --env --add pinned_packages conda=$conda_version\nconda config --env --set verbosity 3\n\n# not activate conda's base environment on startup\nconda config --set auto_activate_base false\n</code></pre></p>"},{"location":"Python/Conda/Config/#requests_ca_bundle","title":"REQUESTS_CA_BUNDLE","text":""},{"location":"Python/Conda/Docker/","title":"Docker","text":"<p>Steps to build the docker image for a conda app <code>my-app</code>.</p>"},{"location":"Python/Conda/Docker/#install-conda","title":"install conda","text":"<pre><code>COPY --chown=user:user docker/.bashenv /home/user/.bashenv\nCOPY --chown=user:user docker/.condarc /home/user/.condarc\nARG CONDA_VER=24.11.0\nARG MAMBA_VER=2.0.4\nRUN conda-pin-package conda=$CONDA_VER &amp;&amp; \\\n    conda config --set ssl_verify /etc/ssl/certs/ca-certificates.crt &amp;&amp; \\\n    conda-install conda mamba==$MAMBA_VER &amp;&amp; \\\n    conda-clean\n</code></pre>"},{"location":"Python/Conda/Docker/#build-conda-package-my-apptarbz2","title":"build conda package <code>my-app.tar.bz2</code>","text":"<p>Note that the <code>--croot</code> folder must be outside of the project folder. <pre><code>export NAME=my-repo #pass value to meta.yaml variable NAME\nconda build recipe --no-anaconda-upload --python 3.9 --croot ../conda-build --no-test\n</code></pre></p>"},{"location":"Python/Conda/Docker/#create-a-new-env-and-install-my-app","title":"create a new env and install <code>my-app</code>","text":"<p>If we can push the conda package to the conda repo we do not need the local repo. <pre><code>conda create --name my-app-0.1.0 --yes\nconda install --name my-app-0.1.0 --yes --quiet -v \\\n--channel /home/user/conda-build --no-update-deps my-app=0.1.0=py39_0 python=3.9\n</code></pre></p>"},{"location":"Python/Conda/Docker/#export-condaenv-file-listing-all-packages","title":"export <code>conda.env</code> file listing all packages","text":"<pre><code>conda activate my-app-0.1.0\nconda list --explicit &gt; conda.env\nmv /path/to/conda.env path/to/my-app/\nconda deactivate\nconda remove --name my-app-0.1.0 --all --yes\n</code></pre> <p>Note that if we cannot send the conda package to the conda repo,  we have to use the local repo to install the conda package: <pre><code>conda install --name my-app-0.1.0 --yes --quiet -v \\\n --channel /home/dev/conda-build \\\n --no-update-deps my-app-0.1.0=py39_0 python=3.9\n</code></pre> In this case, after exporting the conda.env file we should change the path of the new conda package according to the dockerfile content.</p>"},{"location":"Python/Conda/Docker/#copy-local-conda-package-folder-to-under-the-project-folder","title":"copy local conda package folder to under the project folder","text":"<p>Note that the local path of the conda package in <code>conda.env</code> must be updated based on the dockerfile content. <pre><code>cp -r ../conda-build .build/conda\n</code></pre></p>"},{"location":"Python/Conda/Docker/#install-conda-and-all-the-packages-from-condaenv-in-dockerfile","title":"install conda and all the packages from <code>conda.env</code> in dockerfile","text":"<pre><code>conda create --yes --quiet --verbose --name my-app-0.1.0 --file conda.env &amp;&amp; conda-clean\n</code></pre>"},{"location":"Python/Conda/Docker/#build-docker-image","title":"build docker image","text":"<p>run docker build (pass value to ARG via <code>--build-arg</code>) <pre><code>docker build . --build-arg NAME=my-app -f ./docker/linux/my-app.docker \\\n--no-cache --force-rm -t docker.example.com/uat/my-app:0.1.0\n</code></pre> Here we pass the value for the <code>NMAE</code> variable in the dockerfile via the <code>--build-arg</code> command flag  and tag the image as <code>docker.example.com/uat/my-app:0.1.0</code>. </p> <p>In case the build process fails, we can run into the last successful layer to check the issue: <pre><code>docker run -it &lt;last-successful-layer-id&gt; bash -il\n</code></pre></p>"},{"location":"Python/Conda/Docker/#push-docker-image","title":"push docker image","text":"<p>When the build process is finished we can push the docker image to the docker registry. <pre><code>docker push docker.example.com/uat/my-app:0.1.0\n</code></pre></p>"},{"location":"Python/Conda/Docker/#python-mamba-workaround","title":"Python mamba workaround","text":"<pre><code># Workaround for mamba SSL issue https://github.com/mamba-org/mamba/issues/628\nENV REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt\n</code></pre>"},{"location":"Python/Conda/Docker/#python-conda-ssl-config","title":"Python conda ssl config","text":"<pre><code># SSL configuration for httpx https://www.python-httpx.org/compatibility/#ssl-configuration\nENV SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt\nENV MINIFORGE_ROOT=/home/user/miniforge/\n</code></pre>"},{"location":"Python/Conda/Env/","title":"Env","text":""},{"location":"Python/Conda/Env/#list-envs","title":"list envs","text":"<pre><code>conda env list\n</code></pre>"},{"location":"Python/Conda/Env/#delete-an-env","title":"delete an env","text":"<p>The <code>--all</code> flag ensures that all files associated with the environment, including packages and cache, are removed. <pre><code>conda deactivate\nconda remove -n &lt;env-name&gt; # or\nconda remove --name &lt;env-name&gt; --all\n\n# by path if there is no env name\nconda env remove --prefix c:\\tmp\\conda\\my-env\n\n# always good to run\nconda clean --all  # clean Up Conda Metadata, such as unused packages and caches\n</code></pre></p>"},{"location":"Python/Conda/Env/#change-default-env-location","title":"change default env location","text":"<p>Open or create a <code>.condarc</code> file in home directory <code>~/.condarc</code> and add: <pre><code>envs_dirs:\n  - ~/conda-envs/\n</code></pre></p>"},{"location":"Python/Conda/Env/#create-env-and-install-packages","title":"create env and install packages","text":"<p>https://stackoverflow.com/questions/66291897/conda-create-and-conda-install</p> <p>option 1: create an environment, activate, install packages <pre><code>conda create --name env_name python=3.10\nconda activate env_name\nconda install package_name another_package\n</code></pre></p> <p>option 2: create an environment with packages <pre><code>conda create -name env_name python=3.10 package_name another_package\nconda activate env_name\n</code></pre></p>"},{"location":"Python/Conda/Env/#create-env","title":"create env","text":"<pre><code>conda create -n py10 python=3.10\nconda create --name &lt;env&gt; --file &lt;env-file&gt; &amp;&amp; conda-clean\n</code></pre>"},{"location":"Python/Conda/Env/#create-env-with-exported-env-file-not-suggested","title":"create env with exported env-file (not suggested)","text":"<p><pre><code>conda activate &lt;env&gt; &amp;&amp; conda list --explicit &gt; environment.txt\n</code></pre> The output can be used as the input  for creating a conda env when build a docker image. <pre><code>conda create --name &lt;name&gt; --file ~/environment.txt &amp;&amp; conda-clean\n</code></pre> <p>Note that the <code>conda list --explicit</code> command only lists packages installed via Conda, and it doesn't include packages installed using <code>pip</code>.  If you want to include <code>pip</code> packages in your environment export, you would need to use <code>pip freeze</code> to create a requirements.txt file separately. Export Conda and pip Packages <pre><code>conda list --explicit &gt; environment_conda.txt   \npip freeze &gt; environment_pip.txt\n</code></pre></p> <p>Create a new environment using both files: <pre><code>conda create --name my_new_environment --file environment_conda.txt\npip install -r environment_pip.txt\n</code></pre> Keep in mind that managing environments with both Conda and pip can be a bit trickier, as they have different dependency resolution mechanisms. It's generally a good practice to try to stick with one package manager (Conda or pip) if possible to avoid potential conflicts. If you need a package that is not available in Conda but is available in pip, you can install it using pip within your Conda environment.</p>"},{"location":"Python/Conda/Env/#create-env-with-env-yml-file","title":"create env with env yml file","text":"<p><pre><code>conda env export &gt; environment.yml\nconda env create -f environment.yml\n</code></pre> environment.yml will include both conda and pip packages <pre><code>name: env-name\nchannels:\n  - https://conda.mini-forge.com/uat/linux-64/\ndependencies:\n- python\n- pip\n- pip:\n    - pypi-package-name\nprefix: C:\\Users\\user\\conda-envs\\env-name\n</code></pre></p>"},{"location":"Python/Conda/Env/#install-package","title":"install package","text":"<pre><code>conda activate py10\nconda install python=3.10\n</code></pre>"},{"location":"Python/Conda/Env/#install-from-local-conda-package","title":"install from local conda package","text":"<p>assume the conda package is: <code>/home/user/dev/.build/linux-64/my-dev-package-0.1.1-py39_0.tar.bz2</code></p> <p>why does not work??? <pre><code>conda activate dev-env \\\n&amp;&amp; conda install --yes --quiet -v --channel /home/user/dev/.build --no-update-deps my-dev-package=0.1.1=py39_0\n</code></pre> one command (not correct?) <pre><code>conda install -n dev-env --yes --quiet -v \\\n--channel /home/user/dev/.build --no-update-deps my-dev-package=0.1.1=py39_0 python=3.9\n</code></pre></p>"},{"location":"Python/Conda/EnvFile/","title":"Env file","text":"<p>https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#create-env-file-manually</p> <p><code>environment.yml</code> vs <code>meta.yaml</code>: - https://stackoverflow.com/questions/78309675/conda-build-duplication-of-requirements-in-environment-yml-and-meta-yml - <code>environment.yml</code> describes what goes into an environment, encompassing all necessary packages for a project - <code>meta.yaml</code> describes how to build a single Conda package, including its specific dependencies and build process - <code>environment.yml</code> can be exported after building a conda package</p>"},{"location":"Python/Conda/EnvFile/#create-env-from-envyml","title":"create env from env.yml","text":"<pre><code>conda env create -f environment.yml\nconda env create --name conda-env --file environment.yml --force #force creating new env with the input name if there is one\n</code></pre> <p>conda env create --name basic-ml-env --file environment.yml --force environment.yml <pre><code>name: dev\nchannels:\n  - javascript\ndependencies:\n  - python=3.9\n  - bokeh=2.4.2\n  - numpy=1.21.*\n  - nodejs=16.13.*\n  - flask\n  - pip\n  - pip:\n    - Flask-Testing\n</code></pre></p>"},{"location":"Python/Conda/EnvVar/","title":"Env Var","text":""},{"location":"Python/Conda/EnvVar/#custom-managed-env-vars","title":"custom managed env vars","text":"<ul> <li>https://stackoverflow.com/questions/31598963/how-to-set-specific-environment-variables-when-activating-conda-environment</li> <li>https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#windows</li> </ul>"},{"location":"Python/Conda/Index/","title":"Index","text":"<p>The conda index command is particularly useful when you're hosting your own conda package channel,  either on a local server or on a cloud storage service like AWS S3 or Azure Blob Storage.  It allows you to manage and update the channel's index files, ensuring that users can easily discover and install packages from your channel.</p>"},{"location":"Python/Conda/Index/#separated-from-conda-build","title":"separated from conda-build","text":"<p>https://conda.github.io/conda-index/</p>"},{"location":"Python/Conda/Index/#create-index-for-a-channel","title":"create index for a channel","text":"<p>https://docs.conda.io/projects/conda-build/en/stable/concepts/generating-index.html <pre><code>conda index channel_path # conda index channels/conda-forge\n</code></pre></p>"},{"location":"Python/Conda/Install/","title":"Install","text":""},{"location":"Python/Conda/Install/#install-conda","title":"install conda","text":"<ul> <li>install miniforge</li> <li>if <code>'conda' is not recognized as an internal or external command</code>. add conda path to user path env var.</li> </ul>"},{"location":"Python/Conda/Install/#update-conda","title":"update conda","text":"<p>might not work - a fresh install might be needed! <pre><code>conda update -n base conda\nconda update -n base -c https://conda.anaconda.org/conda-forge conda\nconda activate base\nconda update mamba\n</code></pre></p> <p>Upgrade python to a new version in an existing env: <pre><code>conda install python=3.12\nmamba install python=3.12\n</code></pre></p> <p>Might need to manually remove some packages that are not compatible with python 3.12. <pre><code>mamba remove pywin32 gurobi\n</code></pre></p> <p>fix inconsistency: <pre><code>conda activate base\nconda install anaconda\nconda update --all\n</code></pre></p>"},{"location":"Python/Conda/Issue/","title":"Issue","text":""},{"location":"Python/Conda/Issue/#run-conda-init-before-conda-activate","title":"run <code>conda init</code> before <code>conda activate</code>","text":"<p>https://stackoverflow.com/questions/77901825/unable-to-activate-environment-conda-prompted-to-run-conda-init-before-cond solution: <code>source activate base</code> before <code>conda activate my_env</code>? Yes, works.</p>"},{"location":"Python/Conda/Issue/#stuck-on-solving-environment","title":"stuck on <code>solving environment</code>","text":"<p>https://github.com/conda/conda/issues/11919</p> <p>Reason: added too many packages in base</p> <p>Solution: - remove packages un-related to conda in base, or - use libmamba   - Update conda and conda-build     <pre><code>conda update conda\nconda update conda-build\n</code></pre>   - Install libmamba and set the new solver     <pre><code>conda install -n base conda-libmamba-solver\nconda config --set solver libmamba\n</code></pre></p>"},{"location":"Python/Conda/Issue/#wsl-condahttperror-http-000-connection-failed","title":"WSL: CondaHTTPError: HTTP 000 CONNECTION FAILED","text":"<p>restart wsl after install conda: <code>wsl --shutdown</code></p>"},{"location":"Python/Conda/Issue/#conda-env-became-base","title":"conda env became base","text":"<p>https://stackoverflow.com/questions/57028760/why-conda-redefines-base-environment-after-activation-of-another-environment</p> <p>If the <code>conda-build</code> package is installed in the environment name <code>dev</code>, then the <code>dev</code> environment becomes base after activation.</p> <p>This means we should only install <code>conda-build</code> in base env not other envs.</p>"},{"location":"Python/Conda/Issue/#package-with-specific-version-could-not-be-installed","title":"package with specific version could not be installed","text":"<p><code>package libarrow-substrait-16.0.0-h1f0e801_0_cpu requires libabseil &gt;=20240116.2,&lt;20240117.0a0, but none of the providers can be installed</code></p> <p>Solution: try to install the required version till we find the root blocker of the package. In this case try to install <code>libabseil=20240116.2</code> and it will report the causer package and version.</p>"},{"location":"Python/Conda/Mamba/","title":"Mamba","text":"<p>Compared to conda, mamba is faster but larger.</p>"},{"location":"Python/Conda/Mamba/#micromamba","title":"micromamba","text":"<ul> <li>micromamba is a tiny version of the mamba package manager</li> <li>micromamba supports a subset of all mamba or conda commands</li> </ul>"},{"location":"Python/Conda/Mamba/#show-available-versions","title":"show available versions","text":"<pre><code>mamba search -c conda-forge pyarrow\nmamba show conda-forge/pyarrow\n</code></pre>"},{"location":"Python/Conda/Mamba/#update","title":"update","text":"<p>Note: Installing the conda or mamba package in anywhere other than base will lead to surprising behavior. <pre><code>mamba update -n base mamba\nmamba install -n base 'mamba&gt;=1.0'\n</code></pre></p>"},{"location":"Python/Conda/Mamba/#list-env","title":"list env","text":"<pre><code>mamba env list\n</code></pre>"},{"location":"Python/Conda/Mamba/#create-env","title":"create env","text":"<pre><code>mamba create --name dev python=3.11\n# to activate dev by default, in the end of .bashrc add\nmamba activate dev\n# then\nsource ~/.bashrc\n</code></pre>"},{"location":"Python/Conda/Mamba/#install-package","title":"install package","text":"<pre><code>mamba install scipy black numpy pandas -y\nmamba install sqlalchemy=1.* #install latest version of 1.x\n</code></pre>"},{"location":"Python/Conda/Mamba/#update-package","title":"update package","text":"<pre><code>mamba update pandas -y\n</code></pre>"},{"location":"Python/Conda/Mamba/#remove-package","title":"remove package","text":"<pre><code>mamba remove pandas\n</code></pre>"},{"location":"Python/Conda/Mamba/#clear-cache","title":"clear cache","text":"<pre><code>mamba clean --all\n</code></pre>"},{"location":"Python/Conda/Mamba/#error-ssl-certificate-not-ok","title":"error: SSL certificate not OK","text":"<p>in <code>.condarc</code>, we should delete this line <pre><code>ssl_verify: C:\\Users\\usr\\conda-envs\\my.crt\n</code></pre></p>"},{"location":"Python/Conda/Miniforge/","title":"Miniforge (Mambaforge)","text":"<p>https://github.com/conda-forge/miniforge</p>"},{"location":"Python/Conda/Miniforge/#install","title":"install","text":"<pre><code>curl -LO \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\nbash Mambaforge-$(uname)-$(uname -m).sh\n</code></pre>"},{"location":"Python/Conda/Miniforge/#uninstall","title":"uninstall","text":"<pre><code># reverse modified shell rc files\nconda init --reverse --dry-run\nconda init --reverse\n\n# remove the entire Miniforge install directory\nCONDA_BASE_ENVIRONMENT=$(conda info --base)\nrm -rf ${CONDA_BASE_ENVIRONMENT}\n\n# optionally remove hidden files and folders\nrm -rf \"${HOME}/.condarc\"\n</code></pre>"},{"location":"Python/Conda/Repo/","title":"Repo","text":"<p>How create a conda repo: Use <code>Flask</code> to create a basic web app with api functions.</p>"},{"location":"Python/Conda/Repo/#web-api","title":"web api","text":"<ul> <li>post: upload a conda package: save the package file and update index</li> <li>post: move a conda package: move the package file and update index in both channels</li> <li>delete: delete a conda package: delet the package file and update index</li> </ul>"},{"location":"Python/Conda/Repo/#configjson","title":"config.json","text":"<p>setup conda channels</p>"},{"location":"Python/Conda/Repo/#setup-internal-channel","title":"setup internal channel","text":"<p>https://stackoverflow.com/questions/35359147/how-can-i-host-my-own-private-conda-repository - Create the channel: <code>mkdir -p /tmp/my-conda-channel/linux-64</code> - Build your conda package: <code>conda build my-package</code> - Copy the built file to the channel: <code>cp /path-to-conda-bld/linux-64/my-package-1.0.0-py39_0.tar.bz2 /tmp/my-conda-channel/linux-64/</code> - Index the channel: <code>conda index /tmp/my-conda-channel/linux-64/</code> - Install the package from the channel: <code>conda install -c file://tmp/my-conda-channel/ my-package=1.0.0</code></p>"},{"location":"Python/Dash/Callback/","title":"Callback","text":"<p>https://dash.plotly.com/basic-callbacks</p> <p>callback functions must be imported to layout, otherwise there is no response when click the tab.</p>"},{"location":"Python/Dash/Callback/#input","title":"Input","text":"<p>Changes in any of inputs will trigger callback run.</p>"},{"location":"Python/Dash/Callback/#output","title":"Output","text":"<p>Output can be used as the input of another callback.</p>"},{"location":"Python/Dash/Callback/#state","title":"State","text":"<p>State will not trigger callback run. But will provide the current value of the state variables as input.</p>"},{"location":"Python/Dash/Callback/#dashexceptionspreventupdate","title":"dash.exceptions.PreventUpdate","text":"<p>https://dash.plotly.com/advanced-callbacks</p> <p>By raising a <code>PreventUpdate</code> exception, the callback outputs will not be updated.</p> <p>Can be used to prevent update when the callback is not for the current page: <pre><code>app.layout = html.Div(\n  [\n      dcc.Location(id='my-url', refresh=False),\n      html.Div([]),\n  ],\n  className='page',\n)\n\n@app.callback(\n    Output('sales-data', 'data'),\n    Input('my-url', 'pathname'),\n)\ndef my_fun(pathname):\n    if not pathname.endswith('/my-url-path/'):\n        raise PreventUpdate\n</code></pre></p>"},{"location":"Python/Dash/Callback/#dashno_update","title":"dash.no_update","text":"<p>Using dash.no_update to update only some of the callback outputs.</p> <p>Detect which input trigged the callback: https://stackoverflow.com/questions/62642418/is-there-a-way-to-prevent-a-callback-from-firing-in-dash <pre><code>from dash import callback_context, no_update\n\n@app.callback(\n    Output('url', 'search'),\n    Output('picker-id', 'value'),\n    Input('url', 'search'),\n    Input('picker-id', 'value'),\n)\ndef my_update(url_search):\n    changed_inputs = [\n        x['prop_id']\n        for x in callback_context.triggered\n    ]\n    if 'picker-id.value' in changed_inputs:\n        return no_update, 100\n    else:\n        return '?userid=100', no_update\n</code></pre></p>"},{"location":"Python/Dash/Callback/#clientside-callback","title":"clientside callback","text":"<pre><code>app.layout = html.Div(\n    html.Div(id='my-div'),\n    dcc.Tabs(\n        id='tabs-a',\n        value=None,\n        children=[\n            dcc.Tab(label='Tab 1', value='tab1'),\n            dcc.Tab(label='Tab 2', value='tab2'),\n        ],\n    )\n)\n\napp.clientside_callback(\n    '''\n    function(tab_value) {\n        if (tab_value === 'tab1') {\n            document.title = 'Tab DEV'\n        }\n        else if (tab_value === 'tab2') {\n            document.title = 'Tab PRD'\n        }\n    }\n    ''',\n    Output('my-div', 'children'),\n    [Input('tabs-a', 'value')],\n)\n</code></pre>"},{"location":"Python/Dash/Callback/#duplicate-callback-outputs","title":"Duplicate callback outputs","text":"<p>\"Duplicate callback outputs\": This means two or more callbacks are trying to  modify the same output component, leading to a conflict.</p> <p>If the callbacks are truly independent and don't interfere with each other,  use the <code>allow_duplicate=True</code> option on the conflicting outputs to allow multiple callbacks to modify them.</p> <pre><code>import dash\nfrom dash import dcc, html\nimport plotly.express as px\n\napp = dash.Dash(__name__)\n\n# Sample data\ndf = px.data.iris()\n\n# Graph component\ngraph = dcc.Graph(id='graph')\n\n# Buttons\nbutton1 = html.Button('Update Graph 1', id='button1')\nbutton2 = html.Button('Update Graph 2', id='button2')\n\n# Callbacks with allow_duplicate=True\n@app.callback(\n    Output('graph', 'figure'),\n    [Input('button1', 'n_clicks')],\n    allow_duplicate=True\n)\ndef update_graph_1(n_clicks):\n    fig = px.scatter(df, x='sepal_width', y='sepal_length', color='species')\n    return fig\n\n@app.callback(\n    Output('graph', 'figure'),\n    [Input('button2', 'n_clicks')],\n    allow_duplicate=True\n)\ndef update_graph_2(n_clicks):\n    fig = px.bar(df, x='species', y='petal_length')\n    return fig\n\n# App layout\napp.layout = html.Div([graph, button1, button2])\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n</code></pre>"},{"location":"Python/Dash/Cherrypy/","title":"Cherrypy","text":""},{"location":"Python/Dash/Dash/","title":"Dash","text":"<p>https://realpython.com/python-dash/</p> <p>Dash is a framework built on top of Flask and React.js, specifically designed for building interactive web applications with complex user interfaces.  It's particularly useful when you need to create interactive <code>dashboards</code> or <code>data visualization</code> applications.</p>"},{"location":"Python/Dash/Dash/#flask","title":"Flask","text":"<p>Flask is a lightweight web framework for Python that is well-suited for building web applications of various complexities.  It's commonly used for building <code>RESTful APIs</code> and <code>web applications</code>.</p>"},{"location":"Python/Dash/Dash/#when-to-use-which","title":"When to use which","text":"<ul> <li>Use Flask alone for a simpler web application with basic functionality and routing needs. </li> <li>Use Dash and Falsk if app requires more complex interactivity, real-time updates, or data visualization features</li> <li>We can even integrate Dash components into a Flask application if need to combine the simplicity of Flask with the interactive features of Dash</li> </ul>"},{"location":"Python/Dash/DashTable/","title":"DashTable","text":"<p>https://dash.plotly.com/datatable</p>"},{"location":"Python/Dash/DashTable/#format","title":"format","text":""},{"location":"Python/Dash/DashTable/#sorting","title":"sorting","text":"<pre><code>sort_action = 'native'\n</code></pre>"},{"location":"Python/Dash/DashTable/#filtering","title":"filtering","text":"<pre><code>filter_action = 'native'\n</code></pre>"},{"location":"Python/Dash/DashTable/#pagination","title":"pagination","text":""},{"location":"Python/Dash/DashTable/#example","title":"example","text":"<pre><code>import dash\nimport dash_table\n\napp = dash.Dash(__name__)\n\n# Create a DataTable component\ntable = dash_table.DataTable(\n    id='table',\n    columns=[{'name': 'Name', 'id': 'name'}, {'name': 'Age', 'id': 'age'}, {'name': 'Occupation', 'id': 'occupation'}],\n    data=[{'name': 'John Doe', 'age': 30, 'occupation': 'Software Engineer'}, {'name': 'Jane Doe', 'age': 25, 'occupation': 'Doctor'}, {'name': 'Peter Parker', 'age': 20, 'occupation': 'Student'}]\n)\n\n# Add the DataTable component to the app layout\napp.layout = table\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"Python/Dash/DashTable/#export-to-csv","title":"export to csv","text":"<p>https://stackoverflow.com/questions/61203436/export-plotly-dash-datatable-output-to-a-csv-by-clicking-download-link</p>"},{"location":"Python/Dash/DashTable/#export-to-excel","title":"export to excel","text":"<p>https://stackoverflow.com/questions/61485943/dash-data-table-download-to-excel</p>"},{"location":"Python/Dash/DashTable/#filter-to-cols-example","title":"filter to cols example","text":"<p>https://community.plotly.com/t/show-and-tell-dashtable-filtering-by-columns-with-external-controls-sliders-dropdowns-inputs/22523/5 <pre><code>import pandas as pd\nimport random\nimport dash\nimport dash_core_components as dcc\nimport dash_html_components as html\nfrom dash.dependencies import Input, Output\nfrom dash.dash_table import DataTable\nfrom dash.exceptions import PreventUpdate\nfrom jupyter_dash import JupyterDash\n\ndf = pd.DataFrame({\n    'int': [random.randint(1, 100) for i in range(20)],\n    'float': [random.random() * x for x in random.choices(range(100), k=20)],\n    'str(object)': ['one', 'one two', 'one two three', 'four'] * 5,\n    'category': random.choices(['Sat', 'Sun', 'Mon', 'Tue', 'Wed','Thu', 'Fri'], k=20),\n    'datetime': pd.date_range('2019-05-01', periods=20),\n    'bool': random.choices([True, False], k=20),\n})\ndf['category'] = df['category'].astype('category')\n\ndef get_str_dtype(df, col):\n    \"\"\"Return dtype of col in df\"\"\"\n    dtypes = ['datetime', 'bool', 'int', 'float', 'object', 'category']\n    for d in dtypes:\n        try:\n            if d in str(df.dtypes.loc[col]).lower():\n                return d\n        except KeyError:\n            return None\n\napp = JupyterDash(__name__)\n\napp.layout = html.Div([\n    html.Div([\n        html.Div(\n            id='container_col_select',\n            children=dcc.Dropdown(\n                id='col_select',\n                options=[\n                    {\n                       'label': c.replace('_', ' ').title(),\n                       'value': c,\n                    }\n                   for c in df.columns\n                ]\n            ),\n            style={'display': 'inline-block', 'width': '30%', 'margin-left': '7%'}\n        ),\n        # DataFrame filter containers\n        html.Div([\n            html.Div(\n                children=dcc.RangeSlider(id='num_filter', updatemode='drag')\n            ),\n            html.Div(\n                children=html.Div(id='rng_slider_vals')\n            ),\n        ], id='container_num_filter', ),\n        html.Div(\n            id='container_str_filter',\n            children=dcc.Input(id='str_filter')\n        ),\n        html.Div(\n            id='container_bool_filter',\n            children=dcc.Dropdown(\n                id='bool_filter',\n                options=[\n                    {'label': str(tf), 'value': str(tf)}\n                    for tf in [True, False]\n                ]\n            )\n        ),\n        html.Div(\n            id='container_cat_filter',\n            children=dcc.Dropdown(\n                id='cat_filter', \n                multi=True,\n                options=[\n                    {'label': day, 'value': day}\n                    for day in df['category'].unique()\n                ]\n            )\n        ),\n        html.Div([\n            dcc.DatePickerRange(\n                id='date_filter',\n                start_date=df['datetime'].min(),\n                end_date=df['datetime'].max(),\n                max_date_allowed=df['datetime'].max(),\n                initial_visible_month=pd.Timestamp(2019, 5, 10),\n            ),\n        ], id='container_date_filter'),\n    ]),\n\n    DataTable(\n        id='table',\n        columns=[{'name': i, 'id': i} for i in df.columns],\n        style_cell={'maxWidth': '400px', 'whiteSpace': 'normal'},\n        data=df.to_dict('records'),\n    )\n])\n\n\n@app.callback(\n    [\n        Output(x, 'style')\n        for x in [\n            'container_num_filter', 'container_str_filter',\n            'container_bool_filter', 'container_cat_filter', 'container_date_filter',\n        ]\n    ],\n    [\n        Input('col_select', 'value')\n    ],\n)\ndef dispaly_relevant_filter_container(col):\n    if col is None:\n        return [{'display': 'none'} for i in range(5)]\n    dtypes = [['int', 'float'], ['object'], ['bool'],\n              ['category'], ['datetime']]\n    result = [{'display': 'none'} if get_str_dtype(df, col) not in d\n              else {'display': 'inline-block',\n                    'margin-left': '7%',\n                    'width': '400px'} for d in dtypes]\n    return result\n\n\n@app.callback(\n    Output('rng_slider_vals', 'children'),\n    [Input('num_filter', 'value')],\n)\ndef show_rng_slider_max_min(numbers):\n    if numbers is None:\n        raise PreventUpdate\n    return 'from:' + ' to: '.join([str(numbers[0]), str(numbers[-1])])\n\n\n@app.callback([Output('num_filter', 'min'),\n               Output('num_filter', 'max'),\n               Output('num_filter', 'value')],\n              [Input('col_select', 'value')])\ndef set_rng_slider_max_min_val(col):\n    if col is None:\n        raise PreventUpdate\n    if col and (get_str_dtype(df, col) in ['int', 'float']):\n        vmin = df[col].min()\n        vmax = df[col].max()\n        return vmin, vmax, [vmin, vmax]\n    else:\n        return None, None, None\n\n\n@app.callback(\n    Output('table', 'data'),\n    [\n        Input('col_select', 'value'),\n        Input('num_filter', 'value'),\n        Input('cat_filter', 'value'),\n        Input('str_filter', 'value'),\n        Input('bool_filter', 'value'),\n        Input('date_filter', 'start_date'),\n        Input('date_filter', 'end_date'),\n    ]\n)\ndef filter_table(\n    col, numbers, categories, string, bool_filter, start_date, end_date\n):\n    if all([\n        param is None \n        for param in [\n            col, numbers, categories, string, \n            bool_filter, start_date, end_date,\n        ]\n    ]):\n        raise PreventUpdate\n    if numbers and (get_str_dtype(df, col) in ['int', 'float']):\n        dx = df[df[col].between(numbers[0], numbers[-1])]\n        return dx.to_dict('records')\n    elif categories and (get_str_dtype(df, col) == 'category'):\n        dx = df[df[col].isin(categories)]\n        return dx.to_dict('records')\n    elif string and get_str_dtype(df, col) == 'object':\n        dx = df[df[col].str.contains(string, case=False)]\n        return dx.to_dict('records')\n    elif (bool_filter is not None) and (get_str_dtype(df, col) == 'bool'):\n        bool_filter = True if bool_filter == 'True' else False\n        dx = df[df[col] == bool_filter]\n        return dx.to_dict('records')\n    elif start_date and end_date and (get_str_dtype(df, col) == 'datetime'):\n        dx = df[df[col].between(start_date, end_date)]\n        return dx.to_dict('records')\n    else:\n        return df.to_dict('records')\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n</code></pre></p>"},{"location":"Python/Dash/Debug/","title":"Debug","text":"<p>https://towardsdatascience.com/advancing-to-professional-dashboard-with-python-using-dash-and-plotly-1e8e5aa4c668</p> <pre><code>app.run(debug=True, host=host, port=port)\n</code></pre>"},{"location":"Python/Dash/Debug/#debugging-decorator","title":"debugging decorator","text":"<pre><code>debug_mode = True #enable debug\n\ndef print_callback(debug_mode):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            if debug_mode:\n                print(f'===&gt; Func: {func.__name__}')\n                input_triggers = ', '.join([t['prop_id'] for t in dash.callback_context.triggered}])\n                print(f'Triggered by: {input_triggers')\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n@app.callback(...)\n@print_callback(debug_mode)\ndef my_func(trigger):\n    pass\n</code></pre>"},{"location":"Python/Dash/Deploy/","title":"Deploy","text":""},{"location":"Python/Dash/Deploy/#dash_requests_pathname_prefix","title":"DASH_REQUESTS_PATHNAME_PREFIX","text":"<p>use <code>requests_pathname_prefix</code> to remove the default slask <code>/</code>: - https://dash.plotly.com/reference - https://lukesingham.com/how-to-deploy-plotlys-dash-using-shinyproxy/</p> <pre><code>env:\n  - name: DASH_REQUESTS_PATHNAME_PREFIX\n    value: \"/&lt;namespace&gt;/&lt;app-name&gt;/\"\n</code></pre>"},{"location":"Python/Dash/Error/","title":"Error","text":""},{"location":"Python/Dash/Error/#cannot-import-name-dcc-from-partially-initialized-module","title":"cannot import name 'dcc' from partially initialized module","text":"<p><pre><code>cannot import name 'dcc' from partially initialized module 'dash' \n(most likely due to a circular import) (c:/.../dash.py)\n</code></pre> One of the file is dash.py - should not name file <code>dash.py</code> as that would shadow the official dash module.</p>"},{"location":"Python/Dash/Error/#no-module-named-main","title":"No module named main","text":"<p>https://github.com/plotly/dash/issues/1889 <pre><code>Dash is running on http://127.0.0.1:8000/\n\n * Serving Flask app 'app'\n * Debug mode: on\nNo module named main\n</code></pre> Solution: import the app in the <code>main.py</code> and then run it outside of the module/package <code>app.run(host='127.0.0.1', port=8000, debug=True)</code></p>"},{"location":"Python/Dash/Error/#flask-werkzeug-systemexit-3","title":"flask: werkzeug SystemExit: 3","text":"<ul> <li>The SystemExit errorcode 3 is an exception thrown by werkzeug to trigger the reloade</li> <li>To avoid having to do an extra step every time, you can pass \"flask\": true in the config, so this breakpoint is skipped</li> </ul>"},{"location":"Python/Dash/Error/#a-nonexistent-object-was-used-in-an-input-of-a-dash-callback","title":"A nonexistent object was used in an Input of a Dash callback","text":"<p>https://community.plotly.com/t/a-nonexistent-object-was-used-in-an-input-of-a-dash-callback-an-multipage-app/52007</p> <p>in a multipage dashboard: https://stackoverflow.com/questions/71361010/a-nonexistent-object-was-used-in-an-input-of-a-dash-callback-an-multipage-app</p> <p>Solution??? <pre><code>app.config['suppress_callback_exceptions'] = True\n</code></pre></p>"},{"location":"Python/Dash/Error/#error-handling","title":"error handling","text":"<p>https://flask.palletsprojects.com/en/2.2.x/errorhandling/</p> <p>https://code-maven.com/python-flask-catch-exception</p>"},{"location":"Python/Dash/ExcelTable/","title":"Excel table","text":"<p>https://stackoverflow.com/questions/69723198/dash-datatable-drop-down-filter</p> <p>still not good!!!</p>"},{"location":"Python/Dash/Issue/","title":"Issue","text":""},{"location":"Python/Dash/Issue/#challenges","title":"challenges","text":"<p>https://medium.com/analytics-vidhya/5-challenges-when-using-plotly-dash-for-interactive-web-apps-849f442582f7</p> <ul> <li>same input will trigger callbacks for all pages - can we only run callbacks for that page?</li> <li>cannot update the same output using two callbacks - need to merge them into one and use <code>dash.callback_context</code> to determine which input has changes</li> </ul>"},{"location":"Python/Dash/JupyterDash/","title":"JupyterDash","text":"<p>Obsolete and now we can use Dash!</p> <p>https://dash.plotly.com/workspaces/using-dash-in-jupyter-and-workspaces <pre><code>from jupyter_dash import JupyterDash\napp = JupyterDash(__name__)\n</code></pre></p>"},{"location":"Python/Dash/JupyterDash/#example","title":"Example","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom jupyter_dash import JupyterDash\nfrom dash import Dash, dcc, html, Input, Output, State, no_update\nfrom dash.exceptions import PreventUpdate\n\nimport dash_bootstrap_components as dbc\nFA = \"https://use.fontawesome.com/releases/v5.15.4/css/all.css\"\n\napp = JupyterDash(\n    __name__,\n    external_stylesheets=[dbc.themes.BOOTSTRAP, FA],\n)\n\ncountry_radio = html.Div([\n    dbc.RadioItems(\n        id='country-radio',\n        className=\"btn-group\",\n        labelClassName=\"btn btn-secondary\",\n        labelCheckedClassName=\"active\",\n        options=[{'label': i, 'value': i} for i in ['US', 'UK']],\n        value='US',\n    ),\n], className=\"radio-group\", style={\"textAlign\": 'left'})\n\ncity_dropdown = html.Div([\n    dcc.Dropdown(\n        id='select-city',\n        options=[],\n        multi=False,\n    ),\n], className='filter-item')\n\napp.layout = html.Div([\n    country_radio,\n    city_dropdown,\n    html.Div(id='population-text'),\n])\n\ndata = pd.DataFrame({\n    'country': ['US', 'US', 'UK', 'UK'],\n    'city': ['dc', 'el', 'dc', 'ee'],\n    'population': ['3m','2m','1m', '5m'],\n})\n\n@app.callback(\n    Output('select-city', 'options'),\n    Output('select-city', 'value'),\n    Input('country-radio', 'value'),\n    State('select-city', 'options'),\n    State('select-city', 'value'),\n)\ndef set_city_options(\n    country,\n    options,\n    selection,\n):\n    if not country:\n        raise PreventUpdate\n\n    options = options or []\n    selection = selection or []\n    if isinstance(selection, str):\n        selection = [selection]\n\n    updated_list = data.query('country == @country')['city'].unique()\n    updated_options = [dict(label=item, value=item) for item in updated_list]\n    options_updated = options != updated_options\n    if not options_updated:\n        raise PreventUpdate\n\n    updated_selection = np.intersect1d(selection, updated_list).tolist()\n    updated_selection.sort()\n    selection.sort()\n    selection_updated = (\n        updated_selection != selection\n    )\n    if selection_updated:\n        return updated_options, updated_selection\n    else:\n        return updated_options, no_update\n\n@app.callback(\n    Output('population-text', 'children'),\n    Input('country-radio', 'value'),\n    Input('select-city', 'value'),\n)\ndef city_population(\n  country,\n  city,\n):\n    if not country or not city:\n        raise PreventUpdate\n\n    d1 = data.query('country == @country &amp; city == @city')\n    d2 = d1.groupby(['country','city']).agg(lambda col: '/'.join(sorted(set(col))))\n    population = d2['population'][0]\n    return f'The population of {city} in {country} is: {population}'\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n</code></pre>"},{"location":"Python/Dash/Learn/","title":"Learn","text":""},{"location":"Python/Dash/Learn/#charts-and-table-examples","title":"charts and table examples","text":"<p>https://community.plotly.com/t/dash-v1-21-0-plotly-js-2-0-icicle-charts-bar-chart-patterns-clipboard-component-bug-fixes/54892/6</p>"},{"location":"Python/Dash/Learn/#dashboard-example","title":"dashboard example","text":"<ul> <li>https://resources.experfy.com/ai-ml/professional-dashboard-with-dash-css-bootstrap/</li> <li>https://github.com/gabri-al/corporate-dashboard</li> </ul>"},{"location":"Python/Dash/MultiPage/","title":"Multi-page","text":""},{"location":"Python/Dash/MultiPage/#switch-page-based-on-page-url","title":"switch page based on page url","text":"<pre><code>app.layout = html.Div(\n    [\n        dcc.Location(id='url', refresh=False),\n        html.Div(id='page-content'),\n    ]\n)\n\n@app.callback(\n   Output('page-content', 'children'),\n   Input('url', 'pathname'),\n)\ndef load_page(pathname):\n    if pathname == '/':\n        return html.Div(\"Main\")\n    elif pathname == \"/dev\":\n        return html.Div(\"DEV\")\n    else:\n        return no_update\n</code></pre>"},{"location":"Python/Dash/Topic/ClickFigUpdateFilter/","title":"Figure data click triger callback","text":"<p>To create a Dash app where clicking on a bar in a bar chart triggers a callback to  update a Dropdown picker linked to the bar in the plot, you can use the following example code. </p> <pre><code>import dash\nfrom dash import dcc, html\nfrom dash.dependencies import Input, Output\nimport plotly.express as px\nimport pandas as pd\n\n# Sample data\ndf = pd.DataFrame({\n    'Category': ['A', 'B', 'C'],\n    'Value': [10, 20, 15],\n})\n\n# Initialize the Dash app\napp = dash.Dash(__name__)\n\n# Define the layout of the app\napp.layout = html.Div([\n    html.H1(\"Bar Chart with Dropdown Picker\"),\n\n    # Bar chart\n    dcc.Graph(\n        id='bar-chart',\n        figure=px.bar(df, x='Category', y='Value', labels={'Value': 'Count'}),\n        config={'editable': True}\n    ),\n\n    # Dropdown picker\n    dcc.Dropdown(\n        id='region-dropdown',\n        options=[\n            {'label': 'Region 1', 'value': 'region1'},\n            {'label': 'Region 2', 'value': 'region2'},\n            {'label': 'Region 3', 'value': 'region3'}\n        ],\n        value='region1',\n        style={'width': '50%'}\n    )\n])\n\n# Define callback to update Dropdown picker based on clicked bar\n@app.callback(\n    Output('region-dropdown', 'value'),\n    Input('bar-chart', 'clickData')\n)\ndef update_dropdown(click_data):\n    if click_data is None:\n        # If no bar is clicked, return default value\n        return 'region1'\n    else:\n        # Extract information from clicked bar and update Dropdown picker\n        category_clicked = click_data['points'][0]['x']\n        # You can implement your own logic to map the category to the corresponding region value\n        # For simplicity, using a dictionary here\n        category_to_region_mapping = {'A': 'region1', 'B': 'region2', 'C': 'region3'}\n        return category_to_region_mapping.get(category_clicked, 'region1')\n\n# Run the app\nif __name__ == '__main__':\n    app.run_server(debug=True)\n</code></pre> <p>In this example, clicking on a bar in the bar chart will trigger the <code>update_dropdown</code> callback,  which will update the value of the Dropdown picker based on the category of the clicked bar.  You can customize the <code>category_to_region_mapping</code> dictionary to match your specific use case.</p>"},{"location":"Python/Dash/Topic/Framework/","title":"Framework","text":""},{"location":"Python/Dash/Topic/Framework/#how-to-create-a-website-using-dash-including-top-navbar-and-side-toc","title":"how to create a website using dash including top navbar and side toc","text":"<p>does not work <pre><code>import dash\nfrom dash import dcc, html\nimport dash_bootstrap_components as dbc\n\napp = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n\n# Create a top navbar\nnavbar = dbc.NavbarSimple(\n    children=[\n        dbc.NavItem(dbc.NavLink(\"Home\", href=\"#\")),\n        dbc.DropdownMenu(\n            children=[\n                dbc.DropdownMenuItem(\"Page 1\", href=\"#page-1\"),\n                dbc.DropdownMenuItem(\"Page 2\", href=\"#page-2\"),\n            ],\n            label=\"Pages\",\n        ),\n    ],\n    brand=\"Your Website\",\n    brand_href=\"#\",\n    dark=True,\n)\n\n# Create a side toc\nside_toc = dbc.Nav(\n    children=[\n        dbc.NavItem(dbc.NavLink(\"Introduction\", href=\"#introduction\")),\n        dbc.NavItem(dbc.NavLink(\"Data Analysis\", href=\"#data-analysis\")),\n        dbc.NavItem(dbc.NavLink(\"Conclusion\", href=\"#conclusion\")),\n    ],\n    vertical=True,\n)\n\n# Create the page content\npage_content = dbc.Container(\n    children=[\n        dbc.Row(\n            children=[\n                dbc.Col(side_toc, width=2),\n                dbc.Col(\n                    children=[\n                        html.Div([\n                            html.H2(\"Section 1\", id=\"section-1\"),\n                            html.P(\"This is the content of Section 1.\"),\n                        ]),\n                        html.Div([\n                            html.H2(\"Section 2\", id=\"section-2\"),\n                            html.P(\"This is the content of Section 2.\"),\n                        ]),\n                        html.Div([\n                            html.H2(\"Section 3\", id=\"section-3\"),\n                            html.P(\"This is the content of Section 3.\"),\n                        ]),                        \n                    ],                    \n                    width=10,\n                ),\n            ],\n        ),\n    ],\n    fluid=True,\n)\n\n# Create the layout\napp.layout = dbc.Container(\n    children=[navbar, page_content],\n)\n\n# Run the app\nif __name__ == \"__main__\":\n    app.run(debug=True)\n</code></pre></p>"},{"location":"Python/Dash/Topic/NestedDiv/","title":"Nested Div","text":""},{"location":"Python/Dash/Topic/NestedDiv/#example","title":"example","text":"<p>what's the point to add multiple layers of html.Div in this code  <pre><code>html.Div(\n  html.Div(\n    [  ],\n    className=\"info-tab-content\",\n    style={'display': 'flex', 'flexDirection': 'column', 'padding': '7px'}\n  ),\n  className=\"info-tab-content-wrapper\"\n)\n</code></pre></p> <p>There are a few reasons why you might want to add multiple layers of HTML <code>Div</code> elements in this code: - To create a more complex layout. By nesting <code>Div</code> elements, you can create a layout with multiple columns, rows, or other sections. This can be useful for creating complex designs like navigation bars, menus, and product galleries. - To isolate CSS styles. By placing different elements in different <code>Div</code> elements, you can isolate CSS styles to specific parts of your layout. This can make it easier to maintain and update your CSS code. - To improve accessibility. By using <code>Div</code> elements to group related content, you can make it easier for screen readers and other assistive technologies to understand your page layout.</p> <p>In the specific code example you provided, the outer <code>Div</code> element has a class of <code>info-tab-content-wrapper</code> and the inner <code>Div</code> element has a class of <code>info-tab-content</code>. This suggests that the outer <code>Div</code> element is being used to wrap the entire content area of an information tab, while the inner <code>Div</code> element is being used to group the individual elements of the tab content.</p> <p>By nesting the inner <code>Div</code> element within the outer <code>Div</code> element, the developer is able to isolate the CSS styles for the tab content from the rest of the page layout. This makes it easier to maintain and update the CSS code for the tab content, and it also makes the code more accessible to screen readers and other assistive technologies.</p> <p>Here is an example of how the CSS code for the tab content could be isolated using the two <code>Div</code> elements:</p> <pre><code>.info-tab-content-wrapper {\n  /* CSS styles for the outer wrapper element */\n}\n\n.info-tab-content {\n  /* CSS styles for the inner content element */\n}\n</code></pre> <p>By isolating the CSS styles for the tab content, the developer can easily change the appearance of the tab content without affecting the rest of the page layout. This can be useful for creating different themes or styles for the page.</p> <p>Overall, there are many benefits to adding multiple layers of HTML <code>Div</code> elements to your code. By nesting <code>Div</code> elements, you can create more complex layouts, isolate CSS styles, and improve accessibility.</p>"},{"location":"Python/Dash/Topic/PreventUpdate/","title":"Prevent Update","text":"<p>In callback how to prevent update based on context.</p>"},{"location":"Python/Dash/Topic/PreventUpdate/#not-update-non-active-url","title":"Not update non-active url","text":"<p>Do not update when the callback is not for the current page: <pre><code>app.layout = html.Div(\n  [\n      dcc.Location(id='my-url', refresh=False),\n      html.Div([]),\n  ],\n  className='page',\n)\n\n@app.callback(\n    Output('sales-data', 'data'),\n    Input('my-url', 'pathname'),\n)\ndef my_fun(pathname):\n    if not pathname.endswith('/my-url-path/'):\n        raise PreventUpdate\n</code></pre></p>"},{"location":"Python/Dash/Topic/PreventUpdate/#not-update-non-active-tab","title":"Not update non-active tab","text":"<pre><code>@app.callback(\n    [\n        Output('loading-summary', 'children'),\n        Output('error-modal', 'is_open'),\n        Output('error-modal-message', 'children'),\n    ],\n    [\n        Input('summary-tab', 'active_tab'),\n    ],\n)\ndef update_data(\n    active_tab,\n):\n    # active-tab is a tab in summary-tab\n    if active_tab != 'active-tab':\n        raise PreventUpdate\n</code></pre>"},{"location":"Python/Dash/Topic/PreventUpdate/#preventupdate-vs-no_update","title":"PreventUpdate vs no_update","text":"<p>In Dash, <code>PreventUpdate</code> and <code>no_update</code> are both used in callback functions to control the updating behavior of the components in the Dash app. However, they serve slightly different purposes.</p> <ol> <li><code>PreventUpdate</code>:</li> <li>If a callback returns <code>PreventUpdate</code>, it tells Dash to prevent the callback from updating the output. This is useful when you want to prevent a callback from being triggered under certain conditions.</li> <li> <p>For example, you might have a callback that updates a graph based on a user's input, but you want to prevent the graph from updating if the input doesn't meet a certain condition. In such a case, you can return <code>PreventUpdate</code> to prevent the update from happening.</p> <pre><code>@app.callback(\n    Output('output-graph', 'figure'),\n    [Input('input-dropdown', 'value')]\n)\ndef update_graph(selected_value):\n    if selected_value is None or selected_value &lt; 0:\n        raise PreventUpdate\n    # Your code to update the graph based on the valid input\n</code></pre> </li> <li> <p><code>no_update</code>:</p> </li> <li>If a callback returns <code>no_update</code>, it tells Dash to keep the current state of the output unchanged. This is useful when you want to selectively update certain outputs while leaving others unchanged.</li> <li> <p>For example, you might have a callback that updates multiple components, but you only want to update one of them based on a certain condition. You can use <code>no_update</code> for the other components to keep their current state.</p> <pre><code>@app.callback(\n    [Output('output-div', 'children'),\n     Output('output-graph', 'figure')],\n    [Input('input-dropdown', 'value')]\n)\ndef update_components(selected_value):\n    if selected_value is None or selected_value &lt; 0:\n        return 'Invalid input', no_update\n    # Your code to update the output-div and output-graph\n</code></pre> </li> </ol> <p>In summary, <code>PreventUpdate</code> is used to entirely prevent the callback from triggering an update, while <code>no_update</code> is used to selectively keep certain outputs unchanged while allowing others to be updated within the same callback.</p>"},{"location":"Python/Dash/Topic/ShowHide/","title":"Show and hide","text":""},{"location":"Python/Dash/Topic/ShowHide/#showhide-element-based-on-htmldiv-display-style","title":"show/hide element based on html.Div display style","text":"<pre><code>import dash\nfrom dash import dcc, html\nfrom dash.dependencies import Input, Output\n\napp = dash.Dash(__name__)\n\napp.layout = html.Div([\n    dcc.Dropdown(\n        id = 'dropdown-to-show_or_hide-element',\n        options=[\n            {'label': 'Show element', 'value': 'on'},\n            {'label': 'Hide element', 'value': 'off'}\n        ],\n        value = 'on'\n    ),\n    # Create Div to place a conditionally visible element inside\n    html.Div([\n        # Create element to hide/show, in this case an 'Input Component'\n        dcc.Input(\n            id = 'element-to-hide',\n            placeholder = 'something',\n            value = 'Can you see me?',\n        )\n    ], \n    style= {'display': 'block'} # &lt;-- This is the line that will be changed by the dropdown callback\n    )\n])\n\n@app.callback(\n   Output(component_id='element-to-hide', component_property='style'),\n   [Input(component_id='dropdown-to-show_or_hide-element', component_property='value')],\n)\n\ndef show_hide_element(visibility_state):\n    if visibility_state == 'on':\n        return {'display': 'block'}\n    if visibility_state == 'off':\n        return {'display': 'none'}\n\nif __name__ == '__main__':\n    app.run_server(debug=True, port=8059)\n</code></pre>"},{"location":"Python/Dash/Topic/ShowHide/#showhide-radio-items-based-on-tab","title":"show/hide radio items based on tab","text":"<pre><code>import dash\nfrom dash import dcc, html\nfrom dash.dependencies import Input, Output\nimport dash_bootstrap_components as dbc\n\napp = dash.Dash(__name__)\n\nsort_options = html.Div([\n    html.Div([\n        html.Div(\n            'Sort type: ',\n            style={'fontWeight': 'bold', 'flex': '1 0 auto'},\n        ),\n        dbc.RadioItems(\n            id='sort-by',\n            options=[\n                {'label': v, 'value': v}\n                for v in ['name', 'country', 'region']\n            ],\n            value='country',\n            inline=True,\n        )\n    ], style={'display': 'flex', 'marginTop': '5px'}),\n], style={\n    'flex': '0 1 auto',\n    'display': 'flex',\n    'flexDirection': 'column',\n    'alignItems': 'center',\n    'padding': '5px 7px 10px 7px',\n    'marginBottom': '5px',\n    'borderBottom': '1px solid #dee2e6',\n})\n\napp.layout = html.Div([\n    html.Div(id='sort-option-content', children=[\n        sort_options,\n    ]),    \n    dcc.Tabs(id='tabs-example', value='tab1', children=[\n        dcc.Tab(label='Tab 1', value='tab1'),\n        dcc.Tab(label='Tab 2', value='tab2'),\n    ]),\n])\n\n@app.callback(\n    Output('sort-option-content', 'children'),\n    Input('tabs-example', 'value')\n)\ndef update_tab_content(value):\n    if value == 'tab1':\n        return [sort_options]\n    else:\n        return []\n\nif __name__ == '__main__':\n    app.run_server(debug=True, port=8059)\n</code></pre>"},{"location":"Python/Dash/Topic/StoreCache/","title":"dcc.Store and dcc.Cache","text":""},{"location":"Python/Dash/Topic/StoreCache/#when-to-use-store-and-when-to-use-cache","title":"when to use Store and when to use Cache","text":"<p>dcc.Store is a client-side component that can be used to store data in the browser.  It is useful for storing data that needs to be shared between multiple callbacks,  or for storing data that needs to be accessed quickly without making a request to the server.</p> <p>Cache is a server-side mechanism for storing data that can be shared between multiple users and processes.  It is useful for storing data that is expensive to compute or that needs to be accessed frequently.</p> <p>Here are some general guidelines for when to use dcc.Store and when to use cache:</p> <ul> <li> <p>Use dcc.Store when:</p> <ul> <li>You need to store data that needs to be shared between multiple callbacks.</li> <li>You need to store data that needs to be accessed quickly without making a request to the server.</li> <li>You are storing data that is specific to a single user.</li> </ul> </li> <li> <p>Use cache when:</p> <ul> <li>You need to store data that is expensive to compute or that needs to be accessed frequently.</li> <li>You need to store data that needs to be shared between multiple users and processes.</li> <li>You need to store data that persists across browser sessions.</li> </ul> </li> </ul> <p>Here are some specific examples:</p> <ul> <li> <p>Use dcc.Store to:</p> <ul> <li>Store the results of a data processing operation that is performed in a callback.</li> <li>Store the state of a UI component that needs to be synchronized between multiple callbacks.</li> <li>Store the user's preferences.</li> </ul> </li> <li> <p>Use cache to:</p> <ul> <li>Store the results of a database query that is performed frequently.</li> <li>Store the results of a machine learning model that is used to generate predictions.</li> <li>Store the results of a data processing operation that is performed on a large dataset.</li> </ul> </li> </ul> <p>It is important to note that dcc.Store and cache are not mutually exclusive.  You can use both of them in the same app, depending on your specific needs.</p> <p>For example, you could use dcc.Store to store the results of a data processing operation that is performed in a callback,  and then use cache to store the results of the same data processing operation for all users.  This would allow you to avoid performing the data processing operation multiple times for different users.</p> <p>Another example is to use dcc.Store to store the state of a UI component that needs to be synchronized between multiple callbacks,  and then use cache to store the state of the UI component for all users.  This would allow you to avoid sending the state of the UI component back and forth to the server multiple times.</p> <p>Ultimately, the best way to decide whether to use dcc.Store or cache is to consider your specific needs and requirements.</p>"},{"location":"Python/Dash/css/Basic/","title":"Basic","text":""},{"location":"Python/Dash/css/Basic/#syntax","title":"syntax","text":"<pre><code>selector {\n  property: value;\n  property: value;\n}\n</code></pre>"},{"location":"Python/Dash/css/Basic/#types","title":"types","text":"<p>In CSS (Cascading Style Sheets), selectors are used to target and style HTML elements.  There are different types of selectors, and they serve various purposes. </p> <p>Here are explanations of three common types: type selectors, class selectors, and ID selectors:</p> <ol> <li>Type Selectors:</li> <li>Syntax: <code>elementName { /* styles */ }</code></li> <li>Example: <code>p { color: blue; }</code></li> <li> <p>Explanation: Type selectors target HTML elements based on their tag names.      In the example, all <code>&lt;p&gt;</code> (paragraph) elements will have a blue text color.</p> </li> <li> <p>Class Selectors:</p> </li> <li>Syntax: <code>.className { /* styles */ }</code></li> <li>Example: <code>.highlight { background-color: yellow; }</code></li> <li> <p>Explanation: Class selectors target elements with a specific class attribute.      In the example, all elements with the class \"highlight\" will have a yellow background color.      You apply a class to an HTML element using the <code>class</code> attribute: <code>&lt;div class=\"highlight\"&gt;...&lt;/div&gt;</code>.</p> </li> <li> <p>ID Selectors:</p> </li> <li>Syntax: <code>#idName { /* styles */ }</code></li> <li>Example: <code>#header { font-size: 24px; }</code></li> <li>Explanation: ID selectors target a specific element with a unique ID attribute.      In the example, the element with the ID \"header\" will have a font size of 24 pixels.      You apply an ID to an HTML element using the <code>id</code> attribute: <code>&lt;div id=\"header\"&gt;...&lt;/div&gt;</code>.      It's important to note that an ID must be unique within a page, whereas a class can be used on multiple elements.</li> </ol> <p>Examples of usage in HTML:</p> <pre><code>&lt;!-- Type Selector --&gt;\n&lt;p&gt;This is a paragraph.&lt;/p&gt;\n\n&lt;!-- Class Selector --&gt;\n&lt;div class=\"highlight\"&gt;This element has a yellow background.&lt;/div&gt;\n\n&lt;!-- ID Selector --&gt;\n&lt;div id=\"header\"&gt;This is a header.&lt;/div&gt;\n</code></pre> <p>Selectors can also be combined and used in more complex ways to target specific elements or groups of elements.  Understanding and using CSS selectors effectively is crucial for styling web pages efficiently  and maintaining a clear separation of structure and presentation.</p>"},{"location":"Python/Dash/css/CSS/","title":"CSS","text":""},{"location":"Python/Dash/css/CSS/#link-classname-attribute-of-dash-components-to-css","title":"link className attribute of Dash components to css","text":"<pre><code>app.layout = html.Div([\n    html.Div(className=\"x-div\"),\n    html.Div(className=\"y-div\"),\n])\n</code></pre> <p>CSS style <pre><code>.x-div {\n    color: red;\n}\n.y-div {\n    color: steelblue;\n}\n</code></pre></p>"},{"location":"Python/Dash/css/CSS/#custom-css","title":"custom css","text":"<p>https://stackoverflow.com/questions/50844844/python-dash-custom-css - make a <code>assets</code> folder in the same directory as your app.py. Put all your .cssand .js files there. - initialize the app object by giving the <code>__name__</code> as the first argument; use <code>app = dash.Dash(__name__)</code> instead of <code>app = dash.Dash()</code>. - Now Dash will automatically load your CSS and JS files. To force a correct loading order (especially important with CSS), it is recommended to name your files as 01_first.css, 02_some_customization.css, .. 25_load_this_last.css. (the loading order is always alphanumerical)</p>"},{"location":"Python/Dash/css/Example/","title":"Example","text":""},{"location":"Python/Dash/css/Example/#set-position-and-order-of-elements-in-a-container","title":"set position and order of elements in a container","text":"<p>explain this css in dash <pre><code>.my-tab-1 {\n  top: 0;\n  z-index: 102; }\n.my-tab-2 {\n  top: calc(160px - 22px);\n  z-index: 101; }\n</code></pre> The CSS you provided appears to be defining styles for an HTML element with the class name \"compare-tab.\" Let's break down the properties:</p> <ul> <li> <p><code>top: 0;</code>: This property sets the top positioning of the element.    In this case, it is set to <code>0</code>, which means the element will be positioned at the top of its containing element.    If the containing element is the entire page, the element with the class \"compare-tab\" will be at the top of the page.</p> </li> <li> <p><code>z-index: 102;</code>: This property sets the z-index of the element.    The z-index determines the stack order of an element concerning the z-axis (front-to-back order).    Elements with a higher z-index will be displayed in front of elements with a lower z-index.    In this case, the \"compare-tab\" element will be displayed in front of elements with a lower z-index, and potentially behind elements with a higher z-index.</p> </li> </ul> <p>These styles suggest that the element with the class \"compare-tab\" is meant to be positioned at the top of its container, and it has a z-index of 102, which might be relevant if there are other elements on the page with different z-index values. The exact appearance and behavior would also depend on the other styles applied to the element and its context in the HTML structure.</p>"},{"location":"Python/Dash/css/Example/#h-100-and-v-100","title":"h-100 and v-100","text":"<p>In Dash CSS, <code>h-100</code> and <code>v-100</code> are CSS properties that can be used to set the height and width of an element to 100% of its parent container.</p> <ul> <li> <p><code>h-100</code> sets the height of an element to 100% of its parent container. This means that the element will take up the entire height of its parent container.</p> </li> <li> <p><code>v-100</code> sets the width of an element to 100% of its parent container. This means that the element will take up the entire width of its parent container.</p> </li> </ul> <p>These properties are commonly used to create full-width and full-height elements, such as background images and navigation bars.</p>"},{"location":"Python/Dash/css/Example/#code-1","title":"code 1","text":"<p>The <code>html.Div</code> component is being created with a list containing a <code>metric_tab</code> element as its child.  The <code>style</code> attribute is used to apply specific styling to this <code>html.Div</code> component. Let's break down the style: <pre><code>style={\n    'flex': '1 1 75%',\n    'display': 'flex',\n    'flexDirection': 'column',\n    'marginTop': '-18px',\n    'padding': '1px',\n}\n</code></pre></p> <ol> <li> <p><code>'flex': '1 1 75%'</code>: This is setting the flex property of the element.    The <code>flex</code> property is shorthand for three individual properties: <code>flex-grow</code>, <code>flex-shrink</code>, and <code>flex-basis</code>.    In this case, it is set to <code>'1 1 75%'</code>, which means:</p> <ul> <li><code>flex-grow</code>: 1 (the element can grow to fill the available space)</li> <li><code>flex-shrink</code>: 1 (the element can shrink if there is not enough space)</li> <li><code>flex-basis</code>: 75% (initial size of the element before growing or shrinking)</li> </ul> </li> <li> <p><code>'display': 'flex'</code>: This specifies that the element should be treated as a flex container.    Flex containers can be used to control the layout and alignment of their children.</p> </li> <li> <p><code>'flexDirection': 'column'</code>: This sets the direction of the flex container's main axis.    In this case, it's set to 'column', which means the children of the flex container will be placed in a column (top to bottom) layout.</p> </li> <li> <p><code>'marginTop': '-18px'</code>: This sets the top margin of the element to -18 pixels.    A negative margin can be used to adjust the spacing above the element.</p> </li> <li> <p><code>'padding': '1px'</code>: This property sets the padding of the element to 1 pixel on all sides.    Padding is the space between the content of an element and its border.   </p> </li> </ol> <p>So, in summary, this piece of code is styling a <code>html.Div</code> to be a flex container with a vertical column layout for its children (<code>metric_tab</code>). It has a flexible width of 75% of its parent container, can grow or shrink as needed, and has a negative top margin of 18 pixels.</p>"},{"location":"Python/Dash/css/Flex/","title":"Flex","text":"<p>https://community.plotly.com/t/how-to-span-full-height-that-left-in-vertical-dbc-col-space-with-dash-bootstrap-dbc-row/67883</p>"},{"location":"Python/Dash/css/Flex/#flexbox","title":"flexbox","text":"<p>https://developer.mozilla.org/en-US/docs/Learn/CSS/CSS_layout/Flexbox</p>"},{"location":"Python/Dash/css/Flex/#bootstrap-flexbox-utility-classes","title":"Bootstrap Flexbox utility classes","text":"<p>https://getbootstrap.com/docs/5.2/utilities/flex/</p>"},{"location":"Python/Dash/css/Flex/#flex-example","title":"flex example","text":"<p>using bard or chatgpt to explian it <pre><code>style={\n    'flex': '0 1 auto',\n    'display': 'flex',\n    'flexDirection': 'column',\n    'alignItems': 'center',\n    'padding': '5px 7px 10px 7px',\n    'marginBottom': '5px',\n    'borderBottom': '1px solid #dee2e6',\n}\n</code></pre></p>"},{"location":"Python/Dash/css/Scrollbar/","title":"Scrollbar","text":""},{"location":"Python/Dash/css/Scrollbar/#horizontal-scrollbar","title":"horizontal scrollbar","text":"<p>Changing <code>flex-direction: column</code> will get vertical scrollbar. <pre><code>.horizontal-scrollbar {\n  flex-direction: row; }\n  .horizontal-scrollbar &gt; div {\n    overflow-x: scroll;\n    overflow-y: hidden;  /* Hide vertical scrollbar */}}\n  .horizontal-scrollbar &gt; div::-webkit-scrollbar {\n    width: 8px;\n    height: 8px; }\n  .horizontal-scrollbar &gt; div::-webkit-scrollbar-corner {\n    background: transparent; }\n  .horizontal-scrollbar &gt; div::-webkit-scrollbar-track {\n    background: transparent;\n    border-radius: 8px; }\n  .horizontal-scrollbar &gt; div::-webkit-scrollbar-thumb {\n    background: #5e6b73;\n    border-radius: 8px; }\n  .horizontal-scrollbar &gt; div::-webkit-scrollbar-thumb:hover {\n    background: #535e65; }\n  .horizontal-scrollbar &gt; div::-webkit-scrollbar-thumb:active {\n    background: #475157; } \n</code></pre></p>"},{"location":"Python/Dash/css/Scrollbar/#scrollbar-due-to-overfow","title":"scrollbar due to overfow","text":"<p>https://blog.logrocket.com/how-to-prevent-overflow-scrolling-css/</p>"},{"location":"Python/Dash/dbc/Card/","title":"Card","text":"<p>In Dash and the <code>dash-bootstrap-components</code> (dbc) library, <code>dbc.Card</code> is a component  used to create a card or a content container that can display various types of content within a styled box. </p> <p>Cards are often used to present information or components in a visually appealing and organized way.  They can contain text, images, graphs, or other elements, and you can customize their appearance and layout.</p> <p>Here are some key features and characteristics of <code>dbc.Card</code>:</p> <ol> <li> <p>Content Container: It serves as a container for displaying content, which can include text, images, graphs, links, buttons, or any other HTML components.</p> </li> <li> <p>Styling: You can apply styling to the card, such as setting a background color, border, and padding to control its appearance.</p> </li> <li> <p>Header and Footer: You can add header and footer sections to the card for titles, additional information, or buttons.</p> </li> <li> <p>Interactive Elements: You can include interactive elements within the card, like buttons and links, to create interactive dashboards or web applications.</p> </li> <li> <p>Responsive Design: Cards can be used in responsive layouts, making them suitable for different screen sizes and devices.</p> </li> </ol> <p>Here's a simple example of how to use <code>dbc.Card</code> to create a card in a Dash application: <pre><code>import dash\nimport dash_bootstrap_components as dbc\nimport dash_html_components as html\n\napp = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n\napp.layout = dbc.Card(\n    dbc.CardBody([\n        html.H4(\"Card Title\"),\n        html.P(\"This is some text content in the card.\"),\n        dbc.Button(\"Learn More\", color=\"primary\", href=\"#\"),\n    ])\n)\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n</code></pre></p> <p>In this example, we use <code>dbc.Card</code> to create a simple card with a title, text content, and a \"Learn More\" button.  The card is wrapped in <code>dbc.CardBody</code> to encapsulate its content.  You can further customize the appearance and structure of the card according to your application's needs.  Cards are versatile components commonly used for presenting information in a user-friendly and visually appealing way.</p>"},{"location":"Python/Dash/dbc/Container/","title":"Container","text":""},{"location":"Python/Dash/dbc/Container/#scrollbar-issue-without-container","title":"scrollbar issue without container","text":"<p>https://community.plotly.com/t/dbc-row-and-dbc-col-trigger-horizontal-scroll-bar/56356</p>"},{"location":"Python/Dash/dbc/Modal/","title":"Modal","text":""},{"location":"Python/Dash/dbc/Modal/#modal_1","title":"modal","text":"<p><code>dbc.Modal</code> is a component in the <code>dash_bootstrap_components</code> library that allows you to create a modal dialog box in a Dash application using Bootstrap styling.</p> <p>A modal dialog box is a type of popup window that appears on top of the current page and blocks interaction with the rest of the page until the user closes it.  It is often used to display important information or to prompt the user for input.</p> <pre><code>import dash\nimport dash_bootstrap_components as dbc\nimport dash_html_components as html\n\napp = dash.Dash(external_stylesheets=[dbc.themes.BOOTSTRAP])\n\nmodal = dbc.Modal(\n    [\n        dbc.ModalHeader(\"Modal Header\"),\n        dbc.ModalBody(id='modal-message'),\n        dbc.ModalFooter(\n            html.Button(\"Close\", id=\"close-modal\", className=\"ml-auto\")\n        ),\n    ],\n    id=\"modal\",\n    size=\"lg\",\n    centered=True,\n)\n\napp.layout = html.Div(\n    [\n        html.H1(\"Dash Modal Example\"),\n        html.Button(\"Open Modal\", id=\"open-modal\"),\n        modal,\n    ]\n)\n\n@app.callback(\n    dash.dependencies.Output(\"modal\", \"is_open\"),\n    [dash.dependencies.Input(\"open-modal\", \"n_clicks\"), dash.dependencies.Input(\"close-modal\", \"n_clicks\")],\n    [dash.dependencies.State(\"modal\", \"is_open\")],\n)\ndef toggle_modal(open_clicks, close_clicks, is_open):\n    if open_clicks or close_clicks:\n        return not is_open\n    return is_open\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"Python/Dash/dbc/Modal/#send-exception-message-to-modal-body","title":"send exception message to modal body","text":"<pre><code>@app.callback(\n    [\n        Output(\"load-data\", \"children\"),\n        Output('modal', \"is_open\"),\n        Output('modal-message', \"children\")\n    ],\n    [\n        Input('date-range', 'data'),\n        Input('select-type', 'value')\n    ]\n)\ndef load_data(\n    date_range,\n    data_type,\n):\n    if not date_range:\n        raise PreventUpdate\n\n    if not data_type:\n        return [html.Div()], no_update, no_update\n\n    try:\n        return get_data(date_range, data_type), no_update, no_update\n    except Exception as e:\n        msg = str(e)\n        return [html.Div(msg)], True, msg\n</code></pre>"},{"location":"Python/Dash/dbc/Modal/#can-use-modal-to-show-errors-in-callbacks","title":"Can use <code>Modal</code> to show errors in callbacks","text":"<pre><code>  html.Div(\n      [\n          dbc.Modal(\n              [\n                  dbc.ModalHeader(dbc.ModalTitle(\"Error\"), close_button=True),\n                  dbc.ModalBody(id='error-modal-message'),\n                  dbc.ModalFooter(\n                      dbc.Button(\n                          \"Try again\",\n                          id=\"close-error-modal\",\n                          className=\"ms-auto\",\n                          n_clicks=0,\n                      )\n                  ),\n              ],\n              id=\"error-modal\",\n              centered=True,\n              is_open=False,\n          ),\n      ]\n  ),\n\n@app.callback\nOutput('error-modal', 'is_open'),\nOutput('error-modal-message', 'children'),\n\n# get exception stacktrace\nmsg = str(exc) + '\\n' + traceback.format_exc()\nreturn html.Div(\n    msg,\n    className='h-100',\n    style={\n        'fontSize': '1.15rem',\n        'display': 'flex',\n        'justifyContent': 'center',\n        'alignItems': 'center',\n    },\n)\n</code></pre>"},{"location":"Python/Dash/dbc/Navbar/","title":"Navbar","text":""},{"location":"Python/Dash/dbc/Navbar/#example","title":"example","text":"<p>Include a logo, title and a link to another dashboard <pre><code>navbar = html.Div([dbc.Navbar([\n    dbc.Container(\n        dbc.Row([\n            dbc.Col(\n                html.Img(src=app.get_asset_url('logo.png'), height='20px')\n            ),\n            dbc.Col(\n                dbc.NavbarBrand(\n                    'My First Dashboard', \n                    id='page-title', \n                    className='page-title',\n                ),\n                className='text-center',\n            ),\n            dbc.Col([\n                dcc.Link(\n                    'Another Dashboard',\n                    href='dashboard.example.com/another-dashboard',\n                    id='another-dashboard',\n                    className='navbar-link',\n                ),\n            ], className='navbar-links'),\n        ], className='custom-navbar-row', align='center'\n        ), fluid=True\n    )\n], color='#5e6b73', dark=True, className='custom-navbar')])\n</code></pre></p>"},{"location":"Python/Dash/dbc/RadioItems/","title":"RadioItems","text":""},{"location":"Python/Dash/dbc/RadioItems/#example","title":"example","text":"<p>An inline radio items list with a lable <code>Metric:</code>. <pre><code>html.Div([\n    html.Div(\n        'Metric: ',\n        style={\n            'fontWeight': 'bold',\n            'flex': '0 0 auto',\n            'marginRight': '3px',\n        }\n    ),\n    dbc.RadioItems(\n        id='accuracy-metric-radio',\n        options=[\n            {'label': k, 'value': v}\n            for (k, v) in METRICS_DICT.items()\n        ],\n        value=list(METRICS_DICT.values())[0],\n        inline=True,\n    )\n], style={\n    'display': 'flex',\n    'flex': '1',\n    'justifyContent': 'end',\n})\n</code></pre></p>"},{"location":"Python/Dash/dbc/Row/","title":"Row","text":""},{"location":"Python/Dash/dbc/Row/#example","title":"example","text":"<p>Put two figs at left-top and 2 at right-top, and 1 at the bottom</p> <p>Content in <code>[]</code> will be aligned horizontally. But <code>html.Div</code> in a <code>[]</code> will be aligned vertically. <pre><code>import dash\nfrom dash import dcc, html\nimport dash_bootstrap_components as dbc\nimport plotly.express as px\n\napp = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n\n# Sample data and figures\nfig1 = px.scatter(x=[1, 2, 3], y=[4, 5, 6])\nfig2 = px.bar(x=[1, 2, 3], y=[7, 8, 9])\nfig3 = px.line(x=[1, 2, 3], y=[4, 5, 6])\nfig4 = px.scatter(x=[1, 2, 3], y=[7, 8, 9])\nfig5 = px.bar(x=[1, 2, 3], y=[10, 11, 12])\n\napp.layout = dbc.Container([\n    dbc.Row([\n        dbc.Col(dcc.Graph(figure=fig1), width=6),\n        dbc.Col(dcc.Graph(figure=fig2), width=6),\n    ]),\n    dbc.Row([\n        dbc.Col(dcc.Graph(figure=fig3), width=6),\n        dbc.Col(dcc.Graph(figure=fig4), width=6),\n    ]),\n    dbc.Row([\n        dbc.Col(dcc.Graph(figure=fig5), width=12),\n    ]),\n], fluid=True)\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n</code></pre></p>"},{"location":"Python/Dash/dbc/Tabs/","title":"Tabs","text":"<p>https://dash-bootstrap-components.opensource.faculty.ai/docs/components/tabs/</p> <p>https://dash.plotly.com/dash-core-components/tabs</p>"},{"location":"Python/Dash/dbc/Tabs/#example","title":"example","text":"<pre><code>import dash\nfrom dash import dcc, html\nfrom dash.dependencies import Input, Output\nimport dash_bootstrap_components as dbc\nimport datetime\n\n# Create a Dash app\napp = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n\n# Define the layout of the dropdown and date range picker\ndropdown_layout = dbc.FormGroup([\n    dbc.Label(\"Select an item from the dropdown:\"),\n    dcc.Dropdown(\n        id=\"dropdown\",\n        options=[\n            {\"label\": \"Option 1\", \"value\": \"option1\"},\n            {\"label\": \"Option 2\", \"value\": \"option2\"},\n            {\"label\": \"Option 3\", \"value\": \"option3\"},\n        ],\n        value=\"option1\",\n    ),\n])\n\ndate_range_layout = dbc.FormGroup([\n    dbc.Label(\"Select a date range:\"),\n    dcc.DatePickerRange(\n        id=\"date-range\",\n        start_date=datetime.date(2023, 1, 1),\n        end_date=datetime.date(2023, 1, 7),\n    ),\n])\n\n# Create the Tabs component\ntabs = dbc.Tabs([\n    dbc.Tab(label=\"Dropdown\", children=dropdown_layout),\n    dbc.Tab(label=\"Date Range\", children=date_range_layout),\n])\n\n# Define the app layout\napp.layout = dbc.Container([\n    html.H1(\"Dropdown and Date Range Tabs Example\"),\n    tabs,\n])\n\n# Run the app\napp.run_server(mode=\"inline\")\n</code></pre>"},{"location":"Python/Dash/dbc/dash_bootstrap_components/","title":"dash_bootstrap_components","text":""},{"location":"Python/Dash/dbc/dash_bootstrap_components/#components","title":"components","text":"<p>https://dash-bootstrap-components.opensource.faculty.ai/docs/components/</p>"},{"location":"Python/Dash/dcc/DatePickerRange/","title":"DatePickerRange","text":"<p>https://dash.plotly.com/dash-core-components/datepickerrange</p> <ul> <li>Accept either <code>strings</code> in the form YYYY-MM-DD or <code>date</code> objects from the datetime module. </li> <li>Strings are preferred because that's the form dates take as <code>callback arguments</code>. </li> </ul>"},{"location":"Python/Dash/dcc/DatePickerSingle/","title":"DatePickerSingle","text":"<p>https://dash.plotly.com/dash-core-components/datepickersingle</p>"},{"location":"Python/Dash/dcc/DatePickerSingle/#example","title":"example","text":"<pre><code>import dash\nfrom dash import dcc\nfrom datetime import datetime\n\napp = dash.Dash(__name__)\napp.layout = dcc.DatePickerSingle(\n    id='my-single-date-picker',\n    date=datetime(2023, 6, 10),             \n    min_date_allowed=datetime(2023, 6, 5), \n    max_date_allowed=datetime(2023, 6, 15),\n    display_format='YYYY-MM-DD',\n    style={'marginRight': '7px'},\n)\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True)\n</code></pre>"},{"location":"Python/Dash/dcc/Download/","title":"Download","text":""},{"location":"Python/Dash/dcc/Graph/","title":"Graph","text":"<p>dcc.Graph is a component for creating interactive data visualizations.</p>"},{"location":"Python/Dash/dcc/Graph/#example","title":"example","text":"<p>Here is an example of how to use dcc.Graph and dcc.Loading: <pre><code>import dash\nfrom dash import dcc\n\napp = dash.Dash()\n\n# Create a dcc.Graph component.\ngraph = dcc.Graph(\n    id=\"my-graph\",\n    figure={\n        \"data\": [\n            {\n                \"x\": [1, 2, 3],\n                \"y\": [4, 5, 6],\n                \"type\": \"scatter\",\n                \"name\": \"Series 1\",\n            },\n        ],\n        \"layout\": {\"title\": \"My Graph\"},\n    },\n    style={'height': '100%', 'width': '100%'},\n    #style={'height': '500px', 'width': '100%'} # can be relative or absolute size\n)\n\n# Wrap the graph component in a dcc.Loading component.\nloading_graph = dcc.Loading(\n    id=\"loading-graph\",\n    children=graph,\n    type=\"circle\",\n)\n\n# Add the loading_graph component to the layout.\napp.layout = html.Div([\n    loading_graph,\n])\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True)\n</code></pre></p>"},{"location":"Python/Dash/dcc/Graph/#fig_jsondata","title":"fig_json['data']","text":"<p>When working with Dash's <code>dcc.Graph</code> component, the <code>fig_json['data'][0]</code> refers to the first trace or set of data in the graph figure.</p> <pre><code>import dash\nfrom dash imports as dcc, html\nimport plotly.graph_objects as go\n\n# Sample data\nx_data = [1, 2, 3, 4, 5]\ny_data = [10, 11, 12, 13, 14]\n\n# Create a Plotly Figure\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x_data, y=y_data, mode='lines', name='Line'))\n\n# Create a Dash app\napp = dash.Dash(__name__)\n\n# Define the layout of the app\napp.layout = html.Div([\n    dcc.Graph(\n        id='example-graph',\n        figure=fig\n    )\n])\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n</code></pre> <p>In this example:</p> <ul> <li><code>go.Figure()</code> is used to create a Plotly figure.</li> <li><code>fig.add_trace(go.Scatter(...))</code> adds a trace to the figure. In this case, it's a simple scatter plot with lines connecting the points.</li> <li><code>dcc.Graph(id='example-graph', figure=fig)</code> creates a Dash Core Component Graph, and the <code>figure</code> attribute is set to the Plotly figure (<code>fig</code>) we created earlier.</li> </ul> <p>To access the data of the first trace in the graph using <code>fig_json['data'][0]</code>, you can do so by converting the figure to JSON using <code>fig.to_json()</code>:</p> <pre><code>fig_json = fig.to_json()\ntrace_data = fig_json['data'][0]\nprint(trace_data)\n</code></pre> <p>This will give you access to the data of the first trace in the form of a dictionary,  allowing you to retrieve information about the x and y coordinates, mode, name, and other attributes of the trace.</p>"},{"location":"Python/Dash/dcc/Link/","title":"Link","text":""},{"location":"Python/Dash/dcc/Link/#link-to-other-url","title":"link to other url","text":"<p>Without setting <code>refresh=True</code> only the url will change but the web content (linked to external app) will remain the same. Seems does not work! Must use full url! <pre><code>dbc.Col([\n    dcc.Link(\n        'test',\n        href='sales/test',\n        refresh=True, #refresh the page after redirect\n        id='sales-test',\n        className='navbar-link',\n    ),\n    dcc.Link(\n        'prod',\n        href='sales/prod',\n        refresh=True, #refresh the page after redirect\n        id='sales-prod',\n        className='navbar-link',\n    ),\n], className='navbar-links'\n)\n</code></pre> this one? https://stackoverflow.com/questions/66127435/dash-plotly-how-to-redirect-to-external-link-which-will-redirect-to-dash</p> <p>Redirect to a new tab: https://community.plotly.com/t/link-to-an-external-site-in-a-new-browser-tab/7249</p>"},{"location":"Python/Dash/dcc/Loading/","title":"Loading","text":"<p><code>dcc.Loading</code> is a simple component that wraps other components and displays a spinner while the content is loading.  This can be useful for preventing users from seeing empty content while they are waiting for data to load.</p>"},{"location":"Python/Dash/dcc/Location/","title":"Location","text":"<p>https://dash.plotly.com/dash-core-components/location</p> <p>The dcc.Location component represents the <code>location or address bar in your web browser</code>.  Through its href, pathname, search and hash properties you can access different portions of the URL that the app is loaded on.</p>"},{"location":"Python/Dash/dcc/Store/","title":"Store","text":"<ul> <li>This can be used to keep data on the client side.</li> <li>The data in <code>dcc.Store</code> needs to be in <code>JSON</code> format for the browser.</li> </ul>"},{"location":"Python/Dash/html/Div/","title":"html.Div","text":"<p>In Dash, <code>html.Div</code> is a component used to group and organize other components within your layout. It represents a generic HTML division or container.</p> <p>Here are some key points about <code>html.Div</code> in Dash: - Container Element: <code>html.Div</code> is an HTML <code>&lt;div&gt;</code> element. In web development, the <code>&lt;div&gt;</code> element is a generic container that is often used to group and structure content. - Structuring Layout: It helps organize the layout of your Dash application by grouping components together. You can use it to create sections, columns, or other structural divisions in your app. - Styling and CSS: <code>html.Div</code> can be assigned CSS classes and styles to control its appearance. You can set properties such as background color, width, height, margins, and more. - Children Components: You can include other Dash components, including other <code>html.Div</code> elements, as children of an <code>html.Div</code>. This allows you to create nested structures for your layout.</p>"},{"location":"Python/Dash/html/Div/#styling-example","title":"styling example","text":"<p>Adjust the vertical space between two dcc.Loading objects <pre><code>left_tab = html.Div([\n    html.Div([\n        dcc.Loading(\n            id='loading-metrics-breakdown-graph',\n            type='default',\n            color=PRIMARY_COLOR,\n        ),\n    ], style={'margin-top': '10px'}), #adjust the top space\n    html.Div([\n        dcc.Loading(\n            id='loading-benchmark-metrics-graph',\n            type='default',\n            color=PRIMARY_COLOR,\n        ),\n    ], style={'margin-top': '10px'}),\n],\nclassName='h-100 justify-content metrics-breakdown',\nstyle={'position': 'absolute', 'inset': '0'},\n)\n</code></pre></p>"},{"location":"Python/Dash/html/Element/","title":"Element","text":""},{"location":"Python/Dash/html/Element/#htmlhr","title":"html.Hr","text":"<p>a horizontal rule: The HTML<code>&lt;hr&gt;</code> tag is a thematic break element that represents a change in topic or subject matter on a web page.</p>"},{"location":"Python/Dash/html/Span/","title":"Span","text":""},{"location":"Python/Dash/html/Span/#difference-between-htmlspan-and-htmldiv","title":"difference between html.Span and html.Div","text":"<ul> <li><code>Span</code> is an inline element and <code>Div</code> is a block-level element. </li> <li><code>Span</code> elements will flow with the surrounding text, while Div elements will start a new line and take up the full width of their container.</li> </ul> <p>When to use Span:  Span elements are typically used to group together small pieces of content that need to be styled differently from the surrounding text.  For example, you might use a Span element to highlight a word or phrase, or to add a link to a piece of text.</p> <p>When to use Div:  Div elements are typically used to group together larger pieces of content, such as paragraphs, headings, and images.  Div elements can also be used to create more complex layouts, such as navigation bars, menus, and product galleries.</p>"},{"location":"Python/Dash/html/Span/#example-of-div-and-span","title":"example of div and span","text":"<p>Create an inline dropdown and single date picker. <code>Div</code> for vertical components and <code>[]</code> for horizontal components. <pre><code>html.Div([\n    html.Span(\n        'Model: ', style={'marginRight': '7px'}\n    ),\n    dcc.Dropdown(\n        id='model-dropdown',\n        options=[\n            {'label': k, 'value': v} for (k, v) in MODELS_DICT.items()\n        ],\n        value=None,\n        clearable=True,\n        style={'width': '170px'}\n    ),\n    html.Span(\n        'Date: ', style={'margin': '0 7px'}\n    ),\n    dcc.DatePickerSingle(\n        id='creation-date',\n        min_date_allowed=MIN_DATE,\n        max_date_allowed=datetime.now().date(),\n        display_format='YYYY-MM-DD',\n        style={'marginRight': '7px'}\n    )\n], style={\n    'display': 'flex',\n    'alignItems': 'center',\n    'flex': '0 1 auto',\n    'marginTop': '7px',\n    'justifyContent': 'end',\n})\n</code></pre></p>"},{"location":"Python/Dask/Config/","title":"Config","text":"<p>https://docs.dask.org/en/stable/configuration.html</p> <ul> <li><code>num_workers</code> set the number of processes or threads to use (defaults to number of cores)</li> </ul>"},{"location":"Python/Dask/Config/#example","title":"Example","text":"<pre><code>export DASK_DISTRIBUTED__WORKERS__MEMORY__SPILL=0.85\ndask.config.set({\"distributed.workers.memory.spill\": 0.85})\n\nimport dask\ndask.config.config #show config\n</code></pre>"},{"location":"Python/Dask/Config/#threads","title":"threads","text":"<pre><code>from multiprocessing.pool import ThreadPool\ndask.config.set(scheduler='threads', pool=ThreadPool(2)) #num_workers has no effect\n</code></pre>"},{"location":"Python/Dask/Config/#processes","title":"processes","text":"<pre><code>from multiprocessing.pool import ThreadPool\ndask.config.set(scheduler='processes', num_workers=2, pool=ThreadPool(2)) #two workers, 2 threads per worker\n</code></pre>"},{"location":"Python/Dask/Config/#comm","title":"comm","text":"<ul> <li><code>distributed.comm.zstd.threads  0</code>   Number of threads to use. 0 for single-threaded, -1 to infer from cpu count.</li> </ul>"},{"location":"Python/Dask/Config/#scheduler","title":"scheduler","text":"<ul> <li><code>distributed.scheduler.work-stealing  True</code>   Whether or not to move tasks around to balance work between workers dynamically.\\   https://dask.discourse.group/t/understanding-work-stealing/335/9</li> <li><code>distributed.scheduler.work-stealing-interval  100ms</code>   How frequently to balance worker loads.</li> </ul>"},{"location":"Python/Dask/Config/#worker","title":"worker","text":"<ul> <li><code>distributed.worker.lifetime.duration  None</code>   The time (seconds) after creation to close the worker, like \"1 hour\".\\   Can also have other units like \"2 hours\"</li> <li><code>distributed.worker.lifetime.stagger  0 seconds</code>    Workers close with random variation time so not closed all at the same time.</li> <li><code>distributed.worker.lifetime.restart  False</code>   Do we try to resurrect the worker after the lifetime deadline?\\   Mainly used to deal with worker stalls or memory leak</li> </ul>"},{"location":"Python/Dask/Dask/","title":"Dask","text":"<p>https://gist.github.com/fionaRust/c7953629690a8be2e7477f266b113877</p>"},{"location":"Python/Dask/Dask/#dask-tutorial","title":"dask tutorial","text":"<p>https://github.com/dask/dask-tutorial</p>"},{"location":"Python/Dask/Dask/#when-to-use","title":"when to use","text":"<p>Dask provides convenience to parallelize code, better than multiprocessing and joblib.</p> <p>Also compared to pandas, dask can filter parquet file row by row when reading. So if do not need all the data tis can be faster.</p> <p>If load the total file all together, pandas is much faster.</p>"},{"location":"Python/Dask/DataFrame/","title":"DataFrame","text":""},{"location":"Python/Dask/DataFrame/#multiindex","title":"MultiIndex","text":"<p>cavaet: NotImplementedError: Dask dataframe does not yet support multi-indexes. At least not fully supported.</p> <ul> <li>If multiindex is not required, save the dataframe as a parquet file by resetting the index. </li> <li>It's much faster to load a parquet file without multiindex, though the total time including recreating the index is a little longer.</li> <li>Aslo, we can read any level separately much faster - with multiindex, we need to read all the index levels together. </li> </ul> <p>Example <pre><code>%%timeit -r 3 -n 3\nfiles = ['/tmp/data.parquet']\n# directly load all index levels\ndi = dd.read_parquet(\n    files,\n    index=None,\n    columns=['val1', 'val2'],\n    engine='pyarrow',\n    open_file_options=dict(precache_options=dict(method='parquet')),\n).groupby(['idx2']).sum().compute()\n# load all index levels as cols\ndi = dd.read_parquet(\n    files,\n    index=False,\n    columns=['idx1', 'idx2', 'idxn', 'val1', 'val2'],\n    engine='pyarrow',\n    open_file_options=dict(precache_options=dict(method='parquet')),\n).drop(columns=['idx1', 'idxn']).groupby(['idx2']).sum().compute()\n</code></pre></p>"},{"location":"Python/Dask/DataFrame/#partition","title":"Partition","text":""},{"location":"Python/Dask/Deadlock/","title":"deadlock","text":""},{"location":"Python/Dask/Deadlock/#threads","title":"threads","text":"<p><code>dask.base.compute</code> is not thread-safe. So using <code>threads</code> can lead to deadlock.</p>"},{"location":"Python/Dask/Delayed/","title":"Delayed","text":"<p>https://docs.dask.org/en/stable/delayed.html</p>"},{"location":"Python/Dask/Delayed/#specify-number-of-threads","title":"specify number of threads","text":"<pre><code>from multiprocessing.pool import ThreadPool\nimport dask\ndask.config.set(pool=ThreadPool(10))\n</code></pre>"},{"location":"Python/Dask/Delayed/#best-practices","title":"best practices","text":"<p>https://docs.dask.org/en/stable/delayed-best-practices.html</p>"},{"location":"Python/Dask/Delayed/#delayedcompute-vs-clientcompute","title":"delayed.compute vs client.compute","text":"<p>https://github.com/dask/distributed/issues/733</p> <p><code>delayed.compute()</code> is a shortcut of <code>f=client.compute(d); f.result()</code> except worker_client (fixed so no exception)</p>"},{"location":"Python/Dask/Distributed/","title":"Distributed","text":""},{"location":"Python/Dask/Distributed/#default","title":"default","text":"<pre><code>import dask\nfrom dask.distributed import Client\n\nclient = Client() #processes=True, [n_workers=n_cores, threads_per_worker=1]\nclient = Client(processes=False) #[n_workers=1, threads_per_worker=n_cores]\nclient = Client(n_workers=2, threads_per_worker=1) #explicitely set the workers and threads\n\n#client.close()\nclient.restart()\n</code></pre>"},{"location":"Python/Dask/Distributed/#standalone-python-scripts","title":"Standalone Python scripts","text":"<p>must put the code in <code>main</code> <pre><code>if __name__ == '__main__':  # This avoids infinite subprocess creation\n   from dask.distributed import Client\n   client = Client()\n</code></pre></p>"},{"location":"Python/Dask/Example/","title":"dask example","text":"<pre><code>import os\nimport time\nimport socket\nimport matplotlib.pyplot as plt\n\nimport dask\nfrom dask.distributed import Client, LocalCluster\nfrom dask_jobqueue import SLURMCluster # Must run on a Linux machine with SLURM\n\nnum_workers = os.environ.get(\"SLURM_NTASKS\", 2)\n\n# Submit workers as slurm job\n# cluster = SLURMCluster(\n#     cores=os.environ.get(\"SLURM_CPUS_PER_TASK\",2),\n#     processes=1,\n#     memory=\"2GB\",\n#     walltime=\"00:00:10\",\n#     queue=\"batch\",\n#     interface=\"Ethernet0\",\n# )\n# cluster.scale(num_workers)\n\n# Test on windows\ncluster = LocalCluster(n_workers=num_workers, threads_per_worker=1, memory_limit='1GB')\n\n# Connect to distributed cluster and override default\nclient = Client(cluster)\nclient.wait_for_workers(n_workers=num_workers)\n\ndef inc(x):\n    time.sleep(2)\n    return x + 1\n\ndef dbl(x):\n    return x * 2\n\ndef add(x, y):\n    time.sleep(2)\n    return x + y\n\ndata = [1, 2, 3, 4, 5, 6]\n\noutput = []\nfor v in data:\n    a = dask.delayed(inc)(v)\n    b = dask.delayed(dbl)(v)\n    c = dask.delayed(add)(a, b)\n    output.append(c)\n\n# Second approach as a delayed function\ntotal = dask.delayed(sum)(output)\ntotal.visualize(filename='task_graph.svg')\n# parallel execution workers\nt0 = time.time()\nresults = total.compute()\nprint(f'{time.time() - t0:.3f}: {results}')\n\n#### Very important ############\ncluster.close()\n</code></pre>"},{"location":"Python/Dask/Jobqueue/","title":"Jobqueue","text":"<p>http://www.devdoc.net/python/dask-2.23.0-doc/setup/hpc.html</p> <ul> <li><code>dask-jobqueue</code> for use with PBS, SLURM, LSF, SGE and other resource managers</li> <li><code>dask-drmaa</code> for use with any DRMAA compliant resource manager</li> </ul>"},{"location":"Python/Dask/Parquet/","title":"Parquet","text":"<p>better to save parquet files without index!!!</p>"},{"location":"Python/Dask/Parquet/#performance","title":"performance","text":"<ul> <li>If read the whole file, <code>pd.read_parquet</code> is better.</li> <li>If there are row filters <code>dd.read_parquet</code> can be faster??? as <code>pd.read_parquet</code> need to read the whole index first (how about filters on cols not index???).</li> <li>Seems <code>dd</code> read multiple parquet files in parallel, but still a little bit slower than parallel <code>pd</code>, pd.concat takes lots of time for large files</li> </ul>"},{"location":"Python/Dask/Parquet/#read-filters","title":"read <code>filters</code>","text":"<ul> <li>The <code>filters</code> keyword is a row-group-wise action</li> <li>It does not do any filtering within <code>partitions</code></li> <li>when using <code>pyarrow</code>, the filters will apply to the partition as well</li> <li>when do not need all data, it's much fast than using <code>pd.read_parquet</code>? <code>pd.read_parquet</code> is about 2-3x faster</li> <li>when read without filters, the <code>dd</code> requires the loading of all levels of a multi-index df</li> <li>for <code>pd</code> if use_pandas_metadata, we need to include index columns in the column selection, to be able to restore those in the pandas DataFrame</li> </ul>"},{"location":"Python/Dask/Parquet/#read-parquet","title":"read parquet","text":"<p>Note that if there are no filters, cannot read part of the index levels. See: https://github.com/dask/dask/issues/10386 <pre><code>Seems this is due to dask still does not fully support multiindex.\nA workaround is to read all the index levels and then drop the levels that are not needed.\nAnother workaround is resetting the index before saving the data to parquet file\n- seems it's faster to load data without multiindex and\n- we do not need to load all other indexl levels that are not required.\n</code></pre></p> <pre><code>filters = [\n    ('datetime_val', '&gt;=', pd.to_datetime('2023-07-15')),\n    (str_val, '==', 'hello'),\n    ('int_val', 'in', (1, 2)),\n]\nschema = pa.schema[\n    ('datetime_val', pa.timestamp('ns')),\n    ('str_val', pa.string()),\n    ('int_val', pa.int64()),\n    ('float_val', pa.float64()),\n]        \ndf = dd.read_parquet(\n    path=file_path_list,\n    filters=filters,\n    index=False,\n    columns=columns,\n    engine='pyarrow',\n    storage_options=storage_options,\n    open_file_options=dict(precache_options={'method': 'parquet'}),\n    schema=schema,\n).compute()\n</code></pre>"},{"location":"Python/Dask/SLURM/","title":"SLURM","text":"<p>https://www.run.ai/guides/slurm</p> <p>SLURM is a system for managing and scheduling Linux clusters. It is open source, fault tolerant and scalable, suitable for clusters of various sizes.</p>"},{"location":"Python/Dask/SLURM/#example","title":"example","text":"<pre><code>from dask_jobqueue import SLURMCluster\nfrom dask.distributed import Client\nimport dask\n\nimport matplotlib.pyplot as plt\nimport socket\nimport time\nimport os\n\n# Submit workers as slurm job\ncluster = SLURMCluster(\n    cores=os.environ.get(\"SLURM_CPUS_PER_TASK\",2),\n    processes=1,\n    memory=\"4GB\",\n    walltime=\"00:00:10\",\n    queue=\"batch\",\n    interface=\"Ethernet0\",\n)\n\nnum_workers = os.environ.get(\"SLURM_NTASKS\",1)\ncluster.scale(num_workers)\n\n# Connect to distributed cluster and override default\nclient = Client(cluster)\nclient.wait_for_workers()\n\ndef inc(x):\n    time.sleep(2)\n    return x + 1\n\ndef dbl(x):\n    return x * 2\n\ndef add(x, y):\n    time.sleep(2)\n    return x + y\n\ndata = [1, 2, 3, 4, 5, 6]\n\noutput = []\nfor v in data:\n    a = dask.delayed(inc)(v)\n    b = dask.delayed(dbl)(v)\n    c = dask.delayed(add)(a, b)\n    output.append(c)\n\n# Second approach as a delayed function\ntotal = dask.delayed(sum)(output)\ntotal.visualize(filename='task_graph.svg')\n# parallel execution workers\nt0 = time.time()\nresults = total.compute()\nprint(f'{time.time() - t0:.3f}: {results}')\n\n#### Very important ############\ncluster.close()\n</code></pre>"},{"location":"Python/Dask/Scheduler/","title":"Scheduler","text":"<p>https://docs.dask.org/en/latest/scheduling.html</p> <p>https://dask.pydata.org/en/latest/scheduler-overview.html#configuring-the-schedulers</p>"},{"location":"Python/Dask/Scheduler/#schedulers","title":"schedulers","text":"<ul> <li><code>threaded</code> (threads): a scheduler backed by a thread pool</li> <li><code>processes</code> (processes): a scheduler backed by a process pool</li> <li><code>synchronous</code> (single-threaded): a synchronous scheduler, good for debugging</li> <li><code>distributed</code> (distributed): a distributed scheduler for executing graphs on multiple machines</li> </ul>"},{"location":"Python/Dask/Scheduler/#synchronous-scheduler","title":"synchronous scheduler","text":"<p>good for debugging <pre><code># As a context manager\nwith dask.config.set(scheduler='single-threaded'):\n    x.sum().compute()\n\n# Set globally: scheduler='processes', num_workers=4\ndask.config.set(scheduler='single-threaded')\nx.sum().compute()\n</code></pre></p>"},{"location":"Python/Dask/Threaded/","title":"Threaded","text":""},{"location":"Python/Dask/Threaded/#set-threads","title":"set threads","text":"<p>Single machine scheduling: Dask.dataframe will use the threaded scheduler by default with as many threads as you have logical cores in your machine.</p> <p>The number of threads and pool implementation can be controlled by keyword arduments such as: <code>.compute(num_workers=4)</code>.</p> <p>Or set the number of workers via env var: <pre><code>dask.config.set({'num_workers', 4})\ndask.config.set(num_workers = 4)  # `.` should be replaced with `__`\n</code></pre></p>"},{"location":"Python/Dask/Threaded/#hanging","title":"hanging","text":"<p>Numba and Dask aren't currently able to share threadpools, so using multi-threaded Numba and multi-threaded Dask can lead to app hanging.</p> <p>Using single-threaded Dask, or using single-threaded Numba. <pre><code>from umap import UMAP\nUMAP utilizes @numba.njit(parallel=True) #False\n\nfrom numba import njit\n@njit(parallel=True) #False\nos.environ[\"NUMBA_NUM_THREADS\"] = \"1\"\n</code></pre></p>"},{"location":"Python/DataType/Dictionary/","title":"Dictionary","text":""},{"location":"Python/DataType/Dictionary/#merge-dicts","title":"merge dicts","text":"<pre><code>dic1 = {'a':1, 'b':2}\ndic2 = {'e':5, 'f':6}\ndic3 = {**dic1, **dic2}\n</code></pre>"},{"location":"Python/DataType/Dictionary/#df-to-dic","title":"df to dic","text":"<pre><code>#each col to a key\ndf = pd.DataFrame({'a': [1,2],'b': ['x', 'y'], 'c': ['u', 'v']})\ndic = df.to_dict('list')\n\n#each row to a key\ndf = pd.DataFrame({'id': ['p', 'q', 'r'],'a': ['red', 'yellow', 'blue'], 'b': [0.5, 0.25, 0.125]})\ndic = df.set_index('id').T.to_dict('list')\n\n#not work\ndic = {k: list(g) for k, g in df.groupby('id')['a','b']}\n\n#series: id is key, a and b are in 2d array\nxx = df.groupby('id').apply(lambda x:x[['a','b']].values)\n\n#series: similar to previous, values col has a name\nyy = df.groupby('id')['a','b'].apply(lambda x: x.values.tolist()).to_frame('val').reset_index()\n</code></pre>"},{"location":"Python/DataType/Float/","title":"Float","text":""},{"location":"Python/DataType/Float/#_1","title":"==","text":"<p>The default <code>x == y</code> is equivalent to <code>math.isclose(x, y, rel_tol=1e-16, abs_tol=0)</code> - <code>1.0000000000000001 == 1</code> will return <code>True</code> - <code>0 == 0.00000000000000000000000000000000000000001</code> will return <code>False</code></p>"},{"location":"Python/DataType/Float/#_2","title":"!=","text":"<p>https://www.quora.com/How-does-one-correctly-compare-two-floats-in-Python-to-test-if-they-are-equal</p> <p>floating point operation can have up to a 1/2 bit rounding. The rounding is approximately: <code>|real_value \u2212 rounded_value| &lt;= real_value * epsilon / 2</code> where epsilon depends on the underlying type of the floating point: <pre><code>import numpy as np \nepsilon= np.finfo(float).eps \n\nv = 10.0 \na = v / 77 \nb = a * 77 \nv != b # return True\nabs(v - b) &gt; max(abs(v), abs(b)) * epsilon # reture False\n</code></pre></p>"},{"location":"Python/DataType/Int/","title":"Int","text":""},{"location":"Python/DataType/Int/#round-up","title":"round up","text":"<p>https://stackoverflow.com/questions/2356501/how-do-you-round-up-a-number</p> <pre><code>round(2.3) # 2.0\nint(2.3 + .5) # 2\n\nmath.ceil(4.2) # return int (python3) or float (python2)?\nnp.ceil(2.3)\n</code></pre>"},{"location":"Python/DataType/Int/#float-precision","title":"float precision","text":"<pre><code>math.ceil(10000000 * 0.00136) # 13601.0 from 13600.000000000002\n</code></pre>"},{"location":"Python/DataType/Int/#rounding-down","title":"<code>//</code> rounding down","text":"<p><code>floor division</code> (also sometimes known as <code>integer division</code>). divide the first argument by the second and round the result down to the nearest whole number, equvalent to <code>math.floor</code>? - if one of the operands is a float in floor division, then the output will be a float <pre><code>rounded_up = -(-numerator // denominator)\n</code></pre></p>"},{"location":"Python/DataType/List/","title":"List","text":""},{"location":"Python/DataType/List/#add-two-lists","title":"add two lists","text":"<pre><code>l = [1,2] + [3]\n</code></pre>"},{"location":"Python/DataType/List/#duplicat-items-in-list","title":"duplicat items in list","text":"<pre><code>l = [1] * 3\n</code></pre>"},{"location":"Python/DataType/List/#nested-list","title":"nested list","text":"<pre><code>#nested list\na = [[0] * 4 for i in range(3)]\n</code></pre>"},{"location":"Python/DataType/List/#diff-between-two-lists","title":"diff between two lists","text":"<pre><code>s1 = set(l1).difference(l2)\n</code></pre>"},{"location":"Python/DataType/List/#remove-empty-items","title":"remove empty items","text":"<pre><code>l2 = list(filter(None, l1))\n</code></pre>"},{"location":"Python/DataType/List/#check-all-none","title":"check all None","text":"<pre><code>l.count(None)==len(l)\nall(v is None for v in l) #10x slower\n</code></pre>"},{"location":"Python/DataType/List/#remove-duplicate-in-list-and-keep-order","title":"remove duplicate in list and keep order","text":"<pre><code>from collections import OrderedDict\nlst = [1,2,4,2,5]\nunique_list = list(OrderedDict.fromkeys(lst)) #python &lt; 3.7\nunique_list = list(dict.fromkeys(lst))        #python &gt;= 3.7\n</code></pre>"},{"location":"Python/DataType/List/#transpose-list-of-tuples","title":"transpose list of tuples","text":"<pre><code>list_of_tuples = [(1,2), (3,4)]\nlist(zip(*list_of_tuples))\n</code></pre>"},{"location":"Python/DataType/List/#list-of-lists-to-csv","title":"list of lists to csv","text":"<pre><code>csv.writer(open('out.csv', 'w', newline='')).writerows(lls)\n</code></pre>"},{"location":"Python/DataType/Regex/","title":"Regex","text":""},{"location":"Python/DataType/Regex/#find-a-word-after-another-word","title":"find a word after another word","text":"<pre><code>re.compile(r'(?:from|join (\\w+)') #word after from or join\n</code></pre>"},{"location":"Python/DataType/Regex/#replace-empty-cell-by-nan","title":"replace empty cell by NaN","text":"<pre><code>df = df.replace(r'^\\s*$', np.nan, regex=True).fillna(value=np.nan)\n</code></pre>"},{"location":"Python/DataType/Regex/#replace-special-characters","title":"replace special characters","text":"<pre><code>#replace hyphen, en-dash, em-dash by dash\npattern = b'(\\xe2\\x80\\x90|\\xe2\\x80\\x93|\\xe2\\x80\\x94)'\ntxt2 = re.sub(pattern, b'-', txt.encode('utf-8')).decode('utf-8')\n</code></pre>"},{"location":"Python/DataType/Set/","title":"Set","text":""},{"location":"Python/DataType/Set/#intersection","title":"intersection","text":"<pre><code>x = {1,2,3}\ny = {2,3,4}\nz = x &amp; y #{2,3}\n</code></pre>"},{"location":"Python/DataType/String/","title":"String","text":"<pre><code>#join the same string many times\nfmt = ','.join(['%f'] * 3)\n\n#pad space before or after the string\nformat('str', '&lt;16')\nformat('str', '&gt;16')\n</code></pre>"},{"location":"Python/DataType/Tuple/","title":"Tuple","text":""},{"location":"Python/DataType/Tuple/#assigned-value-became-tuple","title":"assigned value became tuple","text":"<pre><code>x = 1,    #x = (1,)\ny = None, #y = (None,)\n</code></pre>"},{"location":"Python/DataType/Zip/","title":"zip etc","text":""},{"location":"Python/DataType/Zip/#zip-pairs","title":"zip pairs","text":"<pre><code>letters = ['a', 'b', 'c']\nnumbers = [0, 1, 2]\nfor l, n in zip(letters, numbers):\n    print(f'Letter: {l}')\n    print(f'Number: {n}')\n\n#unzip\npairs = [(1, 'a'), (2, 'b')]\nnumbers, letters = zip(*pairs)\n</code></pre>"},{"location":"Python/DataType/Zip/#sorting-in-parallel","title":"sorting in parallel","text":"<pre><code>letters = ['b', 'a', 'd']\nnumbers = [2, 4, 3]\ndata = list(zip(letters, numbers))\ndata.sort() #sort by letters\n\n#or\ndata = sorted(zip(letters, numbers)) #sort by letters\n</code></pre>"},{"location":"Python/DataType/Zip/#zip_longest","title":"zip_longest","text":"<pre><code>from itertools import zip_longest\n\nl1 = [1, 2, 3]\nl2 = [1, 2]\n\nlt = list(zip_longest(l1, l2, fillvalue=None))\nprint(lt)  #[(1, 1), (2, 2), (3, None)]\n</code></pre>"},{"location":"Python/DateTime/Conversion/","title":"Conversion","text":""},{"location":"Python/DateTime/Conversion/#valid-freq-values","title":"Valid <code>freq</code> values","text":"<p>https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases</p>"},{"location":"Python/DateTime/Conversion/#pdtimestamp-to-week-datetime-date-for-monday","title":"pd.Timestamp to week datetime (date for Monday)","text":"<pre><code>s = pd.Series(pd.date_range('2023-09-01','2023-09-04', freq='D'))\ns.dt.to_period('W').dt.start_time\ns.dt.to_period('W').dt.to_timestamp()\ns.dt.to_period('W').dt.strftime('%Y-%m-%d')\n</code></pre>"},{"location":"Python/DateTime/Conversion/#pdtimestamp-to-month-datetime","title":"pd.Timestamp to month datetime","text":"<pre><code>s.dt.to_period('M').dt.to_timestamp()\ns.dt.to_period('M').dt.strftime('%Y-%m')\ndf['mth'] = df['dt'].astype('datetime64[M]')\n</code></pre>"},{"location":"Python/DateTime/Conversion/#pdtimestamp-to-quarter-datetime-str","title":"pd.Timestamp to Quarter datetime (str)","text":"<pre><code>s.dt.to_period('Q').dt.to_timestamp()\ns.dt.to_period('Q').dt.strftime('%YQ%q')\ns.dt.to_period('Q').dt.strftime('%Y-%m-%d')\n\nper = pd.Timestamp('2023-07-01').to_period('Q-DEC') #cal year quarter\nper = pd.Timestamp('2023-07-01').to_period('Q-JUN') #fin year quarter\n</code></pre>"},{"location":"Python/DateTime/Conversion/#pdtimestamp-to-calfin-year","title":"pd.Timestamp to cal/fin year","text":"<pre><code>per = pd.Timestamp('2023-07-01').to_period('A-DEC') #cal year\nper = pd.Timestamp('2023-07-01').to_period('A-JUN') #fin year\n</code></pre>"},{"location":"Python/DateTime/Conversion/#quarter-year-to-pdtimestamp","title":"Quarter-Year to pd.Timestamp","text":"<pre><code>qs = df['Quarter'].str.replace(r'(Q\\d) (\\d+)', r'\\2-\\1')\nymd_quarter = pd.PeriodIndex(qs, freq='Q').to_timestamp()\nymd_quarter = pd.to_datetime(qs, errors='coerce')\n</code></pre>"},{"location":"Python/DateTime/Conversion/#dateoffset","title":"DateOffset","text":"<pre><code>#within the same day\n&gt;&gt;&gt; pd.Timestamp(\"2021-08-25\") - pd.DateOffset(months=1)\nTimestamp('2021-07-25 00:00:00')\n\n#offset to the period end\n&gt;&gt;&gt; pd.Timestamp(\"2021-07-01\") - pd.tseries.frequencies.to_offset('A-JUN')\nTimestamp('2022-06-30 00:00:00')\n\n#offset to the period start\n&gt;&gt;&gt; pd.Timestamp(\"2021-07-01\") + pd.tseries.frequencies.to_offset('AS-JUL')\nTimestamp('2022-07-01 00:00:00')\n\n#offset to the period start: Cal Year\n&gt;&gt;&gt; pd.Timestamp(\"2021-01-01\") + pd.tseries.frequencies.to_offset('AS-JAN')\nTimestamp('2022-01-01 00:00:00')\n</code></pre>"},{"location":"Python/DateTime/DateOffset/","title":"DateOffset","text":""},{"location":"Python/DateTime/DateOffset/#pddateoffset","title":"pd.DateOffset","text":"<p><code>day</code> resets the actual day, <code>days</code> adds to the original day <pre><code>print(pd.Timestamp(\"2015-01-01\") - pd.DateOffset(day=3))  # 2015-01-03\nprint(pd.Timestamp(\"2015-01-01\") + pd.DateOffset(day=3))  # 2015-01-03\nprint(pd.Timestamp(\"2015-01-01\") + pd.DateOffset(days=3)) # 2015-01-04\n</code></pre></p>"},{"location":"Python/DateTime/DateOffset/#period","title":"period","text":"<p>Only honor the integer periods <pre><code>t = pd.Timestamp('2022-08-01 12:25:56')\np = t.to_period('30T')\nprint(f'from {p.start_time} to {p.end_time}')\nfrom 2022-08-01 12:25:00 to 2022-08-01 12:54:59.999999999\n</code></pre></p>"},{"location":"Python/DateTime/DateOffset/#normalize","title":"normalize","text":"<pre><code>&gt;&gt;&gt; pd.date_range('2022-08-01 23:59:59', '2022-09-16', freq=pd.tseries.frequencies.to_offset('MS'))\nDatetimeIndex(['2022-08-01 23:59:59'], dtype='datetime64[ns]', freq='MS')\n\n&gt;&gt;&gt; pd.date_range('2022-08-01 23:59:59', '2022-09-16', freq=pd.tseries.frequencies.to_offset('MS'), normalize=True)\nDatetimeIndex(['2022-08-01', '2022-09-01'], dtype='datetime64[ns]', freq='MS')\n\n&gt;&gt;&gt; pd.date_range(pd.Timestamp('2022-08-01 23:59:59') + pd.DateOffset(days=0, normalize=True), '2022-09-16', freq=pd.tseries.frequencies.to_offset('MS'))\nDatetimeIndex(['2022-08-01', '2022-09-01'], dtype='datetime64[ns]', freq='MS')\n</code></pre>"},{"location":"Python/DateTime/DateOffset/#normalize-vs-floord","title":"normalize vs floor('D')","text":"<p>Really??? - <code>pd.Timestamp.normalize()</code> sets the time component to midnight (00:00:00) in the <code>local time zone</code>, but does not change the time zone itself. - <code>pd.Timestamp.floor('D')</code> sets the time component to midnight (00:00:00) in the <code>UTC time zone</code>, and then converts the timestamp to the local time zone if necessary.</p>"},{"location":"Python/DateTime/DateOffset/#pddateoffset-vs-pdtimedleta-vs-pdtseriesoffsetsday","title":"pd.DateOffset vs pd.Timedleta vs pd.tseries.offsets.Day","text":"<ul> <li><code>DateOffset</code> respect calendar arithmetic while <code>Timedelta</code> respects absolute time arithmetic, might be different for datetime with timezone info</li> <li>pd.DateOffset(days=1) increments a date by 1 calendar day (23, 24, or 25 hours depending on day light savings time)</li> <li>pd.Timedelta(days=1) increments a date by 24 hours.</li> </ul> <pre><code>import pandas as pd\ndate = pd.Timestamp('2016-10-30 00:00:00+0300', tz='Europe/Helsinki')\ndate_plus_dateoffset = date + pd.DateOffset(days=1)\ndate_plus_timedelta = date + pd.Timedelta(days=1)\ndate_plus_offsetsday = date + pd.tseries.offsets.Day()\n\nprint(date_plus_dateoffset) #2016-10-31 00:00:00+02:00\nprint(date_plus_timedelta)  #2016-10-30 23:00:00+02:00\nprint(date_plus_offsetsday) #2016-10-30 23:00:00+02:00\n</code></pre>"},{"location":"Python/DateTime/DateTime/","title":"DateTime","text":"<p>Dealing with datetimes like a pro in Pandas</p>"},{"location":"Python/DateTime/DateTime/#pywintypesdatetime","title":"pywintypes.datetime","text":"<pre><code>import pywintypes\nfrom datetime import datetime\n\n#check if dt is pywintypes.datetime\nisinstance(dt, pywintypes.TimeType)\n\n#change pywintypes.datetim to datetime\ndt2 = datetime.fromtimestamp(timestamp=dt.timestamp(), tz=dt.tzinfo)\n</code></pre>"},{"location":"Python/DateTime/DateTime/#datetime_1","title":"datetime","text":"<pre><code>import datetime\nde = datetime.date(year=2017, month=10, day=24)\ntm = datetime.time(hour=4, minute=3, second=10, microsecond=7199)\ndt = datetime.datetime(year=2017, month=10, day=24, \\\n        hour=4, minute=3, second=10, microsecond=7199)\ntd = datetime.timedelta(days=3, minutes = 55)\ndt2 = dt + td\n\n#string to datetime\ndt = datetime.strptime('2020-01-01 13:00:10', '%Y-%m-%d %H:%M:%S')\n</code></pre>"},{"location":"Python/DateTime/DateTime/#datetime64","title":"datetime64","text":"<pre><code>dt64 = np.datetime64(5, 'ns')\ndt64 = np.datetime64(1508887504, 's')\ndt64 = np.datetime64('2017-10-24')\ndt64 = np.datetime64('2017-10-22T12:35:40.123')\n\ntd64 = np.timedelta64(2, 'h')\ntd64 = np.timedelta64(5, 'D')\ntd64 = dt64_1 - dt64_2\n</code></pre>"},{"location":"Python/DateTime/DateTime/#timedelta","title":"timedelta","text":"<pre><code>np.timedelta64(300000000000, 'ns')==timedelta(seconds=300)\n#return false\n\nnp.timedelta64(300000000000, 'ns')==pd.Timedelta(hours=1/12)\n#return true\n</code></pre>"},{"location":"Python/DateTime/DateTime/#timestamp","title":"Timestamp","text":"<pre><code>ts = pd.Timestamp(1239.1238934) #defautls to nanoseconds\nts = pd.Timestamp(1239.1238934, unit='D')\nts = pd.Timestamp('2017-10-24 05')\nts = pd.Timestamp('today')\nts = pd.Timestamp.now()\n\nts = pd.Timestamp(2017, 1, 1, 12)\nts = pd.Timestamp(year=2017, month=1, day=1, hour=12)\n\nts = pd.to_datetime('2017-10-24 05')\nts = pd.to_datetime('today')\ndt64_idx = pd.to_datetime(['2017-1-1', '2017-1-2'])\n\n# dt to dt64 and ts\ndt64 = np.datetime64(dt)\nts = pd.Timestamp(dt) # or pd.to_datetime(dt)\n\n#dt64 to dt and ts\nunix_epoch = np.datetime64(0, 's')\none_second = np.timedelta64(1, 's')\nseconds_since_epoch = (dt64 - unix_epoch) / one_second\ndt = datetime.utcfromtimestamp(seconds_since_epoch)\n\nts = pd.Timestamp(dt64)\n\n#ts to dt and dt64\ndt = ts.to_pydatetime()\ndt64 = ts.to_datetime64()\n</code></pre>"},{"location":"Python/DateTime/DateTime/#timestamp-to-date","title":"timestamp to date","text":"<pre><code>#timestamp series to datetime.date\nts.apply(lambda x: x.date())\n\nts.dt.to_pydatetime()\n\n#timestampIndex to datetime.date\ntsIndex.date\n\ndt64 = np.datetime64('2012-05-01T01:00:00.000000')\n\n#dt64 to ts\nts = pd.Timestamp(dt64)\n\n#dt64 to dt\nseconds = (dt64 - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\ndt = datetime.utcfromtimestamp(seconds)\n\n#dt to dt64\ndt64 = np.datetime64(dt)\n</code></pre>"},{"location":"Python/DateTime/DateTime/#dt-dt64-ts-to-string","title":"dt, dt64, ts to string","text":"<pre><code>#dt to str\ndatetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\ndatetime.now().strftime('%#d/%m/%Y %H:%M:%S') #without zero-padded day in windows\ndatetime.now().strftime('%-d/%m/%Y %H:%M:%S') #without zero-padded day in linux\n\ndt = datetime.strptime(str_date, '%Y-%m-%d %H:%M:%S') #str to datetime\n\n#ts to str\nts.strftime('%Y-%m-%d')\ndf.timestamp.dt.strftime('%Y-%m-%d')\ndf['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S.%f', utc=False) #str to ts\ndf['date'] = pd.to_datetime(df['date'], unit='s') #unix time to dt64\n</code></pre>"},{"location":"Python/DateTime/DateTime/#datetime64ns-vs-m8ns","title":"<code>datetime64[ns]</code> vs <code>&lt;M8[ns]</code>","text":"<p>https://stackoverflow.com/questions/29206612/difference-between-data-type-datetime64ns-and-m8ns - <code>datetime64[ns]</code> is a general dtype, while <code>&lt;M8[ns]</code> is a specific dtype - General dtypes map to specific dtypes, but may be different from one installation of NumPy to the next - datetime64[ns] maps to either M8[ns] depending on the endian-ness of the machine - int64 maps to i8, and int maps to either int32 or int64 depending on the bit architecture of the OS and how <code>NumPy</code> was compiled"},{"location":"Python/DateTime/DateTime/#fg","title":"fg","text":"<pre><code>#add days\ndt = datetime.now()\ndt2 = dt + timedelta(days=1)\n\n#convert datetime to string\ndatetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n#without zero-padded day in windows\ndatetime.now().strftime('%#d/%m/%Y %H:%M:%S')\n#without zero-padded day in linux\ndatetime.now().strftime('%-d/%m/%Y %H:%M:%S')\n\n#convert string to datetime\ndate = datetime.strptime(str_date, '%Y-%m-%d %H:%M:%S')\ndf['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S.%f', utc=False)\n\n#create a date list\npd.date_range(fr, to, freq='D', closed='left')\n\n#set index\ndf.set_index('date', inplace=True)\n\n#convert datetimes to the user\u2019s timezone\ndf.index = df.index.tz_convert('America/Los_Angeles')\n\n#rounding (truncating) datetimes\ndf.index = df.index.floor('1H') #ceiling\n\n#count things in this dataframe\ndf.groupby(['date', 'request']).count()\n\n#calculate the start of a week\ndf['week_start'] = df.index.to_period('W').start_time\n\n#start of next week\ndf['next_week_start'] = df['week_start'] + pd.DateOffset(weeks=1)\n\n#create range\npd.date_range(start_date, periods=10, freq='D')\npd.date_range(start_date, end_date, freq='30min')\n</code></pre>"},{"location":"Python/DateTime/DateTime/#get-year-month-from-date-col","title":"get year, month from date col","text":"<pre><code>#combine group by resulted time series multi-level index in to one col\ndf_date = pd.DataFrame(dict(A=1), ts.index)\ndf['date'] = df_date.index.map('{0[0]}-{0[1]:02}-01'.format).values\n</code></pre>"},{"location":"Python/DateTime/DateTime/#combine-year-month-day-cols-to-date-col","title":"combine year, month, day cols to date col","text":"<pre><code>#if we do not use values the index will be kept\ndts = pd.to_datetime(df[['Year', 'Month', 'Day']]).values\n</code></pre>"},{"location":"Python/DateTime/DateTime/#change-days-to-dt","title":"change days to dt","text":"<pre><code>#old\ndef days2dt(days_since_epoch):\n    epoch = dt.datetime(1980, 1, 6)\n    return [epoch + dt.timedelta(days=x) for x in days_since_epoch]\n\n#new 30x faster\ndef days2dt(days_since_epoch):\n    microseconds = np.around(np.asarray(days_since_epoch) * (24*60*60*10**6))\n    return np.datetime64('1980-01-06') + microseconds.astype('timedelta64[us]')\n</code></pre>"},{"location":"Python/DateTime/DateTime/#change-dt-to-timedelta-seconds-and-back","title":"change dt to timedelta [seconds] and back","text":"<pre><code>dts_str = ['2019-07-01 12:00:00','2020-07-01 12:00:00']\ndatetimes = pd.to_datetime(dts_str, format='%Y-%m-%d %H:%M:%S', utc=False).values\nseconds = pd.to_timedelta(datetimes).total_seconds()\ndts_back = pd.to_datetime(seconds)\n</code></pre>"},{"location":"Python/DateTime/DateTime/#change-ctime-to-datetime","title":"change ctime to datetime","text":"<pre><code>last_modified = os.path.getctime(file) #float 1382189138.4196026\ndatetime.fromtimestamp(last_modified)\n</code></pre>"},{"location":"Python/DateTime/DateTime/#save-df-to-csv-with-fixed-dt-format","title":"save df to csv with fixed dt format","text":"<pre><code>df.to_csv(file, index=False, date_format='%#d/%m/%Y %H:%M:%S')\n</code></pre>"},{"location":"Python/DateTime/DateTime/#round-dt-to-nearest-time","title":"round dt to nearest time","text":"<pre><code>df['dt'] = df['dt'].dt.ceil('30min')\n</code></pre>"},{"location":"Python/DateTime/DateTime/#numpydatetime-to-datetime","title":"numpy.datetime to datetime","text":"<pre><code>def npdt_to_dt(dt):\n    #converts a numpy datetime64 object to a python datetime object\n    timestamp = (dt - np.datetime64('1970-01-01T00:00:00')) / np.timedelta64(1, 's')\n    return datetime.utcfromtimestamp(timestamp)\n</code></pre>"},{"location":"Python/DateTime/DateTime/#days-in-month","title":"days in month","text":"<pre><code>dts = pd.date_range('2020-01-01', periods=10, freq='m')\ndts.to_period().days_in_month\n</code></pre>"},{"location":"Python/DateTime/DateTime/#offset-months","title":"offset months","text":"<pre><code>from pandas.tseries.offsets import MonthBegin, MonthEnd\ndt= pd.to_datetime(['2002-07-02','2002-10-03'])\ndt + MonthBegin(-1) #move backward till get the first month begin\n#['2002-07-01', '2002-10-01']\ndt + MonthEnd(1) #move forward till get the first month end\n#['2002-07-31', '2002-10-31']\n\n#get the first day of each date, MonthBegin does not work\ndf['first_day_in_month'] = df['date'].to_numpy().astype('datetime64[M]')\n</code></pre>"},{"location":"Python/DateTime/DateTime/#pdtimestamp-to-month-begin-datetime64m","title":"pd.Timestamp to month begin (datetime64[M])","text":"<pre><code>#output: numpy.datetime64('2022-01')\nmonth_begin = pd.Timestamp('2022-01-05').to_datetime64().astype('datetime64[M]')\nmonth_begin_with_day = month_begin.astype('datetime64[D]')\n</code></pre>"},{"location":"Python/DateTime/DateTime/#df-col-to-first-day-of-each-month","title":"df col to first day of each month","text":"<pre><code>df = pd.DataFrame({'dt': pd.date_range('2023-01-01', '2023-01-02', periods=2)})\ndf.assign(first_day_of_month=lambda x: x['dt'].dt.to_period('M').dt.to_timestamp())\n</code></pre>"},{"location":"Python/Django/Django/","title":"Django","text":"<p>Python Django and Dash are both web frameworks, but they serve different purposes and are used in different contexts. </p>"},{"location":"Python/Django/Django/#django_1","title":"Django","text":"<ul> <li>Purpose: Django is a high-level Python web framework that is designed for building full-featured, complex web applications.   It follows the Model-View-Controller (MVC) architectural pattern, which is commonly referred to as Model-View-Template (MVT) in Django's case.</li> <li>Features: Django provides a wide range of features out-of-the-box, including an ORM (Object-Relational Mapping) system for database interactions,   authentication, security, routing, templating, forms handling, and more.</li> <li>Use Case: Django is well-suited for building content-rich applications, such as social media platforms, e-commerce websites,   content management systems (CMS), and other applications that require complex data models and business logic.</li> <li>Learning Curve: Due to its comprehensive feature set and MVC/MVT architecture, Django can have a steeper learning curve for beginners.</li> <li>Community and Ecosystem: Django has a large and active community, with many third-party packages and libraries available to extend its functionality.</li> </ul>"},{"location":"Python/Django/Django/#dash","title":"Dash","text":"<ul> <li>Purpose: Dash is a Python web framework specifically designed for building interactive web applications for data visualization and analytics.   It's often used to create interactive dashboards and data-driven applications.</li> <li>Features: Dash provides components for creating interactive web visualizations using HTML, CSS, and JavaScript,   but you primarily work with them in Python. It integrates well with popular data manipulation and visualization libraries like Plotly.</li> <li>Use Case: Dash is ideal for building data-centric applications, such as real-time analytics dashboards, reporting tools,   and other applications that require visualizing and interacting with data.</li> <li>Learning Curve: Dash aims to be user-friendly and focuses on ease of use.   Its learning curve can be relatively smoother compared to more comprehensive frameworks like Django.</li> <li>Community and Ecosystem: Dash has a growing community, and its ecosystem revolves around data visualization and analytics tools like Plotly.   It might have a smaller ecosystem compared to Django, but it's highly specialized for its specific use case.</li> </ul> <p>In summary, Django is a versatile web framework for building complex web applications with a wide range of features, while Dash is specialized for creating interactive data visualization applications and dashboards. Your choice between the two would depend on the nature of your project and whether you need a full-featured web application framework (Django) or a focused data visualization framework (Dash).</p>"},{"location":"Python/Email/ExchangeLib/","title":"Exchangelib","text":"<p>https://www.activestate.com/resources/quick-reads/how-to-install-and-use-exchangelib-python/</p> <p>Exchangelib is a Python client library (cross-platform) that provides an interface for accessing and working with Microsoft Exchange Web Services (EWS).</p>"},{"location":"Python/Email/ExchangeLib/#test-tls-cert","title":"test tls cert","text":"<p>Very helpful with lots of info related to ssl: \\ https://github.com/ecederstrand/exchangelib/issues/798 <pre><code>import requests\nrequests.post('https://owa.example.com/EWS/Exchange.asmx')\n</code></pre></p>"},{"location":"Python/Email/ExchangeLib/#supported-tls-version","title":"supported tls version","text":"<pre><code>import requests\nprint(requests.get('https://www.howsmyssl.com/a/check', verify=True).json()['tls_version'])\n</code></pre>"},{"location":"Python/Email/ExchangeLib/#custom-tls-validation","title":"Custom TLS Validation","text":"<p>Supply a custom \u2018requests\u2019 transport adapter class, if custom TLS validation is needed. <pre><code>class CustomHTTPAdapter(requests.adapters.HTTPAdapter):\n  def cert_verify(self, conn, url, verify, cert):\n      cert_file = {\n          'example.com': '/path/to/example.com.crt',\n          'mail.internal': '/path/to/mail.internal.crt',\n      }[urlparse(url).hostname]\n      super(RootCAAdapter, self).cert_verify(conn=conn, url=url, verify=cert_file, cert=cert)\nBaseProtocol.HTTP_ADAPTER_CLS = CustomHttpAdapter\n\nclass ProxyAdapter(requests.adapters.HTTPAdapter):\n...\nBaseProtocol.HTTP_ADAPTER_CLS = ProxyAdapter\n</code></pre></p>"},{"location":"Python/Email/ExchangeLib/#ssl-error-unsafe-legacy-renegotiation-disabled","title":"SSL error unsafe legacy renegotiation disabled","text":"<p>https://stackoverflow.com/questions/71603314/ssl-error-unsafe-legacy-renegotiation-disabled</p> <p>The full code is here: https://gist.github.com/FluffyDietEngine/94c0137445555a418ac9f332edfa6f4b</p> <ul> <li>Import <code>ssl</code> and <code>urllib3</code> <pre><code>import requests\nimport urllib3\nimport ssl\n</code></pre></li> <li>Create a custom <code>HttpAdapter</code> which uses a custom ssl Context   <pre><code>class CustomHttpAdapter(requests.adapters.HTTPAdapter):\n    \"\"\"Transport adapter that allows us to use custom ssl_context.\"\"\"\n\n    def __init__(self, ssl_context=None, **kwargs):\n        self.ssl_context = ssl_context\n        super().__init__(**kwargs)\n\n    def init_poolmanager(self, connections, maxsize, block=False):\n        self.poolmanager = urllib3.poolmanager.PoolManager(\n            num_pools=connections, \n            maxsize=maxsize,\n            block=block, \n            ssl_context=self.ssl_context,\n        )\n</code></pre></li> <li>Set up an ssl context which enables <code>OP_LEGACY_SERVER_CONNECT</code>, and use it with the custom adapter.   <pre><code>def get_legacy_session():\n    ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n    ctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT\n    session = requests.session()\n    session.mount('https://', CustomHttpAdapter(ctx))\n    return session  \n</code></pre></li> <li>Use it like   <pre><code>with get_legacy_session() as s: \n    response = s.get(url=url)\n</code></pre></li> </ul>"},{"location":"Python/Env/Env/","title":"Env","text":""},{"location":"Python/Env/Env/#install-another-version-of-python-in-linux","title":"install another version of python in linux","text":"<pre><code>sudo apt update\nsudo apt install python3.9\nsudo apt install python3.9-venv\n</code></pre>"},{"location":"Python/Env/Env/#create-and-activate-venv","title":"create and activate venv","text":"<pre><code>python3.9 -m venv venv39   #create\nsource venv39/bin/activate #activate\nexit                       #exit venv\n</code></pre>"},{"location":"Python/Env/Import/","title":"Import","text":"<p>https://tenthousandmeters.com/blog/python-behind-the-scenes-11-how-the-python-import-system-works/</p>"},{"location":"Python/Env/Import/#import-m","title":"<code>import m</code>","text":"<ul> <li>searches for a module named m, </li> <li>creates a module object for that module, and </li> <li>assigns the module object to the variable</li> </ul> <p>A <code>module</code> is anything that Python considers a module and knows how to create a module object for, includeing things like: - Python files,  - directories and  - built-in modules written in C</p> <p>A <code>module object</code> is a Python object that acts as a namespace for the module's names.  The names are stored in the module object's dictionary (available as m.dict), so we can access them as attributes.</p>"},{"location":"Python/Env/Import/#regular-packages","title":"Regular packages","text":"<p>If a directory contains a <code>__init__.py</code> file, it's considered to be a regular package.</p>"},{"location":"Python/Env/Import/#namespace-packages","title":"Namespace packages","text":"<ul> <li>does not need the <code>__init__.py</code> file</li> <li>can place contents of a package across multiple locations</li> </ul>"},{"location":"Python/Env/Inspect/","title":"Inspect","text":""},{"location":"Python/Env/Inspect/#show-source-code","title":"show source code","text":"<pre><code>import inspect\nimport pandas as pd\nprint(inspect.getsource(pd.concat))\n</code></pre>"},{"location":"Python/Env/Module/","title":"Module","text":""},{"location":"Python/Env/Module/#tools","title":"tools","text":"<ul> <li>Redis: Redis' access libraries</li> <li>ElasticSearch: data search engine</li> </ul>"},{"location":"Python/Env/Module/#win32com","title":"win32com","text":"<p>install pywin32 for win32com:   python -m pip install pywin32</p>"},{"location":"Python/Env/Module/#plotly-and-cufflinks","title":"Plotly and Cufflinks","text":"<p>cufflinks is the link between plotly and pandas. <pre><code>import pandas as pd\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n</code></pre></p>"},{"location":"Python/Env/Module/#geographical-plotting","title":"Geographical Plotting","text":"<pre><code>import plotly.plotly as py\nimport plotly.grath_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\nlayout = dict(geo={'scope':'usa'})\nchoromap = go.Figure(data=[data], layout=layout)\niplot(choromap)\n</code></pre>"},{"location":"Python/Env/Module/#patsy","title":"patsy","text":"<p>Patsy is a Python library for describing statistical models (especially linear models) with a small string-based \"formula syntax\".</p> <pre><code>import patsy\ny, X = patsy.dmatrices('y ~ x0 + x1', df)\ncoef, resid, _, _ = np.linalg.lstsq(X, y)\ncoef = pd.Series(coef.squeeze(), index=X.design_info.column_names)\n\n#transferm\ny, X = patsy.dmatrices('v2 ~ C(key2)', df) #categorical data\ny, X = patsy.dmatrices('y ~ I(x0 + x1)', df)\ny, X = patsy.dmatrices('y ~ x0 + np.log(np.abs(x1) + 1)', df)\ny, X = patsy.dmatrices('y ~ standardize(x0) + center(x1)', df)\nnew_X = patsy.build_design_matrices([X.design_info], new_df)\n</code></pre>"},{"location":"Python/Env/Module/#statsmodels","title":"statsmodels","text":"<p>Some kinds of models found in statsmodels include:   * Linear models, generalized linear models, and robust linear models   * Linear mixed effects models   * Analysis of variance (ANOVA) methods   * Time series processes and state space models   * Generalized method of moments <pre><code>import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n#ordinary least squares linear regression\nmodel = sm.OLS(y, X)\nresults = model.fit() #results.params,results.summary()\n\nresults = smf.ols('y ~ col0 + col1 + col2', data=data).fit()\n\n#AR: time series analysis\nmodel = sm.tsa.AR(values)\nresults = model.fit(MAXLAGS)\n</code></pre></p>"},{"location":"Python/Env/Module/#scikit-learn","title":"scikit-learn","text":"<pre><code>from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\n</code></pre>"},{"location":"Python/Env/VsCode/","title":"VS Code","text":""},{"location":"Python/Env/VsCode/#python-lecture-notes","title":"python lecture notes","text":"<p>https://codefellows.github.io/sea-python-401d4/lectures/</p>"},{"location":"Python/Env/VsCode/#tree-chart","title":"tree chart","text":"<p>install code2flow from github:</p> <p>pip install git https://github.com/user/repo.git@branch</p>"},{"location":"Python/Env/VsCode/#vs-code_1","title":"vs code","text":"<p>There are 4 levels of settings in VS Code, which in ascending order of priority are: Default, User, Workspace, and Workspace Folder.</p> <p>Ctrl+Shift+P -&gt; python: select interpreter</p> <p>VS Code Conda issue fix:   - Disable activating terminal automatically, \"python.terminal.activateEnvironment\": false, and exit VSCode   - Open command prompt or power shell outside of VSCode.   - Navigate to your project or workspace directory.   - Activate conda there.   - Launch VSCode from the activated conda environment using code . or code project.code-workspace</p> <p>in launch.json of folder .vscode, add [this will ensure the parent path is added to the path search list]   \"env\": {\"PYTHONPATH\": \"${workspaceRoot}\"},</p>"},{"location":"Python/Env/VsCode/#extension","title":"extension","text":"<p>GitLens in VS code vscode-open-in-github</p>"},{"location":"Python/Env/VsCode/#other","title":"other","text":"<pre><code>#not supported\n++i\n\n#check python version, output will be 32 or 64\nimport struct\nstruct.calcsize('P') * 8\n\n#check module version\nimport pandas\npandas.__version__\n\n#create a dictionary\ndic = dict(zip(keys, vals))\n\n#ifelse\nfruit = 'Apple'\nisApple = True if fruit == 'Apple' else False\n\n#change dataframe two cols to dictionary (key has duplicate)\ndic = {}\nfor x in range(len(df)):\n   key = df.iloc[x,0]\n   val = df.iloc[x,1]\n   dic.setdefault(key, [])\n   dic[key].append(val)\n</code></pre> <p>Use cProfile package in Python to find inefficiencies in your code.</p> <p>In Python, the operation of verifying whether a specific example x belongs to S is efficient when S is declared as a set and is inefficient when S is declared as a list.</p> <p>Using PyPy, Numba or similar tools to compile your Python code into fast, optimized machine code.</p>"},{"location":"Python/ErrorHandling/Exception/","title":"Exception","text":""},{"location":"Python/ErrorHandling/Exception/#except-vs-except-exception","title":"except vs except Exception","text":"<p><pre><code>try:\n    x = 1 / 0\nexcept: # except Exception as exc:\n    x = 0\n</code></pre> We should never use a bare <code>except</code>! </p> <p>https://docs.python.org/3.1/howto/doanddont.html \\ <pre><code>Python has the `except:` clause, which catches all exceptions. \nSince every error in Python raises an exception, using `except:`\ncan make many programming errors look like runtime problems, which hinders the debugging process.\n\nBecause except: catches all exceptions, including SystemExit, KeyboardInterrupt, and GeneratorExit\n(which is not an error and should not normally be caught by user code), using a bare except: is almost never a good idea.\nIn situations where you need to catch all \u201cnormal\u201d errors, such as in a framework that runs callbacks,\nyou can catch the base class for all normal exceptions, Exception.\n</code></pre></p> <p>We should use the most specific exception possible and if we do want to catch all exceptions we should catch <code>Exception</code> which is the base class of all normal exceptions. </p> <p>We should also capture the exception with <code>as exc</code> as it is very useful to have if we are in the debugger inside the except block.</p>"},{"location":"Python/ErrorHandling/Exception/#exception-hierarchy","title":"exception hierarchy","text":"<p>https://docs.python.org/3.10/library/exceptions.html#exception-hierarchy</p>"},{"location":"Python/ErrorHandling/ExceptionNote/","title":"Exception Note","text":""},{"location":"Python/ErrorHandling/ExceptionNote/#add-extra-info-to-exception","title":"add extra info to exception","text":"<p>https://stackoverflow.com/questions/6062576/adding-information-to-an-exception <pre><code>try:\n    raise TypeError('Bad type.')\nexcept Exception as e:\n    raise type(e)('Extra info.') from e\n</code></pre></p> <p>With PEP 678 (Python 3.11) adding notes to exceptions is natively supported <pre><code>try:\n    raise TypeError('Bad type.')\nexcept Exception as e:\n    e.add_note('Extra info.')\n    raise\n</code></pre></p>"},{"location":"Python/Excel/Excel/","title":"excel","text":"<pre><code>python -m pip install xlrd==1.2.0\n</code></pre>"},{"location":"Python/Excel/Excel/#number-to-column-letter","title":"Number to Column Letter","text":"<pre><code>#change number to Excel column letters\ndef ExcelNumToLetter(num):\n    letter = ''\n    dividend = num\n    while dividend &gt; 0:\n       modulo = (dividend - 1) % 26\n       letter = chr(65 + modulo) + letter\n       dividend = int((dividend - modulo) / 26)\n    return letter\n</code></pre>"},{"location":"Python/Excel/Excel/#win32com-error","title":"win32.com error","text":"<p><pre><code>python -m pip install pywin32 #this might be blocked by corporate firewall\n</code></pre> raise AttributeError(\"%s.%s\" % (self.username, attr)) AttributeError: Excel.Application.Workbooks</p> <p>When attributes on Excel.Application do not exist, it is usually because the Excel application is open (possibly hidden) and it is in a modal loop such as editing a cell or an open/save file dialog.</p>"},{"location":"Python/Excel/Excel/#py-call-vba","title":"py call vba","text":"<pre><code>import win32com.client\n\nxlApp = win32com.client.DispatchEx('Excel.Application')\nwb = xlApp.Workbooks.Open(Filename=yourworkbookname.xlsm)\nxlApp.Run('macroName')\n</code></pre>"},{"location":"Python/Excel/Excel/#vba-call-py","title":"vba call py","text":"<pre><code>RetVal = Shell(\"&lt;full path to python.exe&gt; \" &amp; \"&lt;full path to your python script&gt;\")\n</code></pre>"},{"location":"Python/Excel/Excel/#excel-to-csv","title":"Excel to csv","text":"<pre><code>def create_vbs(dir):\n    vbscript = \"\"\"\n           xxxxxxxxxxxx\n    \"\"\"\n    file = os.path.join(dir, 'ExcelToCSV.vbs')\n    with open(file,'wb') as f:\n        f.write(vbscript.encode('utf-8'))\n    return file\n\nexefile = r'C:\\ExcelToCsv.vbs'\nxl_file = r'C:\\WBook.xlsx'\ncsvpath = f'C:\\output'\ncall(['cscript.exe', exefile, xl_file, csvpath, \"Sheet1:Sheet2\"])\n</code></pre> <p>ExcelToCSV.vbs <pre><code>if WScript.Arguments.Count &lt; 3 Then\n    WScript.Echo \"Usage should be: ExcelToCsv &lt;xls/xlsx file&gt; &lt;csv path&gt; &lt;ws_name1:ws_name2&gt;\"\n    Wscript.Quit\nEnd If\n\ncsv_format = 6\n\nxl_file = Wscript.Arguments.Item(0)\ncsv_path = WScript.Arguments.Item(1)\nworksheet_names_str = WScript.Arguments.Item(2)\n\nSet xl = CreateObject(\"Excel.Application\")\nSet wb = xl.Workbooks.Open(xl_file)\n\nSet fso = CreateObject(\"Scripting.FileSystemObject\")\nfor each worksheet_name in Split(worksheet_names_str,\":\")\n    csvfile = FSO.BuildPath(csv_path, worksheet_name &amp; \".csv\")\n    wb.Worksheets(worksheet_name).Activate\n    wb.SaveAs csvfile, csv_format\nnext\n\nwb.Close False\nxl.Quit\n</code></pre></p>"},{"location":"Python/Flask/Request/","title":"Flask request","text":""},{"location":"Python/Flask/Request/#attributes","title":"attributes","text":"<p>In Flask, the <code>request</code> object represents the HTTP request sent by the client to your Flask application.  It contains all the information about the request, such as headers, form data, files, and other metadata.</p> <p>Here are some common attributes and methods of the <code>request</code> object: - <code>request.method</code>: Returns the HTTP method used for the request (e.g., GET, POST, PUT, DELETE). - <code>request.args</code>: Contains the query parameters from the URL. - <code>request.form</code>: Contains the form data submitted with a POST request. - <code>request.files</code>: Contains any files uploaded as part of the request. - <code>request.cookies</code>: Contains any cookies sent with the request. - <code>request.headers</code>: Contains the HTTP headers sent with the request. - <code>request.json</code>: Returns the JSON data sent with the request (if the request body is JSON).</p>"},{"location":"Python/Flask/Request/#example","title":"example","text":"<p>We can access these attributes and methods within the Flask routes to retrieve information  about the incoming request and process it accordingly.  <pre><code>from flask import Flask, request, Response\n\napp = Flask(__name__)\n\n@app.route('/files', methods=['GET'])\ndef api_get():\n    name = request.args.get('name')\n    return f'Hello, {name}!'\n\n@app.route('/files', methods=['POST'])\ndef api_post():\n    file = request.files.get('file')\n    name = request.form.get('name')\n    platform = request.form.get('platform')\n\n    return Response(status=200)\n\nif __name__ == '__main__':\n    app.run()\n</code></pre></p> <p>how to call the api (assume the api is mounted on <code>/api</code>) <pre><code>import requests\ndef publish_file(\n    filepath,\n    *,\n    name='test',\n    platform='linux-64',\n    api_url='https://example.com',\n):\n    url = f'{api_url}/api/files'\n    files = {'file': open(filepath, 'rb')}\n    params = {'name': name, 'platform': platform}\n    r = requests.post(url, files=files, data=params)\n    if r.status_code != 200:\n        try:\n            msg = r.json().get('message')\n        except json.JSONDecodeError:\n            msg = r.text\n        finally:\n            raise Exception(msg)\n\nfilepath = '/path/to/file/data.csv'\npublish_file(filepath)\n</code></pre></p>"},{"location":"Python/Flask/Template/","title":"Template","text":"<p>In Flask, the render_template function is used to render HTML templates and generate dynamic content to be displayed in the web browser.</p>"},{"location":"Python/Flask/Template/#example","title":"example","text":"<pre><code>from flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    title = \"My Flask App\"\n    name = {\n        'first_name': 'John',\n        'last_name': 'Dale',\n    }\n    return render_template('index.html', title=tile, name=name)\n\nif __name__ == '__main__':\n    app.run()\n</code></pre> <p>index.html <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;{{ title }}&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Hello, {{ name }}!&lt;/h1&gt;\n    {% if name['first_name'] == \"\": %}\n    &lt;h1&gt;Hello my love&lt;/h1&gt;\n      {% else %}\n    &lt;h1&gt;Hello {{ name['first_name'] + \" \" + name['last_name'] }}&lt;/h1&gt;\n      {% endif %}\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p>"},{"location":"Python/Format/Black/","title":"Balck","text":"<p>https://black.readthedocs.io/</p> <p>Formatting the code with <code>black</code>: <code>black &lt;filepath&gt;</code>. e.g., <code>black .</code></p>"},{"location":"Python/Format/Black/#vscode-black-formatter-extension","title":"vscode black formatter extension","text":"<p>Format the file on save</p> <p>option 1 (works) - install black-formatter extension - In user settings, add <pre><code>\"black-formatter.args\": [\"-S\"]\n\"python.formatting.provider\": \"none\",\n\"[python]\": {\n    \"editor.defaultFormatter\": \"ms-python.black-formatter\",\n    \"editor.formatOnSave\": true\n}\n</code></pre></p> <p>option 2: works but slow - install <code>black</code> in python env - In user settings, add <pre><code>\"python.formatting.provider\": \"black\",\n\"editor.formatOnSave\": true,\n</code></pre></p>"},{"location":"Python/Format/Black/#config","title":"config","text":"<p>https://black.readthedocs.io/en/stable/usage_and_configuration/the_basics.html#configuration-via-a-file</p> <p>pyproject.toml <pre><code>[tool.black]\nline-length = 88\ntarget-version = ['py39']\nskip-string-normalization = true #disable string double quotes normalization\ninclude = '\\.pyi?$'\n--force-exclude = '''versioneer.py''' #exclide this file either from saving or cli\npreview = false\n</code></pre></p>"},{"location":"Python/Format/Black/#ignore-linelines","title":"ignore line/lines","text":"<pre><code># fmt: skip\n# fmt: off/on\n</code></pre>"},{"location":"Python/Format/Ruff/","title":"Ruff","text":"<p>https://github.com/astral-sh/ruff</p> <p>Ruff (flake8, PyLint) is a Python linting tool that checks your Python codebase for errors, styling issues and complexity. </p> <p>caveat: avoid ruff to automatically fix your code - auto fix without review can be questionable or wrong.</p>"},{"location":"Python/Format/Ruff/#disable-e741","title":"disable E741","text":"<pre><code>[tool.ruff.lint]\nignore = [\"E741\"] # ambiguous-variable-name: l (lowercase L), O (uppercase O), or I (uppercase I)\n</code></pre>"},{"location":"Python/Format/Ruff/#config-file","title":"config file","text":"<pre><code>[tool.ruff]\nline-length = 88\ntarget-version = \"py310\"\n\n# Allow unused variables when underscore-prefixed\ndummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"\n\n# Enable the pycodestyle (`E`) and Pyflakes (`F`) rules by default\n# Unlike Flake8, Ruff doesn't enable pycodestyle warnings (`W`) or\n# McCabe complexity (`C901`) by default.\nselect = [\"E\", \"F\"]\nignore = [\n  \"E402\",  # Module level import not at top of file\n]\n\n# Allow autofix for all enabled rules (when `--fix`) is provided\nfixable = [\"ALL\"]\nunfixable = []\n</code></pre>"},{"location":"Python/Format/Ruff/#double-quotes","title":"double quotes","text":"<ul> <li>black forces double quotes, no option to choose</li> <li>ruff is faster, together with linter, and &gt;99.9% black-compatible style</li> <li>use ruff is possible <code>Shift + Alt + F</code></li> </ul> <p>In <code>settings.json</code> make the changes: <pre><code>    \"[python]\": {\n        \"editor.defaultFormatter\": \"charliermarsh.ruff\" //\"ms-python.black-formatter\"\n    },\n</code></pre></p> <p>Settings <code>in pyproject.toml</code> <pre><code>[tool.ruff.lint]\nignore = [\"E741\"] # ambiguous-variable-name: l (lowercase L), O (uppercase O), or I (uppercase I)\n\n[tool.ruff.format]\nquote-style = \"single\"\n</code></pre></p>"},{"location":"Python/Generator/Generator/","title":"Generator","text":"<p>https://python-course.eu/advanced-python/generators-and-iterators.php</p> <p>If there is a return statement in the code of a generator, the execution will stop with a StopIteration exception error when this code is executed by the Python interpreter.</p>"},{"location":"Python/Generator/Generator/#do-not-use-stopiteration","title":"do not use <code>StopIteration</code>","text":"<p>https://stackoverflow.com/questions/14183803/what-is-the-difference-between-raise-stopiteration-and-a-return-statement-in-gen</p> <p>From Python 3.3 there is no longer a need to explicitly using <code>raise StopIteration</code> in a Generator to end the generator.</p> <p>If you raise a StopIteration exception explicitly in your generator there may be some methods that go on to error because they were expecting to receive a regular return (or an implicit return) and were going to raise StopIteration themselves.</p>"},{"location":"Python/Generator/Generator/#return-type-generator","title":"return type Generator","text":"<p>In Python type hinting, the generator type is represented as Generator[ValueType, SendType, ReturnType]. The second and third parameters represent the type of values that can be sent into the generator and the type of value that the generator can return, respectively.</p> <p>For the specific example, <code>Generator[BinaryFile, None, None]</code>, the type hinting indicates the following: - <code>ValueType</code>: BinaryFile   <code>BinaryFile</code> represents the type of values that the generator yields. In this case, the generator is expected to yield values of the BinaryFile type. - <code>SendType</code>: None   <code>None</code> indicates that the generator cannot receive any values through the <code>send()</code> method. In other words, it does not expect any values to be sent into the generator during its execution. - <code>ReturnType</code>: None   <code>None</code> indicates that the generator does not have a specific return type. Generators in Python do not have a return statement; instead, they yield values using the <code>yield</code> keyword. As a result, their return type is implicitly considered None.</p> <p>In summary, the <code>Generator[BinaryFile, None, None]</code> type hinting represents a generator that yields values of the <code>BinaryFile</code> type, does not accept any values through the <code>send()</code> method, and does not have a specific return value (i.e., its return type is implicitly <code>None</code>).</p>"},{"location":"Python/IO/AzureBlob/","title":"Azure Blob Storage","text":"<p>https://learn.microsoft.com/en-us/azure/storage/files/storage-python-how-to-use-file-storage?tabs=python</p>"},{"location":"Python/IO/AzureBlob/#fsspec","title":"fsspec","text":"<p>If the path ends with <code>/</code>, only folders are returned.</p>"},{"location":"Python/IO/AzureBlob/#adlfs","title":"adlfs","text":"<p>Not correct: performance using <code>AzureBlobFileSystem</code> <pre><code>When reading Parquet files from Azure Blob Storage using pd.read_parquet with the engine set to pyarrow,\nthe performance can **become suboptimal** when reading in parallel due to the limitations of the\nAzureBlobFileSystem and the way PyArrow handles parallel reading.\n\nThe main reason for this performance issue is that the `AzureBlobFileSystem`, which is used to interact with\nAzure Blob Storage, **does not natively support parallel reads efficiently**. This limitation can lead to\nslower performance when multiple threads or processes are trying to read Parquet files in parallel from\nthe same container in Azure Blob Storage.\n</code></pre></p>"},{"location":"Python/IO/AzureBlob/#create-file-system","title":"create file system","text":"<pre><code>from adlfs.spec import AzureBlobFileSystem\nfrom azure.identity.aio import DefaultAzureCredential\ndef get_file_system(\n  account_name: str,\n  credential: object=None,\n) -&gt; AzureBlobFileSystem:\n    # Disable messages from azure.identity.aio\n    logging.getLogger('azure.identity.aio').setLevel(logging.ERROR)\n    credential = credential or DefaultAzureCredential()\n    return AzureBlobFileSystem(\n        account_name=account_name,\n        credential=credential,\n    )\n</code></pre>"},{"location":"Python/IO/AzureBlob/#fsglob-vs-fsls","title":"<code>fs.glob</code> vs <code>fs.ls</code>","text":"<ul> <li><code>fs.glob('container-name/xyz-*.parquet')</code> is supper slow - will scan all files in this container</li> <li><code>fs.ls(path='container-name', prefix='xyz-')</code> is much faster - will only list folders (not subfolders) and files in the path</li> <li>but <code>fs.ls</code> will crash if the folder not exist</li> </ul>"},{"location":"Python/IO/AzureBlob/#adlfs-parallel-performance","title":"adlfs parallel performance","text":"<p><code>pd.read_parquet</code> is 2-3x faster than <code>dd.read_parquet</code>. Do not create the file system for each parallel call - can be slow. <pre><code>def read_parquet_file(\n    *,\n    fs: AzureBlobFileSystem,\n    path: str,\n    columns: list[str],\n    filters: list[tuple] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    The `path` should be in this format `az://&lt;blob-name&gt;/folder/file-name`\n    \"\"\"\n    # Occasionally it will fail to retrieve a token\n    retries = 3     # Number of retries\n    retry_delay = 2 # Seconds to wait between retries\n    while retries &gt; 0:\n        try:\n            with fs.open(path) as f:\n                df = pd.read_parquet(\n                    path=f, columns=columns, filters=filters, engine='pyarrow'\n                )\n            break\n        except ClientAuthenticationError:\n            retries -= 1\n            if retries &gt; 0:\n                time.sleep(retry_delay)\n            else:\n                raise\n        except Exception:\n            raise\n    return df\n\ndef read_parquet_files(\n    paths: list[str],\n    columns: list[str],\n    filters: list[tuple] = None,\n    use_cache: bool = False,\n) -&gt; list[pd.DataFrame]:\n    \"\"\"\n    Read multiple parquet files in parallel into a list of pd.DataFrame\n    \"\"\"\n    if not filters:\n        filters = None\n    # do not create the file system for each thread call, can be very slow\n    fs = get_file_system()\n    if len(paths) == 1:\n        dfs = [\n            read_parquet_file(fs=fs, path=paths[0], columns=columns, filters=filters)\n        ]\n    else:\n        n_jobs = min(cpu_count(), len(paths))\n        with ThreadPool(processes=n_jobs) as pool:\n            dfs = pool.map(\n                lambda path:\n                    read_parquet_file(fs=fs, path=path, columns=columns, filters=filters),\n                paths,\n            )\n    return dfs\n\n# can be 2-3x faster than dd.read_parquet\ndfs = read_parquet_files(\n    paths=paths,\n    columns=columns,\n    filters=filters,\n)\nd1 = pd.concat(dfs, axis=0).reset_index().get(columns)\n\n# can 2-3x slower than pd.read_parquet\nd2 = dd.read_parquet(\n    paths,\n    index=False,\n    columns=columns,\n    filters=filters,\n    engine='pyarrow',\n    storage_options=storage_options,\n    open_file_options=dict(precache_options={'method': 'parquet'}),\n    schema=schema,\n).compute()\n</code></pre></p>"},{"location":"Python/IO/AzureBlob/#adlfs-glob-wildcards-performance","title":"adlfs glob wildcards performance","text":"<p>https://www.gnu.org/software/bash/manual/html_node/Pattern-Matching.html</p> <p>performance - use subfolders such as year/month will siginificantly improve search performance - use wildcard characters in the middle will lead to full scan - search a few sub folders one by one will be much faster</p> <pre><code>files = fs.glob('dev-blob/2021/01/data*].parquet')\nfiles = fs.glob('dev-blob/2021/01/data[0-9][0-9].parquet')\n</code></pre>"},{"location":"Python/IO/AzureBlob/#azure-storage-blob","title":"azure-storage-blob","text":"<ul> <li>can be 2-3x slower than <code>adlfs</code></li> <li><code>azure-storage-blob</code> supports both sync and async versions.</li> </ul> <p>When got this error <pre><code>AttributeError: 'coroutine' object has no attribute 'token'\nsys:1: RuntimeWarning: coroutine 'DefaultAzureCredential.get_token' was never awaited\n</code></pre> It's most likely incorrectly used the async version <code>from azure.identity.aio import DefaultAzureCredential</code>.</p>"},{"location":"Python/IO/AzureBlob/#read-parquet-blob-to-df-performance","title":"read parquet blob to df performance","text":"<p><code>adlfs</code> can be 1.5-2x faster than <code>azure-storage-blob</code>. But <code>AzureBlobFileSystem</code> in parallel can be really slow down if used incorrectly (do not create a separate fs for each parallel run - can be very slow). <pre><code>from adlfs.spec import AzureBlobFileSystem\nfrom azure.storage.blob import ContainerClient\nfrom azure.identity import DefaultAzureCredential\n\ndef read_parquet_fast(path: str, columns: list[str]) -&gt; pd.DataFrame:\n    fs = AzureBlobFileSystem(\n        account_name=account_name,\n        credential=DefaultAzureCredential(),\n    )\n    with fs.open(path) as f:\n        df = pd.read_parquet(path=f, columns=columns)\n    return df\n\ndef read_parquet_slow(path: str, columns: list[str]) -&gt; pd.DataFrame:\n    path = path.split('/', 3)[-1]\n    container_client = = ContainerClient(\n        account_url=f'https://{account_name}.blob.core.windows.net',\n        credential=DefaultAzureCredential(),\n        container_name=container_name,\n    )\n    with io.BytesIO() as data_buffer:\n        blob_client = container_client.get_blob_client(path)\n        blob_client.download_blob().readinto(data_buffer)\n        data_buffer.seek(0)\n        df = pd.read_parquet(data_buffer, columns=columns, engine='pyarrow')\n    return df\n</code></pre></p>"},{"location":"Python/IO/AzureBlob/#azure-data-lake-storage-adls","title":"Azure Data Lake Storage (ADLS)","text":"<p>It's actually created on top of <code>azure-storage-blob</code>.</p> <p>The <code>azure-storage-file-datalake</code> library is specifically designed to interact with Azure Data Lake Storage Gen2 (ADLS Gen2). ADLS Gen2 is an enterprise-grade distributed file system built on top of Azure Blob Storage, providing hierarchical namespace and capabilities for big data analytics.</p> <p>When working with Azure Data Lake Storage Gen2, we should use \"azure-storage-filedatalake\" and for general-purpose object storage in Azure Blob Storage, should use \"azure-blob-storage\"</p>"},{"location":"Python/IO/CSV/","title":"CSV","text":""},{"location":"Python/IO/CSV/#csv-reading-perf","title":"csv reading perf","text":"<ul> <li>https://datapythonista.me/blog/how-fast-can-we-process-a-csv-file</li> <li>https://github.com/datapythonista/bench_csv <pre><code>pandas with C engine                    pandas_c                     1.501\npandas with PyArrow engine and PyArrow  dtypespandas_pyarrow_arrow   0.292\nNumPy with loadtxt function             numpy_loadtxt                1.835\nDuckDB 0.9.2 with SQL API               duckdb_sql                   0.817\nDuckDB 0.10.0 with SQL API              duckdb_sql                   0.289\nDataFusion with SQL API                 datafusion_sql               0.206\nPolars in eager mode                    polars_eager                 0.114\nPolars in lazy mode                     polars_lazy                  0.106\nPolars in streaming mode                polars_streaming             0.115\nPolars with SQL API                     polars_sql                   0.098\n</code></pre></li> </ul>"},{"location":"Python/IO/Excel/","title":"Excel","text":""},{"location":"Python/IO/Excel/#read-excel-into-dataframe-options","title":"read excel into dataframe options","text":"<p>https://stackoverflow.com/questions/28766133/faster-way-to-read-excel-files-to-pandas-dataframe</p>"},{"location":"Python/IO/Excel/#compare-different-options","title":"compare different options","text":"<ul> <li>https://www.linkedin.com/pulse/reading-xlsx-files-quickly-python-amged-elsheikh-su7ic</li> <li>https://hakibenita.com/fast-excel-python</li> </ul>"},{"location":"Python/IO/Excel/#read-excel-using-the-light-fast-calamine-engine","title":"read excel using the light-fast calamine engine","text":"<p>polars is using <code>fastexcel</code>.</p> <p>Calamine is a Rust library designed specifically for reading Excel and OpenDocument Spreadsheet files. It focuses on extracting data from these file formats, not modifying them.</p> <p>install <code>calamine</code> <pre><code>pip install fastexcel\nmamba install python-calamine -y\n</code></pre></p> <p>polars: read excel sheet <pre><code>import polars as pl\n\ndl = pl.read_excel(\n    xlfl,\n    sheet_name=wsnm[0],\n    engine='calamine',\n    read_options={'header_row': 3},\n)\ndl[:2]\n</code></pre></p> <p>The <code>read_options</code> is from <code>fastexcel</code>: https://fastexcel.toucantoco.dev/fastexcel.html - <code>idx_or_name</code>: The index (starting at 0) or the name of the sheet to load. - <code>column_names</code>: Overrides headers found in the document.   If column_names is used, header_row will be ignored. - <code>header_row</code>: The index of the row containing the column labels, default index is 0   If None, the sheet does not have any column labels. - <code>skip_rows</code>: Specifies how many rows should be skipped after the header.   If header_row is None, it skips the number of rows from the start of the sheet. - <code>n_rows</code>: Specifies how many rows should be loaded. If None, all rows are loaded - <code>schema_sample_rows</code>: Specifies how many rows should be used to determine the dtype of a column. If None, all rows will be used. - <code>use_columns</code>: Specifies the columns to use. Can either be:     - None to select all columns     - A list of strings and ints, the column names and/or indices (starting at 0)     - A string, a comma separated list of Excel column letters and column ranges (e.g. \u201cA:E\u201d or \u201cA,C,E:F\u201d, which would result in A,B,C,D,E and A,C,E,F) - <code>dtypes</code>: An optional dict of dtypes. Keys can be column indices or names</p> <p>pandas: read excel sheet <pre><code>import pandas as pd\n\ndp = pd.read_excel(\n    xlfl,\n    sheet_name=wsnm[0],\n    engine='calamine',\n    dtype_backend='pyarrow',\n    skiprows=3,\n    skipfooter=0,\n)\ndp[:2]\n</code></pre></p>"},{"location":"Python/IO/FileSys/","title":"FileSys","text":""},{"location":"Python/IO/FileSys/#path","title":"path","text":"<pre><code>#get dir\ndir = os.path.dirname(sys.argv[0])\npath = os.path.join(dir, csvfile)\n\n#get user dir\ndir = os.path.expanduser('~')\n\n#python file dir\nos.path.dirname(__file__)\n\n#python file name\nos.path.basename(__file__)\n\n#create dir\nif not os.path.exists(directory):\n  os.makedirs(directory)\n\n#get filename from path\nos.path.basename(r'c:\\x.txt')\n</code></pre>"},{"location":"Python/IO/FileSys/#get-files","title":"get files","text":"<pre><code>import os\nfiles = []\nfor (dirpath, dirnames, filenames) in os.walk(dir):\n    files += [os.path.join(dirpath, file) for file in filenames if file[-3:]=='.gz']\nfor filename in files:\n    print(filename)\n</code></pre>"},{"location":"Python/IO/FileSys/#show-list","title":"show list","text":"<pre><code>#print 1d list???\nprint('[' + ', '.join(map(str, mylist)) + ']')\n#print 2d list\nprint(*mylist, sep='')\n</code></pre>"},{"location":"Python/IO/FileSys/#read-write","title":"read / write","text":"<pre><code>with open('path_file.csv', 'r', encoding='utf-8') as f:\n    lines = f.read().splitlines()\n\nwith open('somefile.txt', 'a', newline='') as f:\n    f.write('Hello\\n')\n    w = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n    w.writerow([1,2])\n    df.to_csv(f, index=False)\n\n#write array to csv file\nnp.savetxt('myfile.csv', myarray, delimiter=',')\n\n#read and write together\nwith open(file_in) as csv_in, with open(file_out, 'w') as csv_out:\n    xxx\n</code></pre>"},{"location":"Python/IO/FileSys/#get-first-line-of-csv","title":"get first line of csv","text":"<pre><code>with open(fpath, 'r') as f:\n    r = csv.reader(f)\n    line = next(r)\n</code></pre>"},{"location":"Python/IO/FileSys/#read-csv-in-zip","title":"read csv in zip","text":"<pre><code>from io import TextIOWrapper\nfrom zipfile import ZipFile\n\nwith ZipFile(file,'r') as zip:\n    with zip.open(zip.namelist()[0]) as zipf:\n        f = TextIOWrapper(zipf)\n        lines = f.read().splitlines()\n</code></pre>"},{"location":"Python/IO/FileSys/#show-progress","title":"show progress","text":"<pre><code>print('progress report started')\nfor id, yr in enumerate(range(2000, 2021)):\n    print(f'id: {id:003} year: {yr}\\r', end='')\nprint(' '.join('' for i in range(80)) + '\\r', end='')\nprint('progress report ended')\n</code></pre>"},{"location":"Python/IO/Gzip/","title":"Gzip","text":""},{"location":"Python/IO/Gzip/#read-csv-in-gzip","title":"read csv in gzip","text":"<p>it's much faster to use pyarrow.csv <pre><code>import gzip\n\nfile = 'c:/test/archive.csv.gz'\n# readonly text mode\nwith gzip.open(file, 'rt') as f:\n    df = pd.read_csv(f)\n</code></pre></p>"},{"location":"Python/IO/Gzip/#read-csv-in-zip","title":"read csv in zip","text":"<pre><code>import io\nimport zipfile\n\nfile = 'c:/test/archive.csv.zip'\nwith zipfile.ZipFile(zip_file_path, 'r') as archive:\n    for csv_filename in archive.namelist():\n        with archive.open(csv_filename) as csv_file:\n            text_data = io.TextIOWrapper(csv_file, encoding='utf-8') #convert binary to text\n            df = pd.read_csv(text_data)\n</code></pre>"},{"location":"Python/IO/IO/","title":"io","text":""},{"location":"Python/IO/IO/#path","title":"path","text":"<pre><code>#get dir\ndir = os.path.dirname(sys.argv[0])\npath = os.path.join(dir, csvfile)\n\n#get user dir\ndir = os.path.expanduser('~')\n\n#python file dir\nos.path.dirname(__file__)\n\n#python file name\nos.path.basename(__file__)\n\n#create dir\nif not os.path.exists(directory):\n  os.makedirs(directory)\n\n#get filename from path\nos.path.basename(r'c:\\x.txt')\n</code></pre>"},{"location":"Python/IO/IO/#get-files","title":"get files","text":"<pre><code>import os\nfiles = []\nfor (dirpath, dirnames, filenames) in os.walk(dir):\n    files += [os.path.join(dirpath, file) for file in filenames if file[-3:]=='.gz']\nfor filename in files:\n    print(filename)\n</code></pre>"},{"location":"Python/IO/IO/#show-list","title":"show list","text":"<pre><code>#print 1d list???\nprint('[' + ', '.join(map(str, mylist)) + ']')\n#print 2d list\nprint(*mylist, sep='')\n</code></pre>"},{"location":"Python/IO/IO/#read-write","title":"read / write","text":"<pre><code>with open('path_file.csv', 'r', encoding='utf-8') as f:\n    lines = f.read().splitlines()\n\nwith open('somefile.txt', 'a') as f:\n    f.write('Hello\\n')\n\n#write array to csv file\nnp.savetxt('myfile.csv', myarray, delimiter=',')\n</code></pre>"},{"location":"Python/IO/IO/#get-first-line-of-csv","title":"get first line of csv","text":"<pre><code>with open(fpath, 'r') as f:\n    r = csv.reader(f)\n    line = next(r)\n</code></pre>"},{"location":"Python/IO/IO/#read-csv-in-zip","title":"read csv in zip","text":"<pre><code>from io import TextIOWrapper\nfrom zipfile import ZipFile\n\nwith ZipFile(file,'r') as zip:\n    with zip.open(zip.namelist()[0]) as zipf:\n        f = TextIOWrapper(zipf)\n        lines = f.read().splitlines()\n</code></pre>"},{"location":"Python/IO/Json/","title":"Json","text":""},{"location":"Python/IO/Json/#save-dict-to-json","title":"save dict to json","text":"<pre><code># to json strong\njson_str = json.dumps(data)\n# to json file\nwith open('data.json', 'w') as outfile:\n    json.dump(data, outfile)\n</code></pre>"},{"location":"Python/IO/Json/#load-json-to-dict","title":"load json to dict","text":"<pre><code># json string to dict\njson.loads(json_str)\n# json file to dict\nwith open('data.json', 'r') as inputfile:\n    data = json.load(inputfile)\n</code></pre>"},{"location":"Python/IO/StdOutErr/","title":"StdOutErr","text":""},{"location":"Python/IO/StdOutErr/#print-message-not-shown","title":"print message not shown","text":"<p>can be caused by buffer <pre><code>print('ok', flush=True)\n</code></pre></p>"},{"location":"Python/IO/StdOutErr/#dup-dup2-tmpfile-and-stdout-in-python","title":"dup, dup2, tmpfile and stdout in python","text":"<p>https://stackoverflow.com/questions/8817993/dup-dup2-tmpfile-and-stdout-in-python/8825434#8825434</p> <p>python's sys.stdout is a wrapper of the actual stdout handler so redirect sys.stdout will not affect the stdout messaged from other languages such as C and C++.</p> <p>We need to duplicate the actual stdout (1) using os.dup and os.dup2 to redirect the messages.</p>"},{"location":"Python/IO/StdOutErr/#windows-oserror-incorrect-function","title":"windows: OSError: incorrect function","text":"<p>https://stackoverflow.com/questions/66784941/dup2-and-pipe-shenanigans-with-python-and-windows</p> <p>sys.stdout is an object of type TextIOWrapper, whose sys.stdout.write() method eventually calls either os.write() or the C equivalent, on the file descriptor sys.stdout.fileno(), which is 1.</p> <ul> <li>There are several types of file descriptors: files, sockets, serial ports, terminals, pipes, etc.</li> <li>The C library functions write(), close(), etc... work on mostly any type of file descriptor, but some features only work when the file descriptor is of a suitable type.</li> <li>At creation time, the TextIOWrapper that is sys.stdout examines its file descriptor (1) and determines that it's a terminal.</li> <li>On Windows only, sys.stdout.write() ends up doing some operations that are only valid if the file descriptor 1 is truly a terminal.</li> <li>Once you os.dup2() so that 1 becomes an os.pipe() or file, and not a terminal, the Windows Python implementation crashes when it attempts to do terminal-specific operations on a pipe.</li> </ul> <p>As a workaround, on Windows only, I'm doing <pre><code>if sys.platform == 'win32':\n    sys.stdout.write = lambda z: os.write(\n        sys.stdout.fileno(),\n        z.encode() if hasattr(z, 'encode') else z\n    )\n</code></pre></p>"},{"location":"Python/IO/Stream/","title":"Stream","text":"<p>https://docs.python.org/3/library/io.html</p>"},{"location":"Python/IO/Stream/#parquet-cache","title":"parquet cache","text":"<p>When reading a parquet from a fastapi using <code>pd.read_parquet</code>, if it's stream and has cache, we might get the error like <code>pyarrow.lib.ArrowInvalid: Could not open Parquet input source '&lt;Buffer&gt;': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.</code></p> <p>Because there is too much data???</p>"},{"location":"Python/Issue/Main/","title":"Main","text":""},{"location":"Python/Issue/Main/#unexpected-silent-parallel-run-in-background","title":"unexpected silent parallel run in background","text":"<p>if do not use <code>if __name__ == '__main__'</code> in a package, there can be unexpected silent parallel run in background.</p>"},{"location":"Python/Jupyter/Dashboard/","title":"Dashboard","text":""},{"location":"Python/Jupyter/Dashboard/#output-logging-info-from-dashboard","title":"Output logging info from dashboard","text":""},{"location":"Python/Jupyter/Dashboard/#simple-and-effective","title":"Simple and effective","text":"<pre><code>import traceback\ndef to_log(msg)\n    with open(\"err.log\", \"a\") as f:\n        f.write(f'{msg}\\n\\n')\n\ntry:\n    x = 1 / 0\nexcept Exception as exec:\n    to_log(traceback.format_exc())\n    pass\n</code></pre>"},{"location":"Python/Jupyter/Dashboard/#widgetsoutput","title":"widgets.Output","text":"<p>https://ipywidgets.readthedocs.io/en/latest/examples/Output%20Widget.html <pre><code>self.err_viewer = widgets.Output()\nwith self.err_viewer:\n    self.err_viewer.clear_output()\n    print(error_message)\n</code></pre></p>"},{"location":"Python/Jupyter/Error/","title":"Error","text":""},{"location":"Python/Jupyter/Error/#no-module-named-pysqlite2","title":"No module named 'pysqlite2'","text":"<p>This is because the <code>sqlite3.dll</code> could not be found that triggered the error.</p> <p>Solution: download the dll from <code>https://www.sqlite.org/download.html</code> and put it in the <code>DLLs</code> folder. <pre><code>Traceback (most recent call last):\n  File \"\\lib\\site-packages\\jupyter_server\\services\\sessions\\sessionmanager.py\", line 14, in &lt;module&gt;\n    import sqlite3\n  File \"\\lib\\sqlite3\\__init__.py\", line 57, in &lt;module&gt;\n    from sqlite3.dbapi2 import *\n  File \"\\lib\\sqlite3\\dbapi2.py\", line 27, in &lt;module&gt;\n    from _sqlite3 import *\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n...\n  File \"\\lib\\site-packages\\jupyter_server\\services\\sessions\\sessionmanager.py\", line 17, in &lt;module&gt;\n    from pysqlite2 import dbapi2 as sqlite3  # type:ignore[no-redef]\nModuleNotFoundError: No module named 'pysqlite2'\n</code></pre></p>"},{"location":"Python/Jupyter/Error/#failed-to-fetch-package-metadata-for-plotlydash-jupyterlab","title":"Failed to fetch package metadata for '@plotly/dash-jupyterlab':  <p>The error message indicates that the @plotly/dash-jupyterlab package is not available in the current npm registry. </p>","text":""},{"location":"Python/Jupyter/Excel/","title":"Excel","text":""},{"location":"Python/Jupyter/Excel/#copy-jupyter-notebook-output-to-excel","title":"copy jupyter notebook output to excel","text":"<pre><code>df.to_clipboard(excel=True)\n</code></pre>"},{"location":"Python/Jupyter/Extension/","title":"Extension","text":""},{"location":"Python/Jupyter/Extension/#basic","title":"basic","text":"<pre><code>jupyter labextension list\njupyter labextension enable my-extension\njupyter labextension disable my-extension\njupyter labextension install my-extension\n</code></pre>"},{"location":"Python/Jupyter/Extension/#enable-extension","title":"enable extension","text":"<p>http://localhost:8889/nbextensions <pre><code>pip install jupyter_nbextensions_configurator\njupyter nbextensions_configurator enable --user #not work\n\nset cfigpath=C:\\Users\\sma\\AppData\\Roaming\\Python\\Python38\\site-packages\\jupyter_nbextensions_configurator\\\npython %cfigpath%application.py enable --user #works\n\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user #not work\n\nset contrpath=C:\\Users\\sma\\AppData\\Roaming\\Python\\Python38\\site-packages\\jupyter_contrib_core\\\npython %contrpath%application.py nbextension install --user #works\n</code></pre></p>"},{"location":"Python/Jupyter/Extension/#install-extension","title":"install extension","text":"<p>https://jupyterlab.readthedocs.io/en/stable/user/extensions.html - Add a Table of Content (Table of Content 2)\\   <code>jupyter labextension install @jupyterlab/toc</code>\\   <code>jupyter labextension install \"@jupyterlab/toc@2.0.0\"</code></p>"},{"location":"Python/Jupyter/Issue/","title":"Issue","text":""},{"location":"Python/Jupyter/Issue/#jupyterlab-cells-disapperaed","title":"jupyterlab cells disapperaed","text":"<p>https://github.com/jupyterlab/jupyterlab/issues/17023 - changing the \"Windowing mode\" setting from the default \"full\" to either \"defer\" or \"none\" seems to make the problem go away - Settings -&gt; Advanced Settings Editor -&gt; Notebook -&gt; Windowing Mode setting - \"defer\" not good - there is no way to see some disappeared cells</p>"},{"location":"Python/Jupyter/Jupyter/","title":"Jupyter","text":""},{"location":"Python/Jupyter/Jupyter/#install","title":"install","text":"<p>python -m pip install jupyter</p>"},{"location":"Python/Jupyter/Jupyter/#start-jupyter-notebook","title":"start jupyter notebook","text":"<p>python -m notebook \\   python -m notebook example.ipynb</p>"},{"location":"Python/Jupyter/Jupyter/#change-df-output-width","title":"change df output width","text":"<pre><code>pd.set_option(\"display.width\", 240)\n</code></pre>"},{"location":"Python/Jupyter/Jupyter/#change-cell-width","title":"change cell width","text":"<pre><code>from IPython.core.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;\"))\n</code></pre>"},{"location":"Python/Jupyter/Jupyter/#cell-to-markdown","title":"cell to markdown","text":"<p>Esc + m</p>"},{"location":"Python/Jupyter/Jupyter/#markdown-to-cell","title":"markdown to cell","text":"<p>Esc + y</p>"},{"location":"Python/Jupyter/Jupyter/#get-the-docstring-and-method-list-pop-ups-in-jupyter-notebook","title":"get the Docstring and method list pop-ups in Jupyter Notebook","text":"<p>Use Tab with your cursor directly after a defined variable to see the list of methods. For example, given: my_list = [1,2,3] you could then run that cell to define my_list as a variable, afterwards you could just type: my_list. (notice the dot) and then press Tab to see the list of methods. For the doctrings of functions, use Shift+Tab with your cursor right after the function.</p>"},{"location":"Python/Jupyter/Jupyter/#source-control","title":"source control","text":"<p>use <code>jupytext</code> to synchronize <code>.py</code> and <code>.ipynb</code> files. <pre><code>conda install jupytext                               #install\njupytext --set-formats ipynb,py &lt;file&gt;.ipynb         #create ipynb file\njupytext --set-formats ipynb,py &lt;file&gt;.ipynb --sync  #sync ipynb to .py file\n\njupytext --to notebook &lt;file&gt;.py  #from .py to .ipynb file\njupytext --to py &lt;file&gt;.ipynb     #from .ipynb to .py file\njupytext --to python --output &lt;file&gt;.py &lt;file&gt;.ipynb\n</code></pre></p>"},{"location":"Python/Jupyter/Jupyter/#enable-extension","title":"enable extension","text":"<p>pip install jupyter_nbextensions_configurator   jupyter nbextensions_configurator enable --user #not work   set cfigpath=C:\\Users\\sma\\AppData\\Roaming\\Python\\Python38\\site-packages\\jupyter_nbextensions_configurator\\   python %cfigpath%application.py enable --user #works   pip install jupyter_contrib_nbextensions   jupyter contrib nbextension install --user #not work   set contrpath=C:\\Users\\sma\\AppData\\Roaming\\Python\\Python38\\site-packages\\jupyter_contrib_core\\   python %contrpath%application.py nbextension install --user #works</p> <p>http://localhost:8889/nbextensions</p>"},{"location":"Python/Jupyter/Jupyter/#lux","title":"lux","text":"<pre><code>jupyter nbextension install --py luxwidget\njupyter nbextension enable --py luxwidget\n</code></pre>"},{"location":"Python/Jupyter/Jupyterlab/","title":"Jupyterlab","text":""},{"location":"Python/Jupyter/Jupyterlab/#install-jupyterlab","title":"install jupyterlab","text":"<pre><code>pip install mlflow      #manage ml life cycle\nmlflow ui               #start mlflow\npip install azureml-sdk #azure ml sdk\npip install jupyterlab  #install\njupyter lab             #start server\n</code></pre>"},{"location":"Python/Jupyter/Jupyterlab/#fix-issues","title":"fix issues","text":"<pre><code>jupyter lab clean\njupyter lab update\njupyter lab build\n</code></pre>"},{"location":"Python/Jupyter/Jupyterlab/#extension","title":"extension","text":"<pre><code>jupyter labextension install @jupyterlab/codemirror\njupyter labextension update @krassowski/jupyterlab_go_to_definition\n</code></pre>"},{"location":"Python/Jupyter/Jupyterlab/#jupyter-install-a-kernel","title":"jupyter install a kernel","text":"<pre><code>conda install ipykernel\npython -m ipykernel install --user --name &lt;env-name&gt; --display-name \"&lt;kernel-name&gt;\"\n</code></pre>"},{"location":"Python/Jupyter/Package/","title":"Package","text":""},{"location":"Python/Jupyter/Package/#papermill","title":"papermill","text":"<p>Papermill is a tool for parameterizing and executing Jupyter Notebooks. <pre><code>import papermill as pm\n\npm.execute_notebook(\n   'path/to/input.ipynb',\n   'path/to/output.ipynb',\n   parameters=dict(alpha=0.6, ratio=0.1)\n)\n</code></pre></p>"},{"location":"Python/Jupyter/ShortcutKey/","title":"Shortcut key","text":"<p>https://towardsdatascience.com/optimizing-jupyter-notebook-tips-tricks-and-nbextensions-26d75d502663</p>"},{"location":"Python/Jupyter/ShortcutKey/#cell-to-markdown","title":"cell to markdown","text":"<p>Esc + m</p>"},{"location":"Python/Jupyter/ShortcutKey/#markdown-to-cell","title":"markdown to cell","text":"<p>Esc + y</p>"},{"location":"Python/Jupyter/ShortcutKey/#insert-cell-above","title":"insert cell above","text":"<p>Esc + a</p>"},{"location":"Python/Jupyter/ShortcutKey/#insert-cell-bellow","title":"insert cell bellow","text":"<p>Esc + b</p>"},{"location":"Python/Jupyter/ShortcutKey/#split-cell","title":"split cell","text":"<p>Ctrl + Shift + - </p>"},{"location":"Python/Jupyter/ShortcutKey/#merge-bellow","title":"merge bellow","text":"<p>Shift + m  </p>"},{"location":"Python/Jupyter/ShortcutKey/#commentuncomment-block","title":"comment/uncomment block","text":"<p>Ctrl + /</p>"},{"location":"Python/Logging/AzureLibraries/","title":"Azure Libraries","text":"<p>Configure logging in the Azure libraries for Python</p> <p>https://learn.microsoft.com/en-us/azure/developer/python/sdk/azure-sdk-logging</p>"},{"location":"Python/Logging/AzureLibraries/#set-logging-levels","title":"Set logging levels","text":"<p><pre><code>import logging\n\n#for a specific library\nlogger = logging.getLogger('azure.identity.aio')\nlogger.setLevel(logging.ERROR)\n\n#for all azure-storage-* libraries\nlogger = logging.getLogger('azure.storage')\nlogger.setLevel(logging.INFO)\n\n#for all azure-* libraries\nlogger = logging.getLogger('azure')\nlogger.setLevel(logging.ERROR)\n</code></pre> Note that the azure logger is used by some libraries instead of a specific logger. For example, the azure-storage-blob library uses the azure logger.</p>"},{"location":"Python/Logging/Error/","title":"Error","text":""},{"location":"Python/Logging/Error/#try-catch","title":"try catch","text":"<pre><code>try:\n    run()\nexcept Exception as exc:\n    log(f'Error: {sys.exc_info()[0]}. {sys.exc_info()[1]}, line: {sys.exc_info()[2].tb_lineno}')\nfinally:\n    ok = 1 #do anything else\n</code></pre>"},{"location":"Python/Logging/Error/#print-trace-with-except","title":"print trace with except","text":"<pre><code>import traceback\n\ntry:\n  my_func()\nexcept Exception as exc:\n  errmsg = traceback.format_exc()\n  print(errmsg)\n  return\n</code></pre>"},{"location":"Python/Logging/Error/#use-warnings-module-to-elevate-warnings-to-errors","title":"use <code>warnings</code> module to elevate warnings to errors","text":"<pre><code>warnings.simplefilter('error', FutureWarning)\n</code></pre>"},{"location":"Python/Logging/Logging/","title":"Logging","text":""},{"location":"Python/Logging/Logging/#logging-inheritance","title":"logging inheritance","text":"<p><pre><code>setup_logger(__package__)\nlogger = logging.getLogger(__name__)\n</code></pre> By setting up the logger at the package level in the root namespace, the parent logger of all loggers will be created in all sub-packages.</p> <p>All loggers instantiated in any sub-package using <code>logger = logging.getLogger(__name__)</code> will therefore inherit the configuration that applied to the <code>__package__</code> logger in the root namespace.</p>"},{"location":"Python/Logging/Logging/#logging-handler","title":"logging handler","text":"<p>https://docs.python.org/3/library/logging.handlers.html</p> <p>The <code>StreamHandler</code> class, located in the core logging package, sends logging output to streams such as sys.stdout, sys.stderr or any file-like object (or, more precisely, any object which supports write() and flush() methods).</p> <p>The <code>FileHandler</code> class inherits the output functionality from StreamHandler.</p>"},{"location":"Python/Logging/Logging/#log-formatter","title":"log formatter","text":"<pre><code>def get_log_formatter(message_prefix: str = None) -&gt; logging.Formatter:\n    log_format = ' | '.join([\n        '%(asctime)s',\n        '%(levelname)s',\n        '%(name)s',\n        '%(module)s',\n        '%(funcName)s',\n        '%(lineno)d',\n        f'{message_prefix}%(message)s',\n    ])\n    return logging.Formatter(log_format)\n\ndef set_log_message_prefix(message_prefix: str = None) -&gt; None:\n    \"\"\"Set the log message prefix during run\"\"\"\n    logger = logging.getLogger()\n    log_formatter = get_log_formatter(message_prefix=message_prefix)\n\n    #set new format to each logger\n    #console_handler = logging.StreamHandler() #this handler is already in the handlers list\n    #console_handler.setFormatter(log_formatter)\n    for handler in logger.handlers:\n        handler.setFormatter(log_formatter)\n\ndef set_log_handler_level(log_level: int = logging.NOTSET) -&gt; None:\n    logger = logging.getLogger()\n    for handler in logger.handlers:\n        if log_level != logging.NOTSET:\n            handler.setLevel(log_level)\n</code></pre>"},{"location":"Python/Logging/Logging/#log-to-both-stdout-and-file","title":"log to both stdout and file","text":"<pre><code>import sys\nimport logging\n\nformatter = get_log_formatter()\n\n#root logger\nroot_logger = logging.getLogger()\nroot_logger.setLevel(logging.ERROR)\n\n#package logger\npackage_logger = logging.getLogger('package_log_name')\npackage_logger.setLevel(logging.INFO)\npackage_logger.propagate = False\n\n#console handler\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel(logging.INFO)\nconsole_handler.setFormatter(formatter)\nroot_logger.addHandler(console_handler)\n#if previous console handler not deleted, log will output multiple times\npackage_logger.addHandler(console_handler)\n\n#file handler\nfile_handler = logging.FileHandler('file.log')\nfile_handler.setLevel(logging.DEBUG)\nfile_handler.setFormatter(formatter)\nroot_logger.addHandler(file_handler)\npackage_logger.addHandler(file_handler)\n</code></pre>"},{"location":"Python/Logging/Process/","title":"Process","text":""},{"location":"Python/Logging/Process/#psutil","title":"psutil","text":"<p><code>psutil</code> is a cross-platform library for retrieving information on running processes and system utilization (CPU, memory, disks, network, sensors) in Python.</p> <p>List process pid and name <pre><code>import psutil\nfor proc in psutil.process_iter():\n    try:\n        # Get process name &amp; pid from process object.\n        processName = proc.name()\n        processID = proc.pid\n        print(f'pid: {proc.pid}, name: {proc.name()}')\n    except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n        pass\n</code></pre></p>"},{"location":"Python/Logging/Stdout/","title":"Stdout","text":""},{"location":"Python/Logging/Stdout/#suppress-stdout-and-stderr","title":"Suppress stdout and stderr","text":"<p>https://github.com/facebook/prophet/issues/223</p> <p>stdin/stdout are global variables, shared between threads, and so are file descriptors.</p> <p>This method - works for stdout and stderr from c as well - will fail within multi-thread tasks (without output or with unexpected outputs) <pre><code>class SUPPRESS_STDOUT_STDERR(object):\n    '''\n    A context manager for doing a \"deep suppression\" of stdout and stderr in\n    Python, i.e. will suppress all print, even if the print originates in a\n    compiled C/Fortran sub-function.\n       This will not suppress raised exceptions, since exceptions are printed\n    to stderr just before a script exits, and after the context manager has\n    exited (at least, I think that is why it lets exceptions through).\n    '''\n    def __init__(self):\n        # Open a pair of null files\n        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n        # Save the actual stdout (1) and stderr (2) file descriptors.\n        self.save_fds = [os.dup(1), os.dup(2)]\n\n    def __enter__(self):\n        # Assign the null pointers to stdout and stderr.\n        os.dup2(self.null_fds[0], 1)\n        os.dup2(self.null_fds[1], 2)\n        # https://stackoverflow.com/questions/66784941/dup2-and-pipe-shenanigans-with-python-and-windows\n        if sys.platform == 'win32':\n            sys.stdout.write = lambda z: os.write(\n                sys.stdout.fileno(),\n                z.encode() if hasattr(z, 'encode') else z\n            )\n\n    def __exit__(self, *_):\n        # Re-assign the real stdout/stderr back to (1) and (2)\n        os.dup2(self.save_fds[0], 1)\n        os.dup2(self.save_fds[1], 2)\n        # Close the null files\n        for fd in self.null_fds + self.save_fds:\n            os.close(fd)\n</code></pre></p> <p>https://medium.com/swlh/python-recipes-suppress-stdout-and-stderr-messages-9141ef4b1373</p>"},{"location":"Python/Logging/Stdout/#redirect-stdout-and-stderr-to-file","title":"redirect stdout and stderr to file","text":"<p>https://eli.thegreenplace.net/2015/redirecting-all-kinds-of-stdout-in-python/</p> <p>https://dzone.com/articles/redirecting-all-kinds-stdout</p> <p>https://devpress.csdn.net/python/62fd8a92c677032930803e01.html</p>"},{"location":"Python/Logging/Warning/","title":"Warning","text":""},{"location":"Python/Logging/Warning/#warning-to-error","title":"warning to error","text":"<pre><code>warnings.filterwarnings(\"error\")\n</code></pre>"},{"location":"Python/Logging/Warning/#turn-off-warnings","title":"turn off warnings","text":"<pre><code>warnings.simplefilter(\"ignore\", DeprecationWarning)\nwarnings.simplefilter(\"ignore\", FutureWarning)\nwarnings.simplefilter(\"ignore\", UserWarning)\n</code></pre>"},{"location":"Python/Logging/tool/","title":"tool","text":"<p>Seq for python:\\ https://docs.datalust.co/docs/using-python</p> <p>Seq helm chart:\\ https://docs.datalust.co/docs/using-helm</p> <p>logging:\\ https://www.toptal.com/python/in-depth-python-logging</p>"},{"location":"Python/MSOffice/Access/","title":"Access","text":""},{"location":"Python/MSOffice/Access/#connect-to-access-db","title":"Connect to Access DB","text":"<pre><code># connect to db\ntry:\n    drv = '{Microsoft Access Driver (*.mdb)}'\n    con = pyodbc.connect(f'DRIVER={drv};DBQ=' + dbpath)\nexcept pyodbc.InterfaceError:\n    drv = '{Microsoft Access Driver (*.mdb, *.accdb)}'\n    con = pyodbc.connect('DRIVER={drv};DBQ=' + dbpath)\ncursor = con.cursor()\n\n# query\ncursor.execute(\"Select * from tbl\")\nrows = cursor.fetchall()\n\n# close connection\ncursor.close()\ncon.close()\n</code></pre>"},{"location":"Python/MSOffice/Access/#pyodbc-like-wildcard","title":"pyodbc LIKE wildcard","text":"<p>LIKE wildcard characters between queries run in Access and from an external applications are different: Access uses the asterisk as the wildcard character, \"2019-09-03*\"; External application (like Python) uses the percent sign as the wildcard character, \"2019-09-03%\".</p>"},{"location":"Python/MSOffice/Access/#dao-field-type","title":"DAO field type","text":"<p>http://allenbrowne.com/ser-49.html</p>"},{"location":"Python/MSOffice/Access/#dao-connect-timestamp","title":"DAO connect timestamp","text":"<p><code>df.timestamp</code> should be changed to str <pre><code>is_timestamp = pd.core.dtypes.common.is_datetime_or_timedelta_dtype(series)\nif is_timestamp:\n    changes datetime column to str\n</code></pre></p>"},{"location":"Python/MSOffice/Access/#dao-connect-query","title":"DAO connect query","text":"<pre><code>import os\nimport csv\nimport win32com.client\n\ndef open_db(eng, dbpath, lock=False):\n    if not os.path.isfile(dbpath):\n        log(f'File does not exist.\\n    File: {dbpath}', stop=True)\n    #create DAO connection to the access database\n    eng = win32com.client.Dispatch(\"DAO.DBEngine.120\")\n    mdb = eng.OpenDatabase(dbpath, lock) #True = Lock the database.  Prevent getting to multi-user mode\n    return mdb\n\ndef get_db_fields(table, csvheader):\n    fields = []\n    for field in csvheader:\n        try:\n            fields.append(table.Fields.Item(field))\n        except:\n            log(f'ERROR: Field \"{field}\" in csv could not be found in table \"{table.Name}\"', stop=True)\n    return fields\n\ndef qry_to_list(mdb, qry, header):\n    csv = []\n    rs = mdb.OpenRecordset(qry)\n    fields = get_db_fields(rs, header)\n    while not rs.EOF:\n        csv.append([field.Value for field in fields])\n        rs.MoveNext()\n    rs.Close()\n    return csv\n\ndef list_to_csv(csvheader, csvdata, csvpath):\n    with open(csvpath, 'w', newline='') as f:\n        w = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n        w.writerow(csvheader)\n        for row in csvdata:\n            w.writerow(row)\n</code></pre>"},{"location":"Python/MSOffice/Excel/","title":"Excel","text":"<pre><code>python -m pip install xlrd==1.2.0\n</code></pre> <p><pre><code>#change number to Excel column letters\ndef ExcelNumToLetter(num):\n    letter = ''\n    dividend = num\n    while dividend &gt; 0:\n       modulo = (dividend - 1) % 26\n       letter = chr(65 + modulo) + letter\n       dividend = int((dividend - modulo) / 26)\n    return letter\n\ndef num_to_col(n):\n    col = ''\n    while n &gt; 0:\n        n, r = divmod (n - 1, 26)\n        col = chr(r + ord('A')) + col\n    return col\n\ndef col_to_num(col):\n    n = 0\n    for c in col:\n        n = n * 26 + ord(c) - ord('A') + 1\n    return n\n</code></pre> http://timgolden.me.uk/pywin32-docs/win32api.html</p>"},{"location":"Python/MSOffice/Excel/#win32comclient-error","title":"win32com.client error","text":"<p><pre><code>python -m pip install pywin32 #this might be blocked by corporate firewall\n</code></pre> raise AttributeError(\"%s.%s\" % (self.username, attr)) AttributeError: Excel.Application.Workbooks</p> <p>When attributes on Excel.Application do not exist, it is usually because the Excel application is open (possibly hidden) and it is in a modal loop such as editing a cell or an open/save file dialog.</p>"},{"location":"Python/MSOffice/Excel/#avoid-hidden-rowscols-do-not-use-openpyxl-too-slow-when-its-not-readonly","title":"Avoid hidden rows/cols (DO NOT USE <code>openpyxl</code> - too slow when it's not readonly)","text":"<p>https://towardsdatascience.com/how-to-load-excel-files-with-hidden-rows-and-columns-into-pandas-19d445fa5c47 <pre><code>import openpyxl\n\nhidden_rows_idx = [\n    row - 2\n    for row, dimension in ws.row_dimensions.items()\n    if dimension.hidden\n]\n\n# List of indices corresponding to all hidden columns\nhidden_cols_idx = [\n    string.ascii_uppercase.index(col_name)\n    for col_name in [\n        col\n        for col, dimension in ws.column_dimensions.items()\n        if dimension.hidden\n    ]\n]\n</code></pre></p>"},{"location":"Python/MSOffice/Excel/#python-call-vba-messagebox","title":"Python call VBA MessageBox","text":"<pre><code>import win32api\nimport win32con\n\nwin32api.MessageBox(0, 'My message', 'title', win32con.MB_OK) #0 means on top of other windows\nwin32api.MessageBox(0, 'My message', 'title', win32con.MB_OKCANCEL | win32con.MB_ICONERROR)\n</code></pre>"},{"location":"Python/MSOffice/Excel/#py-call-vba","title":"py call vba","text":"<pre><code>import win32com.client\n\nxlApp = win32com.client.DispatchEx('Excel.Application')\nwb = xlApp.Workbooks.Open(Filename=yourworkbookname.xlsm)\nxlApp.Run('macroName')\n</code></pre>"},{"location":"Python/MSOffice/Excel/#vba-call-py","title":"vba call py","text":"<p>RetVal = Shell(\" \" &amp; \"\")"},{"location":"Python/MSOffice/Excel/#excel-to-csv","title":"Excel to csv","text":"<pre><code>def sheets_to_csv(\n    excel_filepath: str,\n    sheet_names: List[str],\n    csv_dir: str = None,\n) -&gt; None:\n    \"\"\"\n    Convert Excel sheets to csv files.\n    \"\"\"\n    csv_dir = csv_dir or os.path.dirname(excel_filepath)\n    vbsfile = os.path.join(csv_dir, 'ExcelSheetsToCSV.vbs')\n    create_sheets2csv_vbs(vbsfile)\n\n    from subprocess import call\n    call(['cscript.exe', vbsfile, csv_dir, excel_filepath, ':'.join(sheet_names)])\n\n    if os.path.isfile(vbsfile):\n        os.remove(vbsfile)\n</code></pre> <pre><code>with open(filepath,'wb') as f:\n    f.write(inspect.cleandoc(vbscript).encode('utf-8'))\n</code></pre> <p>```vb VBScript SheetsToCSV.vbs If WScript.Arguments.Count &lt; 3 Then     WScript.Echo \"Parameters:  \"     WScript.Quit End If csv_dir = WScript.Arguments.Item(0) xl_file = WScript.Arguments.Item(1) sheet_names = WScript.Arguments.Item(2) Set xl = CreateObject(\"Excel.Application\") Set wb = xl.Workbooks.Open(xl_file) Set fs = CreateObject(\"Scripting.FileSystemObject\") csv_file = csv_dir &amp; \"/\" &amp; fs.GetFileName(xl_file) For Each sheet_name In Split(sheet_names, \":\")     wb.Worksheets(sheet_name).Activate     wb.SaveAs csv_file &amp; sheet_name &amp; \".csv\", 6 'csv_format = 6 Next wb.Close False xl.Quit <pre><code>## Excel column letter to number\n```py\ndef col_num(col: str) -&gt; int:\n    n = 0\n    for c in col:\n        n = n * 26 + ord(c) - 64 #ord('A')=65\n    return n - 1\n\ndef col_rng(rng: str) -&gt; Iterator[int]:\n    cols = rng.split(':')\n    return range(col_num(cols[0]), col_num(cols[-1]) + 1)\n\ndef col_ind(cols: str) -&gt; List[int]:\n    return [i for col in colstr.split(',') for i in col_rng(col)]\n</code></pre>"},{"location":"Python/Math/Sparse/","title":"Sparse matrix","text":""},{"location":"Python/Math/Sparse/#python-sparse-matrix","title":"python sparse matrix","text":"<p>https://stackoverflow.com/questions/44873026/sparse-matrix-with-fast-access</p>"},{"location":"Python/NumPy/LinearFit/","title":"LinearFit","text":""},{"location":"Python/NumPy/LinearFit/#linear-fit-must-pass-x0-y0","title":"linear fit must pass (x0, y0)","text":"<p>create a linear fit that must pass a specified point (x0, y0). - shift the data points so that the desired point (x0, y0) becomes the new origin (0, 0) - we only need to calculate the slope (linear regression), as the intercept of the line passing through the new origin is zero - get the intercept using (x0, y0): <code>y - y0 = k (x - x0) =&gt; y = kx + (y0 - kx0)</code> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef constrained_linear_fit(x, y, x0, y0):\n    \"\"\"\n    Performs a linear fit (y = kx + b) that must pass through the point (x0, y0).\n\n    Args:\n        x: Array of x-values.\n        y: Array of y-values.\n        x0: x-coordinate of the point the line must pass through.\n        y0: y-coordinate of the point the line must pass through.\n\n    Returns:\n        tuple: (slope, intercept) of the fitted line.\n    \"\"\"\n\n    if len(x) != len(y):\n        raise ValueError('x and y arrays must have the same length.')\n\n    # Modified x and y values for fitting\n    x_ = x - x0\n    y_ = y - y0\n\n    # Calculate the slope (k) using a simplified linear regression\n    k = np.sum(x_ * y_) / np.sum(x_**2)\n\n    # Calculate the intercept (b) using the constraint point\n    b = y0 - k * x0\n\n    return k, b\n\n# Example Usage\nx = np.array([-300, -150, 0, 150, 300])\ny = np.array([450, 290, 250, 180, 120])\nx0 = 0    # Desired x-coordinate\ny0 = 250  # Desired y-coordinate\n\nslope, intercept = constrained_linear_fit(x, y, x0, y0)\nprint(f'Slope (k): {slope}')\nprint(f'Intercept (b): {intercept}')\n\n# Generate points for plotting the fitted line\nx_fit = np.linspace(min(x) - 1, max(x) + 1, 100)\ny_fit = slope * x_fit + intercept\n\n# Plotting\nplt.scatter(x, y, label='Data Points')\nplt.plot(x_fit, y_fit, color='red', label=f'Fit: y = {slope:.2f}x + {intercept:.2f}')\nplt.scatter(x0, y0, color='green', label='Constraint Point')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Constrained Linear Fit')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre></p>"},{"location":"Python/NumPy/NumPy/","title":"numpy","text":"<pre><code>x = np.arange(5)\n</code></pre>"},{"location":"Python/NumPy/NumPy/#join-array","title":"join array","text":"<pre><code>#join a sequence of arrays along an existing axis\na = np.array([[5, 6]])\nb = np.array([[1, 2], [3, 4]])\nnp.concatenate((a, b), axis=0)\nnp.concatenate((a, b), axis=None)\n</code></pre>"},{"location":"Python/NumPy/NumPy/#tilerepeat","title":"tile/repeat","text":"<pre><code>a = np.array([1, 2])\nnp.tile(a,2) #duplicate the array n times\nnp.repeat(a,2) #duplicate each element n times\n</code></pre>"},{"location":"Python/NumPy/NumPy/#reshape","title":"reshape","text":"<pre><code>import numpy as np\nd = np.array([[1,2,3],[4,5,6],[7,8,9]])\n\n# row by row\nd.reshape(-1, order='c')\n#array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n# col by col\nd.reshape(-1, order='f')\n#array([1, 4, 7, 2, 5, 8, 3, 6, 9])\n\n#when no intention to change the new array\n# it will be faster to use\nd.ravel(order='c')\nd.ravel(order='f')\n</code></pre>"},{"location":"Python/NumPy/NumPy/#sort","title":"sort","text":"<pre><code>people = np.array(people)\nages = np.array(ages)\nind = ages.argsort()\nsortedPeople = people[ind]\n</code></pre>"},{"location":"Python/NumPy/NumPy/#max-value-and-index","title":"max value and index","text":"<pre><code>#get all max values\nind = np.where(a == a.max())\n#get only first max value\nind = np.unravel_index(a.argmax(), a.shape)\n</code></pre>"},{"location":"Python/NumPy/NumPy/#top-n-min-val","title":"top n min val","text":"<pre><code>#1d arr\nind = np.argpartition(d1, n-1)[:n]\n\n#ind in order\nmin_val = arr[ind]\nmin_val_order = np.argsort(min_val)\nordered_ind = ind[min_val_order]\n\n#2d arr\nflat_ind = np.argpartition(d2.ravel(), n-1)[:n]\nrow_ind, col_ind = np.unravel_index(flat_ind, d2.shape)\n\n#ind in order\nmin_val = arr[row_ind, col_ind]\nmin_val_order = np.argsort(min_val)\nrow_ind, col_ind = row_ind[min_val_order], col_ind[min_val_order]\n</code></pre>"},{"location":"Python/NumPy/NumPy/#apply-function","title":"apply function","text":"<pre><code>def my_func(a):\n    return (a[0] + a[-1]) * 0.5\nb = np.array([[1,2,3], [4,5,6], [7,8,9]])\nnp.apply_along_axis(my_func, 0, b) #col by col\n#out array([4., 5., 6.])\nnp.apply_along_axis(my_func, 1, b) #row by row\n#out array([2.,  5.,  8.])\n</code></pre>"},{"location":"Python/NumPy/NumPy/#elems-in-a-not-in-b","title":"elems in a not in b","text":"<pre><code>#numpy\na = np.array([1,2,3,4,5,6,7])\nb = np.array([2,4,6,8])\nmsk = np.in1d(a, b)\na[~msk]\nnp.setdiff1d(a, b)\n\n#pandas\nmsk = (pd.Index(pd.unique(b)).get_indexer(a) &gt;= 0)\na[~msk]\n</code></pre>"},{"location":"Python/NumPy/csmooth/","title":"csmooth","text":""},{"location":"Python/NumPy/csmooth/#interp","title":"interp","text":"<pre><code>from scipy import interpolate\nfn = interpolate.interp1d(x, y, kind='cubic')\nyn = fn(xn)\n</code></pre>"},{"location":"Python/NumPy/csmooth/#smooth","title":"smooth","text":"<pre><code>from csaps import csaps\nsmooth = 0.1\nyn = csaps(x, y, xn, smooth=smooth)\n</code></pre>"},{"location":"Python/NumPy/curvefit/","title":"curvefit","text":"<pre><code>import numpy as np\nfrom scipy.optimize import curve_fit\ncurve_fit(f, x, y, p0, sigma=sigma, absolute_sigma=True, method='dogbox')\n#p0 is inital value\n#sigma is the inverse of weighting\nminimize: chisq = sum((r / sigma) ** 2)\n</code></pre>"},{"location":"Python/NumPy/curvefit/#polynomial","title":"polynomial","text":"<pre><code>polyfn = np.poly1d(np.polyfit(x, y, 4, w=np.square(y)))\ny_new = polyfn(x_new)\n</code></pre>"},{"location":"Python/NumPy/curvefit/#sigmoid","title":"sigmoid","text":"<pre><code>def sigmoid(x, L, x0, k, b):\n    y = L / (1 + np.exp(-k * (x - x0))) + b\n    return y\n\np0 = [4*max(y), np.max(x), 1, min(y)] #this is an mandatory initial guess\npopt, pcov = curve_fit(sigmoid, x, y, p0, method='dogbox', maxfev=1000)\n\nx_new = list(range(x[-1]+1,x[-1]+21)\ny_new = sigmoid(x_new, *popt)\n</code></pre>"},{"location":"Python/NumPy/curvefit/#example","title":"example","text":"<pre><code>from numpy import exp, loadtxt, pi, sqrt\nfrom lmfit import Model\nimport matplotlib.pyplot as plt\n\ndata = loadtxt('model1d_gauss.dat')\nx = data[:, 0]\ny = data[:, 1]\n\ndef gaussian(x, amp, cen, wid):\n    \"\"\"1-d gaussian: gaussian(x, amp, cen, wid)\"\"\"\n    return (amp / (sqrt(2*pi) * wid)) * exp(-(x-cen)**2 / (2*wid**2))\n\ngmodel = Model(gaussian)\nresult = gmodel.fit(y, x=x, amp=5, cen=5, wid=1)\n\nprint(result.fit_report())\n\nplt.plot(x, y, 'bo')\nplt.plot(x, result.init_fit, 'k--', label='initial fit')\nplt.plot(x, result.best_fit, 'r-', label='best fit')\nplt.legend(loc='best')\nplt.show()\n</code></pre>"},{"location":"Python/NumPy/curvefit/#funcs","title":"funcs","text":"<pre><code>def fn_exp(x, b, x0):\n    #y = e^(-b * (x-x0)\n    #log(y) = -b*x + bx0\n    #Y = Bx + A\n    return np.exp(-b * (x-x0))\ndef p0_exp(x, y):\n    coe = np.polyfit(x, np.log(y), 1, w=range(1,len(y)+1))\n    b = -coe[0]\n    x0 = coe[1] / b\n    return [b, x0]\ndef fn_powlaw(x, a, b):\n    #y = a * x^b\n    #log(y) = log(a) + b*log(x)\n    #Y = A + bX\n    return a * np.power((x), b)\ndef p0_powlaw(x, y):\n    coe = np.polyfit(np.log(x), np.log(y), 1, w=range(1,len(y)+1))\n    a = np.exp(coe[1])\n    b = coe[0]\n    return [a, b]\n</code></pre>"},{"location":"Python/NumPy/tip/","title":"Tip","text":""},{"location":"Python/NumPy/tip/#rows-with-same-values","title":"rows with same values","text":"<pre><code># b is the index for rows with the same value in cols j:k\nd = df.iloc[:,j:k].values\nv = df.iloc[:,[j]].values\nb = np.bitwise_and.reduce(d==v,axis=1)\n</code></pre>"},{"location":"Python/NumPy/tip/#npwhere-converted-datetime64ns-to-object","title":"np.where converted datetime64[ns] to object","text":"<p>avoid use <code>np.where</code> here; use pandas method (cap min to zero) <pre><code>df.val.where(df.val &gt; 0, 0)\n</code></pre></p>"},{"location":"Python/Package/CondaBuild/","title":"CondaBuild","text":""},{"location":"Python/Package/CondaBuild/#conda-build","title":"Conda build","text":""},{"location":"Python/Package/CondaBuild/#metayaml","title":"meta.yaml","text":"<p>https://docs.conda.io/projects/conda-build/en/latest/resources/define-metadata.html</p> <pre><code>{% set name = \"pkg\" %}\n{% set version = \"1.0.0\" %}\n\npackage:\n  name: {{ name }}\n  version: {{ version }}\n\nsource:\n  path: ../\n  url: https://github.com/usr/pkg\n\noutputs:\n\n  - name: {{ name }}\n    build:\n      noarch: python\n      number: 0\n      script: python -m pip install --no-deps --ignore-installed .\n    requirements:\n      host:\n        - python\n        - pip\n      run:\n        - python\n    about:\n      home: https://github.com/usr/pkg\n      summary: 'My python pkg'\n\n  - name: {{ name }}.test\n    requirements:\n      run:\n        - python\n    test:\n      source_files:\n        - tests\n    about:\n      home: https://github.com/usr/pkg\n      summary: 'My python pkg'\n</code></pre>"},{"location":"Python/Package/Dependency/","title":"Dependency","text":""},{"location":"Python/Package/Dependency/#find-package-dependencies","title":"Find package dependencies","text":"<pre><code>pip install pipdeptree\n$ pipdeptree\n</code></pre>"},{"location":"Python/Package/Depreciated/","title":"Depreciated","text":""},{"location":"Python/Package/Depreciated/#future-warning","title":"future warning","text":"<p>identify future warning <pre><code># test_file.py\nimport pytest\npytestmark = pytest.mark.filterwarnings(\"error::FutureWarning\")\n\ndef test_my_func():\n  assert true\n</code></pre> Then run the test on terminal <code>pytest test_file.py &gt; test.log</code>, and check the logs.</p>"},{"location":"Python/Package/Error/","title":"Error","text":""},{"location":"Python/Package/Error/#certifi_win32wrapt_certifipy-no-module-named-wrapt","title":"\"certifi_win32/wrapt_certifi.py\"  No module named 'wrapt'","text":"<p>update both packages: <code>pip install -U python-certifi-win32 wrapt</code></p>"},{"location":"Python/Package/List/","title":"Commonly used packages","text":"<pre><code># Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda_forge    conda-forge\n_openmp_mutex             4.5                       2_gnu    conda-forge\n_py-xgboost-mutex         2.0                       cpu_0    conda-forge\nannotated-types           0.6.0              pyhd8ed1ab_0    conda-forge\nansi2html                 1.9.1           py310hff52083_0    conda-forge\nanyio                     4.3.0              pyhd8ed1ab_0    conda-forge\napache-iotdb              1.1.0                    pypi_0    pypi\nargon2-cffi               21.3.0                   pypi_0    pypi\nargon2-cffi-bindings      21.2.0                   pypi_0    pypi\narrow                     1.2.3                    pypi_0    pypi\nasttokens                 2.2.1                    pypi_0    pypi\nasync-generator           1.10                     pypi_0    pypi\nasync-lru                 2.0.2                    pypi_0    pypi\nattrs                     23.1.0                   pypi_0    pypi\naws-c-auth                0.7.19               h5f1c8d9_1    conda-forge\naws-c-cal                 0.6.12               h2ba76a8_0    conda-forge\naws-c-common              0.9.17               h4ab18f5_0    conda-forge\naws-c-compression         0.2.18               h36a0aea_4    conda-forge\naws-c-event-stream        0.4.2               h161de36_10    conda-forge\naws-c-http                0.8.1               h63f54a0_13    conda-forge\naws-c-io                  0.14.8               h96d4d28_0    conda-forge\naws-c-mqtt                0.10.4               hcc7299c_2    conda-forge\naws-c-s3                  0.5.8                hdec9a15_2    conda-forge\naws-c-sdkutils            0.1.16               h36a0aea_0    conda-forge\naws-checksums             0.1.18               h36a0aea_4    conda-forge\naws-crt-cpp               0.26.8               h5ef9dee_9    conda-forge\naws-sdk-cpp               1.11.267             h51dfee4_8    conda-forge\nazure-core                1.30.1             pyhd8ed1ab_0    conda-forge\nazure-identity            1.16.0             pyhd8ed1ab_0    conda-forge\nazure-storage-blob        12.20.0            pyhd8ed1ab_0    conda-forge\nbabel                     2.12.1                   pypi_0    pypi\nbackcall                  0.2.0                    pypi_0    pypi\nbackports                 1.0                pyhd8ed1ab_3    conda-forge\nbackports.functools_lru_cache 2.0.0          pyhd8ed1ab_0    conda-forge\nbeautifulsoup4            4.12.2                   pypi_0    pypi\nblack                     24.4.2          py310hff52083_0    conda-forge\nbleach                    6.0.0                    pypi_0    pypi\nblinker                   1.8.2              pyhd8ed1ab_0    conda-forge\nbrotli                    1.1.0                hd590300_1    conda-forge\nbrotli-bin                1.1.0                hd590300_1    conda-forge\nbrotli-python             1.1.0           py310hc6cd4ac_1    conda-forge\nbzip2                     1.0.8                hd590300_5    conda-forge\nc-ares                    1.28.1               hd590300_0    conda-forge\nca-certificates           2024.2.2             hbcca054_0    conda-forge\ncachelib                  0.9.0              pyhd8ed1ab_0    conda-forge\ncachetools                5.3.3              pyhd8ed1ab_0    conda-forge\ncairo                     1.18.0               h3faef2a_0    conda-forge\ncertifi                   2024.2.2           pyhd8ed1ab_0    conda-forge\ncffi                      1.15.1                   pypi_0    pypi\ncharset-normalizer        3.3.2              pyhd8ed1ab_0    conda-forge\nclick                     8.1.7         unix_pyh707e725_0    conda-forge\ncomm                      0.1.3                    pypi_0    pypi\ncontourpy                 1.2.1           py310hd41b1e2_0    conda-forge\ncryptography              42.0.7          py310hb1bd9d3_0    conda-forge\ncycler                    0.12.1             pyhd8ed1ab_0    conda-forge\ndash                      2.17.0             pyhd8ed1ab_0    conda-forge\ndash-bootstrap-components 1.6.0              pyhd8ed1ab_0    conda-forge\ndash-extensions           1.0.15             pyhd8ed1ab_0    conda-forge\ndataclass-wizard          0.22.3             pyhd8ed1ab_0    conda-forge\ndebugpy                   1.6.7                    pypi_0    pypi\ndecorator                 5.1.1              pyhd8ed1ab_0    conda-forge\ndefusedxml                0.7.1                    pypi_0    pypi\ndeprecation               2.1.0                    pypi_0    pypi\ndnspython                 2.6.1              pyhd8ed1ab_1    conda-forge\ndocker                    6.1.2                    pypi_0    pypi\neditorconfig              0.12.3             pyhd8ed1ab_0    conda-forge\nemail-validator           2.1.1              pyhd8ed1ab_0    conda-forge\nemail_validator           2.1.1                hd8ed1ab_0    conda-forge\nexceptiongroup            1.1.1                    pypi_0    pypi\nexecuting                 1.2.0                    pypi_0    pypi\nexpat                     2.6.2                h59595ed_0    conda-forge\nfake-useragent            1.1.3                    pypi_0    pypi\nfastapi                   0.111.0            pyhd8ed1ab_0    conda-forge\nfastapi-cli               0.0.3              pyhd8ed1ab_0    conda-forge\nfastjsonschema            2.17.1                   pypi_0    pypi\nflask                     2.3.3              pyhd8ed1ab_0    conda-forge\nflask-caching             2.0.2              pyhd8ed1ab_0    conda-forge\nfont-ttf-dejavu-sans-mono 2.37                 hab24e00_0    conda-forge\nfont-ttf-inconsolata      3.000                h77eed37_0    conda-forge\nfont-ttf-source-code-pro  2.038                h77eed37_0    conda-forge\nfont-ttf-ubuntu           0.83                 h77eed37_2    conda-forge\nfontconfig                2.14.2               h14ed4e7_0    conda-forge\nfonts-conda-ecosystem     1                             0    conda-forge\nfonts-conda-forge         1                             0    conda-forge\nfonttools                 4.51.0          py310h2372a71_0    conda-forge\nfqdn                      1.5.1                    pypi_0    pypi\nfreetype                  2.12.1               h267a509_2    conda-forge\ngettext                   0.22.5               h59595ed_2    conda-forge\ngettext-tools             0.22.5               h59595ed_2    conda-forge\ngflags                    2.2.2             he1b5a44_1004    conda-forge\nglog                      0.7.0                hed5481d_0    conda-forge\nh11                       0.14.0             pyhd8ed1ab_0    conda-forge\nh2                        4.1.0              pyhd8ed1ab_0    conda-forge\nhpack                     4.0.0              pyh9f0ad1d_0    conda-forge\nhttpcore                  1.0.5              pyhd8ed1ab_0    conda-forge\nhttpx                     0.27.0             pyhd8ed1ab_0    conda-forge\nhyperframe                6.0.1              pyhd8ed1ab_0    conda-forge\nicu                       73.2                 h59595ed_0    conda-forge\nidna                      3.7                pyhd8ed1ab_0    conda-forge\nimportlib-metadata        7.1.0              pyha770c72_0    conda-forge\nimportlib_metadata        7.1.0                hd8ed1ab_0    conda-forge\nipykernel                 6.23.3                   pypi_0    pypi\nipython                   8.14.0                   pypi_0    pypi\nisodate                   0.6.1              pyhd8ed1ab_0    conda-forge\nisoduration               20.11.0                  pypi_0    pypi\nitsdangerous              2.2.0              pyhd8ed1ab_0    conda-forge\njedi                      0.18.2                   pypi_0    pypi\njinja2                    3.1.4              pyhd8ed1ab_0    conda-forge\njoblib                    1.4.2              pyhd8ed1ab_0    conda-forge\njsbeautifier              1.14.9             pyhd8ed1ab_0    conda-forge\njson5                     0.9.14                   pypi_0    pypi\njsonpointer               2.4                      pypi_0    pypi\njsonschema                4.17.3                   pypi_0    pypi\njupyter-client            8.3.0                    pypi_0    pypi\njupyter-core              5.3.1                    pypi_0    pypi\njupyter-dash              0.4.2              pyhd8ed1ab_1    conda-forge\njupyter-events            0.6.3                    pypi_0    pypi\njupyter-lsp               2.2.0                    pypi_0    pypi\njupyter-server            2.7.0                    pypi_0    pypi\njupyter-server-terminals  0.4.4                    pypi_0    pypi\njupyter_client            8.6.1              pyhd8ed1ab_0    conda-forge\njupyter_core              5.7.2           py310hff52083_0    conda-forge\njupyterlab                4.0.2                    pypi_0    pypi\njupyterlab-pygments       0.2.2                    pypi_0    pypi\njupyterlab-server         2.23.0                   pypi_0    pypi\nkeyutils                  1.6.1                h166bdaf_0    conda-forge\nkiwisolver                1.4.5           py310hd41b1e2_1    conda-forge\nkrb5                      1.21.2               h659d440_0    conda-forge\nlcms2                     2.16                 hb7c19ff_0    conda-forge\nld_impl_linux-64          2.40                 h41732ed_0    conda-forge\nlerc                      4.0.0                h27087fc_0    conda-forge\nlibabseil                 20240116.2     cxx17_h59595ed_0    conda-forge\nlibarrow                  16.0.0           hefa796f_1_cpu    conda-forge\nlibarrow-acero            16.0.0           hac33072_1_cpu    conda-forge\nlibarrow-dataset          16.0.0           hac33072_1_cpu    conda-forge\nlibarrow-flight           16.0.0           hef0f296_1_cpu    conda-forge\nlibarrow-flight-sql       16.0.0           h9241762_1_cpu    conda-forge\nlibarrow-gandiva          16.0.0           h35c4161_1_cpu    conda-forge\nlibarrow-substrait        16.0.0           h7e0c224_1_cpu    conda-forge\nlibasprintf               0.22.5               h661eb56_2    conda-forge\nlibasprintf-devel         0.22.5               h661eb56_2    conda-forge\nlibblas                   3.9.0       22_linux64_openblas    conda-forge\nlibbrotlicommon           1.1.0                hd590300_1    conda-forge\nlibbrotlidec              1.1.0                hd590300_1    conda-forge\nlibbrotlienc              1.1.0                hd590300_1    conda-forge\nlibcblas                  3.9.0       22_linux64_openblas    conda-forge\nlibcrc32c                 1.1.2                h9c3ff4c_0    conda-forge\nlibcurl                   8.7.1                hca28451_0    conda-forge\nlibdeflate                1.20                 hd590300_0    conda-forge\nlibedit                   3.1.20191231         he28a2e2_2    conda-forge\nlibev                     4.33                 hd590300_2    conda-forge\nlibevent                  2.1.12               hf998b51_1    conda-forge\nlibexpat                  2.6.2                h59595ed_0    conda-forge\nlibffi                    3.4.2                h7f98852_5    conda-forge\nlibgcc-ng                 13.2.0               h77fa898_7    conda-forge\nlibgcrypt                 1.10.3               hd590300_0    conda-forge\nlibgettextpo              0.22.5               h59595ed_2    conda-forge\nlibgettextpo-devel        0.22.5               h59595ed_2    conda-forge\nlibgfortran-ng            13.2.0               h69a702a_7    conda-forge\nlibgfortran5              13.2.0               hca663fb_7    conda-forge\nlibgirepository           1.80.1               h003a4f0_0    conda-forge\nlibglib                   2.80.2               hf974151_0    conda-forge\nlibgomp                   13.2.0               h77fa898_7    conda-forge\nlibgoogle-cloud           2.23.0               h9be4e54_1    conda-forge\nlibgoogle-cloud-storage   2.23.0               hc7a4891_1    conda-forge\nlibgpg-error              1.49                 h4f305b6_0    conda-forge\nlibgrpc                   1.62.2               h15f2491_0    conda-forge\nlibiconv                  1.17                 hd590300_2    conda-forge\nlibjpeg-turbo             3.0.0                hd590300_1    conda-forge\nliblapack                 3.9.0       22_linux64_openblas    conda-forge\nlibllvm16                 16.0.6               hb3ce162_3    conda-forge\nlibnghttp2                1.58.0               h47da74e_1    conda-forge\nlibnl                     3.9.0                hd590300_0    conda-forge\nlibnsl                    2.0.1                hd590300_0    conda-forge\nlibnuma                   2.0.18               h4ab18f5_2    conda-forge\nlibopenblas               0.3.27      pthreads_h413a1c8_0    conda-forge\nlibparquet                16.0.0           h6a7eafb_1_cpu    conda-forge\nlibpng                    1.6.43               h2797004_0    conda-forge\nlibprotobuf               4.25.3               h08a7969_0    conda-forge\nlibre2-11                 2023.09.01           h5a48ba9_2    conda-forge\nlibsecret                 0.18.8               h329b89f_2    conda-forge\nlibsodium                 1.0.18               h36c2ea0_1    conda-forge\nlibsqlite                 3.45.3               h2797004_0    conda-forge\nlibssh2                   1.11.0               h0841786_0    conda-forge\nlibstdcxx-ng              13.2.0               hc0a3c3a_7    conda-forge\nlibthrift                 0.19.0               hb90f79a_1    conda-forge\nlibtiff                   4.6.0                h1dd3fc0_3    conda-forge\nlibutf8proc               2.8.0                h166bdaf_0    conda-forge\nlibuuid                   2.38.1               h0b41bf4_0    conda-forge\nlibwebp-base              1.4.0                hd590300_0    conda-forge\nlibxcb                    1.15                 h0b41bf4_0    conda-forge\nlibxcrypt                 4.4.36               hd590300_1    conda-forge\nlibxgboost                2.0.3            cpu_h7afb2cf_4    conda-forge\nlibxml2                   2.12.6               h232c23b_2    conda-forge\nlibzlib                   1.2.13               hd590300_5    conda-forge\nlz4-c                     1.9.4                hcb278e6_0    conda-forge\nmarkdown-it-py            3.0.0              pyhd8ed1ab_0    conda-forge\nmarkupsafe                2.1.5           py310h2372a71_0    conda-forge\nmatplotlib-base           3.8.4           py310h62c0568_0    conda-forge\nmatplotlib-inline         0.1.7              pyhd8ed1ab_0    conda-forge\nmdurl                     0.1.2              pyhd8ed1ab_0    conda-forge\nmistune                   3.0.1                    pypi_0    pypi\nmore-itertools            9.1.0              pyhd8ed1ab_0    conda-forge\nmsal                      1.28.0             pyhd8ed1ab_0    conda-forge\nmsal_extensions           1.1.0           py310hff52083_1    conda-forge\nmunkres                   1.1.4              pyh9f0ad1d_0    conda-forge\nmypy_extensions           1.0.0              pyha770c72_0    conda-forge\nnbclient                  0.8.0                    pypi_0    pypi\nnbconvert                 7.6.0                    pypi_0    pypi\nnbformat                  5.9.0                    pypi_0    pypi\nncurses                   6.5                  h59595ed_0    conda-forge\nnest-asyncio              1.5.6                    pypi_0    pypi\nnotebook-shim             0.2.3                    pypi_0    pypi\nnumpy                     1.26.4          py310hb13e2d6_0    conda-forge\nopenjpeg                  2.5.2                h488ebb8_0    conda-forge\nopenssl                   3.3.0                hd590300_0    conda-forge\norc                       2.0.0                h17fec99_1    conda-forge\norjson                    3.10.3          py310he421c4c_0    conda-forge\noutcome                   1.2.0                    pypi_0    pypi\noverrides                 7.3.1                    pypi_0    pypi\npackaging                 24.0               pyhd8ed1ab_0    conda-forge\npandas                    1.5.3                    pypi_0    pypi\npandocfilters             1.5.0                    pypi_0    pypi\nparso                     0.8.4              pyhd8ed1ab_0    conda-forge\npathspec                  0.12.1             pyhd8ed1ab_0    conda-forge\npatsy                     0.5.6              pyhd8ed1ab_0    conda-forge\npcre2                     10.43                hcad00b1_0    conda-forge\npexpect                   4.9.0              pyhd8ed1ab_0    conda-forge\npickleshare               0.7.5                   py_1003    conda-forge\npillow                    10.3.0          py310hf73ecf8_0    conda-forge\npip                       24.0               pyhd8ed1ab_0    conda-forge\npixman                    0.43.2               h59595ed_0    conda-forge\nplatformdirs              4.2.1              pyhd8ed1ab_0    conda-forge\nplotly                    5.22.0             pyhd8ed1ab_0    conda-forge\npolars                    0.20.25         py310h031f9ce_0    conda-forge\npooch                     1.8.1              pyhd8ed1ab_0    conda-forge\nportalocker               2.8.2           py310hff52083_1    conda-forge\nprometheus-client         0.17.0                   pypi_0    pypi\nprompt-toolkit            3.0.38                   pypi_0    pypi\nprompt_toolkit            3.0.42               hd8ed1ab_0    conda-forge\npsutil                    5.9.8           py310h2372a71_0    conda-forge\npthread-stubs             0.4               h36c2ea0_1001    conda-forge\nptyprocess                0.7.0              pyhd3deb0d_0    conda-forge\npure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge\npy-xgboost                2.0.3          cpu_pyh995e691_4    conda-forge\npyarrow                   16.0.0          py310h17c5347_0    conda-forge\npyarrow-core              16.0.0      py310h6f79a3a_0_cpu    conda-forge\npycairo                   1.26.0          py310hda9f760_0    conda-forge\npycparser                 2.21                     pypi_0    pypi\npydantic                  2.7.1              pyhd8ed1ab_0    conda-forge\npydantic-core             2.18.2          py310he421c4c_0    conda-forge\npygments                  2.15.1                   pypi_0    pypi\npygobject                 3.48.2          py310h30b043a_0    conda-forge\npyjwt                     2.8.0              pyhd8ed1ab_1    conda-forge\npyparsing                 3.1.2              pyhd8ed1ab_0    conda-forge\npyrsistent                0.19.3                   pypi_0    pypi\npysocks                   1.7.1              pyha2e5f31_6    conda-forge\npython                    3.10.14      hd12c33a_0_cpython    conda-forge\npython-dateutil           2.9.0              pyhd8ed1ab_0    conda-forge\npython-json-logger        2.0.7                    pypi_0    pypi\npython-multipart          0.0.9              pyhd8ed1ab_0    conda-forge\npython-tzdata             2024.1             pyhd8ed1ab_0    conda-forge\npython_abi                3.10                    4_cp310    conda-forge\npytz                      2024.1             pyhd8ed1ab_0    conda-forge\npyyaml                    6.0                      pypi_0    pypi\npyzmq                     25.1.0                   pypi_0    pypi\nrdma-core                 51.0                 hd3aeb46_0    conda-forge\nre2                       2023.09.01           h7f4b329_2    conda-forge\nreadline                  8.2                  h8228510_1    conda-forge\nregex                     2023.6.3                 pypi_0    pypi\nrequests                  2.31.0             pyhd8ed1ab_0    conda-forge\nretrying                  1.3.3                      py_2    conda-forge\nrfc3339-validator         0.1.4                    pypi_0    pypi\nrfc3986-validator         0.1.1                    pypi_0    pypi\nrich                      13.7.1             pyhd8ed1ab_0    conda-forge\ns2n                       1.4.13               he19d79f_0    conda-forge\nscikit-learn              1.4.2           py310h1fdf081_0    conda-forge\nscipy                     1.13.0          py310h93e2701_1    conda-forge\nseaborn                   0.13.2               hd8ed1ab_2    conda-forge\nseaborn-base              0.13.2             pyhd8ed1ab_2    conda-forge\nselenium                  4.10.0                   pypi_0    pypi\nsend2trash                1.8.2                    pypi_0    pypi\nsetuptools                69.5.1             pyhd8ed1ab_0    conda-forge\nshellingham               1.5.4              pyhd8ed1ab_0    conda-forge\nsix                       1.16.0             pyh6c4a22f_0    conda-forge\nsnappy                    1.2.0                hdb0a2a9_1    conda-forge\nsniffio                   1.3.1              pyhd8ed1ab_0    conda-forge\nsortedcontainers          2.4.0                    pypi_0    pypi\nsoupsieve                 2.4.1                    pypi_0    pypi\nsqlalchemy                1.3.24                   pypi_0    pypi\nsqlalchemy-utils          0.37.9                   pypi_0    pypi\nstack_data                0.6.2              pyhd8ed1ab_0    conda-forge\nstarlette                 0.37.2             pyhd8ed1ab_0    conda-forge\nstatsmodels               0.14.1          py310h1f7b6fc_0    conda-forge\ntenacity                  8.3.0              pyhd8ed1ab_0    conda-forge\nterminado                 0.17.1                   pypi_0    pypi\ntestcontainers            3.7.1                    pypi_0    pypi\nthreadpoolctl             3.5.0              pyhc1e730c_0    conda-forge\nthrift                    0.16.0                   pypi_0    pypi\ntinycss2                  1.2.1                    pypi_0    pypi\ntk                        8.6.13       noxft_h4845f30_101    conda-forge\ntomli                     2.0.1              pyhd8ed1ab_0    conda-forge\ntornado                   6.3.2                    pypi_0    pypi\ntraitlets                 5.9.0                    pypi_0    pypi\ntrio                      0.22.0                   pypi_0    pypi\ntrio-websocket            0.10.3                   pypi_0    pypi\ntyper                     0.12.3             pyhd8ed1ab_0    conda-forge\ntyper-slim                0.12.3             pyhd8ed1ab_0    conda-forge\ntyper-slim-standard       0.12.3               hd8ed1ab_0    conda-forge\ntyping-extensions         4.11.0               hd8ed1ab_0    conda-forge\ntyping_extensions         4.11.0             pyha770c72_0    conda-forge\ntzdata                    2024a                h0c530f3_0    conda-forge\nucx                       1.16.0               h209287a_5    conda-forge\nujson                     5.9.0           py310hc6cd4ac_0    conda-forge\nunicodedata2              15.1.0          py310h2372a71_0    conda-forge\nuri-template              1.3.0                    pypi_0    pypi\nurllib3                   2.2.1              pyhd8ed1ab_0    conda-forge\nuvicorn                   0.29.0          py310hff52083_0    conda-forge\nwcwidth                   0.2.6                    pypi_0    pypi\nwebcolors                 1.13                     pypi_0    pypi\nwebencodings              0.5.1                    pypi_0    pypi\nwebsocket-client          1.5.2                    pypi_0    pypi\nwerkzeug                  3.0.3              pyhd8ed1ab_0    conda-forge\nwheel                     0.43.0             pyhd8ed1ab_1    conda-forge\nwrapt                     1.15.0                   pypi_0    pypi\nwsproto                   1.2.0                    pypi_0    pypi\nxgboost                   2.0.3          cpu_pyhb8f9a19_4    conda-forge\nxorg-kbproto              1.0.7             h7f98852_1002    conda-forge\nxorg-libice               1.1.1                hd590300_0    conda-forge\nxorg-libsm                1.2.4                h7391055_0    conda-forge\nxorg-libx11               1.8.9                h8ee46fc_0    conda-forge\nxorg-libxau               1.0.11               hd590300_0    conda-forge\nxorg-libxdmcp             1.1.3                h7f98852_0    conda-forge\nxorg-libxext              1.3.4                h0b41bf4_2    conda-forge\nxorg-libxrender           0.9.11               hd590300_0    conda-forge\nxorg-renderproto          0.11.1            h7f98852_1002    conda-forge\nxorg-xextproto            7.3.0             h0b41bf4_1003    conda-forge\nxorg-xproto               7.0.31            h7f98852_1007    conda-forge\nxz                        5.2.6                h166bdaf_0    conda-forge\nzeromq                    4.3.5                h75354e8_3    conda-forge\nzipp                      3.17.0             pyhd8ed1ab_0    conda-forge\nzlib                      1.2.13               hd590300_5    conda-forge\nzstd                      1.5.6                ha6fb4c9_0    conda-forge\n</code></pre>"},{"location":"Python/Package/Namespace/","title":"Namespace","text":""},{"location":"Python/Package/Namespace/#merging-two-python-packages-into-one-namespace","title":"Merging two Python packages into one namespace","text":"<p>Option 1: pkgutil (legal) - Use a special <code>__init__.py</code> file at the root of the namespace package that explicitly tells Python to extend its path - <code>from pkgutil import extend_path;__path__ = extend_path(__path__, __name__)</code> or - <code>__path__ = __import__('pkgutil').extend_path(__path__, __name__)</code></p> <p>Option 2: no <code>__init__.py</code> - omit the <code>__init__.py</code> file in the top-level directory of the shared namespace - packaging with <code>setuptools</code>, have to use <code>find_namespace_packages</code> <pre><code># In setup.py or pyproject.toml\nfrom setuptools import setup, find_namespace_packages\nsetup(\n    name='my_company-core',\n    ...\n    packages=find_namespace_packages(where='src'),\n    package_dir={'': 'src'},\n)\n</code></pre></p>"},{"location":"Python/Package/Namespace/#inspect-namespaces-in-a-package","title":"inspect namespaces in a package","text":"<ul> <li>use <code>dir</code> to list all objects</li> <li>use <code>inspect</code> to check object types <pre><code>import inspect\nimport polars as pl\n\ndef list_polars_namespaces_to_file(module, file_object, indent=0, max_level=5):\n    \"\"\"\n    Recursively lists all \"namespaces\" (sub-modules and significant objects)\n    within a given module, writing to a file with a maximum level depth.\n    \"\"\"\n    if indent &gt;= max_level:\n        return\n    prefix = \"  \" * indent\n    for name in sorted(dir(module)):\n        if name.startswith(\"_\"):  # Skip private/internal attributes\n            continue\n        obj = getattr(module, name)\n        if inspect.ismodule(obj) and obj.__name__.startswith('polars.'):\n            # If it's a sub-module within polars\n            file_object.write(f\"{prefix}{name}\\n\")\n            list_polars_namespaces_to_file(obj, file_object, indent + 1, max_level)\n        elif inspect.isclass(obj) or inspect.isfunction(obj) or isinstance(obj, (int, float, str, bool, list, dict, set)) or (\n            hasattr(obj, '__module__') and obj.__module__ and obj.__module__.startswith('polars.')\n        ):\n            # Consider classes, functions, common data types, and other polars-related objects\n            file_object.write(f\"{prefix}{name}\\n\")\n\nmax_depth = 5\noutput_filename = \"./polars_namespaces.txt\"\nwith open(output_filename, \"w\") as f:\n    f.write(\"polars\\n\")\n    list_polars_namespaces_to_file(pl, f, indent=1, max_level=max)\n</code></pre></li> </ul>"},{"location":"Python/Package/Oracle/","title":"Oracle","text":"<p>Old one: https://oracle.github.io/python-cx_Oracle/samples/tutorial/Python-and-Oracle-Database-Scripting-for-the-Future.html</p> <p>New one: https://oracle.github.io/python-oracledb/</p>"},{"location":"Python/Package/Package/","title":"Package","text":""},{"location":"Python/Package/Package/#create-package","title":"create package","text":"<p>https://packaging.python.org/en/latest/</p>"},{"location":"Python/Package/Package/#setuppy","title":"setup.py","text":"<p>https://packaging.python.org/en/latest/guides/distributing-packages-using-setuptools/</p> <p><code>setup.py</code> serves two primary functions: - It\u2019s the file where various aspects of your project are configured. The primary feature of setup.py is that it contains a global setup() function. The keyword arguments to this function are how specific details of your project are defined. The most relevant arguments are explained in the section below. - It\u2019s the command line interface for running various commands that relate to packaging tasks. To get a listing of available commands, run python setup.py --help-commands.</p> <p>Create this <code>setup.py</code> before run <code>versioneer install</code> <pre><code>from setuptools import setup\nimport versioneer\n\nsetup(\n    version=versioneer.get_version(),\n    cmdclass=versioneer.get_cmdclass(),\n)\n</code></pre></p>"},{"location":"Python/Package/Package/#setupcfg","title":"setup.cfg","text":"<p><code>setup.cfg</code> is an ini file that contains option defaults for setup.py commands.</p> <p>Use <code>git</code> versioneer:\\ https://github.com/python-versioneer/python-versioneer/blob/master/INSTALL.md</p>"},{"location":"Python/Package/Package/#readmemd","title":"README.md","text":"<p>All projects should contain a readme file that covers the goal of the project.</p>"},{"location":"Python/Package/Package/#manifestin","title":"MANIFEST.in","text":"<p>A <code>MANIFEST.in</code> is needed when you need to package additional files that are not automatically included in a source distribution.</p>"},{"location":"Python/Package/Package/#licensetxt","title":"LICENSE.txt","text":"<p>Every package should include a license file detailing the terms of distribution.</p>"},{"location":"Python/Package/Package/#attempted-relative-import-with-no-known-parent-package","title":"attempted relative import with no known parent package","text":"<p><pre><code>from ..utils import check_date\n</code></pre> The previous line will crash with the error if run the code from a subfolder and import the methods from the parent folder. In this case, function call must be from the parent folder.</p>"},{"location":"Python/Package/Package/#nested-package","title":"nested package","text":"<p>When install a package into a subdirectory of another package as an editable package via pip, we need to use <code>pkgutil</code>. - https://github.com/pypa/sample-namespace-packages/tree/master/pkgutil - in <code>__init__.py</code> of the shared folder must only congtain: <code>__path__ = __import__('pkgutil').extend_path(__path__, __name__)</code> - nested namespaces must contain an identical <code>__init__.py</code> - the directory name must match the name given in the <code>setup.py</code></p>"},{"location":"Python/Package/Pyproject/","title":"pyproject.toml","text":""},{"location":"Python/Package/Pyproject/#use-pyprojecttoml-instead-of-setupcfg","title":"use <code>pyproject.toml</code> instead of <code>setup.cfg</code>:","text":"<p>https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html <pre><code>[build-system]\nrequires = ['setuptools&gt;=42', 'versioneer-518']\nbuild-backend = 'setuptools.build_meta'\n\n[project]\nname = \"pkg\"\ndescription = 'My package description'\ndependencies = [\n    \"requests\",\n    'importlib-metadata; python_version&lt;\"3.8\"',\n]\n\n[project.urls]\nhomepage = 'https://pkg.org'\ndocumentation = 'https://pkg.org/docs/'\nrepository = 'https://github.com/usr/repo'\n\n[project.entry-points]\npkg-run = 'my.pkg.main:cli'\n\n[tool.setuptools]\ninclude-package-data = true\n\n[tool.setuptools.packages.find]\ninclude = ['pkg', 'pkg.*']\nnamespaces = false\n\n[tool.versioneer]\nVCS = \"git\"\nstyle = \"pep440\"\nversionfile_source = \"pkg/_version.py\"\nversionfile_build = \"pkg/_version.py\"\ntag_prefix = \"v\"\nparentdir_prefix = \"pkg-\"\n\n[tool.black]\nline-length = 88\ntarget-version = ['py39']\nskip-string-normalization = true #disable string double quotes normalization\ninclude = '\\.pyi?$'\npreview = false\n</code></pre></p>"},{"location":"Python/Package/Setup/","title":"Setup","text":"<p>https://ianhopkinson.org.uk/2022/02/understanding-setup-py-setup-cfg-and-pyproject-toml-in-python/</p> <p>https://setuptools.pypa.io/en/latest/userguide/declarative_config.html</p>"},{"location":"Python/Package/Setup/#setuppy","title":"setup.py","text":"<pre><code>from setuptools import setup\nimport versioneer\n\nif __name__ == '__main__':\n    setup(\n        version=versioneer.get_version(),\n        cmdclass=versioneer.get_cmdclass(),\n    )\n</code></pre>"},{"location":"Python/Package/Setup/#setupcfg","title":"setup.cfg","text":"<pre><code>[metadata]\nname = dev\ndescription = My dev repo\nurl = https://github.com/usr/repo\n\n[options]\ninclude_package_data = True\npackage_dir =\n    = src\npackages = find:\n\n[options.packages.find]\nwhere = src\n\n[options.entry_points]\nconsole_scripts =\n    pkg-run = my.pkg.main:cli\n</code></pre>"},{"location":"Python/Package/Utils/","title":"Utils","text":""},{"location":"Python/Package/Utils/#list-sub-packages","title":"list sub packages","text":"<pre><code>import pkgutil\nfor m in pkgutil.iter_modules(pytoy.__path__, pytoy.__name__ + \".\"): print(m)\n</code></pre>"},{"location":"Python/Package/Versioneer/","title":"Versioneer","text":"<p>https://stackoverflow.com/questions/53724616/update-version-number-with-versioneer-and-github</p> <p><code>versioneer</code> will get the version number from git tag + state of the repository.</p> <p>https://jacobtomlinson.dev/posts/2020/versioning-and-formatting-your-python-code/</p>"},{"location":"Python/Package/Versioneer/#install","title":"install","text":"<pre><code>mamba install versioneer -y\n</code></pre>"},{"location":"Python/Package/Versioneer/#python-package-version-settings","title":"Python package version settings","text":"<p>Option 1: versioneer (old way) - <code>versioneer install --vendor</code>: generate<code>versioneer.py</code> and <code>_version.py</code> files directly into project's source tree - <code>_version.py</code> dynamically asks the VCS for version information at import time - for source distributions, it replaces the dynamic <code>_version.py</code> with a static one containing the computed version - requires <code>setup.py</code>, <code>setup.cfg/pyproject.toml</code> - <code>versioneer.py</code> in project folder and <code>_version.py</code> in base folder</p> <p>Option 2: setuptools_scm - More modern, works cleanly with pyproject.toml, Git-based, zero-config</p>"},{"location":"Python/Package/Versioneer/#versioneer-vs-setuptools-smc","title":"versioneer vs setuptools-smc","text":"<ul> <li>https://setuptools-git-versioning.readthedocs.io/en/stable/comparison.html</li> <li>https://rse.shef.ac.uk/blog/2023-09-18-python-packaging/#:~:text=Setuptools%2Dscm,on%20now%20deprecated%20setup.py%20.</li> <li>https://discuss.python.org/t/how-to-single-source-a-packages-version-when-publishing-to-pypi-using-github-action/50982/17</li> </ul>"},{"location":"Python/Package/Versioneer/#step-1-create-setuppy","title":"step 1: create <code>setup.py</code>","text":"<p><pre><code>from setuptools import setup\nimport versioneer\n\nsetup(\n    version=versioneer.get_version(),\n    cmdclass=versioneer.get_cmdclass(),\n)\n</code></pre> All other info can be put into the <code>setup.cfg</code> file.</p>"},{"location":"Python/Package/Versioneer/#step-2-create-setupcfg","title":"step 2: create <code>setup.cfg</code>","text":"<p>Still not support pyproject.toml <pre><code>[versioneer]\nVCS = git\nstyle = pep440\nversionfile_source = pkg/_version.py\nversionfile_build = pkg/_version.py\ntag_prefix =\nparentdir_prefix = pkg-\n</code></pre></p>"},{"location":"Python/Package/Versioneer/#step-3-create-pkg__init__py","title":"step 3: create <code>pkg/__init__.py</code>","text":"<p>run <code>versioneer install --vendor</code></p>"},{"location":"Python/Pandas/Advance/","title":"Advance","text":""},{"location":"Python/Pandas/Advance/#adv","title":"adv","text":""},{"location":"Python/Pandas/Advance/#interpolation","title":"interpolation","text":"<pre><code>df = df.set_index(dts)\ndf = df.set_index('fld')\ndf = df.reindex(dts, fill_value=0)\ndf = df.reindex(dts, method=method)\n</code></pre>"},{"location":"Python/Pandas/Advance/#first-val-v","title":"first val &gt;= v","text":"<pre><code>i0 = np.searchsorted(dts, dt0, side='left') #a[i-1] &lt; v &lt;= a[i]\ni1 = np.searchsorted(dts, dt1, side='right') - 1 #a[i-1] &lt;= v &lt; a[i]\n</code></pre>"},{"location":"Python/Pandas/Advance/#num-of-nan-in-a-col","title":"num of nan in a col","text":"<pre><code>df.isna().sum()\ndf['fd'].isna().sum()\n</code></pre>"},{"location":"Python/Pandas/Advance/#bin-label","title":"bin label","text":"<pre><code>#label data according to provided bins\nlbls_new = pd.cut(x=col_dat, bins=bins, include_lowest=True, right=False, labels=lbls).astype(int)\n\n#digitize: assign the index according to provided bins\nbin_ind = np.digitize(val, bins=[10, 20, 40, 50], right=True)\n</code></pre>"},{"location":"Python/Pandas/Advance/#filter-col","title":"filter col","text":"<pre><code>df2 = df[df['tech'].str.contains('|'.join(techs))]\n</code></pre>"},{"location":"Python/Pandas/Advance/#combine-rowscols","title":"combine rows/cols","text":"<pre><code>#same cols append rows\ndf = pd.concat([df1,df2],axis=0,ignore_index=True) #don't use append, too slow\n#same number of rows append cols\ndf = pd.concat([df1.reset_index(drop=True),df2.reset_index(drop=True)],axis=1)\n\n#NOTE: concat drops the name of the merge axis when not aligned\n</code></pre>"},{"location":"Python/Pandas/Advance/#merge","title":"merge","text":"<p>if the col is obj type, the merge does not work!!!\\ https://www.datasciencebytes.com/bytes/2014/11/27/when-joins-go-wrong-check-data-types/</p> <p>merge has similar functionality as the the sql join (inner, left, right, and outer).</p> <p>https://stackoverflow.com/questions/53645882/pandas-merging-101 <pre><code>#left join\ndf = df_left.merge(df_right, how='left', on='user_id')\n\n#left join if null\ndf = df_left.merge(df_right, how='outer', on=['id'], indicator='i')\ndf = df.query('i == \"left_only\"').drop('i', 1)\n\n#inner join\ndf = pd.merge(left, right, how=\"left\", on=[\"key1\", \"key2\"])\n</code></pre></p>"},{"location":"Python/Pandas/Advance/#pivot","title":"pivot","text":"<pre><code>df['dt'] = pd.to_datetime(df['settlementdate'], format='%Y-%m-%d %H:%M', utc=False)\n\ndf1 = pd.DataFrame()\ndf1['date'] = df['dt'].dt.date\ndf1['hour'] = df['dt'].dt.hour\ndf1['value'] = np.ones(df1.shape[0])\n\ndf.pivot_table(columns='id',values='val',aggfunc='mean')\n\ndf2 = pd.pivot_table(df1, index=['date'],\n  columns=['hour'], values=['value'], aggfunc=np.sum, fill_value=0)\n</code></pre>"},{"location":"Python/Pandas/Advance/#aggregation","title":"aggregation","text":"<pre><code>#group\ndf.groupby('day')['total_bill'].mean()\ndf.groupby('day')['total_bill'].aggregate('mean')\ndg = df.groupby(['store'], as_index=False).agg({'col1':'mean', 'col2':'sum', 'col3':'sum'})\n\ndf = df_tips.groupby(by='sex').agg({'total_bill': ['count', 'mean', 'sum']})\ndf.rename(columns={'count': 'count_meals', 'mean': 'average_bill', 'sum': 'total_bills'})\n\n#filter\ndf.groupby('day').filter(lambda x : x['total_bill'].mean() &gt; 20)\ndf.groupby('day').filter(lambda x : x['total_bill'].mean() &gt; 20)['size'].mean()\n\n#get row with max value in col in each group\nind = df.groupby(['a','b'])['version'].transform(max) == df['version']\ndf_new = df[ind]\n\ndf_grouped = df.groupby(['a', 'b'], as_index=False).agg({'version':'max'})\ndf_grouped = df_grouped.rename(columns={'version':'version_max'})\ndf = pd.merge(df, df_grouped, how='left', on=['a', 'b'])\ndf = df[df['version'] == df['version_max']]\n\ndf.groupby('kind').agg({'col1': {'foo': sum()}, 'col2': {'mean': np.mean, 'std': np.std}})\ndf.groupby('kind').agg(min_height=('height', 'min'), max_weight=('weight', 'max'))\n</code></pre>"},{"location":"Python/Pandas/Advance/#transform","title":"transform","text":"<p>transform will created a new col with the same indices (populate the grouped value to all elements in each group)</p> <pre><code>#get row with max value in col in each group\nind = df.groupby(['a','b'])['version'].transform(max) == df['version']\ndf_new = df[ind]\n</code></pre>"},{"location":"Python/Pandas/Advance/#add-a-new-level-to-df-column","title":"add a new level to df column","text":"<pre><code>df.columns = pd.MultiIndex.from_product([['x'], df.columns])\n\n#from tuples\ntuples = [(1, 'red'), (1, 'blue'), (2, 'red'), (2, 'blue')]\npd.MultiIndex.from_tuples(tuples, names=('number', 'color'))\n\n#from arrays\narrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']]\npd.MultiIndex.from_arrays(arrays, names=('number', 'color'))\n</code></pre>"},{"location":"Python/Pandas/Advance/#sum-columns-by-level-in-multiindex-df","title":"Sum columns by level in MultiIndex df","text":"<pre><code>df.groupby(level=0, axis=1).sum()\ndf.sum(level=0, axis=1)\n</code></pre>"},{"location":"Python/Pandas/Basic/","title":"Basic","text":""},{"location":"Python/Pandas/Basic/#basic","title":"basic","text":"<pre><code>df.info()\ndf.describe()\n</code></pre>"},{"location":"Python/Pandas/Basic/#indexing","title":"indexing","text":"<p>https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html</p> <p>avoid what is called chained indexing (not just for performance): <pre><code>dfmi['one']['second'] #bad\ndfmi.loc[:, ('one', 'second')] #good\n</code></pre></p>"},{"location":"Python/Pandas/Basic/#change-column-names","title":"change column names","text":"<pre><code>df.columns = df.columns.str.lower()\n\n#common rows\ndf_index = np.intersect1d(df_percentage.index, df_repeated_EV_num.index)\ndf_index = df_percentage.index.intersection(df_repeated_EV_num.index)\ndf1 = df2.loc[df_index,:] * df3.loc[df_index,df2.columns]\n</code></pre>"},{"location":"Python/Pandas/Basic/#get-one-val-by-row-index-and-col-label","title":"get one val by row index and col label","text":"<pre><code>df.at[df.index[0], 'A']\ndf.loc[df.index[0], 'A']\ndf.get_value(df.index[0], 'A')\n\ndf.iat[0, df.columns.get_loc('A')]\ndf.iloc[0, df.columns.get_loc('A')]\ndf.get_value(0, df.columns.get_loc('A'), takable=True)\n</code></pre>"},{"location":"Python/Pandas/Basic/#replace-col-vals-by-dic","title":"replace col vals by dic","text":"<pre><code>#slower\ndf.replace({'col1': dic})\ndf['col1'].replace(dic, inplace=True)\n\n#faster\ndf['col1'].map(dic) #not matched will be changed to NaN\ndf['col1'].map(di).fillna(df['col1']) #keep unmatched values\n</code></pre>"},{"location":"Python/Pandas/Basic/#drop-duplicate-rows","title":"drop duplicate rows","text":"<pre><code>df = pd.DataFrame({\n    'col1':['A','B','A','B','C'],\n    'col2':[3,4,3,5,6],\n    'col3':[0,0.1,0.2,0.3,0.4],\n})\n\ndf.drop_duplicates(['col1','col2'])[['col1','col2']] #drop col3\n</code></pre>"},{"location":"Python/Pandas/Broadcast/","title":"broadcast","text":""},{"location":"Python/Pandas/Broadcast/#multiindex","title":"multiindex","text":"<p>reindex with level can broadcast the level. also df.div can broadcast only one level.</p> <p>not good, will repeat all combinations: <pre><code>va = np.array([[1991,1992],[6,7],[8,9],[1,2],[3,4]]).T\ndf = pd.DataFrame(va,columns=['a','b','c','x','y']).set_index(['a','b','c'])\n\nd2 = df.groupby(['a','c']).mean()\nd3 = d2.reindex(df.unstack('b').columns, axis=1, level=0)\nd3 = d3.stack(level=1).reorder_levels(['a','b','c'])\n</code></pre></p>"},{"location":"Python/Pandas/Broadcast/#broadcast-with-reindex","title":"broadcast with reindex","text":"<p>get grouped mean and then broadcast back to the original df rows. <pre><code>va = np.array([[1991,1991,1992],[6,7,7],[8,8,10],[1,2,3],[3,4,5]]).T\ndf = pd.DataFrame(va,columns=['a','b','c','x','y']).set_index(['a','b','c'])\n\nia = df.index.get_level_values('a')\nic = df.index.get_level_values('c')\nmi = pd.MultiIndex.from_arrays([ia,ic],names=['a','c'])\nd2 = df.groupby(['a','c']).mean()\nd2.reindex(mi)\nd2.reindex(df.index.droplevel('b')) #better? it seems reindex is faster than loc for single index\nd2.loc[df.index.droplevel('b'),:] #best?\n\n# we can also use merge to combine the grouped mean df to the original df without reindex\n# note df has index a,b,c and d2 has index a,c\ndf_merged = df.merge(d2, left_index=True, right_index=True)\n</code></pre></p>"},{"location":"Python/Pandas/Broadcast/#expand-df-date-ranges-to-individual-rows","title":"Expand df date ranges to individual rows","text":"<pre><code>df = (\n    df\n    .assign(\n        DATE=lambda x:\n            [\n                pd.date_range(row.STARTDATE, row.ENDDATE, freq='d')\n                for _,row in x.iterrows()\n            ]\n    )\n    .explode('DATE')\n    .drop(['STARTDATE', 'ENDDATE'], axis=1)\n)\n</code></pre> <p>At least 10x faster: <pre><code>def explode_date_range(\n    df: pd.DataFrame,\n    start_date_col: str,\n    end_date_col: str,\n    new_date_col: str = 'ts',\n    freq: str = '30T',\n    end_date_offset: pd.DateOffset = None,\n) -&gt; pd.DataFrame:\n    df = df.reset_index(drop=True)\n    if end_date_offset is not None:\n        df[end_date_col] += end_date_offset\n    # Get exploded timestamp column\n    dt = (\n        pd.concat([\n            pd.DataFrame({\n                'i': i,\n                'ts': pd.date_range(start=s, end=e, freq=freq)\n            })\n            for i, (s, e) in enumerate(zip(df[start_date_col], df[end_date_col]))\n        ])\n        .set_index('i')\n        .rename_axis(None, axis=0)\n    )\n    # Re-sample df based on new timestamp column\n    df = (\n        df\n        .drop(columns=[start_date_col, end_date_col])\n        .reindex(dt.index)\n        .assign(ds=dt.ts)\n        .rename(columns={'ds': new_date_col})\n    )\n    return df\n</code></pre></p>"},{"location":"Python/Pandas/Categorical/","title":"categorical","text":"<p>https://towardsdatascience.com/staying-sane-while-adopting-pandas-categorical-datatypes-78dbd19dcd8a</p>"},{"location":"Python/Pandas/Categorical/#benefit","title":"benefit","text":"<ul> <li>reduce memory usage</li> <li>runtime performance optimization</li> <li>library integrations</li> </ul>"},{"location":"Python/Pandas/Categorical/#convert","title":"convert","text":"<pre><code>fruit_cat = df['fruit'].astype('category')\nmy_cat = pd.Categorical(['foo', 'bar', 'baz', 'foo', 'bar'])\nmy_cat_2 = pd.Categorical.from_codes(codes, categories, ordered=True)\nmy_cat_2 = my_cats_2.as_ordered() #change to ordered\n</code></pre>"},{"location":"Python/Pandas/Categorical/#method","title":"method","text":"<p>as_ordered, as_unordered, rename_categories, set_categories, add_categories, remove_categories, remove_unused_categories, reorder_categories <pre><code>my_cat.cat.codes\nmy_cat.cat.categories\nmy_cat.cat.set_categories(['a','b','c','d'])\n#remove unobserved categories\ncat_s3 = cat_s[cat_s.isin(['a', 'b'])]\ncat_s3.cat.remove_unused_categories()\n</code></pre></p>"},{"location":"Python/Pandas/Categorical/#operating-on-categorical-columns","title":"operating on categorical columns","text":"<p>category can be much faster <pre><code>df['str'].str.upper()\ndf['cat'].str.upper() #but became string type again\ndf['cat'].cat.rename_categories(str.upper) #even faster and still cat type\n</code></pre> <code>df['cat'].dtype.categories</code> contains the unique categorical values thus can work on these values directly if there are no appropriate cat functions.</p>"},{"location":"Python/Pandas/Categorical/#merge","title":"merge","text":"<p><code>merge</code> dfs can lead category columns becoming string type  - merge(str, cat) =&gt; str - merge(cat, cat) =&gt; str - df.astype({'cat': df2['cat'].dtype}).merge(df2, on='cat') =&gt; cat</p> <pre><code>import numpy as np\nimport pandas as pd\n\nd1 = pd.DataFrame({\n    'id': [5, 6], \n    'value': pd.Categorical(['b', 'c']),\n})\nd2 = pd.DataFrame({\n    'id': [5, 3, 6],  \n    'value': pd.Categorical(['a', 'b', 'c']),\n})\n\ndf = pd.merge(d1, d2, on='id') # cat\ndf = pd.merge(d1, d2)          # str\ndf = pd.concat([d1, d2])       # str\n</code></pre>"},{"location":"Python/Pandas/Categorical/#groupby","title":"groupby","text":"<p>When group on a categorical datatype, by default it will group on every value in the datatype even if it isn't present in the data itself.</p> <p>Using <code>observed=True</code> to solve the issue: <code>df.groupby('cat', observed=True)['val'].mean()</code></p>"},{"location":"Python/Pandas/ColType/","title":"ColType","text":""},{"location":"Python/Pandas/ColType/#convert-col-to-float","title":"convert col to float","text":"<pre><code>df['val'] = pd.to_numeric(df['val'], errors='coerce')\ndf['val'] = df.astype({'val':'float'})\n</code></pre>"},{"location":"Python/Pandas/ColType/#convert-col-to-string","title":"convert col to string","text":"<p>using <code>map</code> to apply the function <pre><code>df[col] = df[col].map('{:.4f}'.format, na_action='ignore')\n</code></pre></p>"},{"location":"Python/Pandas/ColType/#empty-df-with-dtypes","title":"empty df with dtypes","text":"<pre><code>df = pd.DataFrame({\n    'id': pd.Series(dtype='int'),\n    'name': pd.Series(dtype='str'),\n    'value': pd.Series(dtype='float'),\n    'datetime': pd.Series(dtype='datetime64[ns]'),\n })\n\ndf = pd.DataFrame({\n    c: pd.Series(dtype=t) for c, t in {\n        'a': 'int',\n        'b': 'str',\n        'c': 'float',\n    }.items()\n})\n\n# slower\ncol_types = {\n    'id': 'int',\n    'name': 'str',\n    'value': 'float',\n    'datetime': 'datetime64[ns]',    \n}\ndf = pd.DataFrame(columns=col_types.keys()).astype(col_types)\n</code></pre>"},{"location":"Python/Pandas/Column/","title":"Column","text":""},{"location":"Python/Pandas/Column/#set-columns-in-chained-method","title":"set columns in chained method","text":"<pre><code>df = df.set_axis(columns, axis=1)\n</code></pre>"},{"location":"Python/Pandas/Column/#split-col-and-convert-to-int","title":"split col and convert to int","text":"<p>split a column in a DataFrame based on a delimiter (in this case, \"-\") and convert the first item of the split result to integers <pre><code>df['new_col'] = df['col_with_dash'].str.split('-').str[0].astype(int)\n</code></pre></p>"},{"location":"Python/Pandas/Column/#split-one-col-to-two","title":"split one col to two","text":"<pre><code>df[['cola','colb']] = df['col'].str.rsplit('_', 1, expand=True)\n</code></pre>"},{"location":"Python/Pandas/Column/#newcol-from-two-col-levels-year-and-quarter","title":"newcol from two col levels (year and quarter)","text":"<pre><code>qt_yr = [f'Q{quarter}{str(year)[2:]}' for year, quarter in df.columns]\n</code></pre>"},{"location":"Python/Pandas/Column/#merge-df-col-levels","title":"merge df col levels","text":"<pre><code>data = {\n    ('A', 'X'): [1, 2, 3],\n    ('A', 'Y'): [4, 5, 6],\n    ('B', 'X'): [7, 8, 9],\n    ('B', 'Y'): [10, 11, 12]\n}\ndf = pd.DataFrame(data)\ndf.columns = df.columns.map('_'.join)\ndf.columns = [f'{i}_{j}' if j != '' else f'{i}' for i,j in df.columns]\n\n# use pipe to chain the method\ndf = (\n    df\n    .pipe(\n        lambda x: x.set_axis(x.columns.map('_'.join), axis=1)\n    )\n)\n</code></pre>"},{"location":"Python/Pandas/Column/#newcol-based-on-conditions-on-another-col","title":"newcol based on conditions on another col","text":"<pre><code>#use np.select\ncons = [\n    (df['cnt'] &lt;= 2),\n    (df['cnt'] &gt; 2) &amp; (df['cnt'] &lt;= 9),\n    (df['cnt'] &gt; 9) &amp; (df['cnt'] &lt;= 15),\n    (df['cnt'] &gt; 15),\n]\n# create a list of the values we want to assign for each condition\nlbls = ['band1', 'band2', 'band3', 'band4'] #label for each condition\n# create a new column and use np.select to assign values to it\ndf['band'] = np.select(cons, lbls)\n\n#use pd.cut\n#bins = [-np.inf,2,9,15,np.inf] #or\nbins = [float('-inf'),2,9,15,float('inf')]\nlbls = ['band1', 'band2', 'band3', 'band4]\ndf['band'] = pd.cut(x=df['cnt'], bins=bins, right=True, labels=lbls).astype(str) #change category to str\n</code></pre>"},{"location":"Python/Pandas/DateRange/","title":"Date range","text":""},{"location":"Python/Pandas/DateRange/#startend-dates-to-intervals","title":"start/end dates to intervals","text":"<pre><code>pd.date_range(\"2023-07-01\", \"2023-07-02\", freq=\"30min\", inclusive='left')\n</code></pre>"},{"location":"Python/Pandas/DateRange/#split-date-range-to-intervals","title":"split date range to intervals","text":"<p>df1 has columns <code>datetime</code> and <code>value</code> and df2 has <code>start_date</code>, <code>end_date</code> and <code>value2</code>. The best way to add value2 to value based on the datetime <pre><code>import pandas as pd\n\n# Create sample dataframes\ndf1 = pd.DataFrame({\n    'datetime': ['2023-01-01 00:00:00', '2023-01-01 01:00:00', '2023-01-01 02:00:00'],\n    'value': [10, 20, 30]\n})\n\ndf2 = pd.DataFrame({\n    'start_date': ['2023-01-01 00:00:00', '2023-01-01 01:00:00'],\n    'end_date': ['2023-01-01 01:00:00', '2023-01-01 02:00:00'],\n    'value2': [1, 2]\n})\n\n# Convert datetime columns to datetime type\ndf1['datetime'] = pd.to_datetime(df1['datetime'])\ndf2['start_date'] = pd.to_datetime(df2['start_date'])\ndf2['end_date'] = pd.to_datetime(df2['end_date'])\n\n# Add a unique key to the df2 dataframe\ndf2['key'] = range(len(df2))\n\n# Merge dataframes based on datetime range\nmerged_df = pd.merge(df1, df2, how='left', left_on=[(df1['datetime'] &gt;= df2['start_date']) &amp; (df1['datetime'] &lt;= df2['end_date'])], right_on=['key'])\n\n# Calculate new value column by adding value and value2\nmerged_df['new_value'] = merged_df['value'] + merged_df['value2'].fillna(0)\n\n# Drop unnecessary columns\nmerged_df = merged_df.drop(['start_date', 'end_date', 'value2', 'key'], axis=1)\n\nprint(merged_df)\n</code></pre></p>"},{"location":"Python/Pandas/DateRange/#split-date-range-to-intervals-first","title":"split date range to intervals first?","text":"<p>In general, expanding the date range into timestamp intervals first and then doing a merge - might be faster if you have a large dataset, - since it reduces the number of comparisons that need to be made during the merge. - However, this approach can also significantly increase the size of your dataframe and consume a lot of memory, - so it might not be the best approach for all use cases</p> <p>In contrast, the conditional join approach is - more memory-efficient and - can be faster if you have a relatively small dataset or - if the number of overlapping date ranges is small. - However, if the number of overlapping date ranges is large, - then the conditional join approach might become slower compared to expanding the date range into timestamp intervals first.</p> <p>In general, it's a good idea to test both approaches on your specific data and use case to determine which one is faster and more efficient for your needs.</p>"},{"location":"Python/Pandas/DateTime/","title":"DateTime","text":""},{"location":"Python/Pandas/DateTime/#combine-date-and-time-into-one-col","title":"combine <code>date</code>  and <code>time</code> into one col","text":"<pre><code>df = df.assign(ts=lambda x: pd.to_datetime(x['date'].astype(str) + ' ' + x['time'].astype(str)))\n</code></pre>"},{"location":"Python/Pandas/Difference/","title":"Difference","text":"<p>https://stackoverflow.com/questions/19917545/comparing-two-pandas-dataframes-for-differences</p>"},{"location":"Python/Pandas/Difference/#dfequals","title":"df.equals","text":"<p>Will check both dtypes and data. Return a bool. not very reliable regarding floats</p>"},{"location":"Python/Pandas/Difference/#dfcompare","title":"df.compare","text":"<p>Will only check data. Return a dataframe. same shape and type. not reliable either (we need to consider rounding errors) <pre><code>df.compare(dp).empty #return True if same\n\n# workaround - define a round function\ndef round_df(df, decimal=6):\n    tmp = df.select_dtypes(include=[np.number])\n    df.loc[:, tmp.columns] = np.round(tmp)\n    return df\n</code></pre></p> <p>if all are numbers, using <code>np.allclose</code> <pre><code>np.allclose(df1, df2, rtol=1e-6, atol=1e-6)\n</code></pre></p>"},{"location":"Python/Pandas/Display/","title":"Display","text":""},{"location":"Python/Pandas/Display/#set-options","title":"set options","text":"<p>https://pandas.pydata.org/docs/reference/api/pandas.set_option.html</p>"},{"location":"Python/Pandas/Display/#dispaly-width","title":"Dispaly width","text":"<pre><code>pd.set_option('display.width', 240)\n</code></pre> <p>Other options: - display.max_rows - display.max_columns - display.max_colwidth</p>"},{"location":"Python/Pandas/Display/#styling-dataframes","title":"Styling DataFrames","text":"<p>https://pbpython.com/styling-pandas.html</p>"},{"location":"Python/Pandas/Display/#table-visualization","title":"Table Visualization","text":"<p>https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html</p>"},{"location":"Python/Pandas/Group/","title":"Group","text":""},{"location":"Python/Pandas/Group/#dfgroupbylambda-x-true","title":"df.groupby(lambda x: True)","text":"<p>group the entire df as one group. used when want to calculate agg values based on all rows.</p>"},{"location":"Python/Pandas/Group/#groupby-group_keysfalse","title":"groupby([], group_keys=False)","text":"<p>When - group_keys = True: the keys will be added into the results after <code>apply</code> - group_keys = False: the keys will be removed from the results after <code>apply</code> <pre><code>keys = ['x']\nd = pd.DataFrame([[0, 1, 3],[3, 1, 1],[3, 0, 0],[2, 3, 3],[2, 1, 0]], columns=list('xyz'))\nt = df.groupby(keys,group_keys=True).apply(lambda row: row[['y','z']])\nf = df.groupby(keys,group_keys=False).apply(lambda row: row[['y','z']])\nv = df.groupby(keys).apply(lambda row: row[['y','z']])\nprint('Original DataFrame:\\n',d)\nprint('Result [True]:\\n',t)\nprint('Result [False]:\\n',f)\nprint('Result [None]:\\n',v)\n</code></pre></p>"},{"location":"Python/Pandas/Group/#avoid-using-dfindexlevels-it-contains-all-before-the-groupby","title":"avoid using <code>df.index.levels[]</code> - it contains all before the groupby.","text":"<p>Test code: <pre><code>def test(df):\n    print(f'df.index.levels[0]: {df.index.levels[0]}')\n    print(f'df.index.get_level_values(0): {df.index.get_level_values(0)}')\n    return df\ndx = (\n    pd.DataFrame(\n        data={'id':[1,2],'name':['a', 'b'],'value':[5,6]}\n    )\n    .set_index(['id', 'name'])\n    .groupby('id')\n    .apply(test)\n)\n</code></pre> Results: <pre><code>df.index.levels[0]: Int64Index([1, 2], dtype='int64', name='id')\ndf.index.get_level_values(0): Int64Index([1], dtype='int64', name='id')\ndf.index.levels[0]: Int64Index([1, 2], dtype='int64', name='id')\ndf.index.get_level_values(0): Int64Index([2], dtype='int64', name='id')\n</code></pre></p>"},{"location":"Python/Pandas/Group/#group-by-month-quarter-year","title":"group by month, quarter, year","text":"<pre><code>freq = f'{summ_typ.upper()}S-JUL' #summ_typ = m,q,a [fy]\nbins = pd.date_range(f'2000-07-01', f'2020-07-01', freq=freq) #m,q,a start\nlbls = [dt.strftime('%Y-%m-%d') for dt in bins[:-1]]\ndf['cut'] = pd.cut(x=df.index, bins=bins, right=False, labels=lbls) #category\ndf.groupby(['id','cut']).agg(cnt=('val','count'), max=('val','max'), tot=('rev','sum'))\n</code></pre>"},{"location":"Python/Pandas/Group/#reshape-to-serials-and-group","title":"reshape to serials and group","text":"<pre><code>#df.columns = ['idx1','idx2','idx3','prd1','prd2','prd3']\n#set index\ndf = df.set_index(['idx2','idx2','idx3'])\n#stack val cols\ndf = df.stack().rename_axis(index={None:'prd'}).rename('val')\n#average values based on idx3\ndm = df.groupby(['idx1', 'idx2']).mean()\n#add back idx3\ndm = pd.concat([dm], keys=['avg'], names=['idx3'])\n#correct index order\ndm = dm.reorder_levels(['idx1','idx2','idx3'])\n#combine with original df\ndc = pd.concat([dm,df],axis=0,ignore_index=False)\n#expand prd back to cols\nd2 = dc.unstack('prd').reset_index()\n#d2.columns = ['idx1','idx2','idx3','prd1','prd2','prd3']\n</code></pre>"},{"location":"Python/Pandas/Group/#transform","title":"transform","text":"<p>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.transform.html</p> <p>calculate aggregated value in each group and copy back to each item.</p> <p>calculate the number of types in each group <code>c</code> as a new col: <pre><code>df['size'] = df.groupby('c')['type'].transform(len)\n</code></pre></p>"},{"location":"Python/Pandas/Index/","title":"index","text":"<p>The <code>.loc/[]</code> operations can perform enlargement when setting a non-existent key for that axis.</p>"},{"location":"Python/Pandas/Index/#dfdrop-vs-dfdroplevel","title":"<code>df.drop</code> vs <code>df.droplevel</code>","text":"<ul> <li><code>df.drop</code>: Removes rows or columns based on labels or indices.</li> <li><code>df.droplevel</code>: Drops levels from a multi-level index, simplifying it to a single-level index DataFrame.</li> </ul>"},{"location":"Python/Pandas/Index/#get-values-vs-slice-rows","title":"get values vs slice rows","text":"<p><pre><code>id  name\n1   a       2.0\n2   a       4.0\nName: val, dtype: float64\n</code></pre> Singe value in index returns results excluding the index: <pre><code>&gt;&gt;&gt; s[1]\nname\na    2.0\nName: val, dtype: float64\n</code></pre> Multiple values in index returns results including the index: <pre><code>&gt;&gt;&gt; s[[1]]\nid  name\n1   a       2.0\nName: val, dtype: float64\n</code></pre></p>"},{"location":"Python/Pandas/Index/#cols-to-multiindex","title":"cols to multiindex","text":"<pre><code>mi = df.columns.str.split('_', expand=True)\n</code></pre>"},{"location":"Python/Pandas/Index/#get-level-values","title":"get level values","text":"<pre><code>df.index.get_level_values(0)\ndf.index.get_level_values('level_2')\n</code></pre>"},{"location":"Python/Pandas/Index/#get-levle-unique-values","title":"get levle unique values","text":"<p>caveat:  <code>index.levels</code> does not return updated contents if any rows or columns have been deleted. The MultiIndex keeps all the defined levels of an index, even if they are not actually used, to avoid a recomputation of the levels in order to make slicing highly performant. <pre><code>df.index.levels[0]                               #fastest but should avoid to use, bad\ndf.index.levels[df.index.names.index('level_1')] #do not use it\ndf.index.get_level_values('Level_1').unique()    #slow\ndf.index.unique(level='level_1')                 #suggested\n</code></pre></p>"},{"location":"Python/Pandas/Index/#get-one-value","title":"get one value","text":"<pre><code>df.index.values[1][0] #fastest\ndf.index.values[1][df.index.names.index('id')] #by level name\ndf.index[1][0]  #fast\ndf.index.get_level_values(0)[1] #slow\n</code></pre>"},{"location":"Python/Pandas/Index/#rename-levels","title":"rename levels","text":"<pre><code>df = df.rename_axis(index={None:df.index.name})\ndf = df.rename_axis(columns={None:df.index.name})\n</code></pre>"},{"location":"Python/Pandas/Index/#reorder-levels","title":"reorder levels","text":"<pre><code>#custom reorder MultiIndex\nmi = pd.MultiIndex.from_product([['Value'],regions,scens], names=[None,'Region','Scenario'])\ndf = df.reorder_levels([None,'Region','Scenario'], axis=0).reindex(mi) #index (default)\ndf = df.reorder_levels([None,'Region','Scenario'], axis=1).reindex(mi) #columns\n</code></pre>"},{"location":"Python/Pandas/Index/#modify-one-level","title":"modify one level","text":"<pre><code>df.set_index(df.index.set_levels(df.index.get_level_values('a').dt.date, level='a'))\n</code></pre>"},{"location":"Python/Pandas/Index/#add-a-new-index","title":"add a new index","text":"<pre><code>df = pd.concat([df], keys=['New_Idx'], names=['idx0'])\n</code></pre>"},{"location":"Python/Pandas/Index/#drop-rows-with-duplicate-index","title":"drop rows with duplicate index","text":"<pre><code>df.pipe(lambda x: x[~x.index.duplicated(keep='first')])\n</code></pre>"},{"location":"Python/Pandas/Index/#get-last-day-of-each-month-in-index","title":"get last day of each month in index","text":"<pre><code>df.loc[df.groupby(df.index.to_period('M')).apply(lambda x: x.index.max())]\n</code></pre>"},{"location":"Python/Pandas/Index/#get-level-dtype-in-a-multiindex","title":"get level dtype in a multiindex","text":"<pre><code>[l.dtype for l in df.index.levels]\n</code></pre>"},{"location":"Python/Pandas/Index/#change-level-dtype-in-multiindex","title":"change level dtype in multiindex","text":"<pre><code>df.index = df.index.set_levels(idx.levels[-1].astype(int), level=-1)\n\n# another way (all to int or using a dict)\ndf.index = pd.MultiIndex.from_frame(\n    pd.DataFrame(index=df.index).reset_index().astype(int)\n)\n\n# chained method\ndf = df.set_index(new_index))\n</code></pre>"},{"location":"Python/Pandas/Issue/","title":"Issue","text":""},{"location":"Python/Pandas/Issue/#merge-drops-not-used-index-levels","title":"merge drops not used index levels","text":"<p>caveat: A <code>left merge</code> will drop the index levels not used</p>"},{"location":"Python/Pandas/Issue/#ffill-becomes-bfill","title":"ffill becomes bfill","text":"<p>https://stackoverflow.com/questions/63079668/pandas-how-to-explain-not-sorted-index-reindex-with-fill-values-ffill-behaviour <pre><code>df = pd.DataFrame(\n    np.linspace(1,9,9).reshape(3,3),\n    columns=list('abc'),\n    index=[3,2,1],\n)\n\ndf.reindex([0,1,2,3,4], method='ffill')\n     a    b    c\n0  7.0  8.0  9.0\n1  7.0  8.0  9.0\n2  4.0  5.0  6.0\n3  1.0  2.0  3.0\n4  NaN  NaN  NaN\n\n# this works\ndf.reindex([0, 1, 2, 3, 4]).fillna(method='ffill')\n</code></pre></p>"},{"location":"Python/Pandas/Issue/#pandaserrorsinvalidindexerror","title":"pandas.errors.InvalidIndexError","text":"<p>The index in d2 has duplcate rows. <pre><code>d1 = pd.DataFrame({\n    'date': ['2023-01-01', '2023-01-02'],\n    'code': [1, 1],\n    'value': [np.nan, np.nan],\n}).set_index(['date', 'code'])\nd2 = pd.DataFrame({\n    'date': ['2023-01-01', '2023-01-01'],\n    'code': [1, 1],\n    'value': [10, 20],    \n}).set_index(['date', 'code'])\nd1.fillna(d2)\n\n## remove the duplicates\nmask = d2.index.duplicated(keep='first')\nd2 = d2[~mask]\ndf_duplicate = d2[mask]\n</code></pre></p>"},{"location":"Python/Pandas/Merge/","title":"Merge","text":""},{"location":"Python/Pandas/Merge/#out-merge","title":"out merge","text":"<pre><code>d1.merge(d2, how='outer', on=['i', 'j'], suffixes=('_x', '_y'), indicator=True)`\n</code></pre>"},{"location":"Python/Pandas/Merge/#join-vs-merge","title":"join vs merge","text":"<ul> <li><code>join</code>(... on=[...]) joins index/columns of left to index keys of right</li> <li><code>merge</code> joins index/columns of left to index/columns of right - less restricted, also drops index not in <code>on</code></li> </ul>"},{"location":"Python/Pandas/Merge/#avoid-join-with-duplicate-index","title":"avoid <code>join</code> with duplicate index","text":"<p><code>join</code> with duplicate index will lead to <code>m x n</code> records (an outer join) - will blow out memory for large dataset. <pre><code>d1 = pd.DataFrame({'v1': [1,5,6]}, index=[1,3,3])\nd2 = pd.DataFrame({'v2': [4,6]}, index=[3,3])\nd1.join(d2, how='left')\n</code></pre></p>"},{"location":"Python/Pandas/Merge/#merge-on-index-is-2-3x-faster-than-on-column","title":"merge on index is 2-3x faster than on column","text":"<pre><code>import time\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(11)\nn = 1000000\n\na1 = np.arange(n)\nnp.random.shuffle(a1)\nd1 = (\n    pd.DataFrame({'x': a1, 'y': 2*a1})\n    # .astype({'x': 'category'})\n)\n\na2 = a1.copy()\nnp.random.shuffle(a2)\nd2 = (\n    pd.DataFrame({'x': a2, 'y': 2*a2})\n    # .astype({'x': 'category'})\n)\n\n# merge on column\nt0 = time.time()\nd1.merge(d2, how='left', left_on='x', right_on='x')\nprint(f'Merging on column time: {time.time()-t0:.3f}')\n\n# merge on index\nt0 = time.time()\nd1.set_index('x').merge(d2.set_index('x'), how='left', left_index=True, right_index=True)\nprint(f'Merging on index  time: {time.time()-t0:.3f}')\n</code></pre>"},{"location":"Python/Pandas/MethodChaining/","title":"Method Chaining","text":"<p>https://tomaugspurger.github.io/method-chaining.html</p>"},{"location":"Python/Pandas/NaN/","title":"NaN","text":""},{"location":"Python/Pandas/NaN/#fillna-for-selected-columns-within-groups","title":"fillna for selected columns within groups","text":"<pre><code>df['col'] = df.groupby('grp')['col'].ffill()\n</code></pre>"},{"location":"Python/Pandas/NaN/#dfgroupby","title":"df.groupby","text":"<p><code>df.groupby</code> mean does not support \"skipna=False\"</p>"},{"location":"Python/Pandas/NaN/#drop-nan-from-array","title":"drop nan from array","text":"<pre><code>arr[~pd.isnull(arr)]\n</code></pre>"},{"location":"Python/Pandas/NaN/#check-df-nan-values","title":"check df nan values","text":"<pre><code>sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n</code></pre>"},{"location":"Python/Pandas/Other/","title":"Other","text":""},{"location":"Python/Pandas/Other/#other","title":"other","text":"<p>https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/</p>"},{"location":"Python/Pandas/Other/#pd-version","title":"pd version","text":"<pre><code>#pandas version\npd.__version__\n#versions of Python and dependent packages and OS type\npd.show_versions()\n</code></pre>"},{"location":"Python/Pandas/Other/#change-display-width","title":"change display width","text":"<pre><code>from IPython.core.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container {width:90% !important;}&lt;/style&gt;\"))\npd.set_option('display.max_columns', 10)\n</code></pre>"},{"location":"Python/Pandas/Other/#adddelete-cols","title":"add/delete col/s","text":"<pre><code>#add a new col to the end\ndf['A'] = [1,2,3]\n#insert a col to a specific position\ndf.insert(loc=0, column='A', value=[1,2,3])\n\n#add columns\ndf = pd.concat([df1, df2], axis=1, ignore_index=True)\n\n#delete col 'area'\ndf = df.drop('area', axis=1)\n#using columns parameter\ndf = df.drop(columns='area')\n#keep df using inplace=True\ndf.drop(\"area\", axis=1, inplace=True)\n#delete multiple cols\ndf = df.drop(['A', 'B', 'C'], axis=1)\n\n#keep only a few cols\ndf1 = df[['A','B']]\n</code></pre>"},{"location":"Python/Pandas/Other/#adddelete-rows","title":"add/delete row/s","text":"<pre><code>#add row\ndf.append(df2, ignore_index=True)\ndf = pd.concat([df1,df2],ignore_index=True)\n\ndef df_insert_row(r, df):\n    d1 = pd.DataFrame(np.random.randn(1, df.shape[1]), columns=df.columns)\n    d2 = pd.concat([df.iloc[:r], d1, df.iloc[r:]], axis=0, ignore_index=True)\n\n#delete rows using iloc\ndf = df.iloc[5:,]\n\n#delete rows with labels 0,1,5\ndf = df.drop([0,1,5], axis=0)\n\n#delete the rows with label '2018Q1', index should be set\ndf = df.drop('2018Q1', axis=0)\n\n#drop rows with any NaN values\ndf.dropna()\n\n#drop only if ALL columns are NaN\ndf.dropna(how='all')\n\n#drop rows without at least two **not** NaN values\ndf.dropna(thresh=2)\n\n#drop rows only if NaN in specific columns\ndf.dropna(subset=[1,3,5])\ndf.dropna(subset=['a','d','g'])\n</code></pre>"},{"location":"Python/Pandas/Other/#rename-cols","title":"rename cols","text":"<pre><code>#rename multiple cols with dict\ndf.rename(columns={ 'area': 'place_name', 'Y2001': 'year_2001' }, inplace=True)\n#rename all cols using a function, to lower case\ndf.rename(columns=str.lower)\n#using lambda\ndf.rename(columns=lambda x: x.lower().replace(' ', '_')\n</code></pre>"},{"location":"Python/Pandas/Other/#change-cols-type","title":"change cols type","text":"<pre><code>#change all cols\ndf.astype('int32').dtypes\n#change a few cols\ndf.astype({'col1': 'int32'}).dtypes\n</code></pre>"},{"location":"Python/Pandas/Other/#replacedrop-nan","title":"replace/drop NaN","text":"<pre><code>#replace NaN\ndf = df.fillna(0)\n\ndf.fillna({'a':0, 'b':0}, inplace=True)\n\ndf[['a', 'b']] = df[['a','b']].fillna(value=0)\n\n#drop NaN\ndf = df.dropna(subset=['id'])\n#drop rows where all fields are missing\ndf.dropna(how='all')\n</code></pre>"},{"location":"Python/Pandas/Other/#get-duplicate","title":"get duplicate","text":"<pre><code>df_duplicate = df[df.duplicated(['a'], keep=False)]\n</code></pre>"},{"location":"Python/Pandas/Other/#drop-duplicate","title":"drop duplicate","text":"<pre><code>df = df.drop_duplicates(['first_name','last_name'], keep='first')\n</code></pre>"},{"location":"Python/Pandas/Other/#conditional-select-rows","title":"conditional select rows","text":"<pre><code>df.loc[(df['column_name'] &gt;= A) &amp; (df['column_name'] &lt;= B)]\n\n#get row indexes\ndf.index[df['v'].isin([1,5,6])].tolist()\ndf.index[df['v'].isin(['a','b','c'])].tolist()\n</code></pre>"},{"location":"Python/Pandas/Other/#nparray-to-dateframe","title":"np.Array to dateframe","text":"<pre><code>ar = np.zeros((2, 3),dtype=float)\ndf = pd.DataFrame(ar, columns=['A','B','C'])\n</code></pre>"},{"location":"Python/Pandas/Other/#reshape","title":"reshape","text":"<pre><code>#melt:join all cols\ndf = pd.DataFrame({'id': ['d1', 'd2', 'd3'],'p1': [3, 2, 1], 'p2': [7, 5, 3]})\ndf2 = pd.melt(df, id_vars=['id'], var_name='p', value_name='val')\n\n#stack: join all rows\ndf3 = df.stack()\n\n#unstack: create cols based on index\nindex = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),('two', 'a'), ('two', 'b')])\ns = pd.Series(np.arange(1.0, 5.0), index=index)\ndf1 = s.unstack(level=-1)\ndf0 = s.unstack(level=0)\ndf0.unstack()\n</code></pre>"},{"location":"Python/Pandas/Other/#concat","title":"concat","text":"<pre><code>#append rows\ndf = pd.concat([df1, df2, df3], axis=0, ignore_index=True)\n#append cols\ndf = pd.concat([df1,df2,df3],axis=1)\n#add index with name\ndf = pd.concat([d1,d2], keys=['City','Street'], names=['idx1','idx2'])\n</code></pre>"},{"location":"Python/Pandas/Other/#count-positive-in-each-row","title":"count positive in each row","text":"<pre><code>np.sum(np.where(df[cols] &gt; 0, 1, 0), axis=1)\n</code></pre>"},{"location":"Python/Pandas/Other/#repeat","title":"repeat","text":"<pre><code>df=pd.DataFrame(columns=['a','b'],data=[[1,2],[3,4]])\n#repeat each column 2x\ndf[np.repeat(df.columns.values,2)]\n</code></pre>"},{"location":"Python/Pandas/Other/#split","title":"split","text":"<pre><code>chunks = np.split(df, df.shape[0] / 10**6)\n</code></pre>"},{"location":"Python/Pandas/Other/#idx-to-col","title":"idx to col","text":"<pre><code>df = df.reset_index(level=0)\n</code></pre>"},{"location":"Python/Pandas/Other/#fill-gaps","title":"fill gaps","text":"<pre><code>df.reindex(dt_all, method='ffill')\n</code></pre>"},{"location":"Python/Pandas/Other/#lists-to-df","title":"lists to df","text":"<pre><code>df = pd.DataFrame({'a':l1,'b':l2,'c':l3})\ndf = pd.DataFrame(columns=['a','b','c'],data=np.column_stack([l1,l2,l3]))\n</code></pre>"},{"location":"Python/Pandas/Perf/","title":"Performance","text":""},{"location":"Python/Pandas/Perf/#unique-values","title":"unique values","text":"<p>https://stackoverflow.com/questions/13688784/python-speedup-np-unique</p> <p>The <code>numpy.unique()</code> is based on sorting (quicksort), and the <code>pandas.unique()</code> is based on hash table. - np.unique: sorted slower - pd.unique: not sorted faster</p> <p>General rules: - list of integers/strings: <code>list(set())</code> - pd.Series with integers: <code>pd.unique()</code> - pd.Series with strings: <code>pd.unique()</code> or <code>list(set(s.values))</code> [1.5x faster]</p> <pre><code>%timeit df.get('name').unique() \n1.64 ms \u00b1 25 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n\n%timeit df.name.unique()    \n1.66 ms \u00b1 80.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n\n%timeit df['name'].unique()  \n1.84 ms \u00b1 108 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n\n%timeit list(set(df['name']))\n3.23 ms \u00b1 368 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n%timeit list(set(df['name'].values))\n579 \u00b5s \u00b1 39.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n\n%timeit np.unique(df['name'].values)\n12.7 ms \u00b1 644 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre> <pre><code>import timeit\n\na1 = df.get('name').unique() #fastest\na2 = df.name.unique()        #ok\na3 = df['name'].unique()     #slow\n\nt1 = timeit.Timer(lambda: df.get('name').unique())\nprint(t1.timit(10)\n</code></pre>"},{"location":"Python/Pandas/Perf/#named-agg-is-slow","title":"named agg is slow","text":"<p>https://stackoverflow.com/questions/69799691/named-aggregations-with-pandas-group-by-agg-are-super-slow-why</p> <p>Once you pass in <code>lambda</code>, the operation is no longer vectorized across the groups even though it can be vectorized within each group.</p> <p>bad: <pre><code>df.groupby('A').agg(**{\n  'newname' : ('B', lambda x: x.sum(min_count=1))\n})\n</code></pre></p> <p>good <pre><code>df.groupby('A').sum(min_count=1)\n\nd = df.groupby('A')\npd.DataFrame({\n    'name-b1': d['B'].sum(min_count=1),\n    'name-b2': d['B'].size(),\n})\n</code></pre></p> <p>https://llllllllll.github.io/principles-of-performance/index.html</p> <p>https://speakerdeck.com/pycon2018/jake-vanderplas-performance-python-seven-strategies-for-optimizing-your-numerical-code</p> <p>https://speakerdeck.com/nnja/nina-zakharenko-the-basics-of-memory-management-in-python-north-bay-python-2018</p> <p>https://software.intel.com/en-us/articles/large-matrix-operations-with-scipy-and-numpy-tips-and-best-practices</p> <p>http://conference.scipy.org/proceedings/scipy2018/pdfs/anton_malakhov.pdf</p> <p>http://conference.scipy.org/proceedings/scipy2017/pdfs/oleksandr_pavlyk.pdf</p>"},{"location":"Python/Pandas/Perf/#dfwhere-vs-npwhere","title":"df.where vs np.where","text":"<p>No big differences: np.where will convert <code>datetime64[ns]</code> to object - use df.where if possible <pre><code># 0.6 ms for 48*365*1 rows\n# 60 ms for 48*365*100 rows\nd1 = d.assign(\n    f0=np.where(d['f0'] &lt; 0.1, 0, d['f0'])\n)\n# 1.0 ms for 48*365*1 rows\n# 66 ms for 48*365*100 rows\nd2 = d.assign(\n    f0=lambda x: x['f0'].where(x['f0'] &gt;= 0.1, 0)\n)\n# 1.5 ms for 48*365*1 rows\n# 73 ms for 48*365*100 rows\nd2 = d.assign(\n    f0=lambda x: x['f0'].mask(x['f0'] &lt; 0.1, 0)\n)\n</code></pre></p>"},{"location":"Python/Pandas/Pyarrow/","title":"Pyarrow","text":""},{"location":"Python/Pandas/Pyarrow/#tensorflow-does-not-support-arrow-backend","title":"tensorflow does not support arrow backend","text":""},{"location":"Python/Pandas/Pyarrow/#data-types","title":"data types","text":"<p>https://pandas.pydata.org/docs/user_guide/pyarrow.html</p> <ul> <li>string[pyarrow]: this is equivalent to pd.StringDtype('pyarrow') that can return NumPy-backed nullable types but slow</li> <li>pd.ArrowDtype(pa.string()): will return ArrowDtype much faster</li> <li>bool[pyarrow]</li> <li>int64[pyarrow]</li> <li>uint8[pyarrow]</li> <li>uint64[pyarrow]</li> <li>float32[pyarrow]</li> <li>time64[us][pyarrow]</li> <li>timestamp[s][pyarrow]? should use <code>pd.ArrowDtype(pa.timestamp('s'))</code></li> </ul> <p>pd.StringDtype('pyarrow'):  - This allows pandas to utilize PyArrow's memory-efficient string representation for the data - Additionally, it can return NumPy-backed nullable types, meaning it can handle <code>missing values</code> efficiently using NumPy arrays - string[pyarrow]: This is a shortcut alias for <code>pd.StringDtype('pyarrow')</code></p> <p>pd.ArrowDtype(pa.string()):  - It achieves similar memory efficiency as the other options - it returns ArrowDtype objects instead of NumPy-backed nullable types, soit might be less efficient for handling <code>missing values</code> compared to <code>pd.StringDtype('pyarrow')</code></p>"},{"location":"Python/Pandas/Pyarrow/#convert-backend-arrownumpy","title":"convert backend arrow/numpy","text":"<p>currently not possible using global setting</p> <pre><code>data = {'c1': [3, 2, 1, 0], 'c2': ['a', 'b', 'c', 'd']}\ndf = pd.DataFrame(data)\nd2 = df.convert_dtypes(dtype_backend='pyarrow')\nd3 = d2.convert_dtypes(dtype_backend='numpy_nullable')\nprint(df.dtypes)\nprint(d2.dtypes)\nprint(d3.dtypes)\n</code></pre> <p>Notice that  - df string type is <code>object</code> while - d3 string type is <code>string[python]</code>, preserves <code>pd.NA</code> <pre><code>some_series.astype(str)              # object\nsome_series.astype('string')         # string[python]\nsome_series.astype(pd.StringDtype()) # string[python]\n</code></pre></p>"},{"location":"Python/Pandas/Pyarrow/#pyarrow-table-to-pandaspyarrow","title":"<code>pyarrow table</code> to <code>pandas[pyarrow]</code>","text":"<pre><code>import pyarrow.csv as pv\nd = pv.read_csv('data.csv').to_pandas(types_mapper=pd.ArrowDtype)\n</code></pre>"},{"location":"Python/Pandas/Pyarrow/#pyarrow-backend-issues","title":"pyarrow backend issues","text":"<ul> <li><code>mod</code> and <code>divmod</code> not implmented: https://github.com/pandas-dev/pandas/pull/56694/files   <pre><code>import pandas as pd\nd = pd.DataFrame({'x': [1,2,3]}, dtype='int64[pyarrow]')\n(d['x'] + 2).mod(2) + 1  \n</code></pre></li> </ul>"},{"location":"Python/Pandas/Query/","title":"Query","text":""},{"location":"Python/Pandas/Query/#typeerror-unhashable-type","title":"TypeError: unhashable type","text":"<p>Pandas query error: TypeError: unhashable type: 'numpy.ndarray' - Caused by <code>numexpr</code> package, the default engine.  - Can change engine to: <code>df.query('', engine='python')</code>, but not efficient. - Best to indicate using the python semantics: <code>df.query('', parser='python')</code> - not always work.</p>"},{"location":"Python/Pandas/Query/#vs-and","title":"<code>&amp;</code> vs <code>and</code>","text":"<p>https://stackoverflow.com/questions/21415661/logical-operators-for-boolean-indexing-in-pandas</p> <p><code>&amp;</code> in pandas and numpy is element-wise logical-and while <code>and</code> only works for two boolean values.</p>"},{"location":"Python/Pandas/Question/","title":"question","text":""},{"location":"Python/Pandas/Question/#pandas","title":"pandas","text":"<p>how to sort df based on MultiIndex?</p> <p>stack df without changing cols to index?</p> <p>divide series [df col] by a series with one less index level?</p>"},{"location":"Python/Pandas/Reshape/","title":"reshape","text":""},{"location":"Python/Pandas/Reshape/#melt","title":"melt","text":"<p>unpivot a table col by col. The selected id cols will be repeated and the selected value cols will be stacked col by col.</p> <pre><code>df = pd.DataFrame({'A':['a','b','c'], 'B':[1,3,5], 'C':[2,4,6]})\npd.melt(df, id_vars=['A'], value_vars=['B','C'], var_name='var', value_name='val')\n</code></pre>"},{"location":"Python/Pandas/Reshape/#pivot","title":"pivot","text":"<p>return reshaped DataFrame organized by given index / column values.</p> <pre><code>df = pd.DataFrame({'idx': ['a', 'a', 'b', 'b', 'c', 'c'],\n                   'col': ['B', 'C', 'B', 'C', 'B', 'C'],\n                   'va1': [1, 2, 3, 4, 5, 6],\n                   'va2': ['x', 'y', 'z', 'q', 'w', 't']})\ndf.pivot(index='idx', columns='col', values='va1')\ndf.pivot(index='idx', columns='col', values=['va1', 'va2'])\n\n#back to melt status\ndf_pivot = df_pivot.reset_index()\ndf_pivot.columns.name = None\n</code></pre>"},{"location":"Python/Pandas/Reshape/#stack","title":"stack","text":"<p>stack row by row to a series, with original index and col names being merged to new index.</p> <pre><code>df = pd.DataFrame({'A':['a','b','c'], 'B':[1,3,5], 'C':[2,4,6]})\ndf.stack()\ndf.set_index('A').stack()\n\n#add names for new index and col\ndf.stack().rename_axis(index={None: 'new_idx'}).rename('new_val').reset_index()\n\n#multiIndex columns\ndf.stack(list(range(df.columns.nlevels)))\n</code></pre>"},{"location":"Python/Pandas/Reshape/#unstack","title":"unstack","text":"<p>for df, stack col by col to a series, with col names and original index being merged to new index.</p> <pre><code>df = pd.DataFrame({'A':['a','b','c'], 'B':[1,3,5], 'C':[2,4,6]})\ndf.unstack()\n</code></pre>"},{"location":"Python/Pandas/Sample/","title":"sample","text":""},{"location":"Python/Pandas/Sample/#down-sample","title":"down sample","text":"<p>https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html <pre><code>dfsum = df.resample('A-JUN', label='left').sum()  #annual sum\ndfsum = df.resample('A-JUN', label='left').mean() #annual average\n</code></pre></p>"},{"location":"Python/Pandas/Sample/#up-sample","title":"up sample","text":"<p>https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.resample.Resampler.interpolate.html <pre><code>dfupsmp = df.resample('D').interpolate(method='linear')\ndfupsmp = df.resample('D').interpolate(method='spline', order=2)\n</code></pre></p>"},{"location":"Python/Pandas/Slice/","title":"slice","text":""},{"location":"Python/Pandas/Slice/#filter-performance","title":"filter performance","text":"<p>https://medium.com/@thomas-jewson/faster-pandas-what-is-the-most-performant-filtering-method-a5dbb8f694dc <pre><code>df[df['col'].values == 'xxxxxx'] #fastest\ndf[df.get('col').values == 'x']  #similar\ndf.query('col &gt; 1')              #slowest\n</code></pre></p>"},{"location":"Python/Pandas/Slice/#slicing","title":"slicing","text":"<pre><code>df.ix[[0, 2], 'A'] #obsolete\ndf.at[ridx, 'A']\ndf.iat[1,2] #get a single value\n\n#loc to index\ndf.loc[df.index[[0, 2]], 'A']\n\n#index to loc\ndf.iloc[[0, 2], df.columns.get_loc('A')]\ndf.iloc[[0, 2], df.columns.get_indexer(['A', 'B'])]\n\nmsk = [True, False, True]\ndf.loc[msk, df.columns[2:]]\n\n#multiIndex slicing\nidx = pd.IndexSlice\ndf.loc[idx[:, r_level1], idx[:, 'c_level1']]\n</code></pre>"},{"location":"Python/Pandas/Slice/#select-rows-using-list","title":"select rows using list","text":"<pre><code>df[df['A'].isin([3, 6])]\n</code></pre>"},{"location":"Python/Pandas/Slice/#chained-assignments","title":"chained assignments","text":"<p>The SettingWithCopyWarning was created to flag potentially confusing chained assignments. With chained assignment, it is generally difficult to predict whether a view or a copy is returned. When filtering DataFrames, it is possible slice/index a frame to return either a view, or a copy.</p> <p>such as: <pre><code>df[df['a'] &gt; 2]['b'] = new_val     #new_val not set in df as a copy is made\ndf.loc[df['a'] &gt; 2, 'b'] = new_val #good\n\ndf2 = df[df.A &gt; 5]                 #boolean indexing will return a view\ndf2.loc[df2.C == 5, 'D'] = 123     #should be ---&gt;\ndf2 = df[df.A &gt; 5].copy()          #good\n</code></pre></p>"},{"location":"Python/Pandas/Stat/","title":"stats","text":""},{"location":"Python/Pandas/Stat/#round","title":"round","text":"<p>The round() function rounds values of .5 towards an even integer (Python Docs, n.d. a). So .5 is round up for positive values and round down for negative values.</p> <p>For instance, both round(0.5) and round(-0.5) return 0, while round(1.5) gives 2 and round(-1.5) gives -2. This Python behaviour is a bit different from how rounding usually goes.</p>"},{"location":"Python/Pandas/Stat/#pdquantile","title":"pd.quantile","text":""},{"location":"Python/Pandas/Stat/#1-consider-the-first-and-last-values-are-the-edges","title":"1, consider the first and last values are the edges:","text":"<pre><code>h = (N \u2212 1)*p + 1 #space is between v1 and vn\nEst_Quantile = x\u230ah\u230b + (h \u2212 \u230ah\u230b)*(x\u230ah\u230b + 1 \u2212 x\u230ah\u230b)\n</code></pre> <p>not #2, consider the first and last values are the first and last data centers: <pre><code>h = N*p + 0.5\nEst_Quantile = x\u230ah\u230b + (h \u2212 \u230ah\u230b)*(x\u230ah\u230b + 1 \u2212 x\u230ah\u230b)\n</code></pre></p>"},{"location":"Python/Pandas/Style/","title":"Style","text":"<p>https://pandas.pydata.org/docs/user_guide/style.html</p>"},{"location":"Python/Pandas/Style/#set-precision","title":"set precision","text":"<pre><code>df.style.set_precision(2)\n</code></pre>"},{"location":"Python/Pandas/Style/#format-float","title":"format float","text":"<pre><code>df.style.format('{:.2%}', na_rep='NULL')\ndf.style.format(precision=3, thousands='.', decimal=',').format_index(str.upper, axis=1)\n</code></pre>"},{"location":"Python/Pandas/Style/#hide-index","title":"hide index","text":"<pre><code>df.style.hide_index()\n</code></pre>"},{"location":"Python/Pandas/Style/#highlight-minmaxnull-in-each-col","title":"highlight min/max/null in each col","text":"<pre><code>df.style.highlight_min()\ndf.style.highlight_max()\ndf.style.highlight_null(null_color='red')\ndf.style.set_na_rep('missing').highlight_null(null_color='orange') \n</code></pre>"},{"location":"Python/Pandas/Style/#bar-chart","title":"bar chart","text":"<pre><code>df.style.bar()\n</code></pre>"},{"location":"Python/Pandas/Style/#heat-map","title":"heat map","text":"<pre><code>df.style.background_gradient()\n</code></pre>"},{"location":"Python/Pandas/Style/#set-table-properties","title":"set table properties","text":"<pre><code>df.style.set_properties(**{\n    'border': '1.1px solid blue',\n    'color': 'magenta',\n})\n</code></pre>"},{"location":"Python/Pandas/Style/#custom-style","title":"custom style","text":"<pre><code>def color_abs_red(v, x):\n    \"\"\"\n    Return a string with css property `'color: red'` for abs(val) &gt; x, black otherwise\n\n    v is a scalar and x a non-negative value\n    \"\"\"\n    colors = np.where(np.abs(v) &gt; x, 'color: red', 'color: black')\n    return colors\ndf.style.apply(color_abs_red, x=0.01)\n</code></pre>"},{"location":"Python/Pandas/Test/","title":"Test","text":""},{"location":"Python/Pandas/Test/#create-a-large-dataframe","title":"create a large dataframe","text":"<pre><code>import numpy as np\nimport pandas as pd\nnp.random.seed(0)\n\nnrow = 150000\nncol = 45\ncolumns = [f'c{i}' for i in range(ncol)]\ndf = pd.DataFrame(data=np.random.rand(nrow, ncol),columns=columns)\ndf.to_parquet('data.parquet')\n</code></pre>"},{"location":"Python/Pandas/Text/","title":"text","text":"<p>working with text:\\ https://pandas-docs.github.io/pandas-docs-travis/user_guide/text.html</p>"},{"location":"Python/Pandas/Text/#filter-col","title":"filter col","text":"<pre><code>df = df[df['tech'].str.contains('|'.join(techs))] #only keep selected techs\n</code></pre>"},{"location":"Python/Pandas/Text/#regex-replace","title":"regex replace","text":"<pre><code>#replace 'New' or 'new' by 'New_'\ndf2 = df.replace(to_replace ='[nN]ew', value = 'New_', regex = True)\n\n#use apply\n#remove (*\ndef clean_names(nam):\n    #search for opening bracket in the name followed by any characters repeated any number of times\n    if re.search('\\(.*', nam):\n        pos = re.search('\\(.*', nam).start()  #extract the position of beginning of pattern\n        return nam[:pos]  # return the cleaned name\n    else:\n        return nam\ndf['city'] = df['city'].apply(clean_names) #replace 'city (abc)' by 'city'\n\ntxt2 = re.sub(r'(Ave|Rd|Dr)\\s+#?\\d+\\s*\\n',r'\\1\\n', txt, re.MULTILINE|re.IGNORECASE)\n</code></pre>"},{"location":"Python/Pandas/Text/#re-no-capturing","title":"re no capturing","text":"<p>(?:...)  A non-capturing version of regular parentheses. Matches whatever regular expression is inside the parentheses, but the substring matched by the group cannot be retrieved after performing a match or referenced later in the pattern <pre><code>msk = np.where(df['id'].str.contains(r'^(?:pie|dog|dinner_[^_]+)_[^_]+_.*',flags=re.IGNORECASE))[0]\n</code></pre></p>"},{"location":"Python/Pandas/TimeSeries/","title":"tseries","text":""},{"location":"Python/Pandas/TimeSeries/#offset","title":"offset","text":"<pre><code>from pandas.tseries.offsets import Day, MonthEnd\noffset = MonthEnd()\nts.groupby(offset.rollforward).mean()\nts.resample('M').mean()#faster\n</code></pre>"},{"location":"Python/Pandas/TimeSeries/#period","title":"period","text":"<pre><code>prd = pd.Period(2007, freq='A-DEC')\nrng = pd.period_range('2000-01-01', '2000-06-30', freq='M')\nprd.asfreq('M', how='start') #convert to another period\n</code></pre>"},{"location":"Python/Pandas/TimeSeries/#resample","title":"resample","text":"<pre><code>ts.resample('M').mean()\nts.resample('M', kind='period').mean()\n\n#up-sampling\ndf.resample('D').asfreq()\ndf.resample('D').ffill(limit=2)\n\n#down-sampling (aggregate)\nts.resample('5min', closed='right').sum()\nts.resample('5min', closed='right', label='right', loffset='-1s').sum()\n\n#Open-High-Low-Close (OHLC) resampling\nts.resample('5min').ohlc()\n\n#multiple key\ndf2 = pd.DataFrame({'time': times.repeat(3),\n                    'key': np.tile(['a', 'b', 'c'], N),\n                    'value': np.arange(N * 3.)})\ntime_key = pd.TimeGrouper('5min')\ndf2 = df.set_index('time').groupby(['key', time_key]).sum()\ndf2 = df2.reset_index()\n</code></pre>"},{"location":"Python/Pandas/TimeSeries/#moving-window","title":"moving window","text":"<pre><code>df['val'].rolling(250).mean()\ndf['val'].rolling(250, min_periods=10).std()\ndf['val'].expanding().mean()\n\n#exponentially weighted moving average\ndf['val'].ewm(span=30).mean()\n\n#binary moving window\nspx_rets = spx_px.pct_change()\nreturns = close_px.pct_change()\ncorr = returns.AAPL.rolling(125, min_periods=100).corr(spx_rets)\ncorr = returns.rolling(125, min_periods=100).corr(spx_rets) #for all cols\n\n#user-defined moving window\nfrom scipy.stats import percentileofscore\nscore_at_2percent = lambda x: percentileofscore(x, 0.02)\nresult = returns.AAPL.rolling(250).apply(score_at_2percent)\n</code></pre>"},{"location":"Python/Pandas/TimeSeries/#timezone","title":"timezone","text":"<pre><code>import pytz\npytz.common_timezones[-5:]\ntz = pytz.timezone('America/New_York')\n\n#from naive to localized\nts_utc = ts.tz_localize('UTC')\n#converted to another time zone\nts_utc.tz_convert('America/New_York')\n\n#localize timestamp\nstamp = pd.Timestamp('2011-03-12 04:00')\nstamp_utc = stamp.tz_localize('utc')\nstamp_utc.tz_convert('America/New_York')\n</code></pre>"},{"location":"Python/Pandas/Tip/","title":"tip","text":""},{"location":"Python/Pandas/Tip/#astypeint","title":"astype(int)","text":"<p>will does a conversion toward zero, not rounding, try this <code>s1.round().astype(int)</code></p>"},{"location":"Python/Pandas/Tip/#npwherecond-pdtimestamp-pdtimestamp","title":"np.where(cond, pd.Timestamp(), pd.Timestamp())","text":"<p>will change pd.Timestamp() to inetger, the right way to use where <code>np.where(cond, pd.Timestamp(), np.datetime64())</code></p>"},{"location":"Python/Pandas/Tip/#list-of-lists-to-csv","title":"list of lists to csv","text":"<pre><code>csv.writer(open('out.csv', 'w', newline='')).writerows(lls)\n</code></pre>"},{"location":"Python/Pandas/Tip/#set-index-in-each-group-starting-from-0","title":"set index in each group starting from 0","text":"<pre><code>df['idx'] = df.groupby('grp_id').cumcount()\n</code></pre>"},{"location":"Python/Pandas/To/","title":"to","text":""},{"location":"Python/Pandas/To/#list-of-tuples","title":"list of tuples","text":"<pre><code>list(df.itertuples(index=False, name=None))\n</code></pre>"},{"location":"Python/Pandas/To/#check-type-count-in-col","title":"check type count in col","text":"<pre><code>df['col1'].apply(lambda x: type(x)).value_counts()\ndf['col1'][df['col1'].apply(lambda x: isinstance(x,str))]\ndf['col1'].dt.year.value_counts().sort_index()\n</code></pre>"},{"location":"Python/Pandas/To/#dict-of-list","title":"dict of list","text":"<pre><code>df.groupby('key')['val'].apply(list).to_dict()\ndf.groupby('key')[['va1','va2']].apply(lambda g: g.values.tolist()).to_dict()\n</code></pre>"},{"location":"Python/Pandas/To/#df-to-dic","title":"df to dic","text":"<pre><code>#each col to a key\ndf = pd.DataFrame({'a': [1,2],'b': ['x', 'y'], 'c': ['u', 'v']})\ndic = df.to_dict('list')\n\n#each row to a key\ndf = pd.DataFrame({'id': ['p', 'q', 'r'],'a': ['red', 'yellow', 'blue'], 'b': [0.5, 0.25, 0.125]})\ndic = df.set_index('id').T.to_dict('list')\n\n#not work\ndic = {k: list(g) for k, g in df.groupby('id')['a','b']}\n\n#series: id is key, a and b are in 2d array\nxx = df.groupby('id').apply(lambda x:x[['a','b']].values)\n\n#series: similar to previous, values col has a name\nyy = df.groupby('id')['a','b'].apply(lambda x: x.values.tolist()).to_frame('val').reset_index()\n</code></pre>"},{"location":"Python/Pandas/Transform/","title":"transform","text":"<p>broadcast aggregated value into the group, and function output must be a series. - It can produce a scalar value to be broadcast to the shape of the group - It can produce an object of the same shape as the input group - It must not mutate its input</p> <pre><code>g = df.groupby('key').value\ng.mean()\ng.transform('mean')\ng.transform(lambda x: x.mean())\ng.transform(lambda x: x * 2)\ng.transform(lambda x: x.rank(ascending=False))\n\ndef normalize(x):\n  return (x - x.mean()) / x.std()\ng.apply(normalize)\ng.transform(normalize)\nnormalized = (df['value'] - g.transform('mean')) / g.transform('std')\n</code></pre>"},{"location":"Python/Pandas/Transform/#example","title":"example","text":"<p>We have a df with cols <code>ty</code>, <code>reg</code>, <code>size</code> and <code>val</code>.  We want to get the min/max value in each group of (type, region). <pre><code>ylims = (\n    d.groupby(['ty','reg'])\n    .agg(vmin=('val', 'min'), vmax=('val', 'max'))\n    .assign(\n        tmin=lambda x: x[['vmin']].groupby(['ty']).transform('min'),\n        tmax=lambda x: x[['vmax']].groupby(['ty']).transform('max'),\n    )\n    .drop(columns=['vmin','vmax'])\n)\nylims\n</code></pre></p>"},{"location":"Python/PandasUpgradeTest/Datetime/","title":"Datetime","text":""},{"location":"Python/PandasUpgradeTest/Datetime/#to_datetime","title":"to_datetime","text":"<p>From pandas 2.0, cannot use <code>dayfirst</code> separatedly.  <pre><code># dayfirst applies to all formats (previously not yyyy-mm-dd)\nto_datetime('2023-02-01', dayfirst=True)\n&gt;&gt;&gt; 2023-01-02\n# dayfirst does not apply to yyyy-mm-dd format when use mixed (not compatable with old versions)\nto_datetime('2023-02-01', dayfirst=True, format='mixed')\n&gt;&gt;&gt; 2023-02-01\n</code></pre></p>"},{"location":"Python/PandasUpgradeTest/Datetime/#read_csv","title":"read_csv","text":"<p>int column will be persed as int64, use <code>dtype</code> to set the type to int32. <pre><code>pd.read_csv(fp,\n    index_col=[\n        'name',\n        'year',\n        'quarter',\n        'date',\n        'value',\n    ],\n    dtype={\n        'year': 'int32',\n        'quarter': 'int32',\n    },\n    parse_dates=['date'],\n)\n</code></pre></p>"},{"location":"Python/Parallel/Example/","title":"Example","text":""},{"location":"Python/Parallel/Example/#multiprocessing","title":"multiprocessing","text":"<pre><code># not support asyncio\nfrom multiprocessing.pool import ThreadPool\nn_jobs = min(cpu_count(), len(safe_paths))\n\n# option 1, pool.map actually call map_async\nwith ThreadPool(processes=n_jobs) as pool:\n    dfs = pool.map(lambda path: read_parquet_func(fs, path, columns, filters), paths)\n\n# option 2\nwith ThreadPool(processes=n_jobs) as pool:\n  results = [\n      pool.apply_async(read_parquet_func, args=(fs, path, columns, filters))\n      for path in paths\n  ]\n  dfs = [sync(result.get()) for result in results]\n</code></pre>"},{"location":"Python/Parallel/Example/#multiprocessingpool-issue","title":"<code>multiprocessing.Pool</code> issue","text":"<p>https://stackoverflow.com/questions/72766345/attributeerror-cant-pickle-local-object-in-multiprocessing</p> <p>It has pickle issue: some object such as pywin32 datetime cannot be pickled.</p>"},{"location":"Python/Parallel/Example/#slower-threadpool","title":"slower <code>ThreadPool</code>","text":"<p>https://superfastpython.com/threadpoolexecutor-slower/</p> <p>This is because Python threads are constrained by the Global Interpreter Lock, or GIL. The GIL uses synchronization to ensure that only one thread can execute instructions at a time within a Python process. If the task is CPU bounded then it's worse than a single for loop.</p>"},{"location":"Python/Parallel/Example/#concurrent","title":"concurrent","text":"<pre><code># support both sync and async\nfrom concurrent.futures import ThreadPoolExecutor\n\nwith ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n    futures = [\n        executor.submit(\n            read_parquet_func, fs, path, columns, filters\n        )\n        for path in paths if fs.exists(path)\n    ]\n    dfs = [future.result() for future in futures]\n</code></pre>"},{"location":"Python/Parallel/Example/#joblib","title":"joblib","text":"<pre><code>from joblib import cpu_count, delayed, Parallel\ndelayed_func = [\n    delayed(read_parquet_func)(fs, path, columns, filters)\n    for path in paths if fs.exists(path)\n]\n\nn_jobs = min(cpu_count(), len(delayed_func))\nwith Parallel(n_jobs=n_jobs, prefer='threads') as parallel_pool:\n    dfs = parallel_pool(delayed_func)\n</code></pre>"},{"location":"Python/Parallel/Multiprocessing/","title":"multiprocessing","text":"<ul> <li>threads share memory with the main process</li> <li>processes have independent memory spaces. When a process ends, the OS cleans up its memory</li> </ul>"},{"location":"Python/Parallel/Multiprocessing/#use-multipricessingprocess","title":"Use <code>multipricessing.Process</code>","text":"<pre><code>import os\nimport time\nimport multiprocessing\n\ndef worker(data_size_mb):\n    pid = os.getpid()\n    print(f'[PID {pid}] Allocating {data_size_mb}MB')\n    # Simulate large memory usage\n    big_data = bytearray(data_size_mb * 1024 * 1024)\n    time.sleep(2)\n    print(f'[PID {pid}] Done.')\n    # need to use multiprocessing.Queue or multiprocessing.Pipe to return values\n\nif __name__ == '__main__':\n    processes = []\n    for _ in range(2):  # Spawn 3 separate processes\n        p = multiprocessing.Process(target=worker, args=(100,))  # 100MB each\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n    print('All worker processes completed.')\n</code></pre>"},{"location":"Python/Parallel/Multiprocessing/#use-multiprocessingpool","title":"Use <code>multiprocessing.Pool</code>","text":"<pre><code>import os\nimport time\nimport multiprocessing\n\ndef worker(data_size_mb):\n    pid = os.getpid()\n    print(f'[PID {pid}] Allocating {data_size_mb}MB')\n    # Simulate large memory usage\n    big_data = bytearray(data_size_mb * 1024 * 1024)\n    time.sleep(2)\n    print(f'[PID {pid}] Done.')\n    return f'Process {pid} completed'\n\nif __name__ == '__main__':\n    with multiprocessing.Pool(processes=2) as pool:\n        # should not use a class method as worker!\n        results = pool.map(worker, [100, (100)])  # 2 tasks of 100MB each\n\n        # # different workers\n        # result1 = pool.apply_async(worker1, args=args1)\n        # result2 = pool.apply_async(worker2, args=args2)\n        # results = [result1.get(), result2.get()]\n\n    print('\\nAll tasks completed.')\n    for r in results:\n        print(r)\n</code></pre>"},{"location":"Python/Parallel/Parallel/","title":"Parallel","text":"<p>reuse session\\ err: only one usage of each socket address\\ Sessions are usually easy to use. Just wrap with requests.Session() as s: around your for loop, and then replace requests.get with s.get</p> <p>https://iliauk.wordpress.com/2016/03/07/high-performance-python-sessions-async-multi-tasking/</p>"},{"location":"Python/Parallel/Parallel/#gil-and-issues","title":"GIL and issues","text":"<p>https://realpython.com/python-parallel-processing/</p> <p>Great summary for Python parallel computation.</p>"},{"location":"Python/Parallel/Parallel/#ray","title":"Ray","text":"<p>Ray is decentralized, meaning each machine runs its own scheduler, so any issues with a scheduled task are handled at the level of the individual machine, not the whole cluster.</p>"},{"location":"Python/Parallel/Parallel/#dask","title":"Dask","text":"<p>Dask uses a centralized scheduler that handles all tasks for a cluster.</p>"},{"location":"Python/Parallel/Parallel/#joblib","title":"joblib","text":"<pre><code>#r = Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10)) #one return value\n#r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10)) #several return values\n#res, i = zip(*r)\nmeth = 'threading' #joblib loky (default, not work), threading, and multiprocessing (not work)\nParallel(n_jobs=-3, backend=meth)(\n    delayed(my_func)(p1, p2, yr, cols, new_cols) for yr in range(2000, 2022)\n)\n</code></pre> <p>Another format <pre><code>from joblib import cpu_count, delayed, Parallel\ndelayed_func = [\n    delayed(my_func)(p1, p2, yr)\n    for yr in range(2000, 2022)\n]\n\nn_jobs = min(cpu_count(), len(delayed_func))\nwith Parallel(n_jobs=n_jobs, prefer='threads') as parallel_pool:\n    dfs = parallel_pool(delayed_func)\n</code></pre></p>"},{"location":"Python/Parallel/Parallel/#multi-processing","title":"multi-processing","text":"<p>https://research.wmz.ninja/articles/2018/03/on-sharing-large-arrays-when-using-pythons-multiprocessing.html</p> <pre><code>import numpy as np\nimport multiprocessing as mp\n\n# A global dictionary storing the variables passed from the initializer.\nvar_dict = {}\n\ndef init_worker(X, X_shape):\n    # Using a dictionary is not strictly necessary. You can also use global variables.\n    var_dict['X'] = X\n    var_dict['X_shape'] = X_shape\n\ndef worker_func(i):\n    # Simply computes the sum of the i-th row of the input matrix X\n    X_np = np.frombuffer(var_dict['X']).reshape(var_dict['X_shape'])\n    time.sleep(1) # Some heavy computations\n    return np.asscalar(np.sum(X_np[i,:]))\ndef main():\n    X_shape = (16, 1000000)\n    # Randomly generate some data\n    data = np.random.randn(*X_shape)\n\n    X = RawArray('d', X_shape[0] * X_shape[1])\n\n    # Wrap X as an numpy array so we can easily manipulates its data.\n    X_np = np.frombuffer(X).reshape(X_shape)\n\n    # Copy data to our shared array.\n    np.copyto(X_np, data)\n\n    # Start the process pool and do the computation.\n    # Here we pass X and X_shape to the initializer of each worker.\n    # (Because X_shape is not a shared variable, it will be copied to each child process.)\n    with Pool(processes=4, initializer=init_worker, initargs=(X, X_shape)) as pool:\n        result = pool.map(worker_func, range(X_shape[0]))\n        print('Results (pool):\\n', np.array(result))\n    # Should print the same results.\n    print('Results (numpy):\\n', np.sum(X_np, 1))\n</code></pre>"},{"location":"Python/Performance/Deadlock/","title":"deadlock","text":"<p>https://pythonspeed.com/articles/python-multiprocessing/</p> <p>In Python 3 the multiprocessing library added new ways of starting subprocesses. One of these does a fork() followed by an execve() of a completely new Python process.</p>"},{"location":"Python/Performance/Deadlock/#global","title":"global","text":"<pre><code>from multiprocessing import set_start_method\nset_start_method(\"spawn\")\n</code></pre>"},{"location":"Python/Performance/Deadlock/#local","title":"local","text":"<pre><code>from multiprocessing import get_context\ndef your_func():\n    with get_context(\"spawn\").Pool() as pool:\n        # ... everything else is unchanged\n</code></pre>"},{"location":"Python/Performance/Deadlock/#start-method","title":"Start Method","text":"<p>A start method is the technique used to start child processes in Python. - spawn: start a new Python process. - fork: copy a Python process from an existing process. - forkserver: new process from which future forked processes will be copied.</p>"},{"location":"Python/Performance/Deadlock/#default-start-methods","title":"default Start Methods","text":"<ul> <li>Windows (win32): spawn</li> <li>macOS (darwin): spawn</li> <li>Linux (unix): fork</li> </ul>"},{"location":"Python/Performance/LazyEvaluation/","title":"Lazy Evaludation","text":""},{"location":"Python/Performance/LazyEvaluation/#use-a-class-to-implement-lazy-function","title":"use a class to implement lazy function","text":"<pre><code>class Test():\n    def __init__(self, name):\n        self.name = name\n        self._value = None\n\n    @property\n    def value(self):\n        if self._value is None:\n            self._value = self._get_value()\n        return self._value\n\n    def _get_value(self):\n        val = my_api(self.name)\n        return val\n</code></pre>"},{"location":"Python/Performance/LazyEvaluation/#lazy-package","title":"<code>lazy</code> package","text":"<pre><code>from lazy import lazy\n@lazy\ndef f(x, y):\n  v = 0\n  for i in range(10000):\n    v += x ** y\n  return v\n\nimport time\nt0 = time.time()\nv = f(12,3)\nprint(f'time: {time.time() - t0}, value: {v}')\nt0 = time.time()\nv = f(12,3)\nprint(f'time: {time.time() - t0}, value: {v}')\n</code></pre>"},{"location":"Python/Performance/Numba/","title":"Numba","text":""},{"location":"Python/Performance/Numba/#nb-vs-npwhere","title":"nb vs np.where","text":"<pre><code>np.random.seed(0)\ndf = pd.DataFrame(np.random.rand(100000,2), columns=['x','y'])\n\nimport numba as nb\n@nb.njit\ndef nb_where(x, y):\n    n = len(x)\n    val = np.empty(n, dtype=np.float64)\n    for i in range(n):\n        if x[i] &lt; 0.9 and y[i] &lt; 0.9:\n            val[i] = x[i]\n        else:\n            val[i] = x[i] + y[i]\n    return val\n\nassert(np.where((df.x &lt; 0.9) &amp; (df.y &lt; 0.9), df.x, df.x + df.y)==nb_where(df.x.values, df.y.values)).all()\n\n%%timeit -r 7 -n 1000\nmask = (df.x &gt;= 0.9) | (df.y &gt;= 0.9)\ndf.x[mask] += df.y[mask]\n# 6.16 ms \u00b1 14.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n%timeit -r 7 -n 1000 np.where((df.x &lt; 0.9) &amp; (df.y &lt; 0.9), df.x, df.x+df.y)\n# 1.04 ms \u00b1 1.52 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n%timeit -r 7 -n 1000 nb_where(df.x.values, df.y.values)\n# 286 \u00b5s \u00b1 2.32 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>"},{"location":"Python/Performance/Pandas/","title":"Pandas","text":""},{"location":"Python/Performance/Pandas/#iter-rows-using-zip-with-values-is-400x-faster-than-dfiterrows","title":"iter rows, using <code>zip</code> with values is 400x faster than <code>df.iterrows()</code>.","text":"<p>assume df has columns <code>name</code>, <code>date</code>, and <code>value</code>. <pre><code>%timeit for row in df.iterrows(): pass\n1.67 s \u00b1 70.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n%timeit for row in df.to_records(index=False): pass\n69.2 ms \u00b1 2.89 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n%timeit for n, d, v in zip(df['name'], df['date'], df['value']): pass\n10.9 ms \u00b1 751 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n%timeit for n, d, v in zip(df['name'].values, df['date'].values, df['value'].values): pass\n4.12 ms \u00b1 302 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre></p>"},{"location":"Python/Performance/Pandas/#slow-multiindex-methods","title":"slow multiindex methods","text":"<p>Get the mask of the index in both mi1 and mi2 - can be 2x faster. <pre><code>#slow\niboth = mi1.intersection(mi2)\nmask1 = mi1.isin(iboth)\nmask2 = mi2.isin(iboth)\n\n#fast (joining the index of both dfs will be 8x faster than joining normal cols)\nd1 = pd.DataFrame(index=mi1, data=np.arange(len(mi1)), columns=['id1'])\nd2 = pd.DataFrame(index=mi2, data=np.arange(len(mi2)), columns=['id2'])\ndf = d1.merge(d2, how='inner', left_index=True, right_index=True)\nmask1 = np.full(len(mi1), False, dtype=bool)\nmask1[df.id1] = True\nmask2 = np.full(len(mi2), False, dtype=bool)\nmask2[df.id2] = True\n</code></pre></p>"},{"location":"Python/Performance/Pandas/#pdeval","title":"pd.eval","text":"<pre><code>np.random.seed(0)\ndf = pd.DataFrame(np.random.rand(100000,2), columns=['x','y'])\n\nimport numba as nb\n@nb.njit\ndef nb_and(x, y):\n    n = len(x)\n    val = np.empty(n, dtype=np.bool_)\n    for i in range(n):\n        val[i] = (x[i] &lt; 0.9) &amp; (y[i] &lt; 0.9)\n    return val\n\nnp.allclose(nb_and(df.x.values, df.y.values), (df.x &lt; 0.9) &amp; (df.y &lt; 0.9))\n\n%timeit -r 10 -n 100 nb_and(df.x.values, df.y.values)\n154 \u00b5s \u00b1 2.57 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n\n%timeit -r 10 -n 100 (df.x &lt; 0.9) &amp; (df.y &lt; 0.9)\n483 \u00b5s \u00b1 2.44 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n\n%timeit -r 5 -n 100 df.eval('(x &lt; 0.9) &amp; (y &lt; 0.9)')\n1.58 ms \u00b1 28.4 \u00b5s per loop (mean \u00b1 std. dev. of 5 runs, 100 loops each)\n</code></pre>"},{"location":"Python/Performance/Profiling/","title":"Profiling","text":"<p>In Python, the operation of verifying whether a specific example x belongs to S is efficient when S is declared as a set and is inefficient when S is declared as a list.</p> <p>Using PyPy, Numba or similar tools to compile your Python code into fast, optimized machine code.</p>"},{"location":"Python/Performance/Profiling/#cprofile","title":"cProfile","text":"<p>Use cProfile package in Python to find inefficiencies in your code. <pre><code>python -m cProfile -s time my.py\n</code></pre> But generally it's more convenient to run the code once and save the results to a file. Then do the analysis later.</p>"},{"location":"Python/Performance/Profiling/#cprofile-and-pstats","title":"cProfile and pstats","text":"<pre><code>python -m pstats my.profile\n</code></pre>"},{"location":"Python/Performance/Profiling/#cprofile-and-snakeviz","title":"cProfile and snakeviz","text":"<p>SnakeViz has two visualization styles, 'icicle' and 'sunburst'. <pre><code>1. pip install snakeviz\n2. python -m cProfile -o my.profile my.py\n3. snakeviz my.profile\n</code></pre></p>"},{"location":"Python/Performance/Profiling/#linux","title":"Linux","text":"<p>https://stackoverflow.com/questions/51982417/pandas-mask-where-methods-versus-numpy-np-where <pre><code>perf record python np_where.py\nperf report\n</code></pre></p>"},{"location":"Python/Performance/StackTrace/","title":"Stack trace","text":""},{"location":"Python/Performance/StackTrace/#gnucash","title":"GnuCash","text":"<p>dump stack trace https://wiki.gnucash.org/wiki/Stack_Trace</p>"},{"location":"Python/Performance/TimeIt/","title":"timeit","text":""},{"location":"Python/Performance/TimeIt/#timeit-in-jupyter","title":"timeit in Jupyter","text":"<pre><code>#same line\n%timeit -r 3 -n 100 x+y\n\n#one cell\n%%timeit -r 3 -n 100\nx + y\n</code></pre>"},{"location":"Python/Performance/TimeIt/#run-timeit-in-vscode-debug","title":"run timeit in vscode debug","text":"<pre><code>import timeit\ndef func(df):\n  df.set_index('a')\n  pass\n\nt1 = timeit.Timer(lambda: func(df))\nprint(t1.timeit(5))\nprint(t1.repeat(repeat=3, number=7))\n</code></pre>"},{"location":"Python/Performance/perf1/","title":"perf (I)","text":"<ul> <li>https://scipy.github.io/old-wiki/pages/PerformanceTips</li> <li>https://llllllllll.github.io/principles-of-performance/index.html</li> <li>https://speakerdeck.com/pycon2018/jake-vanderplas-performance-python-seven-strategies-for-optimizing-your-numerical-code</li> <li>https://speakerdeck.com/nnja/nina-zakharenko-the-basics-of-memory-management-in-python-north-bay-python-2018</li> <li>https://software.intel.com/en-us/articles/large-matrix-operations-with-scipy-and-numpy-tips-and-best-practices</li> <li>http://conference.scipy.org/proceedings/scipy2018/pdfs/anton_malakhov.pdf</li> <li>http://conference.scipy.org/proceedings/scipy2017/pdfs/oleksandr_pavlyk.pdf</li> </ul>"},{"location":"Python/Performance/perf1/#example","title":"example","text":"<p>Fast, Flexible, Easy and Intuitive: How to Speed Up Your Pandas Projects</p> <p>https://realpython.com/fast-flexible-pandas</p> <pre><code>Tariff Type   Cents per kWh   Time Range\nPeak          28              17:00 to 24:00\nShoulder      20              07:00 to 17:00\nOff-Peak      12              00:00 to 07:00\n</code></pre> <pre><code>method        time\n.itertuples() 713 ms\n.apply()      272 ms\n.isin()        10 ms\npd.cut()        3 ms\nnp.digitize()   2 ms\n</code></pre>"},{"location":"Python/Performance/perf1/#datetime","title":"DateTime","text":"<pre><code># change String Datetime to Timestamp\ndf['date_time'] = pd.to_datetime(df['date_time'])\n# 50x faster when explicitly using format\ndf['date_time'] = pd.to_datetime(df['date_time'], format='%d/%m/%Y %H:%M:%S')\n</code></pre>"},{"location":"Python/Performance/perf1/#isin","title":".isin()","text":"<pre><code># define hour range Boolean arrays\npeak_hours = df.index.hour.isin(range(17, 24))\n# apply tariffs to hour ranges\ndf.loc[peak_hours, 'cost_cents'] = df.loc[peak_hours, 'energy_kwh'] * 28\n</code></pre>"},{"location":"Python/Performance/perf1/#pdcut","title":"pd.cut()","text":"<pre><code>cents_per_kwh = pd.cut(\n    x=df.index.hour,\n    bins=[0, 7, 17, 24],\n    include_lowest=True,\n    labels=[12, 20, 28]\n).astype(int)\ndf['cost_cents'] = cents_per_kwh * df['energy_kwh']\n</code></pre>"},{"location":"Python/Performance/perf1/#npdigitize","title":"np.digitize()","text":"<pre><code>prices = np.array([12, 20, 28])\nbins = np.digitize(df.index.hour.values, bins=[7, 17, 24])\ndf['cost_cents'] = prices[bins] * df['energy_kwh'].values\n</code></pre>"},{"location":"Python/Performance/perf1/#hdfstore","title":"HDFStore","text":"<pre><code># save to datastore\nds = pd.HDFStore('C:/ds1.h5')\nds['df1'] = df\nds.close()\n# get from datadtore\nds = pd.HDFStore('C:/ds1.h5','r')\ndf = ds['df1']\nds.close()\n</code></pre>"},{"location":"Python/Performance/perf2/","title":"perf (II)","text":""},{"location":"Python/Performance/perf2/#ravel-vs-flatten","title":"ravel() vs flatten()","text":"<p>np.ravel() will avoid copy if possible and thus faster than flatten()</p>"},{"location":"Python/Performance/perf2/#broadcasting-rules","title":"broadcasting rules","text":"<p>when broadcasting is possible, we do not need to use np.tile()</p>"},{"location":"Python/Performance/perf2/#numexpr","title":"numexpr","text":"<p>numexpr is 5x faster than NumPy expression.</p> <pre><code>a = df['a']\nexpr = 'sin(a - 1) + 1'\nresult1 = np.sin(a - 1) + 1\nresult2 = numexpr.evaluate(expr)\nresult3 = pd.eval(expr, engine='numexpr')\n</code></pre>"},{"location":"Python/Performance/perf2/#pdeval-and-pdquery","title":"pd.eval and pd.query","text":"<p>Mainly used for large arrays to save memory thus the speed.</p> <pre><code># 2x faster than df1+df2 and less mem\npd.eval('df1 + df2'))\n\n# pd.eval\nresult1 = (df['A'] + df['B']) / (df['C'] - 1)\nresult2 = pd.eval(\"(df.A + df.B) / (df.C - 1)\")\n\n# df.eval\nresult3 = df.eval('(A + B) / (C - 1)')\n# add/modify col\ndf.eval('D = (A + B) / C', inplace=True)\n\n# df.eval with local variable\ncol_mean = df.mean(1)\nresult1 = df['A'] + col_mean\nresult2 = df.eval('A + @col_mean')\n\n# df.query\nresult1 = df[(df.A &lt; 0.5) &amp; (df.B &lt; 0.5)]\nresult2 = pd.eval('df[(df.A &lt; 0.5) &amp; (df.B &lt; 0.5)]')\nresult2 = df.query('A &lt; 0.5 &amp; B &lt; 0.5')\n\n# df.query with local variable\navg = df['C'].mean()\nresult1 = df[(df.A &lt; avg) &amp; (df.B &lt; avg)]\nresult2 = df.query('A &lt; @avg &amp; B &lt; @avg')\n</code></pre>"},{"location":"Python/Pip/Pip/","title":"Pip","text":""},{"location":"Python/Pip/Pip/#build-whl-package","title":"build whl package","text":"<pre><code>pip wheel . --no-deps -w . --no-cache-dir --verbose\n</code></pre>"},{"location":"Python/Pip/Pip/#install-a-local-package","title":"install a local package","text":"<pre><code>pip install --no-deps --editable .\n</code></pre>"},{"location":"Python/Pip/PipBuild/","title":"Pip build","text":"<ul> <li>https://packaging.python.org/en/latest/guides/distributing-packages-using-setuptools/</li> <li>https://github.com/pypa/sampleproject</li> <li>old: https://medium.com/@jonathan.hoffman91/a-step-by-step-guide-to-building-python-wheels-4ed8160809a2</li> <li>new: https://medium.com/@jonathan_b/building-a-python-wheel-with-pyproject-toml-bbab842e989a</li> </ul>"},{"location":"Python/Pip/PipBuild/#build-whl-package","title":"build whl package","text":"<pre><code>pip wheel . --no-deps -w . --no-cache-dir --verbose\npip wheel . --no-deps -w ../pip/pkg --no-cache-dir --verbose\npip wheel $(Build.SourcesDirectory) --no-deps -w $(Agent.TempDirectory)/pkg --no-cache-dir --verbose\n</code></pre>"},{"location":"Python/Pip/PipBuild/#use-build","title":"use build","text":"<p><pre><code>pip install build setuptools wheel twine\npython -m build --outdir build-output/\npython -m build --wheel --no-isolation --outdir build-output/\n</code></pre> - <code>--no-isolation</code>: will not create a separate env, faster but less reproducible</p>"},{"location":"Python/Polars/CSV/","title":"CSV","text":""},{"location":"Python/Polars/CSV/#read-csv-files","title":"read csv files","text":"<pre><code>import polars as pl\n\ndf = pl.read_csv('data.csv')\ndf = pl.read_csv('data.csv', batch_size=50000)\n\n# lazy and filter\npl.scan_csv('data.csv').filter(pl.col('col_0') == 100).collect()\n\nlazy_df = pl.scan_csv('data.csv')\ndf = lazy_df.filter(\n    (pl.col('ts') == '2015-01-01') &amp; (pl.col('number') == 1)\n).collect().to_pandas()\n</code></pre>"},{"location":"Python/Polars/CSV/#read-csv-150-mb-with-categoricalstring","title":"read csv (150 MB) with categorical/string","text":"<p>For csv file reading, the fastest method is using <code>pv.read_csv(file, convert_options).to_pandas()</code>. <pre><code># Method                                                          Categorical    String   Format\npv.read_csv(file, convert_options=pa_convert_options)             #0.42s         0.32s    pa.Table\npv.read_csv(file, convert_options=pa_convert_options).to_pandas() #0.48s         0.75s    pd.DataFrame*\npl.read_csv(file, dtypes=pl_dtypes).to_pandas()                   #2.82s         2.01s    pd.DataFrame\npv.read_csv(file).to_pandas().astype(pd_dtypes)                   #3.20s         2.09s    pd.DataFrame\npd.read_csv(file, dtype=pd_dtype, parse_dates=['date'])           #16.2s         15.3s    pd.DataFrame\n</code></pre></p> <pre><code>import pandas as pd\nimport polars as pl\nimport pyarrow as pa\nimport pyarrow.csv as pv\n\npd_categorical = 'category' #'string'\npd_dtype = {\n    'country': pd_categorical,\n    'val': 'Float64',\n}\npd_dtypes = pd_dtype.copy()\npd_dtypes['date'] = 'datetime64[ns]'\n\npl_categorical = pl.Categorical # pl.String\npl_dtypes = {\n    'date': pl.Date,\n    'country': pl_categorical,\n    'val': pl.Float64,\n}\n\n# pa_categorical = pa.string()\npa_categorical = pa.dictionary(pa.int32(), pa.string()) # CSV conversion to dictionary only supported for int32 indices\npa_convert_options = pv.ConvertOptions(\n    column_types={\n        'date': pa.timestamp('ns'),\n        'country': pa_categorical,\n        'val': pa.float64(),\n    }\n)\n</code></pre>"},{"location":"Python/Polars/CSV/#write-csv-with-list-type-columns","title":"write csv with list type columns","text":"<pre><code>import polars as pl\n\n# Sample DataFrame\ndf = pl.DataFrame({\n    'id': [1, 2],\n    'values': [[1, 2, 3], [4, 5]],\n    'tags': [['a', 'b'], ['x']],\n})\n\n# Convert the list cols to string\ndf = df.with_columns(\n    pl.col(col).cast(pl.List(pl.String)).list.join(',') #.alias(col)\n    for col, dtype in zip(df.columns, df.dtypes)\n    if dtype == pl.List\n)\n\nprint(df)\n</code></pre>"},{"location":"Python/Polars/Categorical/","title":"Categorical","text":"<p>https://docs.pola.rs/user-guide/concepts/data-types/categoricals/</p>"},{"location":"Python/Polars/Categorical/#categoricalremappingwarning","title":"CategoricalRemappingWarning","text":"<pre><code>Local categoricals have different encodings, expensive re-encoding is done to perform this merge operation.\nConsider using a StringCache or an Enum type if the categories are known in advance.\n</code></pre> <p>solution: using the same <code>Enum</code> to encode the column in both series <pre><code>dtype = pl.Enum([\"Polar\", \"Panda\", \"Brown\"])\ncat_series = pl.Series([\"Polar\", \"Panda\", \"Brown\", \"Brown\", \"Polar\"], dtype=dtype)\ncat2_series = pl.Series([\"Panda\", \"Brown\", \"Brown\", \"Polar\", \"Polar\"], dtype=dtype)\nprint(cat_series.append(cat2_series))\n</code></pre></p>"},{"location":"Python/Polars/Column/","title":"Column","text":""},{"location":"Python/Polars/Column/#rename","title":"rename","text":"<pre><code>df = pl.DataFrame({\n    'foo': [1, 2, 3],\n    'bar': [6, 7, 8],\n    'ham': ['a', 'b', 'c'],\n})\ndf.rename({'foo': 'apple'})\ndf.rename(lambda col: 'c' + col[1:])\n</code></pre>"},{"location":"Python/Polars/Column/#reorder-columns","title":"reorder columns","text":"<p>https://stackoverflow.com/questions/71353113/polars-how-to-reorder-columns-in-a-specific-order</p> <p>Do not use Square bracket indexing: - cannot be parallelized - cannot be optimized <pre><code>df = pl.DataFrame({\n    'z': [1, 2],\n    'x': ['a', 'b'],\n    'y': [True, False],\n})\n\ndf.select(['x', 'y', 'z'])\n</code></pre></p>"},{"location":"Python/Polars/Column/#create-new-column-conditional-on-other-columns","title":"create new column conditional on other columns","text":"<pre><code>df.with_columns(\n    pl.when(pl.col('x') &gt; 0.5).then(0).otherwise(1).alias('y')\n)\n</code></pre>"},{"location":"Python/Polars/Column/#list-of-float-col-to-string","title":"list of float col to string","text":"<pre><code>df.with_columns(\n    pl.col('prices')\n    .list.eval(pl.element().cast(pl.Utf8))\n    .list.join(',')\n    .alias('prices')\n)\n</code></pre>"},{"location":"Python/Polars/Config/","title":"Config","text":""},{"location":"Python/Polars/Config/#polars-print-config","title":"polars print config","text":"<p>when print df align numerical cols right - <code>pl.Config.set_tbl_cell_numeric_alignment('RIGHT')</code></p> <p>numer of lines to show: - <code>pl.Config.set_tbl_rows(20)</code> # number of rows in print</p>"},{"location":"Python/Polars/Config/#show-all-rows-and-columns","title":"show all rows and columns","text":"<p>if n &lt; 0 (eg: -1), display all rows/columns <pre><code>with pl.Config(tbl_rows=-1, tbl_cols=-1):\n    print(df)\n</code></pre></p>"},{"location":"Python/Polars/DataType/","title":"Data type","text":""},{"location":"Python/Polars/DataType/#cast","title":"cast","text":"<p>https://stackoverflow.com/questions/71790235/switching-between-dtypes-within-a-dataframe</p>"},{"location":"Python/Polars/DataType/#string-to-intfloat","title":"string to int/float","text":"<pre><code>df = pl.DataFrame({'val_str': ['1.5', '2.2', '12', '']})\ndf.with_columns(\n    pl.col('val_str').cast(pl.Float64, strict=False).alias('val')\n)\n</code></pre>"},{"location":"Python/Polars/DataType/#intfloat-to-string","title":"int/float to string","text":"<pre><code>df = pl.DataFrame({'val': [1.5, 2.2, 12, None]})\ndf.with_columns(\n    pl.col('val').cast(pl.Utf8, strict=False).alias('val_str')\n)\n</code></pre>"},{"location":"Python/Polars/DataType/#string-to-datetime","title":"string to datetime","text":"<pre><code>df.with_columns(\n    pl.col('date').str.strptime(pl.Datetime, fmt='%Y-%m-%d').cast(pl.Datetime)\n)\n</code></pre>"},{"location":"Python/Polars/DataType/#new-col","title":"new col","text":"<pre><code>df = pl.DataFrame({'val': ['10', '20', '']})\ndf.with_columns([\n    pl.format('Value is {}', pl.col('val'))\n])\n</code></pre>"},{"location":"Python/Polars/Datetime/","title":"Datetime","text":"<ul> <li>https://www.confessionsofadataguy.com/date-and-datetime-manipulation-in-polar</li> <li>https://medium.com/@riellygriffiths/working-with-datetime-data-in-polars-9bb57e7f6304</li> </ul> <pre><code>df = pl.DataFrame({'dt': ['2024-10-01', '2024-10-02']})\n</code></pre>"},{"location":"Python/Polars/Datetime/#string-to-date","title":"string to date","text":"<pre><code>df.with_columns(\n    dt = pl.col('dt').str.to_datetime().cast(pl.Date)\n)\ndf.with_columns(\n    dt = pl.col('dt').str.strptime(pl.Date)\n)\n\nctx = pl.SQLContext(data=df)\nctx.execute('SELECT *, CAST(dt as DATE) as date FROM data', eager=True)\nctx.execute('SELECT *, DATE(dt) as dt FROM data', eager=True)\n</code></pre>"},{"location":"Python/Polars/Datetime/#string-to-datetime","title":"string to datetime","text":"<pre><code>df.with_columns(\n    dt = pl.col('dt').str.to_datetime().cast(pl.Datetime)\n)\n</code></pre>"},{"location":"Python/Polars/Datetime/#date-to-string","title":"date to string","text":"<pre><code>df.with_columns(\n    dt = pl.col('dt').dt.strftime('%Y-%m-%d')\n)\n</code></pre>"},{"location":"Python/Polars/Datetime/#date-to-yeramonthday","title":"date to yera/month/day","text":"<pre><code>df.with_columns([\n    pl.col('dt').dt.year().alias('year'),\n    pl.col('dt').dt.month().alias('month'),\n    pl.col('dt').dt.day().alias('day'),\n])\n</code></pre>"},{"location":"Python/Polars/Datetime/#add-days-to-date-column","title":"add days to date column","text":"<pre><code>df.with_columns(\n    dt = pl.col('dt') + pl.duration(days=1)\n)\n</code></pre>"},{"location":"Python/Polars/From/","title":"From","text":""},{"location":"Python/Polars/From/#default-include_indexfalse","title":"default include_index=False","text":"<pre><code>dl = pl.from_pandas(df, include_index=True)\n</code></pre>"},{"location":"Python/Polars/Group/","title":"Groupby","text":""},{"location":"Python/Polars/Group/#groupby-performance","title":"groupby performance","text":"<p>5x faster <pre><code>l = dl.group_by(['region', 'country', 'city']).agg(pl.col('value').sum())\nf = df.groupby(['region', 'country', 'city'])[['value']].sum())\n</code></pre></p>"},{"location":"Python/Polars/Issue/","title":"Issue","text":""},{"location":"Python/Polars/Issue/#polars-memory-issue","title":"polars memory issue","text":"<p>Allocated memory not released to OS: - https://github.com/pola-rs/polars/issues/23128</p> <p>Workaround: avoid unnecessary memory allocation. For example, <code>with_columns</code> will create a copy of the original columns. </p>"},{"location":"Python/Polars/Issue/#polars-integer-calculation-not-upcasted","title":"polars integer calculation not upcasted","text":"<p>Polars does not automatically upcast integer types during arithmetic <pre><code>from datetime import datetime, timedelta\ndf = (\n    pl.DataFrame({\n        'ts': pl.datetime_range(\n            start=datetime(2025,7,1,2,0,0),\n            end=datetime(2025,7,1,2,20,0),\n            interval='5m',\n            eager=True,\n         )\n    })\n    .with_columns(\n        hour=pl.col('ts').dt.hour(),\n        minute=pl.col('ts').dt.minute(),\n        minutes=pl.col('ts').dt.hour() * 60 + pl.col('ts').dt.minute(),\n    )\n)\nprint(df)\n# shape: (5, 4)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 ts                  \u2506 hour \u2506 minute \u2506 minutes \u2502\n# \u2502 ---                 \u2506 ---  \u2506 ---    \u2506 ---     \u2502\n# \u2502 datetime[\u03bcs]        \u2506 i8   \u2506 i8     \u2506 i8      \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 2025-07-01 02:00:00 \u2506 2    \u2506 0      \u2506 120     \u2502\n# \u2502 2025-07-01 02:05:00 \u2506 2    \u2506 5      \u2506 125     \u2502\n# \u2502 2025-07-01 02:10:00 \u2506 2    \u2506 10     \u2506 -126    \u2502\n# \u2502 2025-07-01 02:15:00 \u2506 2    \u2506 15     \u2506 -121    \u2502\n# \u2502 2025-07-01 02:20:00 \u2506 2    \u2506 20     \u2506 -116    \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"Python/Polars/Lazy/","title":"Lazy","text":""},{"location":"Python/Polars/Lazy/#eager-and-lazy-apis","title":"Eager and Lazy APIs","text":"<p>https://benfeifke.com/posts/the-3-reasons-why-i-switched-from-pandas-to-polars-20230328/ <pre><code>                    Eager API        Lazy API\n---------------------------------------------------\nDataframe Creation  DataFrame()      LazyFrame()\nInput CSV           read_csv()       scan_csv()\nInput Parquet       read_parquet()   scan_parquet()\nOutput Parquet      write_parquet()  sink_parquet()\n</code></pre></p>"},{"location":"Python/Polars/Memory/","title":"Memory","text":""},{"location":"Python/Polars/Memory/#polars-mem","title":"polars mem","text":"<ul> <li>Polars switched to <code>jemalloc</code></li> <li>Fragmentation in <code>mimalloc</code> allocator will lead to memory increase (leak? workaround: <code>MIMALLOC_ABANDONED_PAGE_RESET = 1</code>): https://github.com/pola-rs/polars/issues/18074</li> <li>A memory allocator cannot return the memory to the OS because it is fragmented</li> <li>Run operations with memory allocation in another process (<code>multiprocessing</code>) will help - memory will released once the process finished</li> </ul> <p>free memory: <pre><code>del df\ndel d2\ngc.collect()\n\ndf.estimated_size('mb')\n</code></pre></p>"},{"location":"Python/Polars/Memory/#jemalloc","title":"jemalloc","text":"<p>In jemalloc, memory goes through multiple stages: - <code>Active</code>: currently in use. - <code>Dirty</code>: freed but not yet zeroed; can be reused by the program. - <code>Muzzy</code>: freed and zeroed, but not yet returned to the OS. - <code>Returned</code>: truly unmapped (returned to the OS).</p> <p><code>muzzy_decay_ms:-1</code>: Jemalloc never releases zeroed (muzzy) memory to the OS. why using <code>muzzy_decay_ms:-1</code>? - page faults will significantly slowdown operations - <code>muzzy_decay_ms:-1</code> will disable <code>MADV_DONTNEED</code></p> <p>Setting the parameters via env var before loading polars: <pre><code>_RJEM_MALLOC_CONF=\"background_thread:true,dirty_decay_ms:500,muzzy_decay_ms:-1\"\n</code></pre></p>"},{"location":"Python/Polars/Merge/","title":"Merge","text":""},{"location":"Python/Polars/Merge/#vertical-concat","title":"vertical concat","text":"<ul> <li>all dfs must have the same cols and same order <pre><code>import polars as pl\nd1 = pl.DataFrame({\n    'x': [1, 2, 3],\n    'y': [4, 5, 6]\n})\nd2 = pl.DataFrame({\n    'y': [7, 8, 9],\n    'x': [10, 11, 12]\n})\ntry:\n    df = pl.concat([d1, d2], how='vertical')\n    print(df)\nexcept Exception as e:\n    print(f'Error: {e}')\n# output\n# Error: unable to vstack, column names don't match: \"x\" and \"y\"\n</code></pre></li> </ul>"},{"location":"Python/Polars/Merge/#join","title":"join","text":"<p>lazy join with streaming is 2x faster <pre><code>df.lazy().join(dl.lazy(), on=['id', 'id2'], how='left').filter(\n    (pl.col('day') &lt; pl.col('day_right')) &amp; (pl.col('day_right') - pl.col('day') &lt;= 30)\n).collect()\n\ndf.lazy().join(dl.lazy(), on=['id', 'id2'], how='left').filter(\n    (pl.col('day') &lt; pl.col('day_right')) &amp; (pl.col('day_right') - pl.col('day') &lt;= 30)\n).collect(streaming=True)\n</code></pre></p> <p>join is 2x faster than pandas <pre><code>d1.merge(d2, left_index=True, right_index=True) #index: id, date\nl1.join(l2, on=['id', 'date'], how='inner')\n</code></pre></p>"},{"location":"Python/Polars/Pandas/","title":"From Pandas to Polars","text":"<p>https://www.rhosignal.com/posts/polars-pandas-cheatsheet/</p>"},{"location":"Python/Polars/Parquet/","title":"Parquet","text":""},{"location":"Python/Polars/Parquet/#read_parquet-vs-scan_parquet","title":"<code>read_parquet</code> vs <code>scan_parquet</code>","text":"<ul> <li><code>read_parquet</code> will load all data in RAM an cannot apply any optimization to the scan level</li> <li><code>scan_parquet</code> is recommended when dealing with larger file sizes</li> </ul>"},{"location":"Python/Polars/Parquet/#read-parquet-with-filters","title":"Read parquet with filters","text":"<p>https://github.com/pola-rs/polars/issues/3964 - generate random df - read parquet using duckdb - read parquet using polars - cannot use string datetime (polars will cast column to string) - have to use datetime</p> <p>Performance benchmark (parquet with index 40 MB) - file size is similar to without category - both filter string and category columns are in index - duckdb is not sensitive to index/category - pandas has to read all index columns so became slow - category is the winner <pre><code>    String                                                 Category\n    all    filter_string + columns  filter_date + columns  all    filter_category + columns  filter_date + columns\npl: 2.87s  360ms                    727ms                  762ms  315ms                      524ms\npd: 6.48s  1.23s                    2.92s                  977ms  342ms                      537ms\ndk: 5.81s  575ms                    1.15s                  5.76s  512ms                      988ms\n</code></pre></p> <p>Performance benchmark (parquet without index 40 MB) - file size is similar to without category - best to save parquet file without index with category and read it with pandas. <pre><code>    String                                                 Category\n    all    filter_string + columns  filter_date + columns  all    filter_category + columns  filter_date + columns\npl: 3.21s  445ms                    758ms                  914ms  349ms                      552ms\npd: 3.33s  941ms                    1.10s                  684ms  299ms                      306ms\ndk: 5.57s  597ms                    1.01s                  5.56s  536ms                      983ms\n</code></pre></p> <p>polars <pre><code>import polars as pl\nfrom datetime import datetime\n\ndf = (\n    pl\n    .scan_parquet('df.parquet')\n    .filter(pl.col('dt') &gt;= datetime(2020,4,1))\n    .filter(pl.col('val').is_not_null())\n    .select(['c1', 'c2', 'c4'])\n).collect().to_pandas()\n\nprint(df.shape)\n</code></pre></p> <p>duckdb <pre><code>import duckdb\n\nquery = \"\"\"\nselect\n    c1, c2, c4\nfrom\n    read_parquet('df.parquet')\nwhere\n    dt &gt;= '2020-04-01'\n    and val is not null\n\"\"\"\ndf = duckdb.query(query).to_df()\n\nprint(df.shape)\n</code></pre></p>"},{"location":"Python/Polars/Performance/","title":"Performance","text":"<p>https://medium.com/cuenex/pandas-2-0-vs-polars-the-ultimate-battle-a378eb75d6d1</p> <p>https://able.bio/haixuanTao/data-manipulation-polars-vs-rust--3def44c8</p> <ul> <li>good at groupby and merge</li> </ul>"},{"location":"Python/Polars/Polars/","title":"Polars","text":""},{"location":"Python/Polars/Polars/#install","title":"Install","text":"<pre><code>mamba install -c conda-forge polars-lts-cpu -y\npip install polars-lts-cpu\n</code></pre>"},{"location":"Python/Polars/Polars/#book","title":"book","text":"<p>https://github.com/PacktPublishing/Polars-Cookbook</p>"},{"location":"Python/Polars/SQL/","title":"SQL","text":""},{"location":"Python/Polars/SQL/#read-sql","title":"read sql","text":"<p>For oracle db to get lowercase col names, use <code>as \"col_name\"</code> <pre><code>import sqlalchemy as sa\nimport get_database_config\n\ncfg = get_database_config('my-config-file')\nurl = sa.engine.url.URL.create(**cfg)\nengine = sa.create_engine(url)\n# session = sa.orm.Session(bind=engine)\nSessionLocal = sa.orm.sessionmaker(bind=engine)\n# with SessionLocal() as session:\n#   ...\n\nquery = \"\"\"\n    select SALEDATE, REGION, ID, QUANTITY\n    from db.SALES\n    where SALEDATE = TO_DATE(:date, 'YYYY-MM-DD HH24:MI:SS')\n\"\"\"\ndata_schema = {\n    'SALEDATE': pl.Datetime('ns'),\n    'REGION': pl.Utf8,\n    'ID': pl.Int32,\n    'QUANTITY': pl.Float32,\n}\ndf = pl.read_database(\n    query=query,\n    connection=engine, #or session.connection()\n    schema_overrides=data_schema,\n    execute_options={'parameters': {'date': '2025-05-29 12:00:00'}},\n)\n</code></pre></p>"},{"location":"Python/Polars/Slicing/","title":"Slicing","text":""},{"location":"Python/Polars/Slicing/#category-and-date","title":"category and date","text":"<pre><code>dl.filter(pl.col('code') == '4311251697')   #10.3ms\ndf[df['code'].values == '4311251697']       #12.2ms\n\ndl.filter(pl.col('ts') &gt;= datetime(2022,8,1)).filter(pl.col('ts') &lt; datetime(2022,9,1))      #314ms\ndf[(df['ts'].values &gt;= pd.Timestamp(2022,8,1)) &amp; (df['ts'].values &lt; pd.Timestamp(2022,9,1))] #236ms\n</code></pre>"},{"location":"Python/Polars/Sorting/","title":"Sorting","text":""},{"location":"Python/Polars/Sorting/#sort-values","title":"sort values","text":"<p>2-3x faster <pre><code>dl.sort(['city', 'ts'], descending=False)\ndf.sort_values(by=['city', 'ts'], ascending=True)\n</code></pre></p>"},{"location":"Python/PyArrow/CSV/","title":"CSV","text":"<p>pyarrow.csv is much faster than pandas to read csv. see: https://towardsdatascience.com/stop-using-pandas-to-read-write-data-this-alternative-is-7-times-faster-893301633475</p>"},{"location":"Python/PyArrow/CSV/#benchmark","title":"benchmark","text":"<pre><code>import pandas as pd\nimport pyarrow as pa\nimport pyarrow.csv as pv\n\ndtype = {\n    'name': str,\n    'good': bool,\n    'value':np.float64,\n}\nconvert_options = pv.ConvertOptions(\n    column_types={\n        'name': pa.string(),\n        'date': pa.timestamp('s'),\n        'good': pa.bool_(),\n        'value': pa.float64(),\n    },\n)\n\nd0 = pd.read_csv(f, engine='pyarrow').astype(dtype)              #???\nd1 = pd.read_csv(f, dtype=dtype, parse_dates=['date'])           #2.41 s \u00b1 44.3 ms\nd2 = pv.read_csv(f).to_pandas().astype(dtype)                    #466 ms \u00b1 125 ms\nd3 = pv.read_csv(f, convert_options=convert_options).to_pandas() #276 ms \u00b1 29.7 ms, to pandas\nd4 = pv.read_csv(f, convert_options=convert_options)             #175 ms \u00b1 37.1 ms, to pa.Table\n</code></pre>"},{"location":"Python/PyArrow/CSV/#example","title":"example","text":"<pre><code>pyarrow.csv.read_csv(input_file, read_options=None, parse_options=None, convert_options=None, MemoryPool memory_pool=None)\n</code></pre>"},{"location":"Python/PyArrow/CSV/#read_options","title":"read_options","text":"<p>https://arrow.apache.org/docs/python/generated/pyarrow.csv.ReadOptions.html#pyarrow.csv.ReadOptions <pre><code>pyarrow.csv.ReadOptions(\n    use_threads=None,\n    *,\n    block_size=None,\n    skip_rows=None,\n    skip_rows_after_names=None,\n    column_names=None,\n    autogenerate_column_names=None,\n    encoding='utf8',\n)\n</code></pre></p>"},{"location":"Python/PyArrow/CSV/#parse_options","title":"parse_options","text":"<p>https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html#pyarrow.csv.ParseOptions <pre><code>pyarrow.csv.ParseOptions(\n    delimiter=None,\n    *,\n    quote_char=None,\n    double_quote=None,\n    escape_char=None,\n    newlines_in_values=None,\n    ignore_empty_lines=None,\n    invalid_row_handler=None,\n)\n</code></pre></p>"},{"location":"Python/PyArrow/CSV/#convert_options","title":"convert_options","text":"<p>https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html <pre><code>pyarrow.csv.ConvertOptions(\n    check_utf8=None,\n    *,\n    column_types=None,\n    null_values=None,\n    true_values=None,\n    false_values=None,\n    decimal_point=None,\n    strings_can_be_null=None,\n    quoted_strings_can_be_null=None,\n    include_columns=None,\n    include_missing_columns=None,\n    auto_dict_encode=None,\n    auto_dict_max_cardinality=None,\n    timestamp_parsers=None,\n)\n\n# column types\nconvert_options = csv.ConvertOptions(\n    column_types={\"n_legs\": pa.float64()},\n)\n\n# select columns\nconvert_options = csv.ConvertOptions(\n    include_columns=[\"animals\", \"n_legs\"],\n    include_missing_columns=True,\n)\n\n# datetime parser\nconvert_options = csv.ConvertOptions(\n    timestamp_parsers=[\"%m/%d/%Y\", \"%m-%d-%Y\"],\n    auto_dict_encode=True,\n)\n</code></pre></p>"},{"location":"Python/PyArrow/DataType/","title":"data type","text":"<p>https://arrow.apache.org/docs/python/api/datatypes.html</p> <p>https://arrow.apache.org/docs/python/generated/pyarrow.dictionary.html</p>"},{"location":"Python/PyArrow/DataType/#pyarrow-to-numpy","title":"pyarrow to numpy","text":"<p>Many numpy functions still does not support pyarrow data types. When using pyarrow backend we must convert df cols to numpy first: <pre><code>df[cols].to_numpy(dtype=np.float32)\n</code></pre></p>"},{"location":"Python/PyArrow/Issue/","title":"Issue","text":""},{"location":"Python/PyArrow/Issue/#import-pyarrowlib-as-_lib","title":"import pyarrow.lib as _lib","text":"<p>ImportError: DLL load failed while importing lib: The specified procedure could not be found.</p> <p>solution: <pre><code>pip install --upgrade pyarrow\n</code></pre></p>"},{"location":"Python/PyArrow/Issue/#pyarrow-read-string-column-with-all-none-values","title":"pyarrow read string column with all None values","text":"<p>https://arrow.apache.org/docs/python/generated/pyarrow.Table.html - Be aware that Series of the <code>object</code> dtype don't carry enough information to always lead to a meaningful Arrow type. - In the case that we cannot infer a type, e.g. because the DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to <code>null</code>. - This behavior can be avoided by constructing an explicit schema and passing it to this function</p> <p>Related issues: - https://github.com/apache/arrow/issues/2110 - https://github.com/apache/arrow/issues/19053</p>"},{"location":"Python/PyArrow/Parquet/","title":"Parquet","text":""},{"location":"Python/PyArrow/Parquet/#save-parquet-file-without-index-index-levels-to-columns","title":"save parquet file without index (index levels to columns)","text":"<p>The read speed for parquet files can be much faster without index, by convert index levels to column.  - all index levels have to be loaded - so this is slow - if only need a specific column, other converted index columns will not be loaded <pre><code>df = pd.read_parquet(\n    '/path/to/my.parquet', \n    filters=[('ts', '&gt;=', pd.to_datetime('2023-12-15'))],\n    columns=['ts', 'val1', 'val2'],\n)\n</code></pre></p>"},{"location":"Python/PyArrow/Parquet/#df-and-parquet-bytes-conversion","title":"df and parquet bytes conversion","text":"<pre><code>import os\nimport pandas as pd\n\n# df to parquet bytes\ndf = pd.DataFrame()\ndf_parquet_bytes = df.to_parquet()\ndf_parquet_bytes\nb'PAR1\\x15\\x04\\x15\\x0...'\n\n# parquet bytes to df\nparquet_file = io.BytesIO(df_parquet_bytes)\ndf = pd.read_parquet(parquet_file)\n</code></pre>"},{"location":"Python/PyArrow/Parquet/#save-patable-to-parquet-file","title":"save pa.table to parquet file","text":"<pre><code>import pyarrow as pa\nimport pyarrow.parquet as pq\ntable = pa.table({\n    'n_legs': [2, 2, 4, 4],\n    'animal': ['Flamingo', 'Parrot', 'Dog', 'Horse'],\n})\n\n# directly write to file\npq.write_table(table=table, where='data.parquet', compression='zstd')\n\n# write to buffer first\nbuf = pa.BufferOutputStream()\npq.write_table(table=table, where=buf, compression='zstd')\n\nblob = buf.getvalue()\nbuf = pa.py_buffer(blob)\nwith fs.open('data.parquet', mode='wb') as f:\n    f.write(buf)\n</code></pre>"},{"location":"Python/PyArrow/Parquet/#compression","title":"compression","text":"<p><pre><code>size = []\ntims = []\nimport time\ndf = pd.read_parquet('test.parquet').reset_index()\nfor c in [None, 'snappy', 'gzip', 'brotli']:\n    t0 = time.time()\n    bio = io.BytesIO(df.to_parquet(compression = c))\n    tims.append(time.time() - t0)\n    byts = bio.getbuffer().nbytes\n    size.append(byts)\nfor i, s in enumerate(size):\n    print(f'Size: {s / size[0]:.3f}, Time: {tims[i]:.3f}')\n</code></pre> Output: <pre><code>None   - Size: 1.000, Time:  2.952\nsnappy - Size: 0.961, Time:  2.961\ngzip   - Size: 0.897, Time:  7.333\nbrotli - Size: 0.601, Time: 10.218\n</code></pre></p>"},{"location":"Python/PyArrow/Table/","title":"Table","text":"<p>https://arrow.apache.org/docs/python/generated/pyarrow.Table.html</p> <p>https://arrow.apache.org/cookbook/py/data.html</p>"},{"location":"Python/PyArrow/Table/#get-col-unique-values","title":"get col unique values","text":"<pre><code>pyarrow.Table.column('nmi_code').unique()\n</code></pre>"},{"location":"Python/PyArrow/Table/#select-columns","title":"select columns","text":"<pre><code>table.select(['year'])\n</code></pre>"},{"location":"Python/PyArrow/Table/#drop-columns","title":"drop columns","text":"<pre><code>table.drop_columns('animals')\ntable.drop_columns(['n_legs', 'animals'])\n</code></pre>"},{"location":"Python/PyArrow/Table/#select-rows","title":"select rows","text":"<pre><code>table.take([1,3])\n</code></pre>"},{"location":"Python/PyArrow/Table/#select-rows-based-on-condition","title":"select rows based on condition","text":"<pre><code>import pyarrow.compute as pc\nexpr = pc.field('year') &lt;= 2020\ntable.filter(expr)\n\ntable.filter(pc.field('start_date') &lt; pc.field('end_date'))\n</code></pre>"},{"location":"Python/PyArrow/Table/#get-rows-with-zero-copy","title":"get rows with zero-copy","text":"<pre><code>table.slice(offset=0, length=3)\n</code></pre>"},{"location":"Python/PyArrow/Table/#sort_by","title":"sort_by","text":"<pre><code>table.sort_by('animal')\n</code></pre>"},{"location":"Python/PyArrow/Table/#group_by","title":"group_by","text":"<pre><code>table.group_by('year').aggregate([('n_legs', 'sum')])\n</code></pre>"},{"location":"Python/PySpark/Column/","title":"Column","text":""},{"location":"Python/PySpark/Column/#string-col-name","title":"string col name","text":"<p>Can use column names as strings - DataFrame APIs such as <code>select</code>, <code>groupBy</code>, <code>orderBy</code> etc - No transformations on any column in any function</p>"},{"location":"Python/PySpark/Column/#distinct","title":"distinct","text":"<pre><code>df.select('CustomerID').distinct().count()\n</code></pre>"},{"location":"Python/PySpark/Column/#convert-col-types","title":"convert col types","text":"<pre><code>from pyspark.sql.functions import col, to_timestamp\nd = (\n    df\n    .withColumn('InvoiceNo', col('InvoiceNo').cast('int'))\n    .withColumn('Quantity', col('Quantity').cast('double'))\n    .withColumn('InvoiceDate', to_timestamp('InvoiceDate', 'dd/MM/yyyy HH:mm'))   \n)\nd.show(5,0)\n</code></pre>"},{"location":"Python/PySpark/DataFrame/","title":"DataFrame","text":""},{"location":"Python/PySpark/DataFrame/#from-list","title":"from list","text":"<p>https://stackoverflow.com/questions/65070639/create-empty-dataframe-from-list-is-x4-times-slower-than-from-emptyrdd</p> <p>creating empty DataFrame from list is 4-100 times slower than from emptyRDD() <pre><code>spark.createDataFrame([], schema)\nspark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n</code></pre></p> <p>Reason: - <code>spark.sparkContext.emptyRDD()</code> creates an RDD with zero partitions - <code>spark.createDataFrame([], schema)</code> creates a DataFrame with at least one partition. The overhead is due to tasks on empty partitions.</p>"},{"location":"Python/PySpark/Error/","title":"Error","text":""},{"location":"Python/PySpark/Error/#java-gateway-process-exited-before-sending-its-port-number","title":"Java gateway process exited before sending its port number","text":""},{"location":"Python/PySpark/Error/#caused-by-javanetsockettimeoutexception-connect-timed-out","title":"Caused by: java.net.SocketTimeoutException: connect timed out","text":"<p>Even run Spark in local mode, it communicates over the network just like any other Spark deployment.  This means that the Firewall can potentially affect the communication between different Spark components.</p> <p>In local mode, Spark's <code>driver program</code> communicates with the <code>worker threads</code> over TCP/IP, and the default port used is typically 7077. </p> <p>Reason: Firewall blocked the java binary to access the network</p> <p>Check if there is a firewall blocking the REST API call from the cluster to DIS nodes. </p> <p>Try this??? https://stackoverflow.com/questions/60916259/sparkexception-python-worker-failed-to-connect-back-when-execute-spark-action <pre><code>HADOOP_HOME = C:\\Hadoop\nJAVA_HOME = C:\\Java\\jdk-11.0.6\nPYSPARK_DRIVER_PYTHON = jupyter\nPYSPARK_DRIVER_PYTHON_OPTS = notebook\nPYSPARK_PYTHON = python\n</code></pre></p> <p>try this??? <pre><code>SPARK_LOCAL_IP = 127.0.0.1\n</code></pre></p>"},{"location":"Python/PySpark/Error/#orgapachesparksparkexception-python-worker-failed-to-connect-back","title":"org.apache.spark.SparkException: Python worker failed to connect back","text":"<p>could not find python <pre><code>PYSPARK_PYTHON = c:/my/folder/to/python.exe\nos.environ['PYSPARK_PYTHON'] = 'C:/Users/sma/conda-envs/elchapo/python.exe'\n</code></pre> This solves the previous issue: java.net.SocketTimeoutException: connect timed out</p>"},{"location":"Python/PySpark/FileIO/","title":"File IO","text":""},{"location":"Python/PySpark/FileIO/#csv","title":"csv","text":"<p>with header <pre><code>df = spark.read.csv('data.csv', header=True, escape='\\\"')\ndf = spark.read.load('data.csv', format='csv', header=True)\ndisplay(df.limit(5)) #what is this???\n</code></pre></p> <p>without header <pre><code>from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\ncsv_schema = StructType([\n    StructField('id', IntegerType()),\n    StructField('name', StringType()),\n    StructField(\"date\", DateType()),\n    StructField('price', FloatType()),\n])\n\ndf = spark.read.load('data.csv', format='csv', header=False, schema=csv_schema)\ndisplay(df.limit(5)) #what is this???\n</code></pre></p>"},{"location":"Python/PySpark/Function/","title":"Function","text":""},{"location":"Python/PySpark/Function/#lit","title":"lit","text":"<p>Literal <pre><code>df.select(concat('country', ', ', 'description'))      #wrong\ndf.select(concat('country', lit(', '), 'description')) #correct\ndf.select(concat('country', lit(', '), 'description').alias('cd')) #provide name\n</code></pre></p>"},{"location":"Python/PySpark/Function/#col","title":"col","text":"<p>Return a <code>col</code> expression https://stackoverflow.com/questions/55105363/pyspark-dataframe-column-reference-df-col-vs-dfcol-vs-f-colcol <pre><code>df.withColumn('typ', when(df['id'].isin([1, 2]), 'animal').otherwise('plant')) #ok\ndf.withColumn('typ', when(col('id').isin([1, 2]), 'animal').otherwise('plant')) #better\ndf.withColumn('typ', when(col('id').isin([1, 2]), col('animal')).otherwise(col('plant'))) #select cols, not string\n</code></pre></p> <p><code>col</code> can be re-used as it's not df specific and can be used before the df is assigned. <pre><code>age = col('dob') / 365\nif_expr = when(age &lt; 18, 'underage').otherwise('adult')\n\ndf1 = df.read.csv(path).withColumn('age_category', if_expr)\ndf2 = df.read.parquet(path).select('*', age.alias('age'), if_expr.alias('age_category'))\n</code></pre></p>"},{"location":"Python/PySpark/Group/","title":"Group","text":""},{"location":"Python/PySpark/Group/#select-and-where","title":"select and where","text":"<pre><code>d = (\n    df\n    .select('name', 'price')\n    .where((df['id']==1) | (df['id']==3))\n)\ndisplay(d)\n</code></pre>"},{"location":"Python/PySpark/Group/#count","title":"count","text":"<pre><code>d = (\n    df\n    .select('id', 'name')\n    .groupBy('name')\n    .count()\n)\ndisplay(d)\n</code></pre>"},{"location":"Python/PySpark/Group/#count-distinct","title":"count distinct","text":"<pre><code>d = df.groupBy('Country').agg(countDistinct('CustomerID').alias('country_count')).orderBy(desc('country_count'))\nd.show()\n</code></pre>"},{"location":"Python/PySpark/Group/#minmax","title":"min/max","text":"<pre><code>#d1.agg({'InvoiceDate': 'min', 'InvoiceDate': 'max'}).first()[0] # not work as dict cannot have same keys\nfrom pyspark.sql import functions as f\nd = df.agg(\n    f.min('InvoiceDate').alias('mn'),\n    f.max('InvoiceDate').alias('mx'),\n    f.avg('InvoiceDate').alias('ag'),\n    f.sum('InvoiceDate').alias('sm'),\n).first()\nmn, mx, ag, sm = d #d is a Row, we can also get mn by d.mn or d['mn'] or d[:1]\n</code></pre>"},{"location":"Python/PySpark/Install/","title":"Install","text":""},{"location":"Python/PySpark/Install/#databricks-community-cloud","title":"Databricks Community Cloud","text":"<p>It's free for Community Edition: https://community.cloud.databricks.com/login.html</p>"},{"location":"Python/PySpark/Install/#locally-on-windows","title":"Locally On Windows","text":"<ul> <li>install jdk: https://www.oracle.com/au/java/technologies/downloads/</li> <li>add java <code>bin</code> path to system env var <code>PATH</code> and check java: <code>java --version</code></li> <li>download apache spark: https://spark.apache.org/downloads.html</li> <li>unzip to a folder <code>c:/spark</code> and add to env var \"SPARK_HOME\" and add <code>bin</code> to env path</li> <li>check park version <code>spark-submit --version</code></li> <li>install pyspark <code>pip install pyspark[ml,mllib,sql,pandas_on_spark]</code></li> <li>copy <code>winutils.exe</code> from <code>https://github.com/cdarlint/winutils/tree/master/hadoop-3.3.5/bin</code> to <code>c:/hadoop/bin</code>   and set env var <code>HADOOP_HOME</code> to <code>c:/hadoop</code>???</li> </ul>"},{"location":"Python/PySpark/Learn/","title":"Learn","text":""},{"location":"Python/PySpark/Learn/#basic","title":"Basic:","text":"<ul> <li>https://lnkd.in/g-gCpUyi</li> <li>https://www.youtube.com/watch?v=S2MUhGA3lEw</li> <li>https://www.projectpro.io/apache-spark-tutorial/pyspark-tutorial</li> <li>https://www.youtube.com/watch?v=J62lYtIq0M8</li> </ul>"},{"location":"Python/PySpark/Learn/#basic-compared-to-pandas","title":"basic compared to pandas","text":"<p>https://levelup.gitconnected.com/pyspark-dos-and-donts-15fa4f6433d8</p>"},{"location":"Python/PySpark/Learn/#basic-for-ml","title":"basic for ml","text":"<p>https://www.datacamp.com/tutorial/pyspark-tutorial-getting-started-with-pyspark</p>"},{"location":"Python/PySpark/Learn/#theory","title":"theory","text":""},{"location":"Python/PySpark/Learn/#examples","title":"examples","text":"<p>https://sparkbyexamples.com/pyspark/different-ways-to-create-dataframe-in-pyspark/</p>"},{"location":"Python/PySpark/Performance/","title":"Performance","text":"<p>https://spark.apache.org/docs/latest/sql-performance-tuning.html</p> <p>https://sparkbyexamples.com/spark/spark-performance-tuning/</p> <p>https://engineering.salesforce.com/how-to-optimize-your-apache-spark-application-with-partitions-257f2c1bb414/ - caching - sql hint - partition - dynamic repartition - dynamic resource allocation</p>"},{"location":"Python/PySpark/PySpark/","title":"PySpark","text":""},{"location":"Python/PySpark/PySpark/#job","title":"Job","text":"<p>Code with input and output to process data.</p>"},{"location":"Python/PySpark/PySpark/#stage","title":"Stage","text":"<p>Jobs are divided into stages.</p>"},{"location":"Python/PySpark/PySpark/#task","title":"Task","text":"<p>One task per partition - each stage has some tasks.</p>"},{"location":"Python/PySpark/PySpark/#pyspark-vs-dask","title":"PySpark vs Dask","text":"<p>https://www.coiled.io/blog/moving-from-spark-to-dask</p> <ul> <li>spark for larger data, need to run on a cloud cluster</li> <li>dask for smaller data, faster and easier to integrate into python, can run locally</li> </ul>"},{"location":"Python/PySpark/PySpark/#spark-version","title":"Spark version","text":"<pre><code>print('spark version = ', SparkSession.builder.appName('test').getOrCreate().version)\npip show pyspark\n</code></pre>"},{"location":"Python/PySpark/RDD/","title":"RDD (Resilient Distributed Dataset)","text":""},{"location":"Python/PySpark/RDD/#create-rdd","title":"Create RDD","text":""},{"location":"Python/PySpark/RDD/#cache-rdd","title":"Cache RDD","text":""},{"location":"Python/PySpark/RDD/#rdd-performance","title":"rdd performance","text":"<p>We should try to avoid using RDD as serialization and de-serialization are very expensive operations for Spark applications or any distributed systems -  most of the time is spent only on serialization of data rather than executing the operations.</p>"},{"location":"Python/PySpark/SQL/","title":"SQL","text":""},{"location":"Python/PySpark/SQL/#create-view-and-table","title":"create view and table","text":"<pre><code>df.createOrReplaceTempView(\"viw\") #df to temp view\nsaveAsTable                       #df to table\nspark.catalog.createTable         #empty table\nspark.catalog.createExternalTable #external table\n</code></pre>"},{"location":"Python/PySpark/SQL/#spark-sql-api","title":"Spark SQL API","text":"<pre><code>ds = spark.sql('''\n    SELECT id, name, price\n    FROM products\n    WHERE category IN ('Mountain Bikes', 'Road Bikes')\n''')\ndisplay(ds)\n\ndf = ds.toPandas()\n</code></pre>"},{"location":"Python/PySpark/SQL/#sql-magic","title":"SQL magic","text":"<pre><code>%sql\n\nSELECT category, COUNT(id) AS cnt\nFROM products\nGROUP BY category\nORDER BY category\n</code></pre>"},{"location":"Python/PySpark/Session/","title":"Session","text":""},{"location":"Python/PySpark/Session/#create-session","title":"create session","text":"<pre><code>from pyspark.sql import SparkSession\nspark = (\n    SparkSession\n    .builder\n    .appName('spark-local')\n    .master('local[*]')\n    .config('spark.memory.offHeap.enabled', 'true')\n    .config('spark.memory.offHeap.size', '10g')\n    .getOrCreate()\n)\n...\nspark.stop()\n</code></pre>"},{"location":"Python/PySpark/Session/#buildermaster","title":"builder.master","text":"<p>Sets the Spark master URL to connect to, such as - <code>local[*]</code>: run locally with all cores - <code>local[1]</code>: run locally with 1 core    - much faster than <code>local[*]</code> for <code>spark.createDataFrame</code> but    - for data processing we need multiple threads otherwise can be very slow - <code>spark://&lt;master-ip&gt;:7077</code>: run on a Spark standalone cluster - <code>yarn</code>: yse YARN as cluster manager - cli: <code>spark-submit --master local[2] my_app.py</code></p>"},{"location":"Python/PySpark/Session/#sparkdefaultparallelism","title":"spark.default.parallelism","text":"<p>https://sparkbyexamples.com/spark/difference-between-spark-sql-shuffle-partitions-and-spark-default-parallelism/</p> <ul> <li>only applicable to RDD </li> <li>default value is the number of all cores on all nodes in a cluster</li> <li>on local, it is set to the number of cores on your system</li> </ul> <p>For RDD, transformations like <code>reduceByKey()</code>, <code>groupByKey()</code>, <code>join()</code> triggers the data shuffling.  Set the desired partitions for shuffle operations. <pre><code>spark.conf.set('spark.default.parallelism', 100)\n</code></pre></p>"},{"location":"Python/PySpark/Session/#sparksqlshufflepartitions","title":"spark.sql.shuffle.partitions","text":"<ul> <li>only works with DataFrame</li> <li>default value for this configuration is 200</li> </ul> <p>For DataFrame, transformations like <code>groupBy()</code>, <code>join()</code> triggers the data shuffling.  Set the value using: <pre><code>spark.conf.set('spark.sql.shuffle.partitions', 100)\n</code></pre></p> <p>Set the values in cli <pre><code>spark-submit --conf spark.sql.shuffle.partitions=100 --conf spark.default.parallelism=100\n</code></pre></p>"},{"location":"Python/PySpark/Session/#default-partitions-casued-slow-performance","title":"default partitions casued slow performance","text":"<p>https://stackoverflow.com/questions/34625410/why-does-my-spark-run-slower-than-pure-python-performance-comparison</p> <p>Spark shuffle is a very expensive operation as it moves the data between executors or even between worker nodes in a cluster.  Spark automatically triggers the shuffle when we perform aggregation and join operations on RDD and DataFrame.</p> <p>By default when run spark in SQL Context or Hive Context it will use 200 partitions by default.  This will significantly slow down the performance when running locally. <pre><code>spark.conf.set('spark.sql.shuffle.partitions', 1)\n</code></pre></p>"},{"location":"Python/PySpark/Setup/","title":"Setup","text":""},{"location":"Python/PySpark/Setup/#load-data","title":"load data","text":"<pre><code>#df = spark.read.csv('data.csv', header=True, escape='\\\"')\ndf = spark.read.parquet('data.parquet')\n</code></pre>"},{"location":"Python/PySpark/Setup/#show-top-n-lines","title":"show top n lines","text":"<pre><code>df.show(5)                #truncate to 20 chars\ndf.show(5, 0)             #no truncate\ndf.show(5, vertical=True) #transpose\n</code></pre>"},{"location":"Python/PySpark/Setup/#basic-info","title":"basic info","text":"<pre><code>def sp_shape(df):\n    return df.count, len(df.columns)\n# why not include timestamp cols???\ndf.describe().show() #count, mean, stdev, min, max\ndf.summary().show()  #count, mean, stdev, min, 25%, 50%, 75%, max\n</code></pre>"},{"location":"Python/PySpark/UDF/","title":"UDF","text":""},{"location":"Python/PySpark/UDF/#best-practices","title":"Best Practices","text":"<ul> <li>Optimize for Performance: If performance is critical, consider using Scala UDFs or exploring alternative approaches like vectorized operations or custom accumulators.</li> <li>Thorough Testing: Write comprehensive unit tests for your UDFs to guarantee their correctness and prevent regressions.</li> <li>Clear Documentation: Document your UDFs clearly, including input/output types, purpose, and usage examples. This facilitates collaboration and avoids confusion.</li> <li>Use Sparingly: Opt for UDFs only when necessary. Explore built-in Spark functions or vectorized operations whenever possible for better performance and readability.</li> </ul>"},{"location":"Python/PySpark/UDF/#pros","title":"Pros","text":"<ul> <li>Flexibility: UDFs empower you to introduce custom logic into your PySpark computations, extending built-in functionality. This is particularly valuable when standard functions fall short of your specific requirements.</li> <li>Language Choice: PySpark allows UDFs to be written in both Python and Scala. You can select the language that aligns best with your skillset and project needs.</li> <li>SQL Integration: Once registered, UDFs can be seamlessly used within SQL queries, enabling users familiar with SQL to leverage custom transformations. This streamlines workflows for teams with diverse programming backgrounds.</li> <li>Code Reusability: Defining a UDF once allows you to reuse it throughout your PySpark application. This promotes code maintainability, reduces redundancy, and improves development efficiency.</li> </ul>"},{"location":"Python/PySpark/UDF/#cons","title":"Cons","text":"<ul> <li>Performance Overhead: Python UDFs can introduce a significant performance penalty due to data serialization between the JVM (Java Virtual Machine) and the Python process. This overhead can be mitigated by using Scala UDFs, which are generally more performant but might still not always match the optimization level of native Spark functions.</li> <li>Debugging Challenges: Debugging UDFs, especially within complex Spark operations, can be cumbersome. PySpark's error messages for UDFs might not be as detailed as for native functions, requiring extra effort to pinpoint issues.</li> <li>Versioning Considerations: As UDFs are modified or new ones are added, managing versions and ensuring backward compatibility can become tricky. Careful planning and version control practices are crucial in such scenarios.</li> <li>Type Checking: PySpark's UDF engine doesn't perform automatic type checking. It's your responsibility to ensure that the input and output types of the UDF align with the expected data types. Type mismatches can lead to unforeseen errors in your PySpark application.</li> </ul>"},{"location":"Python/PySpark/Variable/","title":"Variable","text":""},{"location":"Python/PySpark/Variable/#broadcast","title":"Broadcast","text":"<p>A broadcast variable is read-only, created from the driver program object and made available to the nodes that will execute the computation.</p>"},{"location":"Python/PySpark/Variable/#accumulator","title":"Accumulator","text":"<p>An accumulator is also a variable that can be added to with limitation to ensure global sum is correct in parrallel run.</p>"},{"location":"Python/Pytorch/Install/","title":"install pytorch","text":"<pre><code>pip install torch===2.3.0 torchvision===0.18.0 -f https://download.pytorch.org/whl/torch_stable.html\n</code></pre>"},{"location":"Python/Regex/Regex/","title":"regex","text":""},{"location":"Python/Regex/Regex/#find-a-word-after-another-word","title":"find a word after another word","text":"<pre><code>re.compile(r'(?:from|join (\\w+)') #word after from or join\n</code></pre>"},{"location":"Python/Regex/Regex/#replace-empty-cell-by-nan","title":"replace empty cell by NaN","text":"<pre><code>df = df.replace(r'^\\s*$', np.nan, regex=True).fillna(value=np.nan)\n</code></pre>"},{"location":"Python/Regex/Regex/#replace-special-characters","title":"replace special characters","text":"<pre><code>#replace hyphen, en-dash, em-dash by dash\npattern = b'(\\xe2\\x80\\x90|\\xe2\\x80\\x93|\\xe2\\x80\\x94)'\ntxt2 = re.sub(pattern, b'-', txt.encode('utf-8')).decode('utf-8')\n</code></pre>"},{"location":"Python/SQL/ADBC/","title":"ADBC","text":""},{"location":"Python/SQL/ADBC/#arrow-database-connectivity-adbc-driver","title":"arrow database connectivity (adbc) driver","text":"<p>https://arrow.apache.org/blog/2023/01/05/introducing-arrow-adbc/</p>"},{"location":"Python/SQL/Access/","title":"Access","text":""},{"location":"Python/SQL/Access/#pyodbc-like-wildcard","title":"pyodbc LIKE wildcard","text":"<p>LIKE wildcard characters between queries run in Access and from an external applications are different: Access uses the asterisk as the wildcard character, \"2019-09-03*\"; External application (like Python) uses the percent sign as the wildcard character, \"2019-09-03%\".</p>"},{"location":"Python/SQL/Access/#dao-field-type","title":"DAO field type","text":"<p>http://allenbrowne.com/ser-49.html</p>"},{"location":"Python/SQL/Access/#dao-connect-timestamp","title":"DAO connect timestamp","text":"<p>df.timestamp should be changed to str <pre><code>is_timestamp = pd.core.dtypes.common.is_datetime_or_timedelta_dtype(series)\nif is_timestamp:\n    changes datetime column to str\n</code></pre></p>"},{"location":"Python/SQL/Access/#dao-connect-query","title":"DAO connect query","text":"<pre><code>import os\nimport csv\nimport win32com.client\n\ndef open_db(eng, dbpath, lock=False):\n    if not os.path.isfile(dbpath):\n        log(f'File does not exist.\\n    File: {dbpath}', stop=True)\n    #create DAO connection to the access database\n    eng = win32com.client.Dispatch(\"DAO.DBEngine.120\")\n    mdb = eng.OpenDatabase(dbpath, lock) #True = Lock the database.  Prevent getting to multi-user mode\n    return mdb\n\ndef get_db_fields(table, csvheader):\n    fields = []\n    for field in csvheader:\n        try:\n            fields.append(table.Fields.Item(field))\n        except:\n            log(f'ERROR: Field \"{field}\" in csv could not be found in table \"{table.Name}\"', stop=True)\n    return fields\n\ndef qry_to_list(mdb, qry, header):\n    csv = []\n    rs = mdb.OpenRecordset(qry)\n    fields = get_db_fields(rs, header)\n    while not rs.EOF:\n        csv.append([field.Value for field in fields])\n        rs.MoveNext()\n    rs.Close()\n    return csv\n\ndef list_to_csv(csvheader, csvdata, csvpath):\n    with open(csvpath, 'w', newline='') as f:\n        w = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n        w.writerow(csvheader)\n        for row in csvdata:\n            w.writerow(row)\n</code></pre>"},{"location":"Python/SQL/DuckDB/","title":"DuckDB","text":"<p>DuckDB is an in-process SQL OLAP database management system</p> <p>When to use DuckDB - Processing and storing tabular datasets, e.g. from CSV or Parquet files - Interactive data analysis, e.g. Joining &amp; aggregate multiple large tables - Concurrent large changes, to multiple large tables, e.g. appending rows, adding/removing/updating columns - Large result set transfer to client</p> <p>When to not use DuckDB - Non-rectangular data sets, e.g. graphs - High-volume transactional use cases (e.g. tracking orders in a webshop) - Large client/server installations for centralized enterprise data warehousing - Writing to a single database from multiple concurrent processes</p>"},{"location":"Python/SQL/DuckDB/#install","title":"install","text":"<pre><code>mamba install python-duckdb -y\n</code></pre>"},{"location":"Python/SQL/DuckDB/#python-api","title":"python api","text":"<p>https://duckdb.org/docs/api/python/overview.html</p>"},{"location":"Python/SQL/DuckDB/#connection","title":"connection","text":"<pre><code>import duckdb\ncon = duckdb.connect(database=':memory:') #in-memory database\ncon = duckdb.connect(database='my-db.duckdb', read_only=False) #database file (not shared between processes)\ncon = duckdb.connect(database='my-db.duckdb', read_only=True)  #database file (shared between processes)\n</code></pre>"},{"location":"Python/SQL/DuckDB/#query","title":"query","text":"<pre><code>con.execute(\"CREATE TABLE items(item VARCHAR, value DECIMAL(10,2), count INTEGER)\")\ncon.execute(\"INSERT INTO items VALUES ('jeans', 20.0, 1), ('hammer', 42.2, 2)\")\n\ncon.execute(\"SELECT * FROM items\")\nprint(con.fetchall())\n# [('jeans', 20.0, 1), ('hammer', 42.2, 2)]\n\ncon.execute(\"INSERT INTO items VALUES (?, ?, ?)\", ['laptop', 2000, 1])\ncon.executemany(\"INSERT INTO items VALUES (?, ?, ?)\", [['chainsaw', 500, 10], ['iphone', 300, 2]] )\ncon.execute(\"SELECT item FROM items WHERE value &gt; ?\", [400])\n</code></pre>"},{"location":"Python/SQL/DuckDB/#pandas","title":"pandas","text":"<pre><code>import pandas as pd\ncon.register('df_view', df)\ncon.execute('SELECT * FROM df_view')\ncon.fetchall()\n\n# fetch as pandas data frame\ndf = con.execute(\"SELECT * FROM items\").fetchdf()\n# fetch as dictionary of numpy arrays\narr = con.execute(\"SELECT * FROM items\").fetchnumpy()\n</code></pre>"},{"location":"Python/SQL/DuckDB/#force-utc-timezone","title":"force utc timezone","text":"<p>otherwize <code>datetime64[us, UTC]</code> will be converted to local timezone such as <code>datetime64[us, Australia/Brisbane]</code> <pre><code>conn.execute(\"SET timezone = 'UTC'\")  # Force UTC timezone in DuckDB\nconn.register(\"delta_table\", delta_table) # Register as a view\n</code></pre></p>"},{"location":"Python/SQL/DuckDB/#register-vs-create-table-df-as-select-from-df","title":"<code>register</code> vs <code>CREATE TABLE df AS SELECT * FROM df</code>","text":"<pre><code>con = duckdb.connect(':memory:')\ncon.register('df', df)\ncon.execute(\"CREATE TABLE my_table AS SELECT * FROM df\")\n</code></pre> <p><code>duckdb.register('df', df)</code>: - Registers a virtual table: Makes a DataFrame accessible within DuckDB without physically copying data. - No persistence: Virtual table disappears when DuckDB connection closes. - Query-only: Changes to the virtual table don't affect the original DataFrame. - Ideal for: Quick queries and exploration without data duplication.</p> <p><code>conn.execute(\"CREATE TABLE df AS SELECT * FROM df\")</code>: - Creates a persistent table: Stores data physically in DuckDB's database. - Data copied: Data is copied from the DataFrame into DuckDB's storage. - Independent table: Changes to the table within DuckDB don't affect the original DataFrame. - Ideal for: Persisting data, complex queries, sharing data across sessions.</p>"},{"location":"Python/SQL/DuckDB/#performance","title":"performance","text":"<p>have some data (e.g., NEM demand data for different generators and days/intervals). Now there are some new data that can be either updated demand or demand in new days. We need update/insert the new data into the original data.</p> <p>In this case, will it be faster by using duckdb to hold the original data then upsert the new data to duckdb, compared to directly using pandas? write the code to test it!</p> <p>For a parquet file about 10MB, the pandas code is 2-3x faster compared to duckdb.</p> <p>pandas outer merge <pre><code># %%timeit\n# Merge DataFrames, prioritizing df2's values\ndf = pd.merge(d1, d3, how='outer', on=d1.index.names, suffixes=('', '_update'))\n\n# Update sales_amount where values are present in df2\ncols_update = []\nfor col in d1.columns:\n    col_update = f'{col}_update'\n    cols_update.append(col_update)\n    df[col] = df[col_update].fillna(df[col])\n\n# Drop unnecessary columns\ndf1 = df.drop(columns=cols_update)\n</code></pre></p> <p>duckdb upsert: cannot register d1 as a virtual table because it will be modified  <pre><code># %%timeit\n# Connect to a DuckDB database\nconn = duckdb.connect()\n\n# Load DataFrames into DuckDB\nd1t = d1.reset_index()\nconn.execute('CREATE TABLE d1 AS SELECT * FROM d1t')\nconn.register('d2', d3.reset_index())\n\n# Perform the upsert using SQL\nid_cols = ', '.join(d1.index.names)\ndat_cols = ', '.join(d1.columns)\nupd_vals = ', '.join([f'{col} = EXCLUDED.{col}' for col in d1.columns])\nconn.execute(f'CREATE UNIQUE INDEX i1 ON d1 ({id_cols})')\n\nconn.execute(f'''\n    INSERT INTO d1 ({id_cols}, {dat_cols})\n    SELECT {id_cols}, {dat_cols}\n    FROM d2\n    ON CONFLICT ({id_cols}) DO UPDATE SET {upd_vals}\n''')\n\n# Retrieve the updated DataFrame\ndf2 = conn.execute('SELECT * FROM d1').fetchdf()\n\n# Close the connection\nconn.close()\n</code></pre></p>"},{"location":"Python/SQL/MySQL/","title":"sql","text":"<p>To install MySQLdb on Windows go to this link: https://www.lfd.uci.edu/~gohlke/pythonlibs/#mysqlclient \\ Download the appropriate .whl for your Python version: python -m pip install mysqlclient-1.3.13-cp36-cp36m-win_amd64.whl <pre><code>python -m pip install sqlalchemy-access\n</code></pre></p>"},{"location":"Python/SQL/MySQL/#module-time-has-no-attribute-clock","title":"module 'time' has no attribute 'clock'","text":"<p>update sqlalchemy to the latest version as time.clock is removed from python 3.8 and later versions</p>"},{"location":"Python/SQL/MySQL/#connect-to-mdb","title":"Connect to mdb","text":"<pre><code># connect to db\ntry:\n    drv = '{Microsoft Access Driver (*.mdb)}'\n    con = pyodbc.connect(f'DRIVER={drv};DBQ=' + dbpath)\nexcept pyodbc.InterfaceError:\n    drv = '{Microsoft Access Driver (*.mdb, *.accdb)}'\n    con = pyodbc.connect('DRIVER={drv};DBQ=' + dbpath)\ncursor = con.cursor()\n\n# query\ncursor.execute(\"Select * from tbl\")\nrows = cursor.fetchall()\n\n# close connection\ncursor.close()\ncon.close()\n</code></pre>"},{"location":"Python/SQL/MySQL/#mysql-query-to-csv","title":"MySQL query to csv","text":"<pre><code>import pandas as pd\nimport MySQLdb as db\n\ncon = db.connect(host=svr,port=prt,user=usr,passwd=pwd,db=dbn,connect_timeout=30) #sec\nqry = f'''select c1, c2\n          from simulation\n          where c1 = '{a}' and c2 = {b};'''\ndf = pd.read_sql(qry, con=con)\ndf.to_csv('file.csv', mode='w', index=False, header=True)\n</code></pre>"},{"location":"Python/SQL/MySQL/#performance","title":"performance","text":"<p>Executemany is often not much faster than performing individual inserts in a loop. This is true not only for pyodbc but for other Python ODBC layers as well.</p> <p>A commonly-suggested workaround is to use a bulk copy utility such as the bcp utility for Microsoft SQL Server, but Microsoft Access does not have such a stand-alone utility. The MS Access equivalent is to use VBA code to import data from text files via VBA's DoCmd.TransferText method.</p> <p>If you really need to accomplish your task via ODBC from within Python, then one alternative that might offer a modest performance gain would be to</p> <p>create a temporary table with no indexes, possibly in a separate database file, insert the data into the temporary table while autocommit=False with a .commit() at the end, and then INSERT INTO real_table (...) SELECT ... FROM temp_table</p>"},{"location":"Python/SQL/MySQL/#ms-sql-connection-error","title":"MS SQL Connection Error","text":"<p>answer</p> <p>Two thoughts on what to check:</p> <p>1) Your connection string is wrong. There's a way to get a known good connection string directly from the ODBC Administrator program (taken from http://www.visokio.com/kb/db/dsn-less-odbc). These instructions assume you're using an MDB, but the same process will work for a paradox file</p> <p>On a typical client PC, open Control Panel -&gt; Administrative Tools -&gt; Data Sources. Select the File DSN tab and click Add.</p> <p>Select the appropriate driver (e.g. \"Microsoft Access Driver (*.mdb)\") and click Next Click Browse and choose where you want to save the .dsn file (this is a temporary file you are going to delete later). Click Next then Finish.</p> <p>You will be shown the vendor-specific ODBC setup dialog. For example, with Microsoft Access, you might only need to click Select and browse to an existing .mdb file before clicking OK. Browse to the location of the .dsn file and open using Notepad. In the DSN file you might see something similar to: <pre><code>[ODBC]\nDRIVER=Microsoft Access Driver (*.mdb)\nUID=admin\nUserCommitSync=Yes\nThreads=3\nSafeTransactions=0\nPageTimeout=5\nMaxScanRows=8\nMaxBufferSize=2048\nFIL=MS Access\nDriverId=25\nDefaultDir=C:\\\nDBQ=C:\\db1.mdb\nTo convert the above to the full connection strring:\n</code></pre> Omit the first [ODBC] line Put curly braces around all values containing spaces Put all name=value pairs on one line, separated by semicolons. This gives you the full connection string. In this example, the string becomes:</p> <p>DRIVER={Microsoft Access Driver (*.mdb)};UID=admin;UserCommitSync=Yes;Threads=3;SafeTransactions=0;PageTimeout=5;axScanRows=8;MaxBufferSize=2048;FIL={MS Access};DriverId=25;DefaultDir=C:\\;DBQ=C:\\db1.mdb</p> <p>2) 32/64 bit mismatch. I've had troubles when mixing 32-bit python with 64-bit drivers, or vice-versa. You may want to check your Python interpreter and database driver line up.</p>"},{"location":"Python/SQL/MySQL/#simple-obfuscation","title":"simple obfuscation","text":"<p>can save password and other server info in a file in the user's temporal folder. the first time ask the user to input the info and later just read the info from the file. Or do some simple obfuscation.</p> <pre><code>#get password\ngetpass.getpass()\n\n#encode\ndef enco(k, s):\n    s = ''.join([chr((ord(s[i]) + ord(k[i % len(k)])) % 256) for i in range(len(s))])\n    return base64.urlsafe_b64encode(s.encode()).decode()\n#decode\ndef deco(k, s):\n    s = base64.urlsafe_b64decode(s).decode()\n    return ''.join([chr((ord(s[i]) - ord(k[i % len(k)]) + 256) % 256) for i in range(len(s))])\n</code></pre>"},{"location":"Python/SQL/ODBC/","title":"ODBC","text":""},{"location":"Python/SQL/ODBC/#turbodbc-vs-pyodbc","title":"turbodbc vs pyodbc","text":"<p>Turbodbc and pyodbc are both Python libraries used to interact with relational databases through the Open Database Connectivity (ODBC) interface.  While they share the same core functionality, they have some key differences:</p> <p>Performance: - Turbodbc: Generally considered faster, especially for retrieving large datasets. This is primarily due to its use of buffers and NumPy for data type conversions. - pyodbc: Typically slower than turbodbc, but the exact performance difference can vary depending on the specific scenario and database system.</p> <p>Features: - Turbodbc: Supports Apache Arrow columnar data format, potentially improving performance for specific use cases. Offers additional features like batching and query optimization. - pyodbc: More mature and feature-rich library, including support for stored procedures, transactions, and more connection options.</p> <p>Ease of Use: - Turbodbc: Can have a steeper learning curve due to its more advanced features and focus on performance. - pyodbc: Generally considered easier to learn and use, with a simpler API and wider documentation.</p> <p>Community and Support: - Turbodbc: Smaller community and less extensive documentation compared to pyodbc. - pyodbc: Larger community and more active development, leading to better documentation and support resources.</p> <p>Licensing: - Turbodbc: Available under both paid and open-source licenses (Apache 2.0). - pyodbc: Free and open-source (MIT license).</p>"},{"location":"Python/SQL/Oracle/","title":"Oracle","text":""},{"location":"Python/SQL/Oracle/#oracle-database-features-supported-by-python-oracledb","title":"Oracle Database Features Supported by python-oracledb","text":"<p>https://python-oracledb.readthedocs.io/en/latest/user_guide/appendix_a.html</p>"},{"location":"Python/SQL/Oracle/#do-not-include-after-query","title":"do not include <code>;</code> after query","text":"<p>SQL Error: ORA-00933: SQL command not properly ended <pre><code>select * from my_table\n;\n</code></pre></p>"},{"location":"Python/SQL/Oracle/#cannot-use-binding-var-in-pivot","title":"cannot use binding var in pivot","text":""},{"location":"Python/SQL/Postgre/","title":"Postgre","text":""},{"location":"Python/SQL/Postgre/#example","title":"example","text":"<pre><code>import pandas as pd\nfrom contextlib import contextmanager\n\n@contextmanager\ndef db_session():\n    session = Session()\n    try:\n        yield session\n        session.commit()\n    except:\n        session.rollback()\n        raise\n    finally:\n        session.close()\n\ndef exe(query):\n    with db_session() as session:\n        session.execute(query)\n\ndef qry(query):\n    with db_session() as session:\n        result = session.execute(query)\n        data = result.fetchall()\n        cols = result.keys()\n        df = pd.DataFrame(data, columns=cols)\n    return df\n\n# list schemas (namespaces) in current database\ndf = qry('''select nspname as schema_name from pg_namespace;''')\n\n# list tables in \"staging\" schema\ndf = qry('''select table_name from information_schema.tables where table_schema = 'staging';''')\n\n# select top 5 rows from \"staging.sales\" table\ndf = qry('''select * from staging.sales order by insert_timestamp desc limit 5;''')\n\n# select sale records from \"staging.sales\" table based on sale_id\ndf = qry('''select * from staging.sales where sale_id = 123456;''')\n\n# delete sale record\nexe('''delete from staging.sales where sale_id = 123456;''')\n</code></pre>"},{"location":"Python/SQL/PyODBC/","title":"pyodbc","text":""},{"location":"Python/SQL/PyODBC/#faster-way-to-insert-data","title":"faster way to insert data","text":"<p>https://stackoverflow.com/questions/48006551/speeding-up-pandas-dataframe-to-sql-with-fast-executemany-of-pyodbc</p>"},{"location":"Python/SQL/PyODBC/#fast_executemany","title":"fast_executemany","text":"<p>pyodbc needs string even for numerical data when <code>fast_executemany=True</code></p> <p>Insert or update dataframe into table <pre><code># Convert numerical values to string\nfor col in upd_cols:\n    df[col] = df[col].map('{:.4f}'.format, na_action='ignore')\n\n# Standardize column names\nid_cols = [f'[{c}]' for c in id_cols]\nupd_cols = [f'[{c}]' for c in upd_cols]\ncols = id_cols + upd_cols\n\n# Command terms\nstmt_on = ' AND '.join([f'T.{i} = S.{i}' for i in id_cols])\ncols_upd = ','.join([f'T.{c} = S.{c}' for c in upd_cols])\ncols_ins = ','.join(cols)\ncols_val = ','.join([f'S.{c}' for c in cols])\n\n# Create parameter indicators (?, ?, ..., ?) and parameter values\nparam_slots = ','.join(['?'] * df.shape[1])\nparam_values = df.to_records(index=False).tolist()\n\ntable_full_name = f'[{schema}].[{database}].[{table_name}]'\nsql_stmt = f'''\n    MERGE INTO {table_full_name} WITH (XLOCK, ROWLOCK) AS T\n    USING (\n        SELECT * FROM\n        (VALUES ({param_slots})) AS Vtmp ({cols_ins})\n    ) AS S\n        ON {stmt_on}\n    WHEN MATCHED THEN\n        UPDATE SET {cols_upd}\n    WHEN NOT MATCHED BY TARGET THEN\n        INSERT ({cols_ins}) VALUES ({cols_val});\n'''\n# Create engine\nurl = sa.engine.url.URL.create(**cfg)\nengine = sa.create_engine(url, **kwargs, fast_executemany=True)\n# Merge df with table\nwith engine.begin() as cnn:\n    cnn.execute(sql_stmt, param_values)\n</code></pre></p>"},{"location":"Python/SQLAlchemy/BindParam/","title":"BindParam","text":""},{"location":"Python/SQLAlchemy/BindParam/#value","title":"value","text":"<pre><code>query = sql.text(f'''\n    SELECT id, code\n    FROM my_tbl\n    WHERE id &gt;= :id_min AND code = :code\n''')\nquery = query.bindparams(id=id, code=code)\nreturn read_sql(query, session=session)\n</code></pre>"},{"location":"Python/SQLAlchemy/BindParam/#list","title":"list","text":"<pre><code>query.bindparams(\n    bindparam('lst', value=list(values), type_=String, expanding=True),\n)\n</code></pre>"},{"location":"Python/SQLAlchemy/Case/","title":"Case","text":""},{"location":"Python/SQLAlchemy/Case/#cannot-use-case-in-groupby","title":"cannot use <code>case</code> in groupby","text":"<ul> <li>https://groups.google.com/g/sqlalchemy/c/o4H3kalpAFk</li> <li>https://stackoverflow.com/questions/21742713/need-a-query-in-sqlalchemy-with-group-by-case original query <pre><code>SELECT   \n  CASE \n    WHEN (s.[Name] = 'John') THEN 'John' \n    ELSE 'Other' \n  END AS customer_name,\n  SUM(s.[Quantity]) AS sale_quantity,\nFROM [Sales] as s\nGROUP BY \n  CASE \n    WHEN (s.[Name] = 'John') THEN 'John' \n    ELSE 'Other' \n  END  \n</code></pre></li> </ul> <p>slqalchemy code <pre><code>name_col = sa.case(\n    (s.Name=='John', 'John'),\n    else_='Other'\n)\nquery = (\n    session.query(    \n        name_col.label('customer_name'),\n        sa.func.sum(s.Quantity).label('sale_quantity'),\n    )\n    .group_by(name_col)\n)\n</code></pre></p> <p>workaround: using subquery <pre><code>subq = session.query(    \n    sa.case((s.Name=='John', 'John'), else_='Other').label('name'),\n    s.Quantity.label('quantity'),\n).subquery()\nquery = (\n    session.query(    \n        subq.c.name,\n        sa.func.sum(subq.c.quantity).label('sale_quantity'),\n    )\n    .group_by(subq.c.name)\n)\n</code></pre></p>"},{"location":"Python/SQLAlchemy/Case/#sacase-vs-sasqlcase","title":"sa.case vs sa.sql.case","text":"<p><code>sa.case</code> is the <code>sa.sql.case</code> imported from <code>sa.__init__</code>.</p>"},{"location":"Python/SQLAlchemy/Case/#two-formats-of-sqlcase","title":"two formats of sql.case","text":"<p>Accepts multiple 2-tuples passed as positional arguments; each 2-tuple consists of <code>(&lt;sql expression&gt;, &lt;value&gt;)</code>, where the SQL expression is a boolean expression and \"value\" is a resulting value, e.g.: <pre><code>case(\n    (users_table.c.name == 'wendy', 'W'),\n    (users_table.c.name == 'jack', 'J')\n)\n</code></pre> Accepts a Python dictionary of comparison values mapped to a resulting value; this form requires :paramref:<code>.case.value</code> to be present, and values will be compared using the <code>==</code> operator, e.g.: <pre><code>case(\n    {\"wendy\": \"W\", \"jack\": \"J\"},\n    value=users_table.c.name\n)\n</code></pre></p>"},{"location":"Python/SQLAlchemy/Case/#get-min_date-based-on-value-of-another-col","title":"get min_date based on value of another col","text":"<p>The min_date should be date1 if terminated = 0 otherwise min_date = min(date1, date2). To implement this in the code, you can use the following code:</p> <pre><code>import sqlalchemy as sa\nfrom sqlalchemy import sql\n\n# Create a SQLAlchemy engine\nengine = sa.create_engine('mssql+pyodbc://username:password@servername/database')\n\n# Get the metadata for the database\nmetadata = sa.MetaData()\n\n# Get the table object for the table containing the date1, date2, and terminated columns\ntable = sa.Table('table_name', metadata, autoload=True, engine=engine)\n\n# Create a CASE statement to determine the min_date based on the terminated value\nmin_date_case_statement = sql.case(\n    (table.c.terminated == 0, table.c.date1),\n    (table.c.terminated == 1, sql.func.min(table.c.date1, table.c.date2))\n)\n\n# Create a select statement to get all rows where the min_date is less than or equal to input_max\nselect_stmt = sa.select(table).where(min_date_case_statement &lt;= input_max)\n\n# Execute the select statement and get the results\nresults = engine.execute(select_stmt)\n\n# Print the results\nfor row in results:\n    print(row)\n</code></pre> <p>This code will print the results of the select statement to the console. You can modify the code to store the results in a variable or to use them in other ways. ```</p>"},{"location":"Python/SQLAlchemy/DbConnection/","title":"db connection","text":""},{"location":"Python/SQLAlchemy/DbConnection/#encrypt-connection-string-in-sql-server","title":"encrypt connection string in sql server","text":"<ul> <li>Ensure in the datbase server the <code>encryption</code> is enabled</li> <li>In the connection string add <code>mssql+pyodbc</code> and <code>;encrypt=yes;trustservercertificate=no</code></li> </ul> <p>When connecting to MS SQL Server with Python and pyodbc,  the <code>encrypt=yes</code> parameter is not a standalone encryption mechanism.  It works in conjunction with TLS/SSL to enforce encrypted communication between the client and server.</p> <ol> <li>TLS/SSL Configuration:</li> <li>The <code>encrypt=yes</code> parameter is typically used in conjunction with other TLS/SSL-related parameters in the connection string,      such as <code>sslmode</code>, <code>sslkey</code>, <code>sslcert</code>, and <code>sslrootcert</code>.      These parameters specify the keys and certificates required for secure communication.</li> <li>Client-Server Negotiation:</li> <li>When the connection attempt is made with <code>encrypt=yes</code>, the driver initiates a TLS/SSL handshake with the MS SQL Server.</li> <li>The server presents its SSL certificate, which is verified by the client against a trusted root certificate authority (CA).</li> <li>If the server certificate is valid, a secure encrypted communication channel is established using symmetric encryption algorithms.</li> <li>Data Encryption:</li> <li>All data transmitted between the client and server, including the connection string itself, is encrypted using the established TLS/SSL channel.</li> <li>This prevents sensitive information, such as login credentials and database queries, from being intercepted or read in plain text.</li> </ol> <p>Important Considerations: - TrustServerCertificate    - While you can use <code>TrustServerCertificate=YES</code> to bypass server certificate verification,      this is strongly discouraged in production environments due to security risks.      Always validate server certificates against trusted CAs. - Best Practices    - Always use valid SSL certificates issued by trusted CAs.    - Ensure proper configuration of TLS/SSL settings on both client and server sides.    - Stay updated with the latest driver versions and security patches.</p>"},{"location":"Python/SQLAlchemy/DeferredReflection/","title":"DeferredReflection","text":""},{"location":"Python/SQLAlchemy/DeferredReflection/#example-and-issue","title":"example and issue","text":"<p>https://github.com/sqlalchemy/sqlalchemy/issues/5499#issuecomment-1560262233</p>"},{"location":"Python/SQLAlchemy/Issue/","title":"Issue","text":""},{"location":"Python/SQLAlchemy/Issue/#error-code-0x68-104-sqlendtran","title":"Error code 0x68 (104) (SQLEndTran)","text":"<p><code>pyodbc.OperationalError: ('08S01', '[08S01] [Microsoft][ODBC Driver 17 for SQL Server]TCP Provider: Error code 0x68 (104) (SQLEndTran)')</code> - https://www.linuxquestions.org/questions/programming-9/persistent-db-connection-after-waking-up-a-computer-from-sleep-4175615332 - https://techcommunity.microsoft.com/t5/azure-database-support-blog/lesson-learned-359-tcp-provider-error-code-0x68-104/ba-p/3834127</p> <p>Solution:  https://docs.sqlalchemy.org/en/20/dialects/mssql.html#pyodbc-pooling-connection-close-behavior</p> <p>PyODBC uses internal pooling by default, which means connections will be longer lived than they are within SQLAlchemy itself.  As SQLAlchemy has its own pooling behavior, it is often preferable to disable this behavior.  This behavior can only be disabled globally at the PyODBC module level, before any connections are made: <pre><code>import pyodbc\npyodbc.pooling = False\n# don't use the engine before pooling is set to False\nengine = create_engine(\"mssql+pyodbc://user:pass@dsn\")\n</code></pre></p>"},{"location":"Python/SQLAlchemy/MapDbTable/","title":"Map db table","text":"<p>In SQLAlchemy, the <code>declarative_base</code> class is used to create a base class for declarative class definitions. Declarative classes are used to define database tables and their associated mapped classes in a more concise and expressive way. The link between the declarative class and the actual database table is established through a class attribute called <code>__tablename__</code> and the metadata associated with the <code>declarative_base</code> instance.</p> <p>Here's a step-by-step explanation:</p> <ol> <li> <p>Create a <code>declarative_base</code> instance:    First, you need to create a <code>declarative_base</code> instance. This instance is a factory for declarative base classes and will also hold the metadata about the tables.</p> <pre><code>from sqlalchemy.orm import declarative_base\n\nBase = declarative_base()\n</code></pre> </li> <li> <p>Define a Declarative Class:    Define a declarative class by inheriting from the <code>Base</code> class created in step 1. Within this class, define the table columns as class attributes, and specify the table name using the <code>__tablename__</code> attribute.</p> <pre><code>from sqlalchemy import Column, Integer, String\n\nclass User(Base):\n    __tablename__ = 'users'\n    __table_args__ = {'schema': 'sales'}\n\n    id = Column(Integer, primary_key=True)\n    username = Column(String)\n    email = Column(String)\n</code></pre> </li> <li> <p>Table Creation:    Once you have defined your declarative class, you can use the <code>create_all()</code> method on the <code>Base</code> instance to create the associated tables in the database.</p> <pre><code>from sqlalchemy import create_engine\n\n# Replace 'sqlite:///example.db' with your actual database URL\nengine = create_engine('sqlite:///example.db')\n\n# Create tables in the database\nBase.metadata.create_all(engine)\n</code></pre> </li> </ol> <p>Now, the <code>User</code> class is linked to the 'users' table in the database. SQLAlchemy uses the information provided in the declarative class (such as column types and constraints) to generate the appropriate SQL statements for table creation.</p> <p>When you perform queries or insert/update records using instances of the <code>User</code> class, SQLAlchemy knows how to map the class attributes to the corresponding database columns, establishing a connection between your Python code and the database table.</p> <p>Here's an example of how you might use the declarative class to add a user to the database:</p> <pre><code>from sqlalchemy.orm import sessionmaker\n\n# Create a session to interact with the database\nSession = sessionmaker(bind=engine)\nsession = Session()\n\n# Create a new user\nnew_user = User(username='john_doe', email='john@example.com')\n\n# Add the user to the session and commit to the database\nsession.add(new_user)\nsession.commit()\n</code></pre> <p>In this example, the <code>new_user</code> instance is associated with the 'users' table, and the changes are committed to the database through the SQLAlchemy session.</p>"},{"location":"Python/SQLAlchemy/SQLAlchemy/","title":"SQLAlchemy","text":""},{"location":"Python/SQLAlchemy/SQLAlchemy/#print-query-with-binding-values","title":"print query with binding values","text":"<p>sql.text <pre><code>print(query.compile(dialect=None,compile_kwargs={'literal_binds': True}))\n</code></pre></p> <p>sql statement <pre><code>print(query.statement.compile(dialect=None,compile_kwargs={'literal_binds': True}))\n# mssql, mysql, oracle, postgresql, sqlite\nprint(query.statement.compile(dialect=sqlalchemy.dialects.mssql.dialect(),compile_kwargs={'literal_binds': True}))\n</code></pre></p>"},{"location":"Python/SQLAlchemy/SQLAlchemy/#query-template","title":"query template","text":"<pre><code>query = (\n    session\n    .query(\n        t1.c1.label('a'),\n        t2.c2.label('b'),\n        case(\n            (t3.c3 &lt; (x or y), x),\n            else_=t3.c3,\n        ).label('c')\n    )\n    .select_from(t1)\n    .distinct()\n    .join(t2)\n    .join(t3)\n    .filter(\n        t1.c2 == p1,\n        t2.c1 == p2,\n        *[...],\n    )\n)\n</code></pre>"},{"location":"Python/SQLAlchemy/Security/","title":"Security","text":""},{"location":"Python/SQLAlchemy/Security/#encrypt-connection-string","title":"encrypt connection string","text":"<p>database connection string contains username and password so it should be encrypted.</p> <p>Encrypt mssql connection string using ODBC Driver's Encryption: - https://learn.microsoft.com/en-us/sql/connect/odbc/using-always-encrypted-with-the-odbc-driver?view=sql-server-ver16 - https://stackoverflow.com/questions/62390326/pyodbc-ms-sql-server-connection-with-encrypt-yes-not-connecting <pre><code>import sqlalchemy as sa\n\nconn_str = (\n    r'mssql+pyodbc://user:password@server/database?driver=ODBC+Driver+17+for+SQL+Server;Encrypt=yes'\n)\nengine = sa.create_engine(conn_str)\n</code></pre></p>"},{"location":"Python/SQLAlchemy/Session/","title":"Session","text":"<p>https://docs.sqlalchemy.org/en/13/orm/session_basics.html#when-do-i-construct-a-session-when-do-i-commit-it-and-when-do-i-close-it</p>"},{"location":"Python/SQLAlchemy/Session/#get-engine-from-session","title":"get engine from session","text":"<pre><code>session.get_bind()\n</code></pre>"},{"location":"Python/SQLAlchemy/Session/#create-session","title":"create session","text":"<p>we can control when the session should be terminated and the number of sessions. <pre><code>import sqlalchemy as sa\nfrom sqlalchemy.orm import sessionmaker\nfrom fastapi import Depends\n\n# create engine\nurl = sa.engine.url.URL.create(**cfg)\nengine = sa.create_engine(url, **kwargs)\n# note that default pool_recycle=-1 so by default max of 15 connections will be there indefinitely\n# engine = sa.create_engine(host, max_overflow=0, pool_recycle=3600)\n\n# create session\nLocalSession = sessionmaker(bind=engine)\ndef get_session():\n    session = LocalSession()\n    try:\n        yield session\n        session.commit()\n    except:\n        session.rollback()\n        raise\n    finally:\n        session.close()\n\n@router.post('/get', response_model=Union[MyClass, None])\ndef get_task(session: Session = Depends(get_session)) -&gt; Optional[MyClass]:\n    return api.get_task(session)\n</code></pre></p>"},{"location":"Python/SQLAlchemy/Session/#separate-session-from-method-and-class","title":"separate session from method and class","text":"<pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef session_scope():\n    session = Session()\n    try:\n        yield session\n        session.commit()\n    except:\n        session.rollback()\n        raise\n    finally:\n        session.close()\n\ndef run_my_program():\n    with session_scope() as session:\n        my_func(session)\n        my_class().query(session)\n</code></pre>"},{"location":"Python/SQLAlchemy/Session/#terminate-idle-sessions","title":"terminate idle sessions","text":"<p>Can get errors like  <pre><code>pyodbc.OperationalError: ('HYT00',\n'[HYT00] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0) (SQLDriverConnect)')\n\npyodbc.OperationalError: ('08S01',\n'[08S01] [Microsoft][ODBC Driver 17 for SQL Server]TCP Provider: Error code 0x68 (104) (SQLEndTran)')\n</code></pre> We must manage connection being terminated.</p> <p>Setting a rule in your database connection string to terminate idle connections after 1 hour can have both benefits and drawbacks. </p> <p>Benefits: - Improved resource utilization: Terminating idle connections frees up resources (memory, CPU) on the database server, potentially improving performance for active users. - Reduced costs: In cloud environments where you pay per connection, reducing idle connections can lower costs. - Enhanced security: Limiting connection lifetime can mitigate risks associated with long-lived, abandoned connections.</p> <p>Drawbacks: - Increased connection churn: Frequent connection creation and termination can put additional load on the database server, potentially impacting performance. - Potential application disruptions: If applications don't handle connection drops gracefully, users might experience interruptions. - Configuration complexity: Managing different idle connection timeouts for various scenarios can become cumbersome.</p> <p>When to set a 1-hour idle connection timeout: - You have a high volume of short-lived database interactions. - Resource utilization on the server is a concern. - Cost optimization is a priority. - Your applications can handle connection drops effectively.</p> <p>When to avoid setting a 1-hour timeout: - You have long-running database operations or transactions. - Application performance is critical, and connection disruptions cannot be tolerated. - Managing connection timeouts and handling drops adds unnecessary complexity.</p> <p>Alternatives to consider: - Connection pooling: This optimizes connection management by reusing existing connections instead of creating new ones each time. - Longer idle timeouts: Consider setting a timeout that aligns with your typical application usage patterns to balance resource utilization and user experience. - Dynamic timeouts: Implement connection timeouts based on specific contexts (e.g., user type, query type) for more granular control.</p>"},{"location":"Python/SQLAlchemy/Session/#query-parameter-in-saengineurlurlcreate","title":"<code>query</code> parameter in <code>sa.engine.url.URL.create</code>","text":"<p>The <code>query</code> parameter in <code>sa.engine.url.URL.create</code> is a <code>dictionary</code> used to specify  connection options specific to the database dialect and underlying driver (DBAPI) being used.  It allows you to pass additional configuration beyond the core connection details like username, password, host, etc.</p> <p>Important points to remember: - Not all database dialects or drivers support the <code>query</code> parameter - The specific options you can use within the <code>query</code> dictionary depend entirely on the chosen dialect and driver - PostgreSQ does not support <code>query</code>, but has the <code>connect_args</code></p> <p>Here are some resources to help you further: - SQLAlchemy Engine Configuration: https://docs.sqlalchemy.org/20/core/engines.html - Discussion on <code>query</code> parameter keys for PostgreSQL: https://stackoverflow.com/questions/61555363/how-to-connect-to-postgresql-using-sqlalchemy</p>"},{"location":"Python/SQLAlchemy/Sql/","title":"SQL","text":""},{"location":"Python/SQLAlchemy/Sql/#sqlliteral","title":"sql.literal","text":"<p>Cannot use literal in groupby - use sub query. <pre><code>sql.literal('name', Unicode).label('surname')\nsql.literal(dates.end, Date).label('as_of_date')\n</code></pre></p>"},{"location":"Python/SQLAlchemy/Subquery/","title":"Subquery","text":""},{"location":"Python/SQLAlchemy/Subquery/#table-alias","title":"table alias","text":"<pre><code>t = tbl.alias('t')\n</code></pre>"},{"location":"Python/SQLAlchemy/Subquery/#subquery-in-column","title":"Subquery in column","text":"<pre><code>stmt = select([t.c.a, subq.as_scaler()])\nstmt = select([t.c.a, subq.correlate(None).as_scaler()])\n</code></pre>"},{"location":"Python/SQLAlchemy/Subquery/#subquery-in-in-clause","title":"Subquery in <code>In clause</code>","text":"<pre><code>stmt = select([t.c.a, t.c.b]).whereclause(t.c.a.in_(subq))\n</code></pre>"},{"location":"Python/SQLAlchemy/Subquery/#subquery-in-from","title":"Subquery in <code>from</code>","text":"<p>Caveat: - <code>window</code> function will only get the first one in the group - <code>inner join</code> will get all records with the same key columns <pre><code>row_number_col = func.row_number().over(\n    partition_by=(t.Name),\n    order_by=f.EffectiveDate.desc(),\n).label('row_number')\nsubq= session.query(t.Id, t.Name).add_columns(row_number_col).subquery()\ncolumns = [col for col in subq.c where col.name != 'row_number']\nstmt = select(columns).select_from(subq).where(subq.c.row_number == 1)\n</code></pre></p>"},{"location":"Python/SQLAlchemy/Subquery/#subquery-with-join","title":"Subquery with <code>join</code>","text":"<pre><code>subq = (\n    session\n    .query(blabla)\n    .subquery()\n)\nquery = query.join(\n    subq, sql.and_(\n        subq.c.a == t1.a,\n        subq.c.b == t2.b,\n    )\n)\n</code></pre>"},{"location":"Python/SSL/Cert/","title":"Cert","text":""},{"location":"Python/SSL/Cert/#requests-certs","title":"requests certs","text":"<p><code>Requests</code> maintainers bundled their own certs. - Redhat <code>/etc/pki/ca-trust</code> - Ubuntu <code>/etc/ssl/certs/</code> - Windows <code>windows trust store</code></p>"},{"location":"Python/SSL/Cert/#force-using-certs-from-system","title":"force using certs from system","text":"<p><pre><code>ENV REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt\n</code></pre> <code>/etc/ssl/certs/ca-certificates.crt</code> includes both: - certificates provided by Ubuntu distribution and - certificates added by user (/usr/local/share/ca-certificates/)</p> <p>Note: If <code>verify</code> is set to a path to a directory, the directory must have been processed using the c_rehash utility supplied with OpenSSL.</p>"},{"location":"Python/SSL/Cert/#check-https-call-ok","title":"check https call OK","text":"<pre><code>curl -vvvv https://my.example.com/EWS/Exchange.asmx\n</code></pre> <pre><code>import requests\nrequests.post('https://my.example.com/EWS/Exchange.asmx')\n\nimport requests\nrequests.get('https://my.example.com/', verify=False)\nrequests.get('https://my.example.com/', verify='/etc/ssl/certs/ca-certificates.crt')\n</code></pre>"},{"location":"Python/SSL/Cert/#self-signed-certificate-in-certificate-chain","title":"self-signed certificate in certificate chain","text":"<p>Python <code>requests.get</code> will throw the error (in windows): <pre><code>SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1010)')))\n</code></pre> Solution (or add to env vars): <pre><code>import os\nos.environ['REQUESTS_CA_BUNDLE'] = 'c:/cert-path/cert.crt'\n# os.environ['SSL_CERT_FILE'] = 'c:/cert-path/cert.crt' # not required\nrequests.get('https://my.example.com/')\n...\n</code></pre></p>"},{"location":"Python/SSL/Cert/#show-cert-path","title":"show cert path","text":"<pre><code>#option 1\nfrom requests.utils import DEFAULT_CA_BUNDLE_PATH\nprint(DEFAULT_CA_BUNDLE_PATH)\n\n#option2\nimport certifi\ncertifi.where()\n</code></pre>"},{"location":"Python/SSL/Issue/","title":"Issue","text":""},{"location":"Python/SSL/Issue/#az-login-fails-with-certificate-verify-failed-unable-to-get-local-issuer-certificate","title":"az login fails with \"certificate verify failed: unable to get local issuer certificate\"","text":"<p>Azure CLI uses python to connect to server: - https://github.com/Azure/azure-cli/issues/19571 - The proxy intercepts Azure CLI's HTTPS traffic, decrypts and re-encrypts it with its own certificate, but the proxy can't return all intermediate and root CA certificates, leading to the failure. - All intermediate and root CA certificates should be appended to the <code>cacert.pem</code> file and then set the env var <code>REQUESTS_CA_BUNDLE</code> to this file.</p>"},{"location":"Python/SSL/Issue/#python-list-trusted-ca-certificates-path","title":"python list trusted CA certificates path","text":"<p>Output the paths where Python is looking for trusted CA certificates: <pre><code>python -c \"import ssl; print(ssl.get_default_verify_paths())\"\n</code></pre></p> <p>The output on windows may look like this: <pre><code>DefaultVerifyPaths(\n    cafile='C:\\\\Users\\\\x\\\\conda-envs\\\\env1\\\\Library\\\\ssl\\\\cacert.pem',\n    capath='C:\\\\Users\\\\x\\\\conda-envs\\\\env1\\\\Library\\\\ssl\\\\certs',\n    openssl_cafile_env='SSL_CERT_FILE',\n    openssl_cafile='C:\\\\Program Files\\\\Common Files\\\\ssl\\\\cert.pem',\n    openssl_capath_env='SSL_CERT_DIR',\n    openssl_capath='C:\\\\Program Files\\\\Common Files\\\\ssl\\\\certs'\n)\n</code></pre></p> <p>In this case, as we use conda env, the <code>REQUESTS_CA_BUNDLE</code> env var should point to the conda env file (configured for the packages we're using) not the system-wide certificate file.</p>"},{"location":"Python/SSL/Legacy/","title":"SSL error unsafe legacy renegotiation disabled","text":"<p>https://stackoverflow.com/questions/71603314/ssl-error-unsafe-legacy-renegotiation-disabled/72245418#72245418</p>"},{"location":"Python/SSL/Legacy/#enable-using-opensslcng","title":"enable using openssl.cng","text":"<p><pre><code>openssl_conf = openssl_init\n\n[openssl_init]\nssl_conf = ssl_sect\n\n[ssl_sect]\nsystem_default = system_default_sect\n\n[system_default_sect]\nOptions = UnsafeLegacyRenegotiation\n</code></pre> Then <code>export OPENSSL_CONF=/path/to/custom/openssl.cnf</code></p>"},{"location":"Python/SSL/Legacy/#enable-using-openssl-context","title":"enable using openssl context","text":"<pre><code>import requests\nimport urllib3\nimport ssl\n\nclass CustomHttpAdapter (requests.adapters.HTTPAdapter):\n    # \"Transport adapter\" that allows us to use custom ssl_context.\n    def __init__(self, ssl_context=None, **kwargs):\n        self.ssl_context = ssl_context\n        super().__init__(**kwargs)\n    def init_poolmanager(self, connections, maxsize, block=False):\n        self.poolmanager = urllib3.poolmanager.PoolManager(\n            num_pools=connections, maxsize=maxsize,\n            block=block, ssl_context=self.ssl_context)\n\ndef get_legacy_session():\n    ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n    ctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT\n    session = requests.session()\n    session.mount('https://', CustomHttpAdapter(ctx))\n    return session\n\n# use it in place of the requests call:\nget_legacy_session().get(\"some-url\")\n</code></pre>"},{"location":"Python/SciPy/SparseMatrix/","title":"Sparse Matrix","text":""},{"location":"Python/SciPy/SparseMatrix/#diags","title":"diags","text":"<pre><code>import numpy as np\nimport scipy.sparse as sp\nm = sp.diags([1,2,4])\n</code></pre>"},{"location":"Python/SciPy/SparseMatrix/#coo-coordinate-list-format","title":"COO (Coordinate List) Format","text":"<p>The <code>COO</code> format stores a sparse matrix by recording the row and column indices of non-zero elements along with their corresponding values.</p> <pre><code># Data: non-zero values, row indices, column indices\ndata = np.array([4, 5, 7])\nrow_indices = np.array([0, 1, 2])\ncol_indices = np.array([0, 2, 3])\n\n# Create COO sparse matrix\ncoo_matrix = sp.coo_matrix((data, (row_indices, col_indices)), shape=(4, 4))\n\nprint(coo_matrix)\n</code></pre>"},{"location":"Python/SciPy/SparseMatrix/#csr-compressed-sparse-row-format","title":"CSR (Compressed Sparse Row) Format","text":"<p>In the <code>CSR</code> format, the matrix is stored by compressing rows into three arrays: - <code>data</code>: The non-zero values. - <code>indices</code>: The column indices of the non-zero values. - <code>indptr</code>: The index pointers for the start of each row.</p> <pre><code># Data for CSR\ndata = np.array([4, 5, 7])\nrow_indices = np.array([0, 1, 2])\ncol_indices = np.array([0, 2, 3])\n\n# Create CSR sparse matrix\ncsr_matrix = sp.csr_matrix((data, (row_indices, col_indices)), shape=(4, 4))\n\nprint(csr_matrix)\nprint(csr_matrix.data)    # Non-zero values\nprint(csr_matrix.indices) # Column indices of non-zero elements\nprint(csr_matrix.indptr)  # Row pointers\n</code></pre>"},{"location":"Python/SciPy/SparseMatrix/#csc-compressed-sparse-column-format","title":"CSC (Compressed Sparse Column) Format","text":"<p>Similar to the <code>CSR</code> format but optimized for column-based operations. It stores the matrix by compressing columns.</p> <pre><code># Data for CSC\ndata = np.array([4, 5, 7])\nrow_indices = np.array([0, 1, 2])\ncol_indices = np.array([0, 2, 3])\n\n# Create CSC sparse matrix\ncsc_matrix = sp.csc_matrix((data, (row_indices, col_indices)), shape=(4, 4))\n\nprint(csc_matrix)\n</code></pre>"},{"location":"Python/SciPy/SparseMatrix/#dok-dictionary-of-keys-format","title":"**DOK (Dictionary of Keys) Format","text":"<p>The <code>DOK</code> format stores the matrix as a dictionary with keys as <code>(row, column)</code> tuples and values as the non-zero values.</p> <pre><code># Create a DOK sparse matrix\ndok_matrix = sp.dok_matrix((4, 4))\n\n# Assign values (row, column): value\ndok_matrix[0, 0] = 4\ndok_matrix[1, 2] = 5\ndok_matrix[2, 3] = 7\n\nprint(dok_matrix)\n</code></pre>"},{"location":"Python/SciPy/SparseMatrix/#lil-list-of-lists-format","title":"**LIL (List of Lists) Format","text":"<p>The <code>LIL</code> format stores each row as a list of column indices and their corresponding non-zero values.</p> <pre><code># Create a LIL sparse matrix\nlil_matrix = sp.lil_matrix((4, 4))\n\n# Assign values to the matrix\nlil_matrix[0, 0] = 4\nlil_matrix[1, 2] = 5\nlil_matrix[2, 3] = 7\n\nprint(lil_matrix)\n</code></pre>"},{"location":"Python/SciPy/SparseMatrix/#dense-to-sparse-conversion","title":"**Dense to Sparse Conversion","text":"<pre><code># Create a dense matrix\ndense_matrix = np.array([\n    [4, 0, 0, 0],\n    [0, 0, 5, 0],\n    [0, 0, 0, 7],\n])\n\n# Convert to CSR sparse matrix\ncsr_matrix = sp.csr_matrix(dense_matrix)\n\nprint(csr_matrix)\n</code></pre>"},{"location":"Python/SciPy/SparseMatrix/#sparse-matrix-arithmetic","title":"Sparse Matrix Arithmetic","text":"<pre><code># Create two sparse matrices\nA = sp.csr_matrix([[1, 0, 3], [0, 0, 0], [4, 0, 5]])\nB = sp.csr_matrix([[0, 2, 0], [0, 0, 0], [1, 0, 6]])\n\n# Matrix addition\nC = A + B\n\nprint(C)\n</code></pre>"},{"location":"Python/SciPy/SparseOp/","title":"Sparse Matrix Operations","text":""},{"location":"Python/SciPy/SparseOp/#element-wise-multiplication","title":"element-wise multiplication","text":"<pre><code>import numpy as np\nimport scipy.sparse as sp\n\n# create the COO sparse matrix\ndata = np.array([1, 2, 3, 4])\nrows = np.array([0, 1, 2, 0])\ncols = np.array([0, 1, 2, 3])\nm_coo = sp.coo_matrix((data, (rows, cols)), shape=(3,4))\nprint(m_coo.toarray())\n\nmat = m_coo.tocsr()\nvec = np.array([1,2,3,4])\n\n# coo matrix\nm1 = mat.multiply(vec)\nprint(f'm1 type: {type(m1)}')\nprint(m1.toarray())\n\n# csr matrix: good for operation\nm2 = mat.multiply(sp.csr_matrix(vec))\nprint(f'm2 type: {type(m2)}')\nprint(m2.toarray())\n</code></pre>"},{"location":"Python/Serialization/Benchmark/","title":"Benchmark","text":"<p>parquet has a slower dump speed but is much faster when load back the data and size is much smaller.</p>"},{"location":"Python/Serialization/Benchmark/#df-to-bytes-to-df","title":"df to bytes to df","text":"<p>average of 5 runs w/o redis <pre><code>   name  dump (redis)  load (redis)    MB\npickle    0.7   (1.9)   1.0   (3.9) 150.5\nfeather   1.4   (1.8)   0.7   (1.6)  56.7\nparquet   2.9   (3.1)   0.9   (1.4)  26.5\n</code></pre></p>"},{"location":"Python/Serialization/Benchmark/#patable-to-bytes-to-patable","title":"pa.Table to bytes to pa.Table","text":"<p>average of 5 runs w/o redis <pre><code>   name  dump (redis)  load (redis)    MB\n pickle   1.1   (2.7)   0.5   (6.0) 269.3\nfeather   0.5   (0.8)   0.1   (1.2)  56.7\nfea(zstd)       (0.8)         (1.0)  42.1\nparquet   1.8   (2.1)   0.3   (1.0)  26.5\npaq(zstd)       (2.0)         (0.7)  21.0\n</code></pre></p>"},{"location":"Python/Serialization/Benchmark/#code","title":"code","text":"<pre><code>import io\nimport pickle\nfrom functools import partial\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport pyarrow.feather as pf\nimport time\n\ntest_redis = False\n\ndef timeit(func, n=5):\n    t0 = time.time()\n    for i in range(n):\n        func()\n    return (time.time() - t0) / n\n\ndef pkldumps(d):    \n    b = pickle.dumps(d)\n    if test_redis:\n        r.set('df', b)\n    return b\n\ndef pklloads(b):\n    if test_redis:\n        b = r.get('df')    \n    return pickle.loads(b)\n\ndef paqdumps(d):\n    buf = io.BytesIO()\n    #pq.write_table(pa.Table.from_pandas(d), buf)       \n    pq.write_table(d, buf)      \n    b = buf.getvalue()\n    if test_redis:\n        r.set('df', b)\n    return b\n\ndef paqloads(b):\n    if test_redis:\n        b = r.get('df')    \n    buf = pa.BufferReader(b)\n    #return pq.read_table(buf).to_pandas()  \n    return pq.read_table(buf)\n\ndef feadumps(d):\n    buf = io.BytesIO()\n    #d.to_feather(buf)\n    pf.write_feather(d, buf)\n    b = buf.getvalue()\n    if test_redis:\n        r.set('df', b)\n    return b\n\ndef fealoads(b):\n    if test_redis:\n        b = r.get('df')     \n    buf = io.BytesIO(b)\n    #return pd.read_feather(buf)\n    return pf.read_table(buf)\n\ndef hdfdumps(d):\n    buf = io.BytesIO()\n    d.to_hdf(buf, 'bar', mode='w') #cannot be io stream\n    return buf.getvalue()\n\ndef hdfloads(b):\n    buf = io.BytesIO(b)\n    return pd.read_hdf(buf, 'bar', mode='r')\n\ndc = {\n    'pickle': [pklloads, pkldumps],\n    'feather': [fealoads, feadumps],\n    'parquet': [paqloads, paqdumps],\n    #'hdf': [hdfloads, hdfdumps],  not supported\n}\n\nresult = []\nfor name, (loads, dumps) in dc.items():\n    print(name)\n    b = dumps(df)\n    s = len(b) / 1024 / 1024\n    result.append([name, timeit(lambda: dumps(df)), timeit(lambda: loads(b)), s])\ndt = pd.DataFrame(result, columns=['name', 'dump', 'load', 'MB'])\nprint(dt.round(1).to_string(index=False))\n</code></pre>"},{"location":"Python/Serialization/Parquet/","title":"Parquet","text":""},{"location":"Python/Serialization/Parquet/#perf-benchmark","title":"perf benchmark","text":"<p>best is - to bytes: <code>pq.write_table(pa.Table.from_pandas(df), buf)</code> - from bytes: <code>pq.read_table(pa.BufferReader(bytes)).to_pandas()</code> - when considering cache and streaming, it's best to stick to the pyarrow.table not pandas df <pre><code>def paqdumps(d, i):\n    buf = io.BytesIO()\n    if i==0:\n        # 3.15 s \u00b1 88.8 ms\n        d.to_parquet(buf)\n    else:\n        # 3.09 s \u00b1 126 ms\n        table = pa.Table.from_pandas(d)\n        pq.write_table(table, buf)\n    return buf.getvalue()\n\ndef paqloads(b, i):\n    if i==0:\n        # 1.79 s \u00b1 18.9 ms\n        buf = io.BytesIO(b)\n        f = pd.read_parquet(buf)\n    elif i==1:\n        # 1.75 s \u00b1 20.3 ms\n        buf = pa.BufferReader(b)\n        f = pd.read_parquet(buf)\n    elif i==2:\n        # 1.83 s \u00b1 15.5 ms, to_pandas is slow\n        buf = io.BytesIO(b)\n        f = pq.read_pandas(buf).to_pandas()\n    elif i==3:\n        # 1.72 s \u00b1 27.2 ms\n        buf = pa.BufferReader(b)\n        f = pq.read_pandas(buf).to_pandas()\n    elif i==4:\n        # 1.77 s \u00b1 72.9 ms\n        buf = io.BytesIO(b)\n        f = pq.read_table(buf).to_pandas()\n    elif i==5:\n        # 1.70 s \u00b1 21.8 ms\n        buf = pa.BufferReader(b)\n        f = pq.read_table(buf).to_pandas()\n    return f\n\nfor i in range(2):\n    t0 = time.time()\n    b = paqdumps(d, i)\n    print(f'dump {i}: {time.time() - t0:.3f}')\nfor i in range(6):\n    t0 = time.time()\n    f = paqloads(b, i)\n    print(f'load {i}: {time.time() - t0:.3f}')\n</code></pre></p>"},{"location":"Python/Serialization/Performance/","title":"Performance","text":"<p>https://matthewrocklin.com/blog/work/2015/03/16/Fast-Serialization</p> <p>https://gist.github.com/mrocklin/4f6d06a2ccc03731dd5f</p> <ul> <li>msgpack (depreciated)</li> <li>hdfstore</li> </ul>"},{"location":"Python/Serialization/Performance/#pickle","title":"pickle","text":"<p>Now pickle is the way to go.  The data stored within Redis is not portable across different versions of Python and programming languages. <pre><code>%%timeit -r 3 -n 3\npickle.loads(pickle.dumps(d))\n\n%%timeit -r 3 -n 3\nbuf = io.BytesIO(d.to_parquet(compression=None))\nrda = pd.read_parquet(buf)\n</code></pre></p>"},{"location":"Python/Serialization/Performance/#arrow","title":"arrow","text":"<p>https://arrow.apache.org/docs/10.0/python/ipc.html#ipc <pre><code>BATCH_SIZE = 10000\nNUM_BATCHES = 1000\nschema = pa.schema([pa.field('nums', pa.int32())])\nwith pa.OSFile('bigfile.arrow', 'wb') as sink:\n   with pa.ipc.new_file(sink, schema) as writer:\n      for row in range(NUM_BATCHES):\n            batch = pa.record_batch([pa.array(range(BATCH_SIZE), type=pa.int32())], schema)\n            writer.write(batch)\n#load\nwith pa.OSFile('bigfile.arrow', 'rb') as source:\n   loaded_array = pa.ipc.open_file(source).read_all()\n#more efficient load\nwith pa.memory_map('bigfile.arrow', 'rb') as source:\n   loaded_array = pa.ipc.open_file(source).read_all()\n</code></pre></p>"},{"location":"Python/Serialization/Performance/#feather","title":"feather","text":"<p>feather does not support serializing  for the index; you can .reset_index() to make the index into column(s)"},{"location":"Python/Tensorflow/Install/","title":"Install","text":"<p>https://www.tensorflow.org/install/pip#windows-native_1</p>"},{"location":"Python/Tensorflow/Install/#windows-packages-are-not-available-from-conda-forge","title":"windows packages are not available from conda-forge","text":""},{"location":"Python/Tensorflow/Install/#install-via-pip-in-windows","title":"install via pip in windows","text":"<p>https://www.tensorflow.org/install/pip#windows-native_1 <pre><code>pip install --upgrade pip\npip install tensorflow\n\n# verify cpu setup\npython -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n# verify gpu setup\npython -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre></p>"},{"location":"Python/Test/Config/","title":"Config","text":"<p>Pytest will read the <code>setup.cfg</code> file to get testing settings.</p>"},{"location":"Python/Test/Config/#pytest-config","title":"<code>pytest</code> config","text":"<p>setup.cfg <pre><code>[tool:pytest]\ntestpaths = tests\naddopts =\n    -v\n    --tb=short\ndoctest_optionflags =\n    NORMALIZE_WHITESPACE\n    ELLIPSIS\n    FLOAT_CMP\n</code></pre> Explicitly set the test paths to improve the test collection speed.</p>"},{"location":"Python/Test/DocTest/","title":"DocTest","text":"<p>https://docs.python.org/3/library/doctest.html</p> <p>Test examples in doc.</p> <p>Searches for pieces of text that look like interactive Python sessions, and then executes those sessions to verify that they work exactly as shown. </p>"},{"location":"Python/Test/FakeData/","title":"Fake data","text":""},{"location":"Python/Test/FakeData/#python-package-used-to-generate-fake-database-tables","title":"python package used to generate fake database tables","text":"<p>https://github.com/tirthajyoti/pydbgen</p>"},{"location":"Python/Test/Fixture/","title":"fixture","text":""},{"location":"Python/Test/Fixture/#data-path-fixture","title":"data path fixture","text":"<pre><code>import pytest\nfrom pathlib import Path\n\nDATA_PATH = Path(__file__).absolute().parent.joinpath('data')\n\n@pytest.fixture\ndef data_path():\n    return DATA_PATH\n\ndef test_data_path(data_path: Path):\n    ...\n</code></pre>"},{"location":"Python/Test/Fixture/#call-fixture-function-directly","title":"call fixture function directly","text":"<p>solution: https://github.com/pytest-dev/pytest/issues/3950 - bad - default argument: this makes the fixture run at import time, not at test runtime   <pre><code>def data():\n    ...\ndef test_my_func(data=data()):\n    ...\n</code></pre> - extract fixture   <pre><code>  def _data():\n      ...\n  @pytest.fixture\n  def data():\n    return _data()\n</code></pre> - manual wrapping   <pre><code>def _data():\n    ...\ndata = pytest.fixture(_data) # attach fixture to pytest\n</code></pre> - direct invocation: <code>@pytest.fixture</code> keeps the original function in <code>.__wrapped__</code> <pre><code>if __name__ == '__main__':\n    test_my_func(data=data.__wrapped__())\n</code></pre></p>"},{"location":"Python/Test/Package/","title":"Package","text":""},{"location":"Python/Test/Package/#factory_boy","title":"factory_boy","text":"<p>https://github.com/FactoryBoy/factory_boy</p> <p><code>factory_boy</code> is a fixtures replacement, aiming to replace static, hard to maintain fixtures with easy-to-use factories for complex objects.</p>"},{"location":"Python/Test/PyTest/","title":"PyTest","text":""},{"location":"Python/Test/PyTest/#pytest-debug","title":"pytest debug","text":"<ul> <li>https://pytest-with-eric.com</li> <li>https://python-basics-tutorial.readthedocs.io/en/latest/test/pytest/debug.html</li> </ul> <p>options: - <code>-s</code>: or <code>--capture=no</code>. disable the capturing of standard output (stdout) and standard error (stderr) - <code>-v</code>: low(<code>-v</code>)/medium(<code>-vv</code>)/high(<code>-vvv</code>) verbosity - <code>-q</code>: or <code>\u2013quiet</code>. less output</p>"},{"location":"Python/Test/PyTest/#run-from-vscode-terminal","title":"run from vscode terminal","text":"<pre><code>pytest\npytest .\n</code></pre>"},{"location":"Python/Test/PyTest/#run-test-on-file","title":"run test on file","text":"<pre><code>pytest test_myfile.py -sv #print output and test name\npytest test_myfile.py -sv -k \"get\"  #run test method matches 'get'\npytest Test_myfile.py -sv -m \"skip or login\" #run test with specific mark or category, either\npytest Test_myfile.py -sv -m \"skip and login\" #run test with specific mark or category, both, not can be used as well\n</code></pre>"},{"location":"Python/Test/PyTest/#skip-test-method","title":"skip test method","text":"<p>https://docs.pytest.org/en/latest/how-to/skipping.html#skipping-test-functions <pre><code>@pytest.mark.skip(reason='no way of currently testing this')\ndef test_my_method():\n...\n\n#or skip during test\ndef test_my_method():\n    if not valid_config():\n        pytest.skip('unsupported configuration')\n</code></pre></p>"},{"location":"Python/Test/PyTest/#skip-the-whole-test-module","title":"skip the whole test module","text":"<pre><code>if not sys.platform.startswith('win'):\n    pytest.skip('skipping windows-only tests', allow_module_level=True)\n</code></pre>"},{"location":"Python/Test/PyTest/#skip-test-module-if-import-fails","title":"skip test module if import fails","text":"<p>https://docs.pytest.org/en/stable/reference/reference.html#pytest-importorskip-ref <pre><code>docutils = pytest.importorskip('docutils')\n</code></pre></p>"},{"location":"Python/Test/PyTest/#check-log-message","title":"check log message","text":"<p>https://docs.pytest.org/en/7.1.x/how-to/logging.html <pre><code>import logging\nfrom pytest import LogCaptureFixture\nfrom click.testing import CliRunner\nfrom myrepo import main\n\ndef test_cli(caplog: LogCaptureFixture) -&gt; None:\n    caplog.set_level(logging.INFO, logger=__name__)\n</code></pre></p>"},{"location":"Python/Test/UnitTest/","title":"Unit Test","text":""},{"location":"Python/Test/UnitTest/#mock","title":"mock","text":"<p>https://docs.python.org/3/library/unittest.mock.html</p> <p>Mock part of method or object so, for example, you can use the mocked data (instead of getting data from the database) to test part of a function. <pre><code>from unittest.mock import (\n    Mock,\n    patch,\n)\n\n# in namespace dev.db\ndef get_from_db():\n    return pd.DataFrame({'x':[1,2], 'y':[3,4]})\n\ndef get_db_data():\n    df = get_from_db()\n    df += 1\n    return df\n\ndef test_get_db_data():  \n    db_data = pd.DataFrame({'x':[1,2], 'y':[3,4]})\n    with patch.multiple(\n        target=dev.db,\n        get_from_db=Mock(return_value=db_data),\n    ):\n        df = get_db_data()\n</code></pre></p>"},{"location":"Python/Test/caplog/","title":"caplog","text":"<p>https://docs.pytest.org/en/6.2.x/reference.html#caplog</p> <p>Access and control log capturing. Returns a <code>pytest.LogCaptureFixture</code> instance.</p> <p>The <code>caplog</code> is one of pytest's built-in fixtures.  The <code>fixture</code> is an argument that will be automatically passed to the test function by its name.  Thus all we need to do is to define caplog as the function's argument and then use it in the method.</p> <p>Here we test the logger using the <code>caplog</code>. So the log messgaes are written to caplog and we can check the messages later. <pre><code>import logging\nfrom pytest import LogCaptureFixture\n\ndef test_logger(caplog: LogCaptureFixture) -&gt; None:\n    caplog.set_level(logging.INFO, logger=__name__)\n    logger = logging.getLogger(__name__)\n\n    logger.info('Test info.')\n    logger.critical('Test error!', exc_info=1)\n\n    assert 'Test info.' in caplog.text\n    assert 'Test error!' in caplog.text\n</code></pre></p>"},{"location":"Python/Typing/tips/","title":"tips","text":""},{"location":"Python/Typing/tips/#get-list-from-literal","title":"get list from Literal","text":"<pre><code>import typing_extensions\n\nMyType = typing_extensions.Literal['monkey', 'pig']\nMyList = list(typing_extensions.get_args(MyType)) #['monkey', 'pig']\n</code></pre>"},{"location":"Python/Typing/tips/#modern-format","title":"modern format","text":"<p>https://peps.python.org/pep-0585/</p>"},{"location":"Python/Visualization/Chart/","title":"Chart","text":""},{"location":"Python/Visualization/Chart/#graphviz","title":"graphviz","text":"<p>Shapes:  https://graphviz.org/doc/info/shapes.html</p> <ul> <li>Install <code>Graphviz</code></li> <li><code>conda install graphviz python-graphviz pydot</code> <pre><code>os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'\nfrom graphviz import (\n    Graph,\n    Digraph,\n)\n\n#Graph\nf = Graph('test', format='png')\nfor k, v in data.items():\n  f.node(k, v)\nf.edges(['ab', 'ac', 'bd', 'ce', 'cf'])\nf.render('test', view=True)\n\n#Digraph\nf = Digraph(filename='c:/test.gv', format='png')\ndata = {\n    'a': 'CEO',\n    'b': 'Team-A', 'c': 'Team-B',\n    'd': 'Staff-A', 'e': 'Staff-B', 'f': 'Staff-C', \n}\n#nodes\nfor k, v in data.items():\n    if k == 'a':\n        f.node(k, v, shape='oval')        \n    elif k in ['b', 'c']:\n        f.node(k, v, shape='box')\n    else:\n        f.node(k, v, shape='plaintext')\n\n#labels\nwith g.subgraph(name='clusterA') as s:\n    s.attr(rank='same')\n    s.attr(rankdir='LR') #TB\n    s.node('a', 'AA', shape='rnastab')\n    s.node('b', 'BB', shape='signature')\n#labels\nwith g.subgraph(name='clusterB') as s:\n    s.attr(rank='same')\n    s.node('c', 'CC', shape='trapezium')\n    s.node('d', 'DD', shape='Mdiamond')  \n#invisible edge    \ng.edge('a', 'c', ltail='clusterA', lhead='clusterB', style='invis')\n#not constrained\n#a2 -- a0 [constraint=false];\n\n#edges\nf.edge('a', 'b')\nf.edge('a', 'c')\nf.edge('b', 'd')\nf.edge('c', 'e')\nf.edge('c', 'f', label='OK\\nGo')\n\nf.save()\nf.render(view=0, cleanup=1)\nf\n</code></pre></li> </ul>"},{"location":"Python/Visualization/Chart/#cloud-inforstructure-diagrams","title":"cloud inforstructure diagrams","text":"<pre><code>from diagrams import Diagram\nfrom diagrams.aws.compute import EC2\nfrom diagrams.aws.network import ELB\nfrom diagrams.aws.database import RDS\n\nwith Diagram(\"Web Service\", show=False):\n    ELB(\"lb\") &gt;&gt; EC2(\"web\") &gt;&gt; RDS(\"userdb\")\n</code></pre>"},{"location":"Python/Visualization/Chart/#schemdraw","title":"schemdraw","text":"<p>https://stackoverflow.com/questions/71906464/how-to-add-annotations-anywhere-in-flowchart-schemdraw <pre><code>import schemdraw\nfrom schemdraw.flow import *\n\nwith schemdraw.Drawing() as d:\n    d += Start.lable('start')\n    d += Data(w=3,h=2).lable('input')\n    d += Process(w=5).lable('process')\n    d += Decision(w=5).lable('decision')\n    d += Arrow(w=5).right.lable('test')\n</code></pre></p>"},{"location":"Python/Visualization/CodeView/","title":"Code view","text":""},{"location":"Python/Visualization/CodeView/#tree-chart","title":"tree chart","text":"<p>install code2flow from github:\\ <code>pip install git https://github.com/user/repo.git@branch</code></p>"},{"location":"Python/Visualization/Example/","title":"Example","text":""},{"location":"Python/Visualization/Example/#matplotlib","title":"matplotlib","text":"<pre><code>plt.plot(x, y, 'ko', label=\"Original Data\")\nplt.plot(x_fit, y_fit, 'r-', label=\"Fitted Curve\")\nplt.legend()\n</code></pre>"},{"location":"Python/Visualization/Example/#df-secondary-y","title":"df secondary y","text":"<pre><code>ax = df.plot(x='dt',y=['y_left'],figsize=(14,6))\ndf.plot(x='dt',y=['y_right'], ax=ax, secondary_y=True)\n_ = ax.set(xlabel='')\n</code></pre>"},{"location":"Python/Visualization/Example/#plotly","title":"plot.ly","text":"<pre><code>df.iplot()\n</code></pre>"},{"location":"Python/Visualization/Example/#sns-realplotlmplot","title":"sns realplot/lmplot","text":"<pre><code>g = sns.FacetGrid(df, row='scenario', col='month', hue='region', height=1.5, aspect=3, sharex=False)\ng.map(sns.lineplot, 'year', 'val')\n\nsns.relplot(kind='line', data=df, x='year', y='daily', hue='month', col='type', height=6, aspect=1.5)\ng= sns.relplot(kind='line', data=df, x='year', y='daily', hue='month', style='type', height=6, aspect=1.5)\ng._legend.set_bbox_to_anchor([0.36,0.72])\n\nsns.lmplot(data=df, x='val_1', y='val_2', row='year', col='month', hue='type')\n</code></pre>"},{"location":"Python/Visualization/Pandas/","title":"Pandas","text":""},{"location":"Python/Visualization/Pandas/#dfplot","title":"df.plot","text":"<pre><code>ax = df.plot(\n  x='date',\n  y=['forecast_revenue','actual_revenue'],\n  title=f'Cal Year: 2022',\n  figsize=(12,5),\n)\n_ = ax.set_ylabel(\"Revenued ($)\")\n</code></pre>"},{"location":"Python/Visualization/Pandas/#rotate-axis-labels","title":"rotate axis labels","text":"<pre><code>import matplotlib.pyplot as plt\n#set font and plot size to be larger\nplt.rcParams.update({'font.size': 20, 'figure.figsize': (10, 8)})\ndf['rating'].plot(kind='hist', title='Rating')\ndf.plot(kind='scatter', x='speed', y='distance', title='my plot')\n</code></pre>"},{"location":"Python/Visualization/Pandas/#hour-x-axis-with-24-hours","title":"hour x-axis with 24 hours","text":"<pre><code>tod = df.groupby('time')[['min', 'max']].mean().reset_index()\n# # cannot restrict the min/max\n# ax = tod.plot(x='time', y=['min', 'max'], figsize=(12, 6), grid=True)\n# xticks = pd.date_range('2023-01-01', freq='2H', periods=12)\n# ax.set_xticks(xticks.time)\n# ax.xaxis.set_major_formatter(plt.FixedFormatter(xticks.strftime('%H:%M')))\n\ntod['hour'] = tod['time'].apply(lambda x: x.hour + x.minute / 60)\nax = tod.plot(x='hour', y=['min', 'max'], figsize=(12, 6), grid=True)\nxticks = list(range(0, 25, 2))\nxtick_labels = [f'{h:2}:00' for h in xticks]\nax.set_xticks(xticks)\nax.set_xlim(xticks[0], xticks[-1])\nax.xaxis.set_major_formatter(plt.FixedFormatter(xtick_labels))\n</code></pre>"},{"location":"Python/Visualization/Pandas/#day-of-week-profile","title":"day of week profile","text":"<pre><code>dow = df.assign(dow=lambda x: x['ts'].dt.dayofweek).groupby('dow')[['min', 'max']].mean().reset_index()\nax = dow.plot(x='dow', y=['min', 'max'], figsize=(12, 6), grid=True)\nax.set_xticks(range(0,7))\n_ = ax.set_xticklabels(['Mon', 'Tue','Wed', 'Thur', 'Fri', 'Sat', 'Sun'])\n</code></pre>"},{"location":"Python/Visualization/figure/","title":"figure","text":""},{"location":"Python/Visualization/figure/#save-figure","title":"save figure","text":"<pre><code>fig = go.Figure()\n    fig.update_layout(\n        width=18, #pixel, CM_TO_PIXEL = 37.7952755906\n        height=9, #pixel\n        margin=go.layout.Margin(\n            l=10, #default 80\n            r=3, #default 80\n            b=5, #default 80\n            t=10  #default 100\n        ),\n        plot_bgcolor='white',\n        paper_bgcolor='white',\n    )\nfig.write_image(imgfile, scale=1.5)\n</code></pre>"},{"location":"Python/Visualization/figure/#rotate-axis-labels","title":"rotate axis labels","text":"<pre><code>import matplotlib.pyplot as plt\n#set font and plot size to be larger\nplt.rcParams.update({'font.size': 20, 'figure.figsize': (10, 8)})\ndf['rating'].plot(kind='hist', title='Rating')\ndf.plot(kind='scatter', x='speed', y='distance', title='my plot')\n</code></pre>"},{"location":"Python/Visualization/figure/#rotate-axis-labels_1","title":"rotate axis labels","text":"<pre><code>for tick in ax.get_xticklabels():\n    tick.set_rotation(90)\n</code></pre>"},{"location":"Python/Visualization/figure/#faceting","title":"faceting","text":"<p>Faceting is the act of breaking data variables up across multiple subplots and combining those subplots into a single figure <pre><code>g = sns.FacetGrid(df, col='class')\ng = g.map(sns.kdeplot, 'sepal_length')\n</code></pre></p>"},{"location":"Python/Visualization/figure/#pairplot","title":"pairplot","text":"<p>plot a grid of pairwise relationships in a dataset <pre><code>sns.pairplot(iris)\n\nfrom pandas.plotting import scatter_matrix\n\nfig, ax = plt.subplots(figsize=(12,12))\nscatter_matrix(iris, alpha=1, ax=ax)\n</code></pre></p>"},{"location":"Python/Visualization/figure/#heatmap","title":"heatmap","text":"<pre><code>sns.heatmap(iris.corr(), annot=True)\n\n# get correlation matrix\ncorr = iris.corr()\nfig, ax = plt.subplots()\n# create heatmap\nim = ax.imshow(corr.values)\n\n# set labels\nax.set_xticks(np.arange(len(corr.columns)))\nax.set_yticks(np.arange(len(corr.columns)))\nax.set_xticklabels(corr.columns)\nax.set_yticklabels(corr.columns)\n\n# Rotate the tick labels and set their alignment.\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\")\n\n# Loop over data dimensions and create text annotations.\nfor i in range(len(corr.columns)):\n    for j in range(len(corr.columns)):\n        text = ax.text(j, i, np.around(corr.iloc[i, j], decimals=2),\n                       ha=\"center\", va=\"center\", color=\"black\")\n</code></pre>"},{"location":"Python/Visualization/figure/#color-bar","title":"color bar","text":"<pre><code>def get_cbar_colors(dts):\n    z = pd.to_timedelta(dts).total_seconds()\n    zmin = min(z)\n    zmax = max(z)\n\n    tks = np.arange(0, 1.1, 0.1)\n    sec_minmax = zmax - zmin\n    secs = tks * sec_minmax\n    dts = sec2dt(secs, min(dts))\n    lbs = [f'{dt.year}-{dt.month:02}' for dt in dts]\n\n    #cmap = plt.cm.get_cmap('jet',10) #viridis, jet\n    colors = {'FUCHSIA':'#FF00FF','PURPLE':'#800080','MAROON':'#800000','GRAY':'#808080','NAVY':'#000080',\n              'BLUE':'#0000FF','TEAL':'#008080','GREEN':'#008000','LIME':'#00FF00','AQUA':'#00FFFF'}\n    cmap = clrs.ListedColormap(colors.values())\n    normalize = plt.Normalize(vmin=0, vmax=1)\n    colors = [cmap(value) for value in (z - zmin) / (zmax - zmin)]\n\n    return tks, lbs, colors, cmap, normalize\n\ndef plot_cbar(ax, tks, lbs, cmap, normalize):\n    sm = plt.cm.ScalarMappable(cmap=cmap, norm=normalize)\n    sm._A = [] #fake up the array of the scalar mappable.  not required from verion 3.1\n    cbar = plt.colorbar(sm,ax=ax,ticks=tks,label='job create time')\n    cbar.ax.set_yticklabels(lbs)\n</code></pre>"},{"location":"Python/Visualization/module/","title":"module","text":""},{"location":"Python/Visualization/module/#bokeh","title":"Bokeh","text":"<p>Bokeh supports interactive plots using its JavaScript API, BokehJS, to provide dynamic and interactive web-ready visualizations. It is based on the grammar of graphics like R\u2019s ggplot2 and supports streaming, and real-time data.</p>"},{"location":"Python/Visualization/module/#plotly","title":"Plotly","text":"<p>Plotly is a JavaScript based data visualization tool, like Bokeh, with strength in making interactive plots with its robust API</p>"},{"location":"Python/Visualization/module/#dash","title":"Dash","text":"<p>Dash is a framework for building interactive dashboards using pure Python. It is built on top of Flask, Plotly.js, ReactJS.</p>"},{"location":"Python/Visualization/module/#altair","title":"Altair","text":"<p>Altair is a simple statistical visualization python library based on Vega-Lite.</p>"},{"location":"Python/Visualization/module/#plotnine","title":"Plotnine","text":"<p>Plotnine is an implementation of the grammar of graphics based on popular R\u2019s ggplot2</p>"},{"location":"Python/Visualization/module/#missingno","title":"missingno","text":"<p>missingno is a small matplotlib based Python library helping show and explore missing data</p>"},{"location":"Python/Visualization/module/#atoti","title":"atoti","text":"<p>atoti works with all Python notebooks, with enhanced features in Jupyter</p>"},{"location":"Python/Visualization/module/#exploratory-data-analysis-eda","title":"Exploratory Data Analysis (EDA)","text":"<p>https://towardsdatascience.com/comparing-five-most-popular-eda-tools-dccdef05aa4c</p>"},{"location":"Python/Visualization/module/#dataprep","title":"Dataprep","text":"<p>Dataprep (use Bokeh) is a library that provides easy-to-use functions for data cleaning and preprocessing tasks. It offers functions for handling missing values, data type conversions, outlier detection, and more. Dataprep aims to simplify the data preparation process before analysis.</p>"},{"location":"Python/Visualization/module/#autoviz","title":"Autoviz","text":"<p>Autoviz is a library that automatically generates visualizations for data exploration. It analyzes the dataset and generates a variety of charts and plots to help users understand the underlying patterns and relationships in the data. Autoviz can be useful for quickly getting insights from your data without the need for manual chart selection.</p>"},{"location":"Python/Visualization/module/#sweetviz","title":"Sweetviz","text":"<p>Sweetviz is another Python library for exploratory data analysis that focuses on generating detailed visual and statistical summaries of datasets. It provides a high-level API to quickly generate comprehensive reports with minimal code.</p>"},{"location":"Python/Visualization/module/#d-tale","title":"D-Tale","text":"<p>D-Tale (use Plotly) is a library that provides an interactive web-based interface for data exploration and analysis. It allows you to visualize your dataset, compute descriptive statistics, apply filters, and perform various other data manipulation tasks. D-Tale provides an intuitive way to interact with your data and gain insights through its user-friendly interface.</p>"},{"location":"Python/Visualization/module/#dabl","title":"Dabl","text":"<p>Dabl (short for \"Data Analysis Baseline Library\") is a library designed to automate the process of building machine learning models for tabular data. It provides a high-level API that automates tasks such as data preprocessing, feature selection, model training, and evaluation. Dabl aims to simplify the machine learning pipeline for tabular data analysis.</p>"},{"location":"Python/Visualization/module/#quickda","title":"QuickDA","text":"<p>QuickDA is a library that offers interactive visualizations and statistical analysis tools for exploratory data analysis. It provides a set of functions to generate various plots, histograms, box plots, and correlation matrices. QuickDA aims to facilitate quick data analysis by providing concise visualizations and summary statistics.</p>"},{"location":"Python/Visualization/module/#lux","title":"Lux","text":"<p>Lux (use Altair) is a library that enhances the Pandas data analysis experience by automatically generating visualizations and offering data exploration capabilities. It provides an intuitive API for filtering, sorting, and visualizing data in Jupyter Notebook environments. Lux helps users discover patterns and relationships in their data through interactive visualizations.</p>"},{"location":"Python/Visualization/Matplotlib/Axes/","title":"Axes","text":""},{"location":"Python/Visualization/Matplotlib/Axes/#set-methods","title":"set methods","text":"<pre><code>ax.set_title('Value profile', loc='left')\nax.set_xticks(xticks)\nax.set_xticklabels(xtick_labels)\nax.set_xlim(xticks[0], xticks[-1])\n</code></pre>"},{"location":"Python/Visualization/Matplotlib/Axis/","title":"Special","text":""},{"location":"Python/Visualization/Matplotlib/Axis/#broken-axis","title":"broken axis","text":"<p>https://matplotlib.org/stable/gallery/subplots_axes_and_figures/broken_axis.html#sphx-glr-gallery-subplots-axes-and-figures-broken-axis-py</p>"},{"location":"Python/Visualization/Matplotlib/Axis/#rotate-axis-labels","title":"rotate axis labels","text":"<pre><code>for tick in ax.get_xticklabels():\n    tick.set_rotation(90)\n</code></pre>"},{"location":"Python/Visualization/Matplotlib/DateAxis/","title":"Date axis","text":"<pre><code>import numpy\nimport pandas\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nn = 25\nnumpy.random.seed(n)\n\nds = pandas.date_range('2020-01-01', periods=n, freq='m')\ndf = pandas.DataFrame({    \n    't': ds,\n    'v': numpy.random.randn(n), \n})\nfig, ax = plt.subplots(figsize=(10, 6))\ndf.plot(ax=ax, x='t', y=['v'])\n</code></pre>"},{"location":"Python/Visualization/Matplotlib/DateAxis/#rotate","title":"rotate","text":"<pre><code>plt.setp(axes.get_xticklabels(), rotation = 15)\n</code></pre>"},{"location":"Python/Visualization/Matplotlib/DateAxis/#turn-on-grid","title":"turn on grid","text":"<pre><code>ax.xaxis.grid(True, which=\"minor\") #minor\nax.xaxis.grid(which='both', linestyle='dotted') #both\n</code></pre>"},{"location":"Python/Visualization/Matplotlib/DateAxis/#bny","title":"b\\nY","text":"<pre><code>#ax.xaxis.set_major_locator(mdates.MonthLocator())\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b\\n%Y'))\n</code></pre>"},{"location":"Python/Visualization/Matplotlib/DateAxis/#timendate","title":"time\\ndate","text":"<pre><code>ax.plot(df.t, df.v)\ndy_fmt=mdates.DateFormatter('%H:%M\\n%Y-%m-%d')\nhr_fmt=mdates.DateFormatter('%H:%M')\nax.xaxis.set_major_formatter(dy_fmt)\nax.xaxis.set_major_locator(mdates.DayLocator())\nax.xaxis.set_minor_formatter(hr_fmt)\nax.xaxis.set_minor_locator(mdates.HourLocator(byhour=range(2,24,2)))\n</code></pre>"},{"location":"Python/Visualization/Matplotlib/matplotlib/","title":"matplotlib","text":""},{"location":"Python/Visualization/Matplotlib/matplotlib/#config","title":"config","text":"<p>first parameter can be 'figure' , 'axes' , 'xtick' , 'ytick' , 'grid' , 'legend' <pre><code>plt.rc('figure', figsize=(10, 10))\nfont_options = {\n  'family': 'monospace',\n  'weight': 'bold',\n  'size': 'small'}\nplt.rc('font', **font_options)\n</code></pre></p>"},{"location":"Python/Visualization/Matplotlib/matplotlib/#subplot","title":"subplot","text":"<pre><code>fig, axes = plt.subplots(2, 3, sharex=True, sharey=True)\nsubplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n</code></pre>"},{"location":"Python/Visualization/Matplotlib/matplotlib/#annotation","title":"annotation","text":"<pre><code>dat = [\n  (datetime(2007,10, 11), 'Dog'),\n  (datetime(2008, 3, 12), 'Bear'),\n  (datetime(2008, 9, 15), 'Horse')]\nfor date, label in dat:\n  ax.annotate(label, xy=(date, spx.asof(date) + 75),\n    xytext=(date, spx.asof(date) + 225),\n    arrowprops=dict(facecolor='black', headwidth=4, width=2, headlength=4),\n    horizontalalignment='left', verticalalignment='top')\n</code></pre>"},{"location":"Python/Visualization/Matplotlib/matplotlib/#drawing","title":"drawing","text":"<pre><code>rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)\ncirc = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)\npgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)\nax.add_patch(rect)\n</code></pre>"},{"location":"Python/Visualization/Matplotlib/matplotlib/#save-fig","title":"save fig","text":"<pre><code>plt.savefig('figpath.png', dpi=400, bbox_inches='tight')\n</code></pre>"},{"location":"Python/Visualization/Plotly/Annotation/","title":"Annotation","text":""},{"location":"Python/Visualization/Plotly/Annotation/#update-annotation","title":"update annotation","text":"<pre><code>import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=[1, 2, 3], y=[4, 5, 6]))\n# Add an annotation with the same font as the subplot title\nfig.add_annotation(\n    text=\"Annotation Text\",\n    x=2, y=5,\n    # arrowhead=2, arrowsize=1.5,\n    font=dict(\n        family=\"Arial\",  # Set the font family to match the subplot title\n        size=14,  # Set the font size to match the subplot title\n        color=\"black\"  # Set the font color to match the subplot title\n    )\n)\n\nfig.update_layout(\n    title='Subplot Title',\n    xaxis_title='X Axis Title',\n    yaxis_title='Y Axis Title'\n)\nfig.layout.annotations[0].update(text=\"Stackoverflow\",x=1,y=5, xanchor='left', yanchor='bottom')\n\nfig.show()\n</code></pre> <p>https://plotly.com/python/reference/layout/annotations/ - <code>x</code>: X-coordinate of the annotation. - <code>y</code>: Y-coordinate of the annotation. - <code>xref</code> and <code>yref</code>: Specifies whether the coordinates refer to the \"paper\" or \"axes\" domain. - <code>text</code>: The text content of the annotation. - <code>showarrow</code>: Boolean indicating whether to draw an arrow from the annotation to the specified point. - <code>arrowhead</code>: The arrowhead style (options: 0, 1, 2, 3, 4, 5, 6). - <code>arrowcolor</code>: Color of the arrow. - <code>arrowwidth</code>: Width of the arrow. - <code>ax</code>: X-coordinate of the arrow tail. - <code>ay</code>: Y-coordinate of the arrow tail. - <code>bordercolor</code> and <code>borderwidth</code>: The color and width of the annotation border. - <code>bgcolor</code>: The background color of the annotation. - <code>opacity</code>: The opacity of the annotation. - <code>font</code>: Dictionary specifying the font properties of the text. - <code>align</code>: Text alignment within the annotation (options: \"left\", \"center\", \"right\").</p>"},{"location":"Python/Visualization/Plotly/Annotation/#change-subplot-title-location","title":"change subplot title location","text":"<pre><code>fig.layout.annotations[0].update(\n    text='Stackoverflow',\n    x=-0.01, y=1,\n    xref='paper', yref='paper',\n    xanchor='left', yanchor='bottom',\n)\n</code></pre>"},{"location":"Python/Visualization/Plotly/AxisTicks/","title":"Axis Ticks","text":"<ul> <li>tickmode: linear, array</li> <li>tickformat: %, datetime</li> <li>tickangle: default is <code>auto</code></li> <li>exponentformat</li> <li>tickformatstops</li> <li>place ticks and gridlines between categories</li> </ul>"},{"location":"Python/Visualization/Plotly/AxisTicks/#tick-label-angle","title":"tick label angle","text":"<pre><code>tickangle=60,\n</code></pre>"},{"location":"Python/Visualization/Plotly/AxisTicks/#axis-line-and-tick-labels","title":"axis line and tick labels","text":"<pre><code>showline=False,      # Hide axis line\nshowticklabels=True  # Show axis tick labels\n</code></pre>"},{"location":"Python/Visualization/Plotly/AxisTicks/#axis-date","title":"axis date","text":"<pre><code>xaxis=dict(\n    title=dict(text='date'),\n    #side='top',   #show axis on top\n    mirror=True,   #also show the axis line on the other side\n    ticks='outside',\n    showline=True,\n    linecolor='black',\n    gridcolor='lightgrey',\n    dtick='d1', #M1\n    tickformat='%d\\n%b\\n%Y', #%b\\n%Y\n)\n</code></pre>"},{"location":"Python/Visualization/Plotly/AxisTicks/#axis-range","title":"axis range","text":"<pre><code>fig.update_layout(\n    xaxis_range=[1,12],\n    yaxis_range=[1,10],\n)\n</code></pre>"},{"location":"Python/Visualization/Plotly/AxisTicks/#axis-ticks_1","title":"axis ticks","text":"<pre><code>fig.update_xaxes(\n    tickvals=x,             # Specify the tick values\n    ticktext=xtick_labels,  # Specify the tick labels\n)\n\nfig.update_layout(\n    xaxis=dict(\n        tickmode='array',       # Enable the custom tick values and labels\n        tickvals=x,             # Specify the tick values\n        ticktext=xtick_labels,  # Specify the tick labels\n    )\n)\n</code></pre>"},{"location":"Python/Visualization/Plotly/AxisTicks/#example","title":"example","text":"<p>https://plotly.com/python/tick-formatting/ <pre><code>import plotly.graph_objects as go\n\nfig = go.Figure(go.Scatter(\n    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n    y = [28.8, 28.5, 37, 56.8, 69.7, 79.7, 78.5, 77.8, 74.1, 62.6, 45.3, 39.9],\n))\n\nfig.update_layout(\n    xaxis = dict(\n        tickmode='linear',\n        tick0=0.5,\n        dtick=0.5,\n    )\n)\n\nfig.show()\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/AxisTicks/#tickformat","title":"Tickformat","text":"<pre><code>fig.update_layout(\n    title='Time Series with Custom Date-Time Format',\n    xaxis_tickformat='%d %B (%a)&lt;br&gt;%Y',\n    yaxis_tickformat='%',\n)\n</code></pre>"},{"location":"Python/Visualization/Plotly/AxisTicks/#ticksuffix","title":"ticksuffix","text":"<p>Add a suffix to the tick labels <pre><code>fig.update_layout(\n    xaxis = dict(        \n        linecolor='#909497',\n        gridcolor='rgba(0,0,0,0)',\n        side='top',\n        range=x_range,\n        fixedrange=True,     \n        ticks='outside',\n        tickmode='array',        \n        ticklen=4,\n        tickwidth=1,\n        tickcolor='#909497',\n        ticksuffix='%',       \n    )\n)\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/BarPlot/","title":"Bar Plot","text":""},{"location":"Python/Visualization/Plotly/BarPlot/#basic","title":"basic","text":"<pre><code>for col in df.columns:\n    col_name = ' '.join(col) if isinstance(col, tuple) else col\n    fig.add_trace(\n        go.Bar(\n            x=df.index.get_level_values('ts'),\n            y=df.get(col),\n            name=col_name,\n            meta=col_name,\n            hovertemplate=hover_template,\n        ),\n    )\n</code></pre>"},{"location":"Python/Visualization/Plotly/BarPlot/#group-gender-bars-together","title":"group gender bars together","text":"<p>The gridline can be put in the midle of each group or in the middle of two groups by using <code>tickson='boundaries'</code> <pre><code>import plotly.graph_objects as go\n\ndf = pd.DataFrame({\n    'country': ['USA', 'USA', 'USA', 'Canada', 'Canada', 'Canada', 'UK', 'UK', 'UK'],\n    'gender': ['Male', 'Female', 'Other', 'Male', 'Female', 'Other', 'Male', 'Female', 'Other'],\n    'value': [10, 15, 5, 8, 12, 4, 7, 11, 3],\n})\n\nfig = go.Figure()\n\nfor gender in df['gender'].unique():\n    data = df[df['gender'] == gender]\n    fig.add_trace(go.Bar(\n        x=data['value'],\n        y=data['country'],\n        name=gender,\n        legendgroup=gender,\n        orientation='h',  # Set orientation to horizontal\n    ))\n\nfig.update_layout(\n    barmode='group',\n    bargap=0.2,       #space between groups\n    bargroupgap=0.05,   #space between bars in the same group\n    title='Grouped Bar Chart by Gender',\n    xaxis=dict(\n        title='Value',\n        side='top',\n        # ticksuffix=' ',\n        # tickprefix='%',\n        tickson='boundaries',\n        # gridwidth=0.5,\n        # griddash='dot',\n        # gridcolor='rgb(230,230,230)',\n    ),\n    yaxis=dict(\n        title='Country',\n    ),\n)\n\nfig.show()\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/Datetime/","title":"Datetime","text":""},{"location":"Python/Visualization/Plotly/Datetime/#x-axis-datetime-format","title":"x-axis datetime format","text":"<p>https://plotly.com/python/time-series/ <pre><code>import plotly.express as px\ndf = px.data.stocks()\nfig = px.line(\n    df, x=\"date\", y=df.columns,\n    hover_data={\"date\": \"|%B %d, %Y\"},\n    title='custom tick labels',\n)\nfig.update_xaxes(\n    dtick=\"M1\",\n    tickformat=\"%b\\n%Y\",\n)\nfig.show()\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/Express/","title":"Express","text":""},{"location":"Python/Visualization/Plotly/Express/#line","title":"line","text":"<p>https://plotly.com/python-api-reference/generated/plotly.express.line</p> <p>Note: <code>width</code> and <code>height</code> are in <code>pixel</code> not <code>mm</code>. 1 pixel = 0.2645833333 mm and 1 mm = 3.7795275591 pixel.</p> <p>how to set the legend and color??? <pre><code>fig = px.line(d1, x='ts', y='val', width=18*60, height=7*60)\nfig.add_scatter(x=d2['ts'], y=d2['val'], mode='lines')\nfig.show()\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/FigUpdate/","title":"Figure Update","text":""},{"location":"Python/Visualization/Plotly/FigUpdate/#update-trace-based-on-name","title":"Update trace based on name","text":"<p>Update trace and change its name <pre><code># create trace\nfig.add_scatter(\n    x=df['ts'],\n    y=df['val'],\n    name=f'My Trace_Name',\n    marker_color='#006da0',\n    line_shape='hv',\n)\n# find previous trace name\nprevious_trace_name = 'Trace_Name'\nfor trace in fig.data:\n    trace_name = trace['name']\n    if 'Trace_Name' in trace_name:\n        previous_trace_name = trace_name\n# update trace\nfig.update_traces(\n    overwrite=True,\n    x=list(forecast_actual['ts']),\n    y=list(forecast_actual[model]),\n    name=f'current_trace_name', #update name\n    selector=dict(name=previous_trace_name),\n)\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/GrathObject/","title":"Graph object","text":""},{"location":"Python/Visualization/Plotly/GrathObject/#rangeselector","title":"rangeselector","text":"<p>https://plotly.com/python-api-reference/generated/plotly.graph_objects.layout.xaxis.rangeselector.html</p> <p>https://stackoverflow.com/questions/75327529/how-to-change-default-rangeselector-in-plotly-python</p>"},{"location":"Python/Visualization/Plotly/Hover/","title":"Hover","text":"<p>Show extra info when the cursor is over the figure data.</p>"},{"location":"Python/Visualization/Plotly/Hover/#example","title":"example","text":"<pre><code>fig = go.Figure(\n    go.Bar(\n        x=df['population'],\n        y=df['country'],\n        orientation='h',\n        name='population',\n        marker=dict(color=df['color']),\n        customdata=np.stack((\n            df['country'],\n            df['area'],\n            df['population'],\n        ), axis=-1)\n    )\n)\n\nhover_items = [\n    '&lt;span style=\"font-size: 1rem;\"&gt;&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;/span&gt;',\n    '&lt;span style=\"font-size: 1rem;\"&gt;Area: &lt;i&gt;%{customdata[1]}&lt;/i&gt;&lt;/span&gt;',\n    '&lt;span style=\"font-size: 0.9rem;\"&gt;Population: %{customdata[2]}&lt;/span&gt;'\n]\n\nfig.update_traces(\n    hovertemplate='&lt;br&gt;'.join(hover_items) + '&lt;extra&gt;&lt;/extra&gt;'\n)\n</code></pre>"},{"location":"Python/Visualization/Plotly/Layout/","title":"Layout","text":""},{"location":"Python/Visualization/Plotly/Layout/#figupdate_layout","title":"fig.update_layout","text":"<pre><code>fig.update_layout(          \n    autosize=True,\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    margin=dict(\n        t=20,\n        b=20,\n        l=20,\n        r=20,\n    ), #the default margin might be too large\n    legend=dict(\n        orientation='h',\n        xanchor='left',\n        x=0.01,\n        yanchor='top',\n        y=0.98,\n    ),\n    xaxis=dict(\n        # title=dict(text='date'),\n        mirror=True,\n        # ticks='outside',\n        showline=True,\n        linecolor='black',\n        gridcolor='lightgrey',\n        dtick='M1',\n        tickformat='%b\\n%Y',\n    ), \n    xaxis2=dict(\n        # title=dict(text='date'),\n        mirror=True,\n        # ticks='outside',\n        showline=True,\n        linecolor='black',\n        gridcolor='lightgrey',\n        dtick='M1',\n        tickformat='%b\\n%Y',\n    ),         \n    yaxis=dict(\n        title=dict(text='mean'),\n        mirror=True,\n        ticks='outside',\n        showgrid=False,\n        showline=True,\n        linecolor='black',\n        gridcolor='lightgrey',\n        # side='left',\n    ),\n    yaxis2=dict(\n        title=dict(text='std'),\n        mirror=True,\n        ticks='outside',\n        showgrid=False,\n        showline=True,\n        linecolor='black',\n        gridcolor='lightgrey',\n        # side='left',\n    ),    \n)  \n</code></pre>"},{"location":"Python/Visualization/Plotly/Legend/","title":"Legend","text":""},{"location":"Python/Visualization/Plotly/Legend/#legend-position-and-orientation","title":"legend position and orientation","text":"<pre><code>fig.update_layout(\n    legend=dict(\n        xanchor='left',\n        x=0.01,\n        yanchor='top',\n        y=0.99,\n        orientation='h',    \n    )\n)\n</code></pre>"},{"location":"Python/Visualization/Plotly/Legend/#multiple-legends","title":"multiple legends","text":"<p>https://plotly.com/python/legend/?_gl=1lf1qk7_gaODEwNTk5NTE4LjE2OTkzMTEyNDI._ga_6G7EE0JNSC*MTY5OTMxMTI0MS4xLjEuMTY5OTMxMTMzNS41Mi4wLjA.#adding-multiple-legends <pre><code>fig = go.Figure(\n    data=[\n        go.Scatter(x=df_germany.year, y=df_germany.gdpPercap, name=\"Germany\"),\n        go.Scatter(x=df_france.year, y=df_france.gdpPercap, name=\"France\"),\n        go.Scatter(\n            x=df_averages_europe.index,\n            y=df_averages_europe.gdpPercap,\n            name=\"Europe\",\n            legend=\"legend2\",\n        ),\n        go.Scatter(\n            x=df_averages_americas.index,\n            y=df_averages_americas.gdpPercap,\n            name=\"Americas\",\n            legend=\"legend2\",\n        ),\n    ],\n    layout=dict(\n        title=\"GDP Per Capita\",\n        legend={\n            \"title\": \"By country\",\n            \"xref\": \"container\",\n            \"yref\": \"container\",\n            \"y\": 0.65,\n            \"bgcolor\": \"Orange\",\n        },\n        legend2={\n            \"title\": \"By continent\",\n            \"xref\": \"container\",\n            \"yref\": \"container\",\n            \"y\": 0.85,\n            \"bgcolor\": \"Gold\",\n        },\n    ),\n)\n</code></pre></p> <p>Worked example <pre><code>import pandas as pd\nimport plotly.graph_objs as go\n\ndf = pd.DataFrame({\n    'model': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],\n    'period': ['AM', 'AM', 'PM', 'PM', 'AM', 'AM', 'PM', 'PM'],\n    'dow': [1,2,1,2,1,2,1,2],\n    'mw': [100, 120, 130, 150, 105, 125, 135,160],    \n})\n\n# Create a list of unique values for the 'model' and 'period' columns\nmodels = df['model'].unique()\nperiods = df['period'].unique()\n\n# Define a color scale for models and line style for each period\ncolors = ['blue', 'orange']\nline_styles = ['solid', 'dash', 'dot', 'dashdot']\n\n# Create a figure\nfig = go.Figure()\n\n# Iterate over unique values of 'model' and 'period' to create separate lines\nfor i, model in enumerate(models):\n    color = colors[i]   \n    for j, period in enumerate(periods):\n        line_style = line_styles[j]\n        data_subset = df.query('model == @model &amp; period == @period')\n        fig.add_trace(go.Scatter(\n            x=data_subset['dow'], \n            y=data_subset['mw'], \n            mode='lines',\n            line=dict(color=color, dash=line_style),\n            showlegend=False,\n        ))\n# Add legend1. If visibility='legendonly', legend will grey out\nfor i, model in enumerate(models):\n    color = colors[i]\n    data_subset = df.query('model == @model &amp; period == @periods[0]')\n    fig.add_trace(go.Scatter(\n        x=data_subset['dow'].iloc[:1], \n        y=data_subset['mw'].iloc[:1],  \n        mode='lines',\n        line=dict(color=color, dash='solid'),\n        name=f'{model}',\n        visible=True,\n        legend='legend',\n    ))  \n# Add legend2\nfor j, period in enumerate(periods):\n    line_style = line_styles[j]\n    data_subset = df.query('model == @models[0] &amp; period == @period')\n    fig.add_trace(go.Scatter(\n        x=data_subset['dow'].iloc[:1], \n        y=data_subset['mw'].iloc[:1], \n        mode='lines',\n        line=dict(color='black', dash=line_style),\n        name=f'{period}',\n        visible=True,\n        legend='legend2',\n    ))        \n\n# Update the layout to add a legend\nfig.update_layout(\n    legend={\n        \"title\": 'Model',\n        # \"xref\": \"container\",\n        # \"yref\": \"container\",\n        'x': 1.01,\n        'y': 0.99,\n        # \"bgcolor\": \"Gold\",\n    },\n    legend2={\n        \"title\": 'Period',\n        # \"xref\": \"container\",\n        # \"yref\": \"container\",\n        'x': 1.01,\n        'y': 0.45,\n        # \"bgcolor\": \"Orange\",\n    },    \n)\n\n# Show the plot\nfig.show()\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/Legend/#grouped-legend","title":"grouped legend","text":"<p>https://plotly.com/python/legend/?_gl=11j02exd_gaODEwNTk5NTE4LjE2OTkzMTEyNDI._ga_6G7EE0JNSC*MTY5OTMxMTI0MS4xLjEuMTY5OTMxMTY4NS42MC4wLjA.#grouped-legend-items <pre><code>import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=[1, 2, 3],\n    y=[2, 1, 3],\n    legendgroup=\"group\",  # this can be any string, not just \"group\"\n    legendgrouptitle_text=\"First Group Title\",\n    name=\"first legend group\",\n    mode=\"markers\",\n    marker=dict(color=\"Crimson\", size=10)\n))\n\nfig.add_trace(go.Scatter(\n    x=[1, 2, 3],\n    y=[2, 2, 2],\n    legendgroup=\"group\",\n    name=\"first legend group - average\",\n    mode=\"lines\",\n    line=dict(color=\"Crimson\")\n))\n\nfig.add_trace(go.Scatter(\n    x=[1, 2, 3],\n    y=[4, 9, 2],\n    legendgroup=\"group2\",\n    legendgrouptitle_text=\"Second Group Title\",\n    name=\"second legend group\",\n    mode=\"markers\",\n    marker=dict(color=\"MediumPurple\", size=10)\n))\n\nfig.add_trace(go.Scatter(\n    x=[1, 2, 3],\n    y=[5, 5, 5],\n    legendgroup=\"group2\",\n    name=\"second legend group - average\",\n    mode=\"lines\",\n    line=dict(color=\"MediumPurple\")\n))\n\nfig.update_layout(title=\"Try Clicking on the Legend Items!\")\n\nfig.show()\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/LinePlot/","title":"Line Plot","text":""},{"location":"Python/Visualization/Plotly/LinePlot/#line-styles","title":"line styles","text":"<pre><code>import numpy as np\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 10, 4, 15, 9])\nline_styles = ['0', '8,3', '8,3,2,3', '3,2', '14,2', '8,3,2,3,2,3', '4,3', '6,3,2,3,2,3,2,3', '2,5']\ncnt = len(line_styles)\n\nfor i, line_stsyle in enumerate(line_styles):\n    fig.add_trace(go.Scatter(\n        x=x,\n        y=y + 5*i,\n        mode='lines',\n        line=dict(color='blue', dash=line_styles[i], width=2 * (1 - i * 0.4 / cnt)),\n    ))\n\nfig.show()\n</code></pre>"},{"location":"Python/Visualization/Plotly/LinePlot/#custom-line-styles","title":"custom line styles","text":"<pre><code>def get_line_widths(cnt, width=2):\n    line_widths = [width * (1 - i * 0.4 / cnt) for i in range(cnt)]\n    return line_widths\n\ndef get_line_styles(cnt):\n    linestyles = [\n        '0', '8,3', '8,3,2,3', '3,2', '14,2',\n        '8,3,2,3,2,3', '4,3', '6,3,2,3,2,3,2,3', '2,5'\n    ]\n    line_styles = (\n        linestyles * (cnt // len(linestyles)) + linestyles[:cnt % len(linestyles)]\n    )\n    return line_styles\n</code></pre>"},{"location":"Python/Visualization/Plotly/LinePlot/#example","title":"example","text":"<p>Plot two columns from two dfs as lines and set the line color, legend and figure size.</p> <p>if name is not set will be default to the df val col name and line will be using default color rotation <pre><code>fig = go.Figure()\nfig.add_trace(go.Scatter(x=df1['ts'], y=df1['val'], mode='lines', name='line-1', line=dict(color='blue')))\nfig.add_trace(go.Scatter(x=df2['ts'], y=df2['val'], mode='lines', name='line-2', line=dict(color='red')))\nfig.update_layout(\n    xaxis_title='ts',\n    yaxis_title='val',\n    width=28 * 37.795,  # Convert cm to pixels (1 cm = 37.795 pixels)\n    height=14 * 37.795,  # Convert cm to pixels (1 cm = 37.795 pixels)\n)  \nfig.show()\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/SecondYaxis/","title":"Second y-axis","text":""},{"location":"Python/Visualization/Plotly/SecondYaxis/#syncing-axes-ticks-does-not-work-in-5161","title":"Syncing Axes Ticks does not work in 5.16.1","text":"<pre><code>import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(specs=[[{'secondary_y': True}]])\nfig.add_trace(\n    go.Scatter(\n        x=df['date'], \n        y=df['price'], \n        mode='lines', \n        name='price', \n        line=dict(color='blue'),\n    ), \n)\nfig.add_trace(\n    go.Scatter(\n        x=df['date'], \n        y=df['quantity'], \n        yaxis='y2',\n        mode='lines', \n        name='quantity', \n        line=dict(color='red'),\n    ),     \n)\nfig.update_layout(          \n    plot_bgcolor='white',\n    width=28 * 37.795,  # Convert cm to pixels (1 cm = 37.795 pixels)\n    height=14 * 37.795,  # Convert cm to pixels (1 cm = 37.795 pixels)\n    legend=dict(\n        xanchor='left',\n        x=0.01,\n        yanchor='top',\n        y=0.99,        \n    ),\n    xaxis=dict(\n        # title=dict(text='date'),\n        mirror=True,\n        # ticks='outside',\n        showline=True,\n        linecolor='black',\n        gridcolor='lightgrey',\n    ),   \n    yaxis=dict(\n        title=dict(text='price'),\n        mirror=True,\n        # ticks='outside',\n        showgrid=False,\n        showline=True,\n        linecolor='black',\n        gridcolor='lightgrey',\n        side='left',\n    ),\n    yaxis2=dict(\n        title=dict(text='quantity'),\n        mirror=True,\n        # ticks='outside',\n        showgrid=False,\n        showline=True,\n        linecolor='black',\n        gridcolor='lightgrey',\n        side='right',\n        overlaying='y',\n        tickmode='sync', # does not work\n    ),    \n)  \nfig.show()\n</code></pre>"},{"location":"Python/Visualization/Plotly/StackedLine/","title":"stacked line chart","text":"<p>https://plotly.com/python-api-reference/</p>"},{"location":"Python/Visualization/Plotly/StackedLine/#example","title":"example","text":"<pre><code>fig = plotly_fig(df, 'fig', 'x', 'y')\nfig.write_image('c:/dev/my_figure.png', scale=1.5)\n</code></pre>"},{"location":"Python/Visualization/Plotly/StackedLine/#chart","title":"chart","text":"<pre><code>def plotly_fig(\n    df: pd.DataFrame,\n    title: str,\n    xtitle: str,\n    ytitle: str,\n    xax_cfg: dict | None,\n    yax_cfg: dict | None,\n    lgd_cfg: dict | None,\n    fig_cfg: dict | None,\n) -&gt; go.Figure:\n    '''\n    create a plotly figure based on inputs\n    '''\n\n    xax_cfg = merge_dicts(xax_cfg, default_xax_cfg)\n    yax_cfg = merge_dicts(yax_cfg, default_yax_cfg)\n    lgd_cfg = merge_dicts(lgd_cfg, default_lgd_cfg_rv)\n    fig_cfg = merge_dicts(fig_cfg, default_fig_cfg, add=lgd_cfg)\n\n    yfmt = '%{y:,.2f}' #'$%{y:,.2f}', '%{y:,.2%}'\n    yhover = f'&lt;br&gt;&lt;b&gt;{ytitle}:&lt;/b&gt; {yfmt}'\n    hover_template = (\n        '&lt;b&gt;X:&lt;/b&gt; %{x|%Y-%m-%d}'\n        '&lt;br&gt;&lt;b&gt;T:&lt;/b&gt; %{meta}'\n        f'{yhover}&lt;extra&gt;&lt;/extra&gt;'\n    )\n\n    fig = go.Figure()\n    dts = df.index.get_level_values(xtitle)\n    for col in df.columns:\n        fig.add_trace(\n            go.Scatter(\n                x=dts,\n                y=df.get(col),\n                name=col,\n                mode='lines',\n                line = dict(width=0.5),\n                line_shape='hv',\n                fill='tonexty',\n                meta=col,\n                stackgroup='one',\n                hovertemplate=hover_template,\n            ),\n        )\n\n    fig.update_xaxes(**xax_cfg)\n    fig.update_yaxes(**yax_cfg)\n    fig.update_layout(**fig_cfg)\n\n    return fig\n</code></pre>"},{"location":"Python/Visualization/Plotly/StackedLine/#layaout","title":"layaout","text":"<pre><code>default_font = 'DejaVu Sans'\ndefault_fig_cfg = dict(\n    width=680,\n    height=303,\n    title=dict(\n        x=0.5,\n        y=0.995,\n        yanchor='top',\n        text=' &amp; '.join(title).replace('_', ' ').title(),\n        font_family=default_font,\n        font_size=14,\n        font_color='#5e6a71',\n    ),\n    margin=go.layout.Margin(\n        l=10, #default 80\n        r=3, #default 80\n        b=5, #default 80\n        t=25  #default 100\n    ),\n    hovermode='x',\n    plot_bgcolor='white',\n    paper_bgcolor='white',\n)\n</code></pre>"},{"location":"Python/Visualization/Plotly/StackedLine/#x-axis","title":"x-axis","text":"<pre><code>default_xax_cfg = dict(\n    title=dict(\n        #text=xtitle,\n        font_family=default_font,\n        font_size=12,\n        standoff=0,  #distance between label and xtitle\n    ),\n    tickfont = dict(\n        size = 10,\n    ),\n    autorange=True,\n    fixedrange=False,\n    gridcolor='#a0afb8',\n    dtick='M1',\n    tickformat='%b\\n%Y-Q%q',\n    linecolor='#5e6a71',\n    showspikes=True,\n)\n</code></pre>"},{"location":"Python/Visualization/Plotly/StackedLine/#y-axis","title":"y-axis","text":"<pre><code>deafult_yax_cfg = dict(\n    title=dict(\n        text=ytitle,\n        font_family=default_font,\n        font_size=12,\n        standoff=4,\n    ),\n    tickfont = dict(\n        size = 10,\n    ),\n    gridcolor='#a0afb8',\n    showspikes=True,\n)\n</code></pre>"},{"location":"Python/Visualization/Plotly/StackedLine/#legend","title":"legend","text":"<p>https://plotly.com/python/reference/#layout-legend <pre><code>default_lgd_cfg_th = dict(\n    x=0.0,\n    y=0.955,\n    xanchor='left',\n    yanchor='bottom',\n    orientation='h', #top horizontal\n    font=dict(family='Calibri', size=10)\n)\n\ndefault_lgd_cfg_rv = dict(\n    x=1.0,\n    y=1.0,\n    xanchor='left',\n    yanchor='top',\n    orientation='v', #right vertical\n    font=dict(family=default_font, size=10)\n)\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/Style/","title":"Style","text":"<pre><code>%%html\n\n&lt;style&gt;\n.box_style{\n    font-color: #5e6a71;\n    background-color:white;\n}\n.button-select{\n    color: white;\n    font-family: \"Calibri\";\n    background-color: #2670A9;\n}\n&lt;! widget.Tab header width auto scale \u254c&gt;\n.jupyter-widgets.widget-tab &gt; .p-TabBar .p-TabBar-tab {\n    flex: 0 1 auto\n}\n&lt;/style&gt;\n</code></pre>"},{"location":"Python/Visualization/Plotly/SubPlots/","title":"SubPlots","text":""},{"location":"Python/Visualization/Plotly/SubPlots/#subplots","title":"SubPlots","text":"<p>https://plotly.com/python-api-reference/generated/plotly.subplots.make_subplots.html</p>"},{"location":"Python/Visualization/Plotly/SubPlots/#vertical-space-between-subplots","title":"vertical space between subplots","text":"<p>Notice when to use <code>row_heights/vertical_spacing</code> and when to use <code>column_widths/horizontal_spacing</code>. <pre><code>from plotly.subplots import make_subplots\nfig = make_subplots(\n    rows=4,\n    cols=1,\n    row_heights=[0.2, 0.2, 0.2, 0.4], \n    shared_xaxes=True, \n    specs=[[{'secondary_y': True}]]*4,\n    vertical_spacing=0.02,\n)\nprint(fig.layout) # inspect the axes name for each subplot\n\n# the specs must be a 2D (rows, cols) list, add title for each subplot as well\nfig = make_subplots(\n    rows=1,\n    cols=2,\n    column_widths=[0.5, 0.5],\n    shared_yaxes=True,\n    specs=[[{'type': 'xy'}, {'type': 'xy'}]],\n    horizontal_spacing=0.05,\n    subplot_titles=('Weekday', 'Weekend'),\n)\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/SubPlots/#secondary-y-axis","title":"secondary y-axis","text":"<pre><code>import numpy as np\nimport plotly.graph_objects as go\n\nx = np.linspace(1, 7, 12)\ny1 = 100+10*np.random.rand(12)\ny2 = 2+2*np.random.rand(12)\ny3 = 1+3*np.random.rand(12)\n\nt1 = go.Bar(x=x, y=y1, name='y1')  \nt2 = go.Scatter(x=x, y=y2, name='y2', yaxis='y2', mode='lines+markers', line=dict(color='orange'))  \nt3 = go.Scatter(x=x, y=y3, name='y3', yaxis='y2', mode='lines+markers', line=dict(color='red'))  \n\nlayout = go.Layout(    \n    width=850, \n    height=450,\n    title='Bar-line chart with secondary y-axis', \n    yaxis=dict(title='y1'),            \n    yaxis2=dict(title='y2', overlaying='y', side='right'),\n)  \n\nfig = go.Figure(data=[t1, t2, t3], layout=layout)  \nfig.show()\n</code></pre>"},{"location":"Python/Visualization/Plotly/SubPlots/#secondary-y-axis-using-subplots","title":"secondary y-axis using subplots","text":"<pre><code>import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(specs=[[{'secondary_y': True}]])\n\nfig.add_trace(\n    go.Scatter(x=[1, 2, 3], y=[50, 20, 100], name='y1'),\n    secondary_y=False,\n)\nfig.add_trace(\n    go.Scatter(x=[1, 2, 3], y=[-20, 50, -100], name='y2'),\n    secondary_y=True,\n)\nfig.update_layout(\n    width=750,\n    height=450,\n    title_text='Secondary y-axis',\n    xaxis=dict(title_text='xaxis title'),\n    yaxis=dict(title_text='yaxis left'),\n    yaxis2=dict(title_text='yaxis right', overlaying='y', tickmode='sync', scaleanchor='y'),\n)\n\nfig.show()\n</code></pre>"},{"location":"Python/Visualization/Plotly/SubPlots/#subplots-with-multiple-y-axes","title":"subplots with multiple y-axes","text":"<p>https://community.plotly.com/t/can-subplot-support-multiple-y-axes/38891/21</p> <p>For each subplot, we should specify whether it's a simple x-y chart or has a second y-axis. <pre><code>import numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfig = make_subplots(\n    rows=2,\n    cols=1, \n    vertical_spacing=0.05, \n    shared_xaxes=True,\n    specs=[\n        [{'type': 'xy'}], \n        [{'secondary_y': True}],\n    ],\n)\n\nx = np.linspace(1, 7, 12)\ny1 = 1+3*np.random.rand(12)\ny2 = 2+2*np.random.rand(12)\ny3 = 3+1*np.random.rand(12)\ny4 = 4+1*np.random.rand(12)\ntrcolor = ['#636efa', '#EF553B', '#00cc96', '#ab63fa']\nfig.add_trace(\n    go.Scatter(x=x, y=y1, name='y1'), \n    row=1, col=1,\n)\nfig.add_trace(\n    go.Scatter(x=x, y=y2, name='y2'), \n    row=2, col=1,\n)\nfig.add_trace(\n    go.Scatter(x=x, y=y3, name='y3', showlegend=False), \n    row=2, col=1, \n    secondary_y=True,\n)\nfig.add_trace(\n    go.Scatter(x=x, y=y4, name='y4', xaxis='x2', yaxis='y4')\n)\n\nfig.update_layout(\n    title_text='Subplots with multiple yaxes', \n    title_x=0.5,\n    margin_l=2,\n    width=750,\n    height=450,\n    xaxis_domain= [0.3, 1], \n    xaxis2_title='common xaxis_title',\n    xaxis2_domain=[0.3, 1],\n    yaxis4=dict(\n        anchor= 'free',\n        overlaying= 'y2',\n        side ='left',\n        position=0.17,\n        title_text='y4',\n        titlefont_color=trcolor[3],      \n        tickfont_color=trcolor[3],\n    ),\n)\n\nfor j, ax in enumerate(['yaxis', 'yaxis2', 'yaxis3']):\n    fig.layout[ax].update(\n        title_text=f'y{j+1}', #merge the initial dict fig.layout[ax] with a new ax related dict\n        titlefont_color=trcolor[j],      \n        tickfont_color=trcolor[j],\n    )\nfig.show()\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/SubPlots/#legend-for-each-subplot","title":"legend for each subplot","text":"<p>https://community.plotly.com/t/associating-subplots-legends-with-each-subplot-and-formatting-subplot-titles/33786/8</p> <p>use <code>legendgroup</code> <pre><code>fig.append_trace(go.Scatter(\n    x=df.query(\"country == 'Canada'\")['year'],\n    y=df.query(\"country == 'Canada'\")['lifeExp'],\n    name = 'Canada',\n    legendgroup = '1'\n), row=1, col=1)\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/Trace/","title":"Trace","text":""},{"location":"Python/Visualization/Plotly/Trace/#visibility","title":"visibility","text":"<p>https://stackoverflow.com/questions/71260018/plotly-hiding-the-trace-initially</p> <p>Add a trace for legend only <pre><code>import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=[1, 2, 3, 4, 5],\n    y=[1, 2, 3, 4, 5],\n))\n\nfig.add_trace(go.Scatter(\n    x=[1, 2, 3, 4, 5],\n    y=[5, 4, 3, 2, 1],\n    visible='legendonly', #True, False and 'legendonly'\n))\n\nfig.show()\n</code></pre></p>"},{"location":"Python/Visualization/Plotly/Trace/#hide-legend-items","title":"hide legend items","text":"<p>https://plotly.com/python/legend/#hiding-legend-items <pre><code>import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=[1, 2, 3, 4, 5],\n    y=[1, 2, 3, 4, 5],\n    showlegend=False,\n))\n\nfig.add_trace(go.Scatter(\n    x=[1, 2, 3, 4, 5],\n    y=[5, 4, 3, 2, 1],\n))\n\nfig.update_layout(showlegend=True)\n\nfig.show()\n</code></pre></p>"},{"location":"Python/Visualization/Seaborn/Heatmap/","title":"Heatmap","text":"<p>ridge plot\\ https://python.plainenglish.io/ridge-plots-with-pythons-seaborn-4de5725881af</p>"},{"location":"Python/Visualization/Seaborn/Heatmap/#create-df","title":"create df","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.DataFrame(np.random.random((10,7)), columns=['a','b','c','d','e','f','g'])\nsns.heatmap(df)\n</code></pre>"},{"location":"Python/Visualization/Seaborn/Heatmap/#example","title":"example","text":"<pre><code>sns.heatmap(iris.corr(), annot=True)\n\n# get correlation matrix\ncorr = iris.corr()\nfig, ax = plt.subplots()\n# create heatmap\nim = ax.imshow(corr.values)\n\n# set labels\nax.set_xticks(np.arange(len(corr.columns)))\nax.set_yticks(np.arange(len(corr.columns)))\nax.set_xticklabels(corr.columns)\nax.set_yticklabels(corr.columns)\n\n# Rotate the tick labels and set their alignment.\nplt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n\n# Loop over data dimensions and create text annotations.\nfor i in range(len(corr.columns)):\n    for j in range(len(corr.columns)):\n        text = ax.text(\n            j, i, np.around(corr.iloc[i, j], decimals=2),\n            ha='center', va='center', color='black'\n        )\n</code></pre>"},{"location":"Python/Visualization/Seaborn/Heatmap/#special","title":"special","text":"<pre><code>#annotate each cell with value\nsns.heatmap(df, annot=True, annot_kws={'size': 6})\n\n#grid lines\nsns.heatmap(df, linewidths=1, linecolor='blue')\n\n#remove x-axis labels\nsns.heatmap(df, xticklabels=False)\n\n#replace x-axis labels\nxtls = ['q','r','s','t','u','v','w']\nsns.heatmap(df, xticklabels=xtls)\n\n#remove color bar\nsns.heatmap(df, cbar=False)\n\n#skip axis labels\nsns.heatmap(df, xticklabels=3) #keep first in 3\n</code></pre>"},{"location":"Python/Visualization/Seaborn/Heatmap/#custom-date-labels","title":"custom date labels","text":"<pre><code>#df date must be string\nfig, ax = plt.subplots(figsize=(10,8))\ng = sns.heatmap(df, ax=ax, cmap='coolwarm', vmin=df.min().min(), vmax=df.max().max())\nfig.suptitle('My first heatmap')\n#custom date labels\ngetattr(ax, 'set_xticks')(xtks)      #locations of xtls in df.columns\ngetattr(ax, 'set_xticklabels')(xtls) #a list of date strings\nax.tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True)\n</code></pre>"},{"location":"Python/Visualization/Seaborn/catplot/","title":"catplot","text":"<p>https://seaborn.pydata.org/generated/seaborn.catplot.html</p> <ul> <li> <p><code>order</code>, <code>hue_order</code>: lists of strings, optional\\   Order to plot the categorical levels in; otherwise the levels are inferred from the data objects.</p> </li> <li> <p><code>row_order</code>, <code>col_order</code>: lists of strings, optional\\   Order to organize the rows and/or columns of the grid in, otherwise the orders are inferred from the data objects.</p> </li> </ul>"},{"location":"Python/Visualization/Seaborn/catplot/#catplot-with-bar-type","title":"catplot with bar type","text":"<p>Use catplot() to combine a barplot() and a FacetGrid - data should not be in index: <pre><code>smoker = list(data['smoker'].unique())\ntime = list(data['time'].unique())\ng = sns.catplot(\n    kind='bar',\n    data=tips,\n    x='sex',\n    order=['male', 'dfemale'], #order for the x value\n    y='total_bill',\n    row='smoker',\n    row_order=smoker,\n    col='time',\n    col_order=time,\n    sharey=False,\n    ci=None,                   #remove error bars    \n    height=4,\n    aspect=0.7,\n)\n#ylim, gridline, annotation\n# get the (min, max) of the val column for each group (smoker, time)\nylims = (\n    d.groupby(['smoker','time'])\n    .agg(vmin=('val', 'min'), vmax=('val', 'max'))\n    .assign(\n        tmin=lambda x: x[['vmin']].groupby(['smoker']).transform('min'),\n        tmax=lambda x: x[['vmax']].groupby(['smiker']).transform('max'),\n    )\n    .drop(columns=['vmin','vmax'])\n    .assign(tmin=lambda x: x.tmin * 0.99)\n    .reindex(pd.MultiIndex.from_product([smoker, time])) #keep row/col order\n    .to_records(index=False)\n)\nfor i, ax in enumerate(g.axes.ravel()):\n    ax.set_ylim(ylims[i])\n    ax.grid(visible=True, which='major', color='black', linewidth=0.075)\n    for c in ax.containers:\n        labels = [f'{v.get_height():.3f}' for v in c]\n        ax.bar_label(c, labels=labels, label_type='edge', rotation=90, fontsize=8)\n</code></pre></p>"},{"location":"Python/Visualization/Seaborn/lineplot/","title":"lineplot","text":""},{"location":"Python/Visualization/Seaborn/lineplot/#line-styles","title":"line styles","text":"<pre><code>line_styles = ['-', '--', '..', '-.', '-..-', '--...', '-    ', '--   ', '--..', 'o-', 's-', '^-', '&gt;--', '&lt;--', '*-', 'h-', '+-', '-', '-', None]\n</code></pre>"},{"location":"Python/Visualization/Seaborn/lineplot/#line-plot-with-figure-size","title":"line plot with figure size","text":"<p>will automatically plot x, y, z based on index ts. <pre><code>df = pd.DataFrame({'ts':ts, 'x': x, 'y': y, 'z': z}).set_index(ts)\nplt.figure(figsize=(18,7))\nsns.lineplot(data=df)\nplt.show()\n</code></pre></p>"},{"location":"Python/Visualization/Seaborn/lineplot/#lineplot-with-hue-and-style","title":"lineplot with hue and style","text":"<p>ToD profile <pre><code>tod = df.groupby(['quarter', 'time'])[['QLD', 'NSW']].mean().reset_index()\ntod['hour'] = tod['time'].apply(lambda x: x.hour + x.minute / 60)\ntod = tod.set_index(['quarter', 'time', 'hour']).stack().rename_axis(index={None: 'model'}).rename('val').reset_index()\n\nsns.set_style(\"whitegrid\")\n\nplt.figure(figsize=(12, 6))\ng = sns.lineplot(data=tod, x='hour', y='val', hue='quarter', style='model', legend='brief')\n\n# Set xticks\nxticks = list(range(0, 25, 2))\nxtick_labels = [f'{h:02}:00' for h in xticks]\nplt.xticks(xticks, xtick_labels)\n_ = plt.xlim(xticks[0], xticks[-1])\n\n# Set the legend position to the right-top outside corner\n_ = g.legend(loc='upper left', bbox_to_anchor=(1.05, 1.1))\n</code></pre></p>"},{"location":"Python/Visualization/Seaborn/seaborn/","title":"seaborn","text":"<p>https://seaborn.pydata.org/tutorial/axis_grids.html</p> <pre><code>#show NaN values in df\nsns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n\n#show numbers count in a df col\nsns.set_style('whitegrid')\nsns.countplot(x='c1', data=df)\nsns.countplot(x='c1', hue='c2', data=df, pallete='RdBu_r')\n\n#distribution of df col\nsns.distplot(df['c1'].dropna(), kde=False, bins=30)\ndf['c1'].plot.dist(bins=30)\ndf['c1'].hist(bins=30, figsize=(10,4))\ndf['c1'].iplot(kind='hist', bins=30)\n</code></pre>"},{"location":"Python/Visualization/Seaborn/seaborn/#bar","title":"bar","text":"<pre><code>sns.barplot(x='tip_pct', y='day', data=df, orient='h')\nsns.barplot(x='tip_pct', y='day', data=df, hue='time', orient='h')\n</code></pre>"},{"location":"Python/Visualization/Seaborn/seaborn/#hist","title":"hist","text":"<pre><code>sns.distplot(values, bins=100, color='k')\n</code></pre>"},{"location":"Python/Visualization/Seaborn/seaborn/#scatter","title":"scatter","text":"<pre><code>sns.regplot('m1', 'unemp', data=trans_data)\nsns.pairplot(trans_data, diag_kind='kde', plot_kws={'alpha': 0.2})\n</code></pre>"},{"location":"Python/Visualization/Seaborn/seaborn/#facetgrid","title":"facetgrid","text":"<p>Faceting is the act of breaking data variables up across multiple subplots and combining those subplots into a single figure <pre><code>g = sns.FacetGrid(df, col='class')\ng = g.map(sns.kdeplot, 'sepal_length')\n</code></pre></p> <pre><code>sns.factorplot(kind='bar', x='day', y='tip_pct',\n               col='smoker', hue='time', data=tips[tips.tip_pct &lt; 1])\n\n#split row and col\ng = sns.FacetGrid(tip, row='sex', col='time', hue='smoker', height=4)\ng.map(plt.scatter, \"total_bill\", \"tip\")\ng.add_legend()\n\n#add hue\ng = sns.FacetGrid(tip, row='day', row_order = tip.day.value_counts().index, height=1.5, aspect=4)\ng.map(sns.distplot, \"total_bill\", hist=False)\n\n#settings\ng.set_titles(row_template = '{row_name}', col_template = '{col_name}') #set new titles\ng.set_axis_labels(\"Total bill (US Dollars)\", \"Tip\")\ng.set(xticks=[10, 30, 50], yticks=[2, 6, 10])\ng.fig.subplots_adjust(wspace=.02, hspace=.02)\n</code></pre>"},{"location":"Python/Visualization/Seaborn/seaborn/#pairplot","title":"pairplot","text":"<p>plot a grid of pairwise relationships in a dataset <pre><code>sns.pairplot(iris)\n\nfrom pandas.plotting import scatter_matrix\n\nfig, ax = plt.subplots(figsize=(12,12))\nscatter_matrix(iris, alpha=1, ax=ax)\n</code></pre></p>"},{"location":"Python/Visualization/Seaborn/seaborn/#relplot","title":"relplot","text":"<pre><code>g = sns.relplot(kind='line', data=df, x='fye', y='val',\n                row='scen', hue='reg', style='fit', height=4, aspect=1.5,\n                facet_kws={'sharey': True, 'sharex': False})\ng._legend.set_bbox_to_anchor([0.35,0.92])\n\n[plt.setp(ax.texts, text=\"\") for ax in g.axes.flat] #remove the original texts\ng.set_titles(row_template = '{row_name}', col_template = '{col_name}') #set new titles\n\n#settings\ng.set_axis_labels(\"Total bill (US Dollars)\", \"Tip\")\ng.set(xticks=[10, 30, 50], yticks=[2, 6, 10])\ng.fig.subplots_adjust(wspace=.02, hspace=.02)\n</code></pre>"},{"location":"Python/Visualization/Seaborn/seaborn/#custom-func","title":"custom func","text":"<pre><code>from scipy import stats\ndef quantile_plot(x, **kwargs):\n    quantiles, xr = stats.probplot(x, fit=False)\n    plt.scatter(xr, quantiles, **kwargs)\ng = sns.FacetGrid(tips, col=\"sex\", height=4)\ng.map(quantile_plot, \"total_bill\")\n\ndef qqplot(x, y, **kwargs):\n    _, xr = stats.probplot(x, fit=False)\n    _, yr = stats.probplot(y, fit=False)\n    plt.scatter(xr, yr, **kwargs)\ng = sns.FacetGrid(tips, col=\"smoker\", height=4)\ng.map(qqplot, \"total_bill\", \"tip\")\n</code></pre>"},{"location":"Python/Visualization/Seaborn/seaborn/#color-bar","title":"color bar","text":"<pre><code>def get_cbar_colors(dts):\n    z = pd.to_timedelta(dts).total_seconds()\n    zmin = min(z)\n    zmax = max(z)\n\n    tks = np.arange(0, 1.1, 0.1)\n    sec_minmax = zmax - zmin\n    secs = tks * sec_minmax\n    dts = sec2dt(secs, min(dts))\n    lbs = [f'{dt.year}-{dt.month:02}' for dt in dts]\n\n    #cmap = plt.cm.get_cmap('jet',10) #viridis, jet\n    colors = {'FUCHSIA':'#FF00FF','PURPLE':'#800080','MAROON':'#800000','GRAY':'#808080','NAVY':'#000080',\n              'BLUE':'#0000FF','TEAL':'#008080','GREEN':'#008000','LIME':'#00FF00','AQUA':'#00FFFF'}\n    cmap = clrs.ListedColormap(colors.values())\n    normalize = plt.Normalize(vmin=0, vmax=1)\n    colors = [cmap(value) for value in (z - zmin) / (zmax - zmin)]\n\n    return tks, lbs, colors, cmap, normalize\n\ndef plot_cbar(ax, tks, lbs, cmap, normalize):\n    sm = plt.cm.ScalarMappable(cmap=cmap, norm=normalize)\n    sm._A = [] #fake up the array of the scalar mappable. not required from verion 3.1\n    cbar = plt.colorbar(sm,ax=ax,ticks=tks,label='job create time')\n    cbar.ax.set_yticklabels(lbs)\n</code></pre>"},{"location":"Python/Web/Cherrypy/","title":"Cherrypy","text":"<p>We can leverage CherryPy as the web server and Flask as the application framework within that server. </p> <p>CherryPy as the Web Server: * CherryPy is a full-fledged web framework with its own routing and request handling mechanisms. However, it also offers the flexibility to function purely as a lightweight WSGI (Web Server Gateway Interface) compliant web server.</p> <p>Flask as the WSGI Application: * Flask, on the other hand, is a minimalistic web framework that focuses on building web applications but relies on a separate WSGI server to handle the underlying communication with web clients.</p> <p>Integration Approach: 1. Create Flask App: Develop your web application logic using Flask's routing, templating, and other functionalities. 2. WSGI Conversion:  Convert your Flask application into a WSGI callable object. Flask applications inherently follow the WSGI specification, so you typically don't need extensive modifications. 3. CherryPy Configuration: Configure CherryPy to act as the WSGI server and mount your Flask application at a specific URL path. CherryPy provides mechanisms to mount WSGI applications like Flask.</p> <p>Benefits: * Leverage CherryPy's Features: You can potentially benefit from CherryPy's built-in features like static file serving or advanced session management alongside your Flask application.</p> <p>Drawbacks: * Increased Complexity:  This approach adds a layer of complexity compared to using Flask's built-in development server for simple deployments. * Potential Conflicts:  There might be subtle conflicts between routing or configuration options between CherryPy and Flask if not managed carefully.</p> <p>Alternatives: * Consider Gunicorn: Gunicorn is a popular WSGI server specifically designed for production use with Python web frameworks like Flask. It offers a simpler and more focused solution compared to using CherryPy as a WSGI server. * Standalone Flask Server: For development or testing, you can often leverage Flask's built-in development server by running your Flask application directly. This is a quick and easy way to get started without needing a separate WSGI server.</p> <p>Choosing the Right Approach: The decision of whether to use CherryPy as a WSGI server with Flask depends on your specific needs. If your application requires features beyond Flask's core functionality and you're comfortable managing some additional complexity, then using CherryPy as the WSGI server can be an option. However, for many scenarios, using Gunicorn or Flask's built-in development server might be more straightforward and suitable.</p>"},{"location":"Python/Web/Cherrypy/#cherrypy-and-flask","title":"Cherrypy and Flask","text":"<p>CherryPy handling static files and Flask handling application </p> <p>app.py <pre><code>from flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello_world():\n    return \"Hello from Flask!\"\n</code></pre></p> <p>server.py <pre><code>import cherrypy\n\nfrom .app import app  # Assuming app.py is in the same directory\n\n# Define the static directory path\nclass HelloWorld(object):\n  @cherrypy.expose\n  def index(self):\n    return \"Hello world!\"\n\n  @cherrypy.expose\n  def goodbye(self):\n    return \"Goodbye world!\"\n\n# Optional: Additional configuration\ncherrypy.config.update({\n    'server.socket_host': '0.0.0.0',\n    'server.socket_port': 5000\n})\n\ncfg = {}\ncfg['/static'] = {\n    'tools.staticdir.dir': str(static).replace('\\\\','/'),\n    'tools.staticdir.on': True,\n}\n\n# Mount static handler\ncherrypy.tree.mount(HelloeWorld(), '/', cfg)\n# Mount Flask app at the root path (`/app`)\ncherrypy.tree.mount(app, '/api')\n\n# Start the CherryPy server\ncherrypy.engine.start()\ncherrypy.engine.block()\n</code></pre></p>"},{"location":"Python/Web/Post/","title":"Post","text":""},{"location":"Python/Web/Post/#run-post","title":"run post","text":"<pre><code>import requests\nimport json\n\nurl = 'https://task.test.com/api/my/url'\n\ndef do_post(data):\n    resp = requests.post(url, json={'id': None, 'val': 12})\n    #json to dic: dic = json.loads(resp.tex)\n    print(resp.text)\n</code></pre>"},{"location":"Python/Web/Post/#async-post","title":"async post","text":"<pre><code>import aiohttp\nimport asyncio\nimport json\n\nurl = 'https://task.test.com/api/my/url'\n\nasync def do_posts():\n    date_ranges = [\n        ['2017-01-05', '2017-04-01'],\n        ['2017-04-01', '2017-07-01'],\n    ]\n    async with aiohttp.ClientSession() as session:\n        post_tasks = []\n        async for date_range in aiter(date_ranges):\n            post_tasks.append(do_post(session, url, date_range))\n        # now execute them all at once\n        await asyncio.gather(*post_tasks)\n\nasync def do_post(session, url, date_range):\n    async with session.post(\n        url,\n        json={\n            'id': None,\n            'start_date': date_range[0],\n            'end_date': date_range[1],\n        }\n    ) as response:\n        data = await response.text()\n        d = json.loads(data)\n        print(f'-&gt; Created post task: {d[\"task_id\"]}')\n\nasync def aiter(lst):\n    for val in lst:\n        yield val\n\nasyncio.run(po_posts())\n</code></pre>"},{"location":"Python/Web/Selenium/","title":"Selenium","text":"<ul> <li>https://pypi.org/project/selenium/</li> <li>https://selenium-python.readthedocs.io/locating-elements.html</li> </ul>"},{"location":"Python/Web/Selenium/#issues","title":"issues","text":"<p>https://www.stablebuild.com/blog/install-chromium-in-an-ubuntu-docker-container - installing Chromium is no longer supported using <code>apt</code> on Ubuntu - snap does not work from within a Docker container - install from ppa (do you trudt it): https://fosspost.org/chromium-deb-package-ubuntu - not possible!</p>"},{"location":"Python/Web/Selenium/#how-to-install","title":"how to install","text":"<p>The versions of the chrome-driver and browser must match!</p> <p>discussions: - https://github.com/password123456/setup-selenium-with-chrome-driver-on-ubuntu_debian - https://stackoverflow.com/questions/22476112/using-chromedriver-with-selenium-python-ubuntu</p> <p>chromium browser download - https://packages.debian.org/sid/amd64/chromium/download</p> <p>chromium and chromedriver download (right place to download packages for 18.04) - http://ftp.ubuntu.com/ubuntu/ubuntu/pool/universe/c/chromium-browser/ - chromium-browser_112.0.5615.49-0ubuntu0.18.04.1_amd64.deb - chromium-chromedriver_112.0.5615.49-0ubuntu0.18.04.1_amd64.deb</p> <p>install them in dockerfile <pre><code>USER root\n\n# install version-pinned chromium-browser and chromedriver\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y libnss3 libnssutil3 libnspr4 &amp;&amp; \\\n    url=\"http://ftp.ubuntu.com/ubuntu/ubuntu/pool/universe/c/chromium-browser/\" &amp;&amp; \\\n    curl -L -o chromium-browser.deb ${url}chromium-browser_112.0.5615.49-0ubuntu0.18.04.1_amd64.deb &amp;&amp; \\\n    apt-get -f install -y ./chromium-browser.deb &amp;&amp; \\\n    rm -rf chromium-browser.deb &amp;&amp; \\\n    curl -L -o chromium-chromedriver.deb ${url}chromium-chromedriver_112.0.5615.49-0ubuntu0.18.04.1_amd64.deb &amp;&amp; \\\n    apt-get -f install -y ./chromium-chromedriver.deb &amp;&amp; \\\n    rm -rf chromium-chromedriver.deb\n\nUSER user\n\n# Add to path var\nENV CHROMEDRIVER_PATH=\"/usr/lib/chromium-browser/chromedriver\" \\\n    PATH=\"${CHROMEDRIVER_PATH}:${PATH}\"\n</code></pre></p>"},{"location":"Python/Web/Selenium/#use-it-in-python","title":"use it in python","text":"<pre><code>from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions\n\n# Setup Chrome driver\n# executable_path = os.environ.get('CHROMEDRIVER_PATH')\n# service = Service(executable_path=executable_path)\nservice = Service() # executable_path is not required, added to path var\nchrome_options = Options()\nchrome_options.add_argument('--no-sandbox')  # Avoid sandbox issues\nchrome_options.add_argument('--headless')    # Run in headless mode\nchrome_options.add_argument('--disable-dev-shm-usage')  # Avoid resource limits\ndriver = webdriver.Chrome(service=service, options=chrome_options)\n\n# get web content\ndriver.get(url)\nWebDriverWait(driver, timeout).until(\n    expected_conditions.presence_of_element_located((By.ID, 'password'))\n)\ndriver.find_element('id', 'username').send_keys(config['username'])\ndriver.find_element('id', 'password').send_keys(config['password'])\ndriver.find_element('id', 'submit_row').submit()\n</code></pre>"},{"location":"Python/Web/Selenium/#disable-error-sending-stats-to-plausible","title":"disable <code>Error sending stats to Plausible</code>","text":"<pre><code>WARNING\nselenium.webdriver.common.selenium_manager\nError sending stats to Plausible: error sending request for url (https://plausible.io/api/event)\n</code></pre> <p>Solution:  - disable Selenium from sending anonymous usage statistics - https://www.selenium.dev/documentation/selenium_manager/#data-collection - set the <code>SE_AVOID_STATS</code> environment variable to <code>true</code>, or -  set <code>avoid-stats = true</code> in config file</p>"},{"location":"Python/Web/Selenium/#webdriverexception-message-service-chromedriver-unexpectedly-exited-status-code-was-127","title":"WebDriverException: Message: Service chromedriver unexpectedly exited. Status code was: 127","text":"<p>https://stackoverflow.com/questions/49323099/webdriverexception-message-service-chromedriver-unexpectedly-exited-status-co - Reason: some dependencies are missing.  - Solution: check and install missing depencdencies</p>"},{"location":"Python/Web/Selenium/#check-chromedriver-dependency-issue","title":"check <code>chromedriver</code> dependency issue","text":"<p>https://stackoverflow.com/questions/49323099/webdriverexception-message-service-chromedriver-unexpectedly-exited-status-co <pre><code>user@xyz:~$ /path/to/chromedriver --version\n/path/to/chromedriver: error while loading shared libraries: libnss3.so: cannot open shared object file: No such file or directory\n#will show all depedencies\nuser@xyz:~$ ldd /path/to/chromedriver\n        linux-vdso.so.1 (0x00007ffdd63e1000)\n        libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fe8b09af000)\n        libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fe8b09aa000)\n        libglib-2.0.so.0 =&gt; /lib/x86_64-linux-gnu/libglib-2.0.so.0 (0x00007fe8b0870000)\n        libnss3.so =&gt; not found\n        libnssutil3.so =&gt; not found\n        libnspr4.so =&gt; not found\n        libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007fe8b0787000)\n        libxcb.so.1 =&gt; /lib/x86_64-linux-gnu/libxcb.so.1 (0x00007fe8b075d000)\n        libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fe8b073d000)\n        libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fe8b0514000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007fe8b1a88000)\n        libpcre.so.3 =&gt; /lib/x86_64-linux-gnu/libpcre.so.3 (0x00007fe8b049e000)\n        libXau.so.6 =&gt; /lib/x86_64-linux-gnu/libXau.so.6 (0x00007fe8b0496000)\n        libXdmcp.so.6 =&gt; /lib/x86_64-linux-gnu/libXdmcp.so.6 (0x00007fe8b048e000)\n        libbsd.so.0 =&gt; /lib/x86_64-linux-gnu/libbsd.so.0 (0x00007fe8b0476000)\n        libmd.so.0 =&gt; /lib/x86_64-linux-gnu/libmd.so.0 (0x00007fe8b0469000)\n</code></pre></p>"},{"location":"Python/Web/Selenium/#screenshot-available-via-screen","title":"Screenshot: available via screen","text":"<pre><code>driver = webdriver.PhantomJS()\ndriver.set_window_size(1920,1080)\ntry:\n    driver.get('http://example.com/')\nexcept Exception as exc:\n    driver.save_screenshot('screenshot.png')\ndriver.close()\n</code></pre>"},{"location":"Python/Web/Selenium/#get-form","title":"Get form","text":"<pre><code>WebDriverWait(driver, timeout).until(\n    EC.presence_of_element_located((By.ID, 'username'))\n)\nform = driver.find_element_by_tag_name('form')\nform.find_element_by_id('username').send_keys('usr')\nform.find_element_by_id('password').send_keys('pwd')\nform.submit()\n</code></pre>"},{"location":"Python/Web/Selenium/#list-all-elments-ids-slow-but-works","title":"List all elments Ids - slow but works","text":"<pre><code>from selenium import webdriver\n\ndriver = webdriver.Firefox()\ndriver.get('http://example.com')\n\nelems = driver.find_elements_by_xpath('//*[@id]')\nfor elem in elems:\n    print(elem.get_attribute('id'))\n</code></pre>"},{"location":"Python/Web/Selenium/#input-usernamepassword-and-submit","title":"Input username/password and submit","text":"<pre><code>driver.find_element_by_name(\"login\").send_keys(\"usr\")\ntime.sleep(0.2)\ndriver.find_element_by_name(\"passwd\").send_keys(\"pwd\")\ntime.sleep(0.4)\ndriver.find_element_by_class_name(\"signinbtn\").click()\n</code></pre>"},{"location":"Python/Web/Selenium/#ubuntu-2204","title":"ubuntu 22.04","text":"<p>how-to-install-chromium-without-snap: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap - For 22.04, use the package from Debian bullseye instead of buster - see metrizable's answer: https://github.com/googlecolab/colabtools/issues/3347</p>"},{"location":"Python/Web/Web/","title":"Web","text":"<p>https://towardsdatascience.com/data-science-skills-web-scraping-javascript-using-python-97a29738353f</p> <p>https://duo.com/decipher/driving-headless-chrome-with-python</p> <p>https://www.datacamp.com/community/tutorials/learn-build-dash-python Dash is Python framework for building web applications. It built on top of Flask, Plotly.js, React and React Js. It enables you to build dashboards using pure Python. Dash is open source, and its apps run on the web browser.</p>"},{"location":"Python/Web/Web/#requests-pool","title":"requests pool","text":"<pre><code>import requests\nfrom threading import Lock\n\nrequestssessions = []\nthreadlock = Lock()\n\nwith threadlock:\n    if len(requestssessions) == 0:\n        s = requests.Session()\n    else:\n        s = requestssessions.pop()\ntry:\n    resp = s.get(url)\n    resp.raise_for_status() #raise HTTP errors\n    data = resp.json()\nexcept requests.exceptions.HTTPError as e:\n    log('Error: ' + str(e)) #it wasn't a 200 error\n    data = None\nwith threadlock:\n    requestssessions.append(s)\n</code></pre>"},{"location":"Python/Web/Web/#requests_html","title":"requests_html","text":"<p>requests_html serves as an alternative to Selenium and PhantomJS, and provides a clear syntax similar to the awesome requests package\\ http://theautomatic.net/2019/01/19/scraping-data-from-javascript-webpage-python</p> <pre><code>from requests_html import HTMLSession\nsession = HTMLSession()\nresp = session.get(url)\nresp.html.absolute_links #get absolute links\nresp.html.render() #run JavaScript code on webpage\nsoup = bsoup(resp.html.html, 'lxml')\n</code></pre>"},{"location":"Python/Web/Web/#soup-example","title":"soup example","text":"<pre><code>htmltext = get_url_text(url)\nsoup = bsoup(htmltext, 'lxml')\ndivs = soup.find_all('div', class_='my-dataset')\nfor div in divs:\n    tbls = div.find_all('table')\n    if len(tbls) == 0:\n        reg = div.find('a').get_text().lower()\n    else:\n        for tbl in tbls:\n            tb = [[td.get_text() for td in tr.find_all('td')] for tr in tbl.find_all('tr')]\n            df = pd.DataFrame(columns=tb[0],data=tb[1:])\n</code></pre>"},{"location":"Python/Web/Web/#get-with-header","title":"get with header","text":"<pre><code>import os\nimport sys\nimport json\nfrom requests_html import HTMLSession\nfrom bs4 import BeautifulSoup as bsoup\n\nstr_url = 'https://www.vv'\n\nprint(f'\\n{str_url}')\n\n#*****worked similar to php file_get_contents\n# get json\nimport urllib.request\nhdr = {'X-Requested-With': 'XMLHttpRequest'}\nreq = urllib.request.Request(str_url, headers=hdr)\nresp = urllib.request.urlopen(req)\nhtmltext = resp.read()\ndata = json.loads(htmltext.decode('utf-8'))\nprint(data[0])\n#****************\n\n#****will only get html after js run\nsession = HTMLSession()\nresp = session.get(str_url, headers=hdr)\nresp.html.render() #run JavaScript code on webpage\nprint(resp.html.html)\n#************\n\nsoup = bsoup(resp.html.html, 'lxml')\ntables = soup.find_all('table')\n\nfor tbl in tables:\n    tbl_body = tbl.find('tbody')\n    rows = tbl_body.find_all('tr')\n    for row in rows:\n        cols = row.find_all('td')\n        vals = [ele.text.strip() for ele in cols]\n        print(vals)\n</code></pre>"},{"location":"Python/Web/Web/#wbdriver","title":"wbdriver","text":"<pre><code>from selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.support.ui import WebDriverWait\n\nversion = 'old'\n\nif version == 'old':\n    #create driver\n    chrome_options = Options()\n    chrome_options.add_argument('--headless')\n    #chrome_options.add_argument('--log-level=3')\n    chrome_options.add_argument('accept=\"application/json\"')\n    chrome_options.add_argument('user-agent=\"Chrome\"')\n    chrome_options.add_argument('x-requested-with=XMLHttpRequest')\n\n    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])\n    #chrome_options.binary_location = r'C:\\Users\\usr\\AppData\\Local\\Google\\Chrome\\Application'\n    chrome_driver = r'C:\\chromedriver.exe'\n    driver = webdriver.Chrome(executable_path=chrome_driver, options=chrome_options)\n\n    urls = ['https://www.vvv', 'https://www.ccc']\n\n    for str_url in urls:\n        print(f'\\n{str_url}')\n        driver.get(str_url)\n        #driver.Navigate().GoToUrl(str_url);\n        print(driver.page_source)\n        sys.exit('finished test')\n\n        #soup = bsoup(driver.page_source,'lxml')\n        #tables = soup.find_all('table')\n        #table_body = tables[0].find('tbody')\n\n        #rows = table_body.find_all('tr')\n        #for row in rows:\n        #    cols = row.find_all('td')\n        #    vals = [ele.text.strip() for ele in cols]\n        #    print(vals)\n\n    #---\n    #trs = driver.find_elements_by_xpath('//div[@id=\"app\"]//table//tr')\n    #for tr in trs:\n    #    tds = tr.find_elements_by_tag_name('td')\n    #    print ([td.text for td in tds])\n    #elem = driver.find_element_by_id('app')\n    #-----\n\n    driver.quit()\n\nelif version == 'new':\n\n    #create driver\n    chrome_options = Options()\n    chrome_options.add_argument('--headless')\n    #chrome_options.add_argument('--log-level=3')\n\n    chrome_options.add_argument('accept=\"application/json, text/javascript, */*; q=0.01\"')\n    chrome_options.add_argument('user-agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36\"')\n\n    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])\n    #chrome_options.binary_location = r'C:\\Users\\usr\\AppData\\Local\\Google\\Chrome\\Application'\n\n    chrome_driver = r'C:\\chromedriver.exe'\n    #driver = webdriver.Chrome(executable_path=chrome_driver, chrome_options=chrome_options)\n\n    service = webdriver.chrome.service.Service(chrome_driver)\n    service.start()\n\n    urls = ['https://www.asxenergy.com.au/options/au-electricity/HNZ','https://www.asxenergy.com.au/futures_au']\n\n    for i, str_url in enumerate(urls):\n        print(f'\\n{str_url}')\n        driver = webdriver.Remote(service.service_url, desired_capabilities=chrome_options.to_capabilities())\n        driver.get(str_url)\n        #driver.Navigate().GoToUrl(str_url);\n\n        #driver.find_element_by_tag_name('body').send_keys(Keys.CONTROL + 'u')\n        print(driver.page_source)\n\n        #wait = WebDriverWait(driver, 10)\n        #elem =driver.find_element_by_css_selector('#my-id')\n\n        xx = driver.execute_script('https://www.asxenergy.com.au/js/options.js?:711')\n\n        #elem = driver.find_element_by_xpath(\"//body\")\n        #elem.send_keys(Keys.CONTROL, 'U')\n\n        print('\\nxxxxxxxxxxxxxxxxx\\n\\n\\n'+ xx)\n\n        soup = bsoup(driver.page_source,'lxml')\n        driver.quit()\n\n        tables = soup.find_all('table')\n        table_body = tables[i].find('tbody')\n\n        rows = table_body.find_all('tr')\n        for row in rows:\n            cols = row.find_all('td')\n            vals = [ele.text.strip() for ele in cols]\n            print(vals)\n</code></pre>"},{"location":"Python/Web/bs4/","title":"bs4","text":""},{"location":"Python/Web/bs4/#find-href-in-tag-a-with-text-test","title":"find href in tag <code>a</code> with text <code>test</code>","text":"<p><code>a class=\"page-link Pagination__pagelink--3V5Te\" href=\"/property/qld/north-lakes-4509?bedsMin=5&amp;amp;soldHistory=false&amp;amp;rentedHistory=true&amp;amp;types=House&amp;amp;sort=dateSoldNewest&amp;amp;page=1\"&gt;\u00ab&lt;/a&gt;</code> <pre><code>from bs4 import BeautifulSoup\nfrom selenium import webdriver\n\n# Open Firefox browser and go to website using Selenium\nbrowser = webdriver.Firefox()\nbrowser.get(test_url)\n\n# Get webpage\nsoup = BeautifulSoup(browser.page_source, 'html.parser')\n\n# Find href of tag with test `test`\ntags = soup.find_all('a')\nfor tag in tags:\n    txt = tag.string\n    if txt is not None and txt.startswith('test'):\n        print(tag['href'])\n</code></pre></p>"},{"location":"Python/Web/bs4/#find-href-in-tag-a-with-class-xyz","title":"find href in tag <code>a</code> with class <code>xyz</code>","text":"<p><code>a class=\"xyz\" href=\"/x/y/z;page=1\"&gt;1&lt;/a&gt;</code> <pre><code>tags = soup.find_all('a', class_='xyz')\n</code></pre></p>"},{"location":"Python/Web/bs4/#find-div-after-another-div","title":"find div after another div","text":"<p>'''html</p> 780 01 Nov 2022 <p>'''</p> <pre><code>soup = BeautifulSoup(html, 'html.parser')\n\n# Find the value div\nvalue_div = soup.find('div', class_='History--xyz')\nvalue = value_div.text.strip() #780\n\n# Find the next sibling div element after the value_div\ndate_div = value_div.find_next_sibling('div', class_='text-secondary')\ndate = date_div.text.strip() #01 Nov 2022\n</code></pre>"},{"location":"SQL/Database/","title":"Database","text":"<ul> <li>backup</li> <li>scalability</li> <li>security: firewall, encrypted databases, dynamic data masking</li> </ul>"},{"location":"SQL/Database/#types","title":"types","text":"<p>https://db-engines.com/en/ranking/time+series+dbms - Relational DBMS: Oracle, MySQL, SQLServer, PostgreSQL - Key-value stores: Redis, Amazon DynamoDB, Azure CosmosDB, Memcached - Document stores: MongoDB, Amazon DynamoDB, Databricks - Time Series DBMS: InfluxDB, IoTDB, Prometheus - Graph DBMS: Neo4j, Azure CosmosDB - Search engines: ElasticSearch, OpenSearch - Object oriented DBMS - RDF stores - Wide column stores - Multivalue DBMS - Native XML DBMS - Spatial DBMS - Event Stores - Content stores - Navigational DBMS</p>"},{"location":"SQL/Database/#database-performance","title":"database performance","text":"<p>avoid bad queries/schema design</p>"},{"location":"SQL/Test/","title":"Test","text":""},{"location":"SQL/Test/#test-connection-query","title":"test connection query","text":"<p>https://stackoverflow.com/questions/3668506/efficient-sql-test-query-or-validation-query-that-will-work-across-all-or-most <pre><code>select 1           # mysql, sqlserver, postgresql, sqlite \nselect 1 from dual # oracle\n</code></pre></p>"},{"location":"SQL/Test/#using-sqlalchemy","title":"using sqlalchemy","text":"<pre><code>from sqlalchemy import create_engine, text\n\nconn_str = '***'\nengine = create_engine(conn_str)\nwith engine.connect() as conn:\n    rows = conn.execute(text('select 1')).fetchall()\n    for row in rows:\n        print(row)\n</code></pre>"},{"location":"SQL/Access/cs/","title":"Basic","text":""},{"location":"SQL/Access/cs/#number-of-record-count-in-query","title":"Number of record count in query","text":"<pre><code>Cnt = DCount(\"*\", \"Qry_xxx\")\n</code></pre> <p>After call 'Set rs = CurrentDb.OpenRecordset(strSQL)', we must first check if the records are empty by rs.EOF\\ if not, to get the number of records, we need to call 'rs.MoveLast' before use cnt = rs.RecordCount</p>"},{"location":"SQL/Access/cs/#introduction-to-data-types-and-field-properties","title":"Introduction to data types and field properties","text":"<p>https://support.office.com/en-us/article/introduction-to-data-types-and-field-properties-30ad644f-946c-442e-8bd2-be067361987c</p>"},{"location":"SQL/Access/cs/#update-t1-based-on-t2","title":"update t1 based on t2","text":"<pre><code>UPDATE t1\nINNER JOIN t2 ON t1.id = t2.id\nSET t1.Unit = t2.Unit, t1.Region = t2.Region\nWHERE t1.Station = 'AGL'\n</code></pre>"},{"location":"SQL/Access/custom/","title":"Custom","text":""},{"location":"SQL/Access/custom/#custom","title":"custom","text":"<p>Create and manage custom categories and groups in the Navigation Pane</p>"},{"location":"SQL/Access/custom/#create-custom-category","title":"Create custom category","text":"<ul> <li>Right-click the menu at the top of the Navigation Pane and then click Navigation Options</li> <li>In the Navigation Options dialog box, under the Categories list, click Add Item</li> <li>Type a name for the new category and then press ENTER</li> </ul>"},{"location":"SQL/Access/custom/#create-group","title":"Create group","text":"<ul> <li>Open Navigation Options and under the Groups for \"Group Name\" list,</li> <li>click Add Group and then type a name for the group</li> </ul>"},{"location":"SQL/Access/error/","title":"Error","text":""},{"location":"SQL/Access/error/#add-a-new-line-in-form-label","title":"Add a new line in form label","text":"<p>In a label caption, use the Shift+ Enter keys.</p>"},{"location":"SQL/Access/error/#operation-must-use-an-updateable-query","title":"Operation must use an updateable query","text":"<p>This happens when update and join are used together.\\ Solution: Add DISTINCTROW after UPDATE, example\\ <pre><code>UPDATE DISTINCTROW g \nINNER JOIN v ON v.name = g.name AND v.id = g.id\nSET gs.val = v.val\n; \n</code></pre></p>"},{"location":"SQL/Access/error/#cannot-open-database-it-may-not-be-a-database-or-the-file-may-be-corrupt","title":"Cannot Open Database\". It may not be a database ... or the file may be corrupt","text":"<p>reached the limit: 2 gigabytes, minus the space needed for system objects.</p>"},{"location":"SQL/Access/error/#error-2501-the-runsql-action-was-canceled","title":"error: 2501-The RunSQL action was canceled","text":"<p>The error is due to the Application.SecurityAutomation that is not set to \"low\" on forehand.</p> <p>If you never accessed the file manually then it is not a trusted document for you yet and the run time error appears.</p> <p>Hence, 2 alternatives: 1. You manually open it, enable content and make it a trusted document and then do your thing in VBA. 2. Or: before you start the query, insert: <pre><code>app.AutomationSecurity = msoAutomationSecurityLow \napp.DoCmd.RunSQL \"blahblah\"\napp.AutomationSecurity = msoAutomationSecurityByUI\n</code></pre></p>"},{"location":"SQL/Access/error/#subform-object-is-empty","title":"Subform object is empty","text":"<p>The name in the VBA code must be the name of control [the yellow rectangle], e.g. xxx_subform.Requery</p>"},{"location":"SQL/Access/error/#empty-query-records","title":"Empty query records","text":"<p>when the query result is empty the control's form is not accessible, using 'subform.Form' will get a crash. in this case, we need to directly check the number of records in the query result by using 'n = DCount(\"*\", \"Qry_duplicate_bus_gen\")'</p> <p>when incorrectly set the data source for the main form and the data source record is empty, the form does not show, the combobox in the form does not update, and the control's form [subform.Form] could not accessed.</p>"},{"location":"SQL/Access/error/#building-sql-queries-with-datetime-criteria-in-vba","title":"Building SQL Queries with Date/Time Criteria in VBA","text":"<p>https://codekabinett.com/rdumps.php?Lang=2&amp;targetDoc=date-time-data-type-vba-access</p> <p>Date values need to be properly formatted for embedding in the SQL-String of a query in VBA.</p> <p>The Jet-/Ace-Database-Engine, however, is stricter about dates. It will only recognize date-strings in either the US-Date-Format (MM/DD/YYYY) or the international ISO-Format (YYYY-MM-DD).</p> <p>To deal with this issue, we have to explicitly format any date value in the international ISO-Date-Format by using the Format-Function for your date: <pre><code>sql = \"SELECT * FROM tbl \" &amp; _\n      \"WHERE DateValue &gt;= #\" &amp; Format(aDateVariable, \"yyyy-mm-dd\") &amp; \"# \" &amp; _\n      \"  AND DateValue &lt; #\" &amp; Format(anotherDateVariable, \"yyyy-mm-dd hh:nn:ss\") &amp; \"#;\"\n</code></pre></p>"},{"location":"SQL/Access/error/#daodbengine120","title":"dao.dbengine.120","text":"<p>Python error: pywintypes.com_error: \"Operation unavailable\", \"Class not registered\"</p> <p>the problem (and issue) is that installing office 2016 does not install an \u201cexposed\u201d version of the .120 engine.</p> <p>The simple solution here is to install the data connectivity components for office, or a version of the Access runtime.</p> <p>Install: Microsoft Access Database Engine 2010 Redistributable\\ https://www.microsoft.com/en-ca/download/details.aspx?id=13255</p>"},{"location":"SQL/Access/error/#error-3033-you-do-not-have-the-necessary-permissions-to-use","title":"Error 3033: You do not have the necessary permissions to use \u2026","text":"<p>To overcome this slight problem, you need to create a shortcut and link the MDW (Microsoft Access Workgroup information file) with the MDB.</p> <p>\"C:\\Program Files\\Microsoft Office\\Office16\\MSACCESS.EXE\" \"c:\\test.mdb\" /wrkgrp \"C:\\Users\\sma\\AppData\\Roaming\\Microsoft\\Access\\System.mdw\" /user</p> <p>Solution: incorrectly tried to export Msys... table.  </p>"},{"location":"SQL/Access/error/#cannot-delete-from-specified-tables","title":"cannot delete from specified tables","text":"<p>Delete cannot combine with select statement; join with distinctrow is OK!</p>"},{"location":"SQL/Access/export/","title":"ExportObjs","text":"<pre><code>Sub ClearErrs()\n    mdb = \"C:\\acc.mdb\"\n\n    qry1 = \"update tbl set col=0 where col2 = 'xyz';\"\n    qry2 = \"delete * from tbl where col = 'xyz';\"\n\n    Dim db As Access.Application\n    Set db = GetObject(mdb)\n    db.DoCmd.RunSQL qry1\n    db.DoCmd.RunSQL qry2\n\n    MsgBox \"All Done!\"\nEnd Sub\n\nPublic Sub MainFn()\n    Dim dbTo As String\n    dbTo = \"C:\\acc.mdb\"\n\n    'rename objects\n    RnmObjs dbTo    \n\n    'delete objects\n    DelObjs dbTo\n\n    'export objects\n    ExpObjs dbTo\n\n    MsgBox \"All Done!\"\nEnd Sub\n\nSub RnmObjs(mdb As String)\n    On Error GoTo Error_Handler\n    Dim db As Access.Application\n    Set db = GetObject(mdb)\n\n    'tables\n    RenameObject db, \"tbl\", \"nam_old\", \"nam_new\"\n\nError_Exit:\n    On Error Resume Next\n    db.Application.Quit\n    Set db = Nothing\n    Exit Sub\n\nError_Handler:\n    MsgBox \"Error \" &amp; Err.Number &amp; vbCrLf &amp; Err.Description, vbCritical, \"Error in RenameObjects!\"\n    Resume Error_Exit\nEnd Sub\n\nSub DelObjs(mdb As String)\n    On Error GoTo Error_Handler\n    Dim db As Access.Application\n    Set db = GetObject(mdb)\n\n    'tables\n    DeleteObject db, \"tbl\", \"tbl_x\"\n\nError_Exit:\n    On Error Resume Next\n    db.Application.Quit\n    Set db = Nothing\n    Exit Sub\n\nError_Handler:\n    MsgBox \"Error \" &amp; Err.Number &amp; vbCrLf &amp; Err.Description, vbCritical, \"Error in DeleteObjects!\"\n    Resume Error_Exit\nEnd Sub\n\nSub ExpObjs(dbTo As String)\n    On Error GoTo Error_Handler\n\n    'tables\n    ExportObject dbTo, \"tbl\", \"tbl_x\", True\n\n    'queries\n    ExportObject dbTo, \"qry\", \"insert_qry\"\n\n    'forms\n    ExportObject dbTo, \"frm\", \"x_subform\"\n\nError_Exit:\n    On Error Resume Next\n    Exit Sub\n\nError_Handler:\n    MsgBox \"Error \" &amp; Err.Number &amp; vbCrLf &amp; Err.Description, vbCritical, \"Error in ExportObjects!\"\n    Resume Error_Exit\nEnd Sub\n\nSub RenameObject(db As Access.Application, typ As String, nam_old As String, nam_new As String)\n    db.DoCmd.Rename nam_old, ObjTyp(typ), nam_new\nEnd Sub\n\nSub DeleteObject(db As Access.Application, typ As String, nam As String)\n    db.DoCmd.DeleteObject ObjTyp(typ), nam\nEnd Sub\n\nSub ExportObject(db As String, typ As String, nam As String, Optional strucOnly As Boolean = False)\n    DoCmd.TransferDatabase acExport, \"Microsoft Access\", db, ObjTyp(typ), nam, nam, strucOnly\nEnd Sub\n\nFunction ObjTyp(typ As String) As AcObjectType\n    Select Case typ\n        Case \"frm\"\n            ObjTyp = acForm\n        Case \"mco\"\n            ObjTyp = acMacro\n        Case \"mod\"\n            ObjTyp = acModule\n        Case \"qry\"\n            ObjTyp = acQuery\n        Case \"rpt\"\n            ObjTyp = acReport\n        Case \"tbl\"\n            ObjTyp = acTable\n        Case Else\n            ObjTyp = acDefault\n    End Select\nEnd Function\n\nPublic Sub ExportObjects(db As String)\n    On Error GoTo Error_Handler\n\n     ' Tables\n    For Each tdf In CurrentDb.TableDefs\n        If Left(tdf.Name, 4) = \"MSys\" Then    'Ignore/Skip system tables\n            ExportObject \"tbl\", obj.Name, db\n        End If\n    Next tdf\n\n    ' Queries\n    For Each qdf In CurrentDb.QueryDefs\n        If Left(qdf.Name, 1) = \"~\" Then     'Ignore/Skip system generated queries\n            ExportObject \"qry\", obj.Name, db\n        End If\n    Next qdf\n\n    ' Forms etc\n    For Each obj In CurrentProject.AllForms, AllReports, AllMacros, AllModules\n        ExportObject \"frm\", obj.Name, db\n    Next obj\n\nError_Handler_Exit:\n    On Error Resume Next\n    Set qdf = Nothing\n    Set tdf = Nothing\n    Set obj = Nothing\n    Exit Sub\n\nError_Handler:\n    MsgBox \"Error Number: \" &amp; Err.Number &amp; vbCrLf &amp; _\n           \"Error Description: \" &amp; Err.Description, _\n           vbCritical, \"Error in Export Objects!\"\n    Resume Error_Handler_Exit\nEnd Sub\n</code></pre>"},{"location":"SQL/Access/io/","title":"io","text":""},{"location":"SQL/Access/io/#importexportspecification","title":"ImportExportSpecification","text":"<p>without specify the 'Width', the fields will be alphabetically ordered.</p> <p>if there are no indexed fields, PrimaryKey should be 'Auto', otherwise it should be 'key1 key2 ...'</p> <p>note that 'AppendToTable' rather than 'Destination' should be used. <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&gt;\n&lt;ImportExportSpecification Path=\"C:\\Users\\Public\\zzz.csv\" xmlns=\"urn:www.microsoft.com/office/access/imexspec\"&gt;\n    &lt;ImportText TextFormat=\"Delimited\" FirstRowHasNames=\"false\" FieldDelimiter=\",\" TextDelimiter=\"\" CodePage=\"437\" AppendToTable=\"MyNewTable\"&gt;\n        &lt;DateFormat DateOrder=\"YMD\" DateDelimiter=\"-\" TimeDelimiter=\":\" FourYearDates=\"true\" DatesLeadingZeros=\"false\"/&gt;\n        &lt;NumberFormat DecimalSymbol=\".\"/&gt;\n        &lt;Columns PrimaryKey=\"{Auto}\"&gt;\n            &lt;Column Name=\"Col1\" FieldName=\"id\" Indexed=\"YESDUPLICATES\" SkipColumn=\"false\" DataType=\"Long\" Width=\"2\"/&gt;\n            &lt;Column Name=\"Col2\" FieldName=\"textfield\" Indexed=\"NO\" SkipColumn=\"false\" DataType=\"YesNo\" Width=\"1\"/&gt;\n        &lt;/Columns&gt;\n    &lt;/ImportText&gt;\n&lt;/ImportExportSpecification&gt;\n</code></pre></p>"},{"location":"SQL/Access/io/#load-csv-file-to-db-table","title":"load csv file to db table","text":""},{"location":"SQL/Access/migrate/","title":"migrate","text":"<p>http://code.activestate.com/recipes/52267-reverse-engineer-ms-accessjet-databases/ <pre><code># jet2sql.py - M.Keranen &lt;mksql@yahoo.com&gt; [07/12/2000]\n# --------------------------------------------------------------------\n# Creates ANSI SQL DDL from a MS Jet database file, useful for reverse\n# engineering database designs in E/R tools.\n#\n# Requires DAO 3.6 library.\n# --------------------------------------------------------------------\n# Usage: python jet2sql.py infile.MDB outfile.SQL\n\nimport sys, string, pythoncom, win32com.client\n\nconst = win32com.client.constants\ndaoEngine = win32com.client.Dispatch('DAO.DBEngine.36')\n\nclass jetReverse:\n    def __init__ (self, infile):\n\n        self.jetfilename=infile\n        self.dtbs = daoEngine.OpenDatabase(infile)\n\n        return\n\n    def terminate(self):\n        return\n\n    def writeTable(self, currTabl):\n        self.writeLine('\\ncreate table ' + chr(34) + currTabl.Name + chr(34),\"\",1)\n        self.writeLine('(',\"\",1)\n\n        # Write Columns\n        cn=0\n        for col in currTabl.Fields:\n            cn = cn +1\n            self.writeColumn(col.Name, col.Type, col.Size, col.Required, col.Attributes, col.DefaultValue, col.ValidationRule, currTabl.Fields.Count-cn)\n\n        # Validation Rule\n        tablRule = currTabl.ValidationRule\n        if tablRule &lt;&gt; \"\":\n            tablRule = \"    check(\" + tablRule + \") \"\n            self.writeLine(\"\",\",\",1) # add a comma and CR previous line\n            self.writeLine(tablRule,\"\",0)\n\n        # Primary Key\n        pk=self.getPrimaryKey(currTabl)\n        if pk &lt;&gt; \"\":\n            self.writeLine(\"\",\",\",1) # add a comma and CR previous line\n            self.writeLine(pk,\"\",0)\n\n        # End of table\n        self.writeLine(\"\",\"\",1) # terminate previous line\n        self.writeLine(');',\"\",1)\n\n        # Write table comment\n        try: sql = currTabl.Properties(\"Description\").Value\n        except pythoncom.com_error: sql=\"\"\n        if sql &lt;&gt; \"\":\n           sql = \"comment on table \" + chr(34) + currTabl.Name + chr(34) + \" is \" + chr(34) + sql + chr(34) +\";\"\n           self.writeLine(sql,\"\",1)\n\n        # Write column comments\n        for col in currTabl.Fields:\n            try: sql = col.Properties(\"Description\").Value\n            except pythoncom.com_error: sql=\"\"\n            if sql &lt;&gt; \"\":\n               sql = \"comment on column \" + chr(34) + currTabl.Name + chr(34) + \".\" + chr(34) + col.Name + chr(34) + \" is \" + chr(34) + sql + chr(34) + \";\"\n               self.writeLine(sql,\"\",1)\n\n        # Write Indexes\n        self.writeIndexes(currTabl)\n\n        return\n\n    def writeColumn(self, colName, colType, length, requird, attributes, default, check, colRix):\n        # colRix: 0 based index of column from right side. 0 indicates rightmost column\n\n        if colType == const.dbByte: dataType = \"Byte\"\n        elif colType == const.dbInteger: dataType = \"Integer\"\n        elif colType == const.dbSingle: dataType = \"Single\"\n        elif colType == const.dbDouble: dataType = \"Double\"\n        elif colType == const.dbDate: dataType = \"DateTime\"\n        elif colType == const.dbLongBinary: dataType = \"OLE\"\n        elif colType == const.dbMemo: dataType = \"Memo\"\n        elif colType == const.dbCurrency: dataType = \"Currency\"\n        elif colType == const.dbLong:\n            if  (attributes &amp; const.dbAutoIncrField): dataType = \"Counter\"\n            else: dataType = \"LongInteger\"\n        elif colType == const.dbText:\n            if length == 0: dataType = \"Text\"\n            else: dataType = \"char(\"+str(length)+\")\"\n        elif colType == const.dbBoolean:\n            dataType = \"Bit\"\n            if default == \"Yes\": default = \"1\"\n            else: default = \"0\"\n        else:\n            if length == 0: dataType = \"Text\"\n            else: dataType = \"Text(\"+str(length)+\")\"\n\n        if default &lt;&gt; \"\":\n            defaultStr = \"default \" + default + \" \"\n        else: defaultStr = \"\"\n\n        if check &lt;&gt; \"\":\n            checkStr = \"check(\" + check + \") \"\n        else:\n            checkStr = \"\"\n\n        if requird or (attributes &amp; const.dbAutoIncrField):\n            mandatory = \"not null \"\n        else:\n            mandatory = \"\"\n\n        sql = \"    \" + chr(34) + colName + chr(34) + \" \" + dataType + \" \" + defaultStr + checkStr + mandatory\n        if colRix &gt; 0:\n            self.writeLine(sql,\",\",1)\n        else:\n            self.writeLine(sql,\"\",0)\n\n        return\n\n    def getPrimaryKey(self, currTabl):\n\n        # Get primary key fields\n        sql = \"\"\n        for idx in currTabl.Indexes:\n           if idx.Primary:\n              idxName = idx.Name\n              sql = \"    primary key \"\n              cn=0\n              for col in idx.Fields:\n                  cn=cn+1\n                  sql = sql + chr(34) + col.Name + chr(34)\n                  if idx.Fields.Count &gt; cn : sql = sql + \",\"\n        return sql\n\n    def writeIndexes(self, currTabl):\n\n        # Write index definition\n        nIdx = -1\n        for idx in currTabl.Indexes:\n            nIdx = nIdx + 1\n            idxName = idx.Name\n            tablName = currTabl.Name\n            if idx.Primary:\n                idxName = tablName + \"_PK\"\n            elif idxName[:9] == \"REFERENCE\":\n               idxName = tablName + \"_FK\" + idxName[10:]\n            else:\n                idxName = tablName + \"_IX\" + str(nIdx)\n\n            sql = \"create \"\n            if idx.Unique: sql = sql + \"unique \"\n            if idx.Clustered: sql = sql + \"clustered \"\n            sql = sql + \"index \" + chr(34) + idxName + chr(34)\n            sql = sql + \" on \" + chr(34) + tablName + chr(34) + \" (\"\n\n            # Write Index Columns\n            cn=0\n            for col in idx.Fields:\n                cn = cn + 1\n                sql = sql + chr(34) + col.Name + chr(34)\n                if col.Attributes &amp; const.dbDescending:\n                    sql = sql + \" desc\"\n                else:\n                    sql = sql + \" asc\"\n                if idx.Fields.Count &gt; cn: sql = sql + \",\"\n\n            sql=sql + \" );\"\n\n            self.writeLine(sql,\"\",1)\n        return\n\n    def writeForeignKey(self, currRefr):\n\n        # Export foreign key\n        sql = \"\\nalter table \" + chr(34) + currRefr.ForeignTable + chr(34)\n        self.writeLine(sql,\"\",1)\n\n        sql = \"    add foreign key (\"\n        cn = 0\n        for col in currRefr.Fields:\n            cn = cn + 1\n            sql = sql + chr(34) + col.ForeignName + chr(34)\n            if currRefr.Fields.Count &gt; cn: sql = sql + \",\"\n\n        sql = sql + \")\"\n        self.writeLine(sql,\"\",1)\n\n        sql = \"    references \" + chr(34) + currRefr.Table + chr(34) + \" (\"\n        cn = 0\n        for col in currRefr.Fields:\n            cn = cn + 1\n            sql = sql + chr(34) + col.Name + chr(34)\n            if currRefr.Fields.Count &gt; cn: sql = sql + \",\"\n\n        sql = sql + \")\"\n        if (currRefr.Attributes &amp; const.dbRelationUpdateCascade) &lt;&gt; 0:\n           sql = sql + \" on update cascade\"\n        if (currRefr.Attributes &amp; const.dbRelationDeleteCascade) &lt;&gt; 0:\n           sql = sql + \" on delete cascade\"\n        sql=sql+\";\"\n        self.writeLine(sql,\"\",1)\n\n        return\n\n    def writeQuery(self, currQry):\n\n        sql = \"\\ncreate view \" + chr(34) + currQry.Name + chr(34) + \" as\"\n        self.writeLine(sql,\"\",1)\n\n        # Write Query text\n        sql=string.replace(currQry.SQL,chr(13),\"\") # Get rid of extra linefeeds\n        self.writeLine(sql,\"\",1)\n\n        # Write Query comment\n        try: sql = currQry.Properties(\"Description\").Value\n        except pythoncom.com_error: sql=\"\"\n        if sql &lt;&gt; \"\":\n            sql =  \"comment on table \" + chr(34) + currQry.Name + chr(34) + \" is \" + chr(34) + sql + chr(34)\n            self.writeLine(sql,\"\",1)\n\n        return\n\n    def writeLine(self,strLine, delimit, newline):\n        # Used for controlling where lines terminate with a comma or other continuation mark\n        sqlfile.write(strLine)\n        if delimit: sqlfile.write(delimit)\n        if newline: sqlfile.write('\\n')\n        return\n\n\nif __name__ == '__main__':\n    if len(sys.argv)&lt;2:\n        print \"Usage: jet2sql.py infile.mdb outfile.sql\"\n    else:\n        jetEng = jetReverse(sys.argv[1])\n        outfile = sys.argv[2]\n\n        sqlfile = open(outfile,'w')\n\n        print \"\\nReverse engineering %s to %s\" % (jetEng.jetfilename, outfile)\n\n        # Tables\n        sys.stdout.write(\"\\n   Tables\")\n        for tabl in jetEng.dtbs.TableDefs:\n            sys.stdout.write(\".\")\n            if tabl.Name[:4] &lt;&gt; \"MSys\" and tabl.Name[:4] &lt;&gt; \"~TMP\":\n                jetEng.writeTable(tabl)\n\n        # Relations / FKs\n        sys.stdout.write(\"\\n   Relations\")\n        for fk in jetEng.dtbs.Relations:\n            sys.stdout.write(\".\")\n            jetEng.writeForeignKey(fk)\n\n        # Queries\n        sys.stdout.write(\"\\n   Queries\")\n        for qry in jetEng.dtbs.QueryDefs:\n            sys.stdout.write(\".\")\n            jetEng.writeQuery(qry)\n\n        print \"\\n   Done\\n\"\n\n        # Done\n        sqlfile.close()\n        jetEng.terminate()\n</code></pre></p>"},{"location":"SQL/Access/other/","title":"other","text":"<p>MDB via C#</p> <p>MDB via Python</p>"},{"location":"SQL/Access/other/#check-outlook-version","title":"Check Outlook version","text":"<p>Open Outlook 2013 and click File in the top left hand corner. Now click Office Account, then About Office 2013. The version type will be displayed on the right hand side. You will have either 32-bit or 64-bit.</p>"},{"location":"SQL/Access/other/#code","title":"Code","text":"<pre><code>--list all objects in access\nSELECT * FROM MSysObjects\n\n--datetime format\nSELECT * from nslp \nwhere ProfileName  = 'NSLP' and ProfileArea = 'VICAGL' \nand SettlementDate between #07/01/2017# and #06/30/2018#;\n\n--cross join\nSELECT *\nFROM (select id from details) b, (select detail from details) c;\n\n--distinct item\nSELECT distinct a.ID, a.Name, a.Value\nFROM t_Area as a;\n</code></pre> <pre><code>' Close and Delete object\nDoCmd.Close acTable, strTblNam 'close the table if it is open\nDoCmd.DeleteObject acTable, strTblNam 'delete the table if it exists\n</code></pre> <pre><code>' Check if table exists\nFunction TableExists(strTableName As String)\n    Dim i As Integer\n    Dim Exists As Boolean\n    Exists = False\n    CurrentDb.TableDefs.Refresh\n    For i = 0 To CurrentDb.TableDefs.Count - 1\n        If strTableName = CurrentDb.TableDefs(i).Name Then\n            Exists = True 'Table Exists\n            Exit For\n        End If\n    Next i\n    TableExists = Exists\nEnd Function\n</code></pre> <pre><code>' Check if object is open\nFunction ObjectIsOpen(strName As String, intObjectType As Integer) As Boolean\n' intObjectType can be: 0-acTable, 1-acQuery, 2-acForm, 3-acReport, 4-acMacro, 5-acModule\n    On Error Resume Next\n    ObjectIsOpen = (SysCmd(acSysCmdGetObjectState, intObjectType, strName) &lt;&gt; 0)\n\n    If Err &lt;&gt; 0 Then\n        ObjectIsOpen = False\n    End If\nEnd Function\n</code></pre> <p>Note that the order of the day and month in the date (default mm/dd/yyyy) is independent of the system culture setting.</p>"},{"location":"SQL/Access/other/#export-objects","title":"Export Objects","text":"<pre><code>Sub ExportObject(typ As String, nam As String, db As String)\n    DoCmd.TransferDatabase acExport, \"Microsoft Access\", db, ObjTyp(typ), nam, nam, False\nEnd Sub\n</code></pre>"},{"location":"SQL/Access/tabinfo/","title":"table","text":""},{"location":"SQL/Access/tabinfo/#c","title":"C","text":"<pre><code>var conStr = \"Provider=Microsoft.Jet.OLEDB.4.0;Data Source=\" + \"database.mdb\";\nvar con = new OleDbConnection(conStr);\ncon.Open();\n\nDataTable dataTable = con.GetOleDbSchemaTable(OleDbSchemaGuid.Tables, new object[] { null, null, null, \"TABLE\" });\nint numTables = dataTable.Rows.Count;\nfor (int tableIndex = 0; tableIndex &lt; numTables; ++tableIndex) {\n    String tableName = dataTable.Rows[tableIndex][\"TABLE_NAME\"].ToString();\n    DataTable schemaTable = con.GetOleDbSchemaTable(OleDbSchemaGuid.Columns, new object[] { null, null, tableName, null });\n    foreach (DataRow row in schemaTable.Rows) {\n        String fieldName = row[\"COLUMN_NAME\"].ToString(); //3\n        String fieldType = row[\"DATA_TYPE\"].ToString(); // 11\n        String fieldDescription = row[\"DESCRIPTION\"].ToString(); //27\n    }\n}\n</code></pre>"},{"location":"SQL/Access/tabinfo/#create-table","title":"create table","text":"<pre><code>CREATE TABLE [tn](\n   [Auto]                  COUNTER\n  ,[Byte]                  BYTE\n  ,[Integer]               SMALLINT\n  ,[Long]                  INTEGER\n  ,[Single]                REAL\n  ,[Double]                FLOAT\n  ,[Decimal]               DECIMAL(18,5)\n  ,[Currency]              MONEY\n  ,[ShortText]             CHAR\n  ,[LongText]              MEMO\n  ,[PlaceHolder1]          MEMO\n  ,[DateTime]              DATETIME\n  ,[YesNo]                 BIT\n  ,[OleObject]             IMAGE\n  ,[ReplicationID]         UNIQUEIDENTIFIER\n  ,[Required]              INTEGER NOT NULL\n  ,[Unicode Compression]   MEMO WITH COMP\n  ,[Indexed]               INTEGER\n  ,CONSTRAINT [PrimaryKey] PRIMARY KEY ([Auto])\n  ,CONSTRAINT [Unique Index] UNIQUE ([Byte],[Integer],[Long])\n);\nCREATE INDEX [Single-Field Index] ON [tn]([Indexed]);\nCREATE INDEX [Multi-Field Index] ON [tn]([Auto],[Required]);\nCREATE INDEX [IgnoreNulls Index] ON [tn]([Single],[Double]) WITH IGNORE NULL;\nCREATE UNIQUE INDEX [Combined Index] ON [tn]([ShortText],[LongText]) WITH IGNORE NULL;\n\ncreate table [mlf_gen] (\n  Generator char(255), Voltage_kV char(8), DUID char(50), ConnectionPoint_ID char(50), \n  TNI_code char(50), MLF_2021_22 float, Region char(3)\n);\n\nALTER TABLE [aemo_mlf_2022_gen] \n  ADD CONSTRAINT PK_mlf_gen PRIMARY KEY ([Generator],[ConnectionPoint_ID]);\nCREATE INDEX [Single-Field Index] ON [mlf_gen]([DUID]);\nCREATE UNIQUE INDEX [Combined Index] ON [mlf_gen]([Generator],[ConnectionPoint_ID]) WITH IGNORE NULL;\n</code></pre>"},{"location":"SQL/Access/tips/","title":"tips","text":""},{"location":"SQL/Access/tips/#add-row_number","title":"add row_number","text":"<pre><code>select t.*, \n      (select count(*) from t \n       where a.sort_field&gt;=sort_field\n      ) as rid \nfrom t as a order by a.sort_field;\n</code></pre>"},{"location":"SQL/Access/tips/#max-of-columns","title":"max of columns","text":"<pre><code>select max(v) as maxv\nfrom ( \n  (select v1 as v from t)\n  union\n  (select v2 as v from t)\n  union\n  (select v3 as v from t)\n) as a\n</code></pre>"},{"location":"SQL/Access/vbasql/","title":"vba","text":""},{"location":"SQL/Access/vbasql/#pass-parameters","title":"pass parameters","text":"<pre><code>SELECT [case] &amp; '_' &amp; Format(start_year,\"yy\") &amp; Format(end_year,\"yy\") AS Expr1, \n       [startyear] AS start_year, \n       [endyear] AS end_year\nFROM tbl\nWHERE id=[idx];\n</code></pre> <pre><code>Set cmd.ActiveConnection = CurrentProject.Connection\ncmd.CommandText = \"Insert_Tbl\"\ncmd.Execute , Array(txtCase, (year - 1) &amp; \"-07-01\", year &amp; \"-07-01\", txtCase), adCmdStoredProc\n</code></pre>"},{"location":"SQL/Access/vbasql/#get-table-row-count","title":"get table row count","text":"<pre><code>Cnt = DCount(\"*\", \"TableName\")\nCnt = DCount(\"[FieldName]\", \"TableName\")\n'will not be used\nDim rs As DAO.Recordset\nSet rs = db.OpenRecordset(\"SELECT Count([FieldName]) AS RecCount FROM TableName;\")\nr.MoveLast\nCnt = rs.RecordCount\nrs.Close\n</code></pre>"},{"location":"SQL/Access/vbasql/#delete-with-inner-join","title":"delete with inner join","text":"<pre><code>DELETE FROM Table_A \nWHERE myid IN \n( SELECT myid \n  FROM Table_B \n  WHERE Table_A.myid = Table_B.myid \n);\n</code></pre>"},{"location":"SQL/DuckDB/Datetime/","title":"Datetime","text":""},{"location":"SQL/DuckDB/Datetime/#extract-date-from-timestamp","title":"extract date from timestamp","text":"<pre><code>cast(timeending - interval '30 minutes' as date) as date_updated\n</code></pre>"},{"location":"SQL/DuckDB/Datetime/#extract-yearmonth-etc-from-datetimestamp","title":"extract year/month etc from date/timestamp","text":"<pre><code>extract(part from date) # year, month, day, hour, minute, second\nextract(year from timeending - interval '30 minutes') as year\n</code></pre>"},{"location":"SQL/DuckDB/DuckDB/","title":"DuckDB","text":""},{"location":"SQL/DuckDB/DuckDB/#duckdb-vs-sqlite","title":"DuckDB vs Sqlite","text":"<p>https://marclamberti.com/blog/duckdb-getting-started-for-beginners</p> <p>DuckDB is optimized for queries that apply aggregate calculations across large numbers of rows,  whereas SQLite is optimized for fast scanning and lookup for individual rows of data. </p> <p>If you need to perform analytical queries, go with DuckDB otherwise, use SQLite.</p>"},{"location":"SQL/IoTDB/Cluster/","title":"Cluster","text":"<p>https://www.bookstack.cn/read/iotdb-1.0-en/4871cfe01be44070.md</p>"},{"location":"SQL/IoTDB/Docker/","title":"Docker","text":""},{"location":"SQL/IoTDB/Docker/#command","title":"command","text":"<pre><code># create and run a iotdb Docker container\ndocker run -d -p 6667:6667 -p 31999:31999 -p 8181:8181 --name iotdb apache/iotdb\n# run commands inside the running iotdb Docker container\ndocker exec -it iotdb /bin/bash\n# change to iotdb folder\ncd /iotdb/\n</code></pre>"},{"location":"SQL/IoTDB/Docker/#ports","title":"ports","text":"<p>The ports that IoTDB uses by defaut: - 6667: RPC port - 31999: JMX port - 8181: Monitor port - 5555: Data sync port - 9003: internal metadata rpc port (for cluster) - 40010: internal data rpc port (for cluster)</p>"},{"location":"SQL/IoTDB/Query/","title":"Query","text":""},{"location":"SQL/IoTDB/Query/#insert","title":"insert","text":"<pre><code>insert into root.ln.wf01.wt01 (timestamp, id, val) VALUES (1, 10, null),(2, 10, 100),(3, 12, null),(4, 1, 150),(5, 13, null),(6, 13, 200)\n</code></pre>"},{"location":"SQL/MSSQL/ANSI_NULLS/","title":"ANSI NULLS","text":""},{"location":"SQL/MSSQL/ANSI_NULLS/#test-ansi-nulls-on-off","title":"test ANSI NULLS ON / OFF","text":"<pre><code>CREATE TABLE #tmpx (i int, v float);\nINSERT INTO #tmpx VALUES (1, 1.1), (2, null);\n\nSET ANSI_NULLS ON;\nGO\n\nDECLARE @val float;\nset @val=1;\n\nselect * from #tmpx where v &lt;&gt; @val;\nGO\n\n# column with column comparison does not work\nselect t2.* from #tmpx as t2\njoin (select * from #tmpx) as t1\non t1.i = t2.i\nwhere t1.v &lt;&gt; t2.i\nGO\n\nDECLARE @json1 nvarchar(100);\nSET @json1 = N'[{\"i\": 1, \"v\": null}, {\"i\": 2, \"v\": 3.3}]';\n\nMERGE #tmpx WITH (XLOCK, ROWLOCK) AS t\nUSING ( \n    SELECT * FROM OPENJSON(@json1) \n        WITH (i int, v float) \n) AS j\nON t.i = j.i\nWHEN MATCHED AND t.v &lt;&gt; j.v\nTHEN UPDATE\n    SET t.v = j.v\nWHEN NOT MATCHED BY TARGET THEN\n    INSERT (i, v) VALUES (j.i, j.v)\n;\nGO\n</code></pre>"},{"location":"SQL/MSSQL/Basic/","title":"Basic","text":""},{"location":"SQL/MSSQL/Basic/#temporal-table","title":"temporal table","text":"<pre><code>CREATE TABLE #tmp1 (id INT, name VARCHAR(25), value float)\nINSERT INTO #tmp1 VALUES (1, 'John', 1.1), (2, 'Ana', 2.2)\nSELECT * FROM #tmp1\nDROP TABLE #tmp1\n</code></pre>"},{"location":"SQL/MSSQL/Basic/#json-variable","title":"json variable","text":"<pre><code>DECLARE @json1 nvarchar(100)\nSET @json1 = N'[{\"a\": \"1\"}, {\"a\": NULL}]'\n</code></pre>"},{"location":"SQL/MSSQL/Basic/#merge","title":"merge","text":"<p><pre><code>MERGE INTO tbl as T\nUSING (\n  SELECT * FROM\n       (VALUES (1, 2, 3), (2, 4, 5)) AS v\n       WITH (id, col1, col2)\n) AS S\nON T.id=S.id\nWHEN MATCHED THEN\n  UPDATE SET\n    col_a=S.col_a, col_b=S.col_b\nWHEN NOT MATCHED BY TARGET THEN\n  INSERT (id, cola, colb)\n  VALUES (S.id, S.cola, S.colb)\nWHEN NOT MATCHED BY SOURCE THEN\n  DELTE\n;\n</code></pre> <pre><code>def upsert_df(\n    df : pd.DataFrame,\n    table_name : str,\n    id_cols: Set[str],\n    connection_url: str,\n) -&gt; None:\n    engine = create_engine(connection_url, fast_executemany=True)\n\n    # Command terms\n    cols = df.columns.tolist()\n    stmt_on = ' AND '.join([f'T.{i} = S.{i}' for i in id_cols])\n    cols_upd = ','.join([f'T.{c} = S.{c}' for c in cols if c not in id_cols])\n    cols_ins = ','.join(cols)\n    cols_val = ','.join([f'S.{c}' for c in cols])\n\n    # fill `NULL` values with None\n    def fill_null(val: list) -&gt; tuple:\n        def is_null(v):\n            return isinstance(v, type(pd.NA)) or (v in ['NULL', np.nan, ''])\n        return tuple(None if is_null(v) else v for v in val)\n\n    # create parameter indicators (?, ?, ..., ?) and parameter values\n    param_slots = ','.join(['?'] * df.shape[1])\n    param_values = [fill_null(row.tolist()) for _, row in df.iterrows()]\n\n    sql_stmt = f'''\n        MERGE INTO {table_name} WITH (XLOCK, ROWLOCK) AS T\n        USING (\n            SELECT * FROM (VALUES ({param_slots})) as v ({cols_ins})\n        ) AS S\n            ON {stmt_on}\n        WHEN MATCHED THEN\n            UPDATE SET {cols_upd}\n        WHEN NOT MATCHED BY TARGET THEN\n            INSERT ({cols_ins}) VALUES ({cols_val})\n        ;\n        '''\n\n    # merge df with table\n    with engine.begin() as cnn:\n        cnn.execute(cmd, param_values)\n</code></pre></p>"},{"location":"SQL/MSSQL/CET/","title":"CET","text":""},{"location":"SQL/MSSQL/CET/#join-performance-in-cet","title":"join performance in cet","text":"<p>when join cet tables, by sorting the join columns, the performance can increase 30-60x. - https://stackoverflow.com/questions/3995958/adding-an-index-to-a-cte - sort the joinning columns in the CET <code>ORDER BY Field1, Field2 OFFSET 0 ROWS</code> - sqlalchemy: <code>.orderby(Field1, Field2).offset(0)</code></p>"},{"location":"SQL/MSSQL/CET/#how-to-only-execute-cet-conditionally","title":"how to only execute cet conditionally","text":"<p>https://stackoverflow.com/questions/58723615/how-to-prevent-cte-execution-until-reached-or-not-exists</p>"},{"location":"SQL/MSSQL/DateTime/","title":"Date time","text":"<p>https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008-r2/ms180878(v=sql.105)?redirectedfrom=MSDN</p> <p>New version:</p> <p>https://docs.microsoft.com/en-us/sql/relational-databases/native-client-odbc-date-time/data-type-support-for-odbc-date-and-time-improvements?view=sql-server-ver16</p>"},{"location":"SQL/MSSQL/DateTime/#iso-8601-format-not-good-for-new-version","title":"ISO 8601 Format (not good for new version)","text":"<p>Date: - <code>YYYYMMDD</code></p> <p>Datetime: - <code>YYYY-MM-DDThh:mm:ss[.nnnnnnn][{+|-}hh:mm]</code> - <code>YYYY-MM-DDThh:mm:ss[.nnnnnnn]Z</code> (UTC, Coordinated Universal Time)</p>"},{"location":"SQL/MSSQL/DateTime/#datetime2-in-new-version","title":"DateTime2 in new version","text":"<pre><code>Datetime2   SQL_TYPE_TIMESTAMP 'yyyy-mm-dd hh:mm:ss[.9999999]'\n</code></pre>"},{"location":"SQL/MSSQL/DateTime/#convert-string-to-date","title":"convert string to date","text":"<p>option 1: <code>cast</code>. Can't set the format <pre><code>SELECT CAST('01' + 'Sep25' AS DATE) as dd_mon_yy; -- 2025-09-01\nSELECT CAST('01' + 'Jan50' AS DATE) as dd_mon_yy; -- 1950-01-01 two digit year cutoff is 2049\n</code></pre></p> <p>option 2: <code>convert</code> https://learn.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql?view=sql-server-ver16 <pre><code>SELECT CONVERT(DATE, '01' + 'Sep25', 6) AS dd_mon_yy;\nSELECT CONVERT(DATE, '01' + 'Jan50', 6) as dd_mon_yy; -- 1950-01-01 two digit year cutoff is 2049\n</code></pre></p>"},{"location":"SQL/MSSQL/Duplicate/","title":"Duplicate","text":""},{"location":"SQL/MSSQL/Duplicate/#duplicate-records","title":"duplicate records","text":"<pre><code>select\n    product_name,\n    start_date,\n    end_date,\n    STRING_AGG(p_type, ', ') AS typ,\n    STRING_AGG(CAST(p_price AS VARCHAR(10)), ', ') AS price,\n    count(*) as cnt\nfrom my_tbl\nwhere p_price &gt; 10\ngroup by product_name, start_date, end_date\nhaving count(*) &gt; 1;\n</code></pre>"},{"location":"SQL/MSSQL/Error/","title":"Error","text":""},{"location":"SQL/MSSQL/Error/#ssms-wont-open","title":"ssms won't open","text":"<ul> <li>export log:</li> <li>ssms.exe -log c:\\log.txt</li> <li>check error in log:</li> <li>Could not load file or assembly 'Microsoft.VisualStudio.Shell.Interop.8.0, Version=15.0.0.0</li> <li>check Interp file version:</li> <li>powershell - [System.Reflection.Assembly]::LoadFrom(\"Full Path to Microsoft.VisualStudio.Shell.Interop.8.0.dll\").GetName()</li> <li>Common7\\IDE\\PublicAssemblies</li> <li>Common7\\IDE\\PrivateAssemblies\\Interop</li> <li>replace the 8.0 version by the 15.0 version</li> </ul>"},{"location":"SQL/MSSQL/Error/#pyodbc-fast_executemany-data-type-varchar-to-numeric-error","title":"pyodbc fast_executemany data type varchar to numeric error","text":"<p>Error converting data type varchar to numeric 8114 when Inserting into SQL Server using select from values using fast_executemany</p> <p>https://github.com/mkleehammer/pyodbc/issues/976</p> <p>fast_executemany needs to determine the types of all the parameters in advance so it can allocate the parameter data array. As your trace shows, pyODBC has trouble determining those types because they're ambiguous, and the errors follow. The best solution (which also avoids sending needless data to the server) is to not do that.</p> <p>https://towardsdatascience.com/how-i-made-inserts-into-sql-server-100x-faster-with-pyodbc-5a0b5afdba5</p> <p>Solution: According to the Github issue from the pyodbc repository [2], pyodbc internally passes all decimal values as strings because of some discrepancies and bugs related to decimal points used by various database drivers.</p> <p>This means that when my data has a value of 0.021527 or 0.02, both of those values may not be accepted because my SQL Server data type was specified as NUMERIC(18,3). <pre><code>df['val'] = [format(v, '.3f') for v in df['val']]\n</code></pre></p>"},{"location":"SQL/MSSQL/ForeignKey/","title":"Foreign key","text":""},{"location":"SQL/MSSQL/ForeignKey/#listed-all-referencing-tables","title":"listed all referencing tables","text":"<pre><code>SELECT OBJECT_NAME(f.parent_object_id) AS referencing_table, \n       COL_NAME(fc.parent_object_id, fc.parent_column_id) AS referencing_column, \n       OBJECT_NAME(f.referenced_object_id) AS referenced_table, \n       COL_NAME(fc.referenced_object_id, fc.referenced_column_id) AS referenced_column\nFROM sys.foreign_keys AS f\nINNER JOIN sys.foreign_key_columns AS fc ON f.object_id = fc.constraint_object_id\nwhere OBJECT_NAME(f.referenced_object_id) = 'referenced_table' \n  and COL_NAME(fc.referenced_object_id, fc.referenced_column_id) = 'referenced_column'\nORDER BY referencing_table;\n</code></pre>"},{"location":"SQL/MSSQL/Index/","title":"Index","text":"<p>https://www.red-gate.com/simple-talk/databases/sql-server/learn/using-covering-indexes-to-improve-query-performance/</p>"},{"location":"SQL/MSSQL/Lag/","title":"Lag","text":"<p>shift row value to next row or previous row</p>"},{"location":"SQL/MSSQL/Lag/#shift-to-next-row","title":"shift to next row","text":"<pre><code>WITH cte_netsales AS (\n  SELECT \n    year,\n    month, \n    SUM(net_sales) net_sales\n  FROM \n    sales.netsales_brands\n  WHERE \n    year &gt;= 2018\n  GROUP BY \n    year, month\n)\nSELECT \n  year,\n  month,\n  net_sales,\n  LAG(net_sales,1) OVER (\n    ORDER BY year, month\n  ) pre_mth_sales\nFROM \n  cte_netsales;\n</code></pre>"},{"location":"SQL/MSSQL/Lag/#shift-to-next-row-with-partition","title":"shift to next row with partition","text":"<pre><code>WITH cte_sales AS (\n  SELECT \n    year,\n    month,\n    brand,\n    net_sales,\n    LAG(net_sales,1) OVER (\n      PARTITION BY brand\n      ORDER BY year, month\n    ) pre_sales\n  FROM \n    sales.netsales_brands\n  WHERE\n    year &gt;= 2018\n)\nSELECT\n  year,\n  month, \n  brand,\n  net_sales, \n  pre_sales,\n  FORMAT((net_sales - pre_sales)  / pre_sales, 'P') vs_pre_mth\nFROM\n  cte_sales;\n</code></pre>"},{"location":"SQL/MSSQL/Latest/","title":"Get latest records","text":""},{"location":"SQL/MSSQL/Latest/#latest-in-each-group","title":"latest in each group","text":"<p>daily revenue values with update_date for each state <pre><code>SELECT \n    region, \n    country, \n    year(value_timestamp) as [year], \n    month(value_timestamp) as [month], \n    sum(revenue) as revenue \nFROM (\n    SELECT \n        [state],\n    [country],\n    [region],\n    DATEADD(HOUR,10,value_timestamp) as value_timestamp,\n    [revenue],\n    row_number() over(partition by state, update_date order by update_date desc) as rn\n    FROM [dev].[annual_revenue]\n    WHERE value_type = 'forecast'\n) t\nWHERE t.rn = 1\nGROUP BY country, region, year(value_timestamp), month(value_timestamp)\nORDER BY region, country, year(value_timestamp), month(value_timestamp)\n;\n</code></pre></p>"},{"location":"SQL/MSSQL/PartitionBy/","title":"PartitionBy","text":"<p>https://www.sqlshack.com/sql-partition-by-clause-overview/</p> <p>PartitionBy will calculate the aggregated value in each partition and broadcast the value back to each row; can add original col as well - so the row number will not decrease.</p>"},{"location":"SQL/MSSQL/PartitionBy/#get-latest-version","title":"get latest version","text":"<pre><code>SELECT\n  *\nFROM (\n  SELECT\n    *,\n    ROW_NUMBER() OVER (PARTITION BY column_to_group_by ORDER BY version_ DESC) AS rn\n  FROM\n    your_table\n) AS t\nWHERE\n  rn = 1\n</code></pre>"},{"location":"SQL/MSSQL/PartitionBy/#examples","title":"examples","text":"<pre><code>IF OBJECT_ID(N'tempdb..#tmpx') IS NOT NULL\nBEGIN\n    DROP TABLE #tmpx\nEND\n;\n\nCREATE TABLE #tmpx (i int, j int, v float);\nINSERT INTO #tmpx VALUES (1, 1, 1), (1, 1, 2), (1, 2, 3);\n\nSELECT i, \n       AVG(v) AS AvgV, \n       MIN(v) AS MinV \nFROM #tmpx\nGROUP BY i;\n\nSELECT i, v,\n       AVG(v) OVER(PARTITION BY i order by j desc) AS AvgV, \n       MIN(v) OVER(PARTITION BY i) AS MinV\nFROM #tmpx;\n\n-- why the average different???\nSELECT i, v,\n       AVG(v) OVER(PARTITION BY i order by j asc) AS AvgV, \n       MIN(v) OVER(PARTITION BY i) AS MinV\nFROM #tmpx;\n</code></pre>"},{"location":"SQL/MSSQL/PartitionBy/#combine-with-groupby","title":"combine with groupby","text":"<pre><code>SELECT i, j,\n    AVG(AVG(v)) OVER(PARTITION BY i order by j) AS AvgV\nFROM  #tmpx\nGROUP BY i, j;\n</code></pre>"},{"location":"SQL/MSSQL/Performance/","title":"Performance","text":"<p>https://learn.microsoft.com/en-us/troubleshoot/sql/database-engine/performance/troubleshoot-slow-running-queries</p>"},{"location":"SQL/MSSQL/Performance/#execution-plan","title":"Execution Plan","text":"<p>In SSMS, there are options to check the estimated/actual/cached execution plan. </p>"},{"location":"SQL/MSSQL/TSL/","title":"TLS","text":""},{"location":"SQL/MSSQL/TSL/#tls-12-support","title":"tls 1.2 support","text":"<p>https://learn.microsoft.com/en-gb/troubleshoot/sql/database-engine/connect/tls-1-2-support-microsoft-sql-server</p>"},{"location":"SQL/MSSQL/TSL/#python-connection-ubuntu-2204-and-old-sql-server","title":"python connection: ubuntu 22.04 and old sql server","text":"<p>openssl 3.0:  https://github.com/microsoft/msphpsql/issues/1112</p> <p>issue: <pre><code>sqlalchemy.exc.OperationalError: (pyodbc.OperationalError) (\n'HYT00', '[HYT00] [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired (0) (SQLDriverConnect)'\n)\n</code></pre></p> <p>config: - SQL Server: 12.0.4100.1 - ODBC Driver 17 (18)</p> <p>Solutions: - option 1: upgrade server to latest version - option 2: modify <code>/etc/ssl/openssl.cnf</code> <pre><code>[system_default_sect]\nMinProtocol = TLSv1.2\n# CipherString = DEFAULT@SECLEVEL=1\nCipherString = DEFAULT:@SECLEVEL=0\n</code></pre>   Change last <code>CipherString</code> line from <code>CipherString = DEFAULT:@SECLEVEL=2</code> to <code>CipherString = DEFAULT:@SECLEVEL=0</code></p> <p>dockerfile   <pre><code>USER root\n\n# workaround for ubuntu 22.04 and ms odbc driver 17 with sql server 12.0\n# https://github.com/microsoft/msphpsql/issues/1112\nRUN head -n -1 /etc/ssl/openssl.cnf &gt; openssl_temp_file &amp;&amp; \\\n    echo \"CipherString = DEFAULT:@SECLEVEL=0\" &gt;&gt; openssl_temp_file &amp;&amp; \\\n    mv openssl_temp_file /etc/ssl/openssl.cnf\n\nUSER user\n</code></pre></p>"},{"location":"SQL/MSSQL/TableEdit/","title":"Table edit","text":""},{"location":"SQL/MSSQL/TableEdit/#delete-table-data","title":"delete table data","text":"<pre><code>delete from my_table_name;\n</code></pre>"},{"location":"SQL/MSSQL/TableEdit/#reset-autoincrement-in-sql-server-after-delete","title":"Reset AutoIncrement in SQL Server after Delete","text":"<pre><code>SELECT MAX(my_column_name) as vmax FROM my_table_name #get max val\nDBCC CHECKIDENT('db.schema.my_table_name', RESEED, vmax-1) #reset autoincrement seed\n</code></pre>"},{"location":"SQL/MSSQL/Terminal/","title":"Terminal","text":"<pre><code>sqlcmd -S &lt;svr&gt; -U &lt;usr&gt; -P \"&lt;pwd&gt;\"\nquery\ngo\n</code></pre>"},{"location":"SQL/MSSQL/TmpTable/","title":"Tmp Table","text":""},{"location":"SQL/MSSQL/TmpTable/#create-and-delete-tmp-table","title":"create and delete tmp table","text":"<pre><code>CREATE TABLE #tmp_tbl (\n    name VARCHAR(30)\n);\n\n\nINSERT INTO #tmp_tbl\nVALUES ('Ai'),('Cob'),('Gle');\n\nDROP TABLE #tmp_tbl;\n</code></pre>"},{"location":"SQL/MSSQL/Variable/","title":"Variable","text":""},{"location":"SQL/MSSQL/Variable/#set-and-use-variable","title":"set and use variable","text":"<pre><code>declare @start_date as date = '2012-01-01';\n\nselect *\nfrom my_table\nwhere recorddate &gt;= @date_start;\n</code></pre> <pre><code>define start_date = TO_DATE('2012-01-01','yyyy-mm-dd');\n\nselect *\nfrom my_table\nwhere recorddate &gt;= &amp;date_start;\n</code></pre>"},{"location":"SQL/MSSQL/Variable/#pipeline-variable-ignore-filter-if-null","title":"pipeline variable: ignore filter if null","text":"<pre><code>(@{pipeline().parameters.p} is null OR p = @{pipeline().parameters.p})\n</code></pre>"},{"location":"SQL/MSSQL/admin/","title":"admin","text":""},{"location":"SQL/MSSQL/admin/#stop-server","title":"stop server","text":"<p>SQL Server agent is dependent on SQL Service. \\ So before stopping MSSQL, Server agent service needs to be stopped.  <pre><code>sc stop sqlserveragent\nsc stop mssqlserver\n</code></pre></p>"},{"location":"SQL/MSSQL/admin/#create-a-user","title":"create a user","text":"<p>or use UI and add the roles: public and sysadmin    CREATE USER Mary WITH PASSWORD = '**';</p>"},{"location":"SQL/MSSQL/admin/#show-users","title":"show users","text":"<pre><code>select name as username,\n       create_date,\n       modify_date,\n       type_desc as type,\n       authentication_type_desc as authentication_type\nfrom sys.database_principals\nwhere type not in ('A', 'G', 'R', 'X')\n      and sid is not null\n      and name != 'guest'\norder by username;  \n</code></pre>"},{"location":"SQL/MSSQL/admin/#create-db","title":"create db","text":"<pre><code>CREATE DATABASE test;\n</code></pre>"},{"location":"SQL/MSSQL/admin/#show-dbs","title":"show dbs","text":"<pre><code>SELECT name FROM master.sys.databases;\nSELECT name FROM master.dbo.sysdatabases;\n\n--exlude system dbs\nSELECT * FROM master.sys.databases d WHERE d.database_id &gt; 4\nSELECT * FROM master.sys.databases d WHERE name NOT IN ('master', 'tempdb', 'model', 'msdb');\n</code></pre>"},{"location":"SQL/MSSQL/admin/#get-table-cols","title":"get table cols","text":"<pre><code>Select table_name, column_name, data_type, ordinal_position \nFrom INFORMATION_SCHEMA.COLUMNS \nWhere table_catalog = 'cat' and table_schema = 'sch' and TABLE_NAME = 'tbl';\n</code></pre>"},{"location":"SQL/MSSQL/admin/#get-table-keys","title":"get table keys","text":"<pre><code>SELECT TABLE_CATALOG, TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, CONSTRAINT_NAME\nFROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE\nWHERE TABLE_CATALOG = 'CAT' AND TABLE_SCHEMA ='SCH' AND TABLE_NAME = 'TBL'\norder by ORDINAL_POSITION;\n</code></pre>"},{"location":"SQL/MSSQL/admin/#show-table-info","title":"show table info","text":"<pre><code>sp_help '[database].[schema].[table]';\n</code></pre>"},{"location":"SQL/MSSQL/migrate/","title":"migrate","text":"<ul> <li>download and install MySQL ODBC Connector</li> <li>download and install SQL Server Migration Assistant for MySQL platform</li> </ul>"},{"location":"SQL/MSSQL/migrate/#from-mysql","title":"from mysql","text":""},{"location":"SQL/MSSQL/migrate/#zero-datetime","title":"Zero datetime","text":"<p>With data type datetime (Transact-SQL) you can store date values from 1753-01-01 onward, with datetime2 (Transact-SQL) from year 0001 on, but never from year 0000.</p> <p>zero datetime ['0000-00-00 00:00:00'] will cause migration error in ssma: Column 'xx' does not allow DbNull value.</p> <p>solution: change the value to 1900-01-01 00:00:00 in the mysql tables.</p> <pre><code>-- list table cols with default zero datetime\nSELECT c.table_schema, c.TABLE_NAME, c.COLUMN_NAME, c.DATA_TYPE, c.IS_NULLABLE, c.COLUMN_DEFAULT\nFROM INFORMATION_SCHEMA.COLUMNS as c\ninner join INFORMATION_SCHEMA.Tables as t\non t.table_schema = c.table_schema and t.table_name = c.table_name\nWHERE t.Table_Type = 'BASE TABLE' and c.IS_NULLABLE = 'NO' and c.COLUMN_DEFAULT like '0000-00-00%';\n\n-- change datatime to 1900-01-01 00:00:00\nupdate 24c_lite.bids_forecast\nset BIDOFFERDATE = '1900-01-01 00:00:00'\nif BIDOFFERDATE = '0000-00-00 00:00:00';\n</code></pre>"},{"location":"SQL/MSSQL/migrate/#failed-on-large-table-about-5gb","title":"failed on large table about 5GB","text":"<p>I had the same error in SSMA while migrating a table from Oracle to SQL Server. The table had 1000 million records and the migration used to fail after 90% of transfer.</p> <p>By making the changes in project setting BATCH SIZE under TOOLS &gt; PROJECT SETTINGS &gt; GENERAL&gt; MIGRATION to 1000 and TIME OUT to 1000, it completed successfully.</p>"},{"location":"SQL/MSSQL/migrate/#connection-closed","title":"Connection closed","text":"<p>Cannot truncate target table. Reason: ExecuteReader requires an open and available Connection. The connection's current state is closed.</p> <p>solution: Go to Tools &gt; Project Settings &gt; General and in the left pane click on Migration and then in the right pane, in the section Misc, change Data migration timeout in minutes property to something larger (the default value was 15; I changed it to 300).</p> <p>By making the changes in project setting BATCH SIZE under TOOLS &gt; PROJECT SETTINGS &gt; GENERAL&gt; MIGRATION to 1000 and TIME OUT to 1000, it completed successfully.</p>"},{"location":"SQL/MSSQL/migrate/#login-failed-for-user","title":"Login Failed for User","text":"<p>Microsoft SQL Server, Error: 18456</p> <p>check and update user permissions: grant public and sysadmin roles</p> <p>and also add server login in configuration: * Open your SQL Server Management Studio * Right click on the database server and go to properties * Choose the security option and check \"SQL Server and Windows authentication mode\" * Enable TCP/IP connection in SQL Configuration Manager * Restart your SQL server</p>"},{"location":"SQL/MSSQL/qry/","title":"query","text":"<pre><code>USE [test]\nDECLARE @Command nvarchar(max), @ConstraintName nvarchar(max), @TableName nvarchar(max), @ColumnName nvarchar(max)\nSET @TableName = 'rec.get'\nSET @ColumnName = 'LAST_UPDATE'\nSELECT @ConstraintName = name\n    FROM sys.default_constraints\n    WHERE parent_object_id = object_id(@TableName)\n        AND parent_column_id = columnproperty(object_id(@TableName), @ColumnName, 'ColumnId')\n\nSELECT @Command = 'ALTER TABLE ' + @TableName + ' DROP CONSTRAINT ' + @ConstraintName  \nEXECUTE sp_executeSQL @Command\n\nSELECT @Command = 'ALTER TABLE ' + @TableName + ' ADD CONSTRAINT ' + @ConstraintName + ' DEFAULT (''1900-01-01'') FOR ' + @ColumnName \nEXECUTE sp_executeSQL @Command\n</code></pre>"},{"location":"SQL/MSSQL/setting/","title":"setting","text":""},{"location":"SQL/MSSQL/setting/#memory","title":"memory","text":"<p>By default, SQL Server\u2019s max memory is 2147483647 \u2013 a heck of a lot more than you actually have.  Trivia time \u2013 that\u2019s the max number for a signed 32-bit integer.  SQL Server will just keep using more and more memory until there\u2019s none left on the system.</p> <p>Our simple \u201cstarter\u201d rule of thumb is to leave 4GB or 10% of total memory free, whichever is LARGER on your instance to start with, and adjust this as needed. <pre><code>Server -&gt; Properties -&gt; Memory -&gt; Max server memory\nT-SQL: EXEC sys.sp_configure 'max server memory (MB)', '2048'; RECONFIGURE;\n</code></pre></p>"},{"location":"SQL/MSSQL/setting/#logging","title":"logging","text":"<p>You can't do without transaction logs in SQL Server, under any circumstances. The engine simply won't function.</p> <p>You CAN set your recovery model to SIMPLE [other than FULL] on your dev machines - that will prevent transaction log bloating when tran log backups aren't done.</p> <p>There is a third recovery mode not mentioned above. In cases where you are going to be doing any type of bulk inserts, you should set the DB to be in \"BULK/LOGGED\". This makes bulk inserts move speedily along and can be changed on the fly [A new database inherits its recovery model from the model database]. <pre><code>USE master;\nALTER DATABASE model SET RECOVERY BULK_LOGGED; --change\nALTER DATABASE model SET RECOVERY SIMPLE; --change it back\n</code></pre></p>"},{"location":"SQL/MSSQL/ssma/","title":"ssma for mysql","text":""},{"location":"SQL/MSSQL/ssma/#timeout-for-large-tables","title":"Timeout for large tables","text":"<p>change batch-size to 1000 and time-out to max value 1440 in default project settings.</p>"},{"location":"SQL/MSSQL/ssma/#sql-server-does-not-exist-or-access-denied","title":"SQL Server does not exist or access denied","text":"<p>To check it:   - Open SQL Server Configuration Manager from start program.   - Expand SQL Server Network Configuration   - Click on Protocols for XXXX   - Right Click on TCP/IP and open properties      * Enable TCP/IP      * Double click TCP?IP, In IP Address Tab, Set Port 1433 in the last option (IPAll)</p>"},{"location":"SQL/MSSQL/ssma/#port-is-not-added-in-the-firewall-exception-list","title":"Port is not added in the firewall exception list","text":"<ul> <li>Go to Control Panel -&gt; Open Adminstrative Tool -&gt; Select Windows firewall with Advanced Security</li> <li>From the left panel, click on the Inbound Rules and from right panel select New Rule\u2026</li> <li>In New Inbound Rule Wizard window, select Port and click on Next button</li> <li>In the next tab, enter \u20181433\u2019 in Specific local ports and click on Next button</li> <li>Under What action should be taken when a connection matches the specified condition? section, select Allow the connection option and click on Next</li> <li>Check Domain, Private, Public under which does this rule apply section? and click on Next</li> <li>Enter the Name and click on the Finish button</li> </ul>"},{"location":"SQL/MSSQL/tip/","title":"tip","text":""},{"location":"SQL/MSSQL/tip/#max-of-columns","title":"max of columns","text":"<pre><code>select f1, max(d) vmax, f3\nfrom t unpivot (d for v in (v1,v2,v3)) as u\ngroup by f1, f3 \ngo\n\nSELECT [Other Fields],\n  (SELECT Max(v) FROM (VALUES (date1), (date2), (date3),...) AS value(v)) as [MaxDate]\nFROM [YourTableName]\n</code></pre>"},{"location":"SQL/MSSQL/DBA/Table/","title":"Table","text":""},{"location":"SQL/MSSQL/DBA/Table/#get-table-description","title":"Get table description","text":"<pre><code>SELECT\n  obj.name AS table_name, col.name AS column_name, ep.value AS column_description\nFROM sys.objects obj\nINNER JOIN sys.schemas s\n  ON obj.schema_id = s.schema_id\nINNER JOIN sys.columns col\n  ON obj.object_id = col.object_id\nLEFT JOIN sys.extended_properties ep\n  ON obj.object_id = ep.major_id AND col.column_id = ep.minor_id AND ep.name = 'MS_Description'\nWHERE\n  obj.type_desc = 'USER_TABLE' AND s.name = 'schema-name' AND obj.name = 'table-name'\n</code></pre>"},{"location":"SQL/MSSQL/DBA/Table/#delete-versioned-table-records-in-sql-server","title":"Delete versioned table records in sql server","text":"<ul> <li> <ol> <li>Upscale: As the database is full, upscale quota limit</li> </ol> </li> <li> <ol> <li>Disable system versioning temporarily</li> </ol> </li> <li> <ol> <li>Batch Delete Data from <code>Main Table</code>: Execute the batch delete script on the main temporal table (<code>my_schema.sales</code>) to move old records to the history table without overwhelming the log.</li> </ol> </li> <li> <ol> <li>Batch Delete Data from <code>History Table</code>: Temporarily disable versioning and execute the batch delete script on the history table (<code>my_schema_history.sales</code>).</li> </ol> </li> <li> <ol> <li>Re-enable system versioning and link to the history table again</li> </ol> </li> <li> <ol> <li>Shrink the Database: Run the <code>DBCC SHRINKDATABASE ('My_DB')</code> command to reclaim the unused space from the data and log files. This is the step that will physically reduce the database size.</li> </ol> </li> <li> <ol> <li>Downscale (Optional): Once the size has been safely reduced and the database is below the old quota limit, you can safely return to the Azure Portal and scale the database back down to its original size limit, if you wish to save cost.</li> </ol> </li> </ul>"},{"location":"SQL/MSSQL/DBA/Table/#disable-system-versioning-temporarily","title":"Disable system versioning temporarily","text":"<pre><code>ALTER TABLE [my_schema].[sales]\nSET (SYSTEM_VERSIONING = OFF);\n</code></pre>"},{"location":"SQL/MSSQL/DBA/Table/#delete-records","title":"Delete records","text":"<pre><code>WHILE 1=1\nBEGIN\n    DELETE TOP (20000)\n    FROM [my_schema].[sales]\n    WHERE as_of_timestamp &lt; '2024-01-01';\n    IF @@ROWCOUNT = 0 BREAK;\nEND\n</code></pre>"},{"location":"SQL/MSSQL/DBA/Table/#delete-all-records-if-no-longer-in-use","title":"Delete all records if no longer in use","text":"<pre><code>TRUNCATE TABLE [my_schema].[sales];\n</code></pre>"},{"location":"SQL/MSSQL/DBA/Table/#re-enable-system-versioning-and-link-to-the-history-table-again","title":"Re-enable system versioning and link to the history table again","text":"<pre><code>ALTER TABLE [my_schema].[sales]\nSET (SYSTEM_VERSIONING = ON (HISTORY_TABLE = [my_schema_history].[sales]));\n</code></pre>"},{"location":"SQL/MSSQL/DBA/Table/#check-table-type","title":"Check table type","text":"<pre><code>SELECT\n    s.name AS SchemaName,\n    t.name AS TableName,\n    t.temporal_type AS TemporalType,\n    t.temporal_type_desc AS TemporalTypeDesc\nFROM sys.tables AS t\nJOIN sys.schemas AS s\n    ON t.schema_id = s.schema_id\nLEFT JOIN sys.tables AS ht\n    ON t.history_table_id = ht.object_id\nORDER BY\n    s.name, t.name;\n</code></pre>"},{"location":"SQL/MSSQL/DBA/Table/#check-table-space-usage","title":"Check table space usage","text":"<pre><code>SELECT\n    s.name AS schema_name,\n    t.name AS table_name,\n    SUM(a.total_pages) * 8 / 1024 / 1024 AS used_gb,\n    p.rows AS row_count\nFROM\n    sys.tables t\nJOIN\n    sys.schemas s ON t.schema_id = s.schema_id\nJOIN\n    sys.indexes i ON t.object_id = i.object_id\nJOIN\n    sys.partitions p ON i.object_id = p.object_id AND i.index_id = p.index_id\nJOIN\n    sys.allocation_units a ON p.partition_id = a.container_id\nWHERE\n    t.is_ms_shipped = 0  -- Exclude system tables\n    AND i.type &lt;= 1      -- Include only heap (0) or clustered index (1)\nGROUP BY\n    s.name, t.name, p.rows\nORDER BY\n    s.name, t.name;\n</code></pre>"},{"location":"SQL/MongoDB/Sharding/","title":"Database Sharding","text":"<p>https://www.mongodb.com/features/database-sharding-explained</p> <p>Sharding is a method for distributing a single dataset across multiple databases, which can then be stored on multiple machines. </p>"},{"location":"SQL/MySQL/InnoDB/","title":"InnoDB","text":"<p>DROP a huge InnoDB table, there will be some unavoidable some downtime for the server.</p> <p>https://dev.to/jung/why-you-simply-don-t-drop-a-huge-innodb-table-in-production-18j2</p> <p>This is better: <pre><code>DELETE FROM tbl;\nOPTIMIZE TABLE tbl;\nDROP TABLE tbl;\n</code></pre></p>"},{"location":"SQL/MySQL/PerconaToolkit/","title":"percona-toolkit","text":""},{"location":"SQL/MySQL/PerconaToolkit/#pt-online-schema-change-with-partitions","title":"pt-online-schema-change with partitions","text":"<p>Will create triggers to synchronize the changes between old and new tables. It might be slow but the downtime is minimized.</p> <p>https://www.percona.com/blog/2015/09/09/testing-mysql-partitioning-pt-online-schema-change/</p> <pre><code>#direct command\nALTER TABLE test.tbl PARTITION BY HASH(id) partitions 4;\n\n#pt-online\npt-online-schema-change h=localhost, D=test, t=tbl --recursion-method none --execute \n--alter \"PARTITION BY HASH(id) partitions 4\"\n</code></pre> <p>For testing, if we want to 1) keep the original table; 2) do not swap the tables; 3) do not drop the triggers, we should use <pre><code>pt-online-schema-change h=localhost, D=test, t=tbl --recursion-method none --execute \n--no-swap-tables --no-drop-old-table --no-drop-new-table --no-drop-triggers  \n--alter \"PARTITION BY HASH(id) partitions 4\"\n</code></pre></p>"},{"location":"SQL/MySQL/adm/","title":"Admin","text":"<p>by default, mysql8 enabled -log-bin, can be disabled by skip-log-bin</p> <pre><code>--drop db\nDROP DATABASE [IF EXISTS] db_name;\n--drop table\nDROP TABLE IF EXISTS table1, table2;\n</code></pre>"},{"location":"SQL/MySQL/adm/#restart","title":"restart","text":"<p>mysql did not release memory, restart can release 1.5GB</p>"},{"location":"SQL/MySQL/adm/#check-version","title":"check version","text":"<pre><code>SHOW VARIABLES LIKE '%version%';\n\nshow create table my_table_name;\n</code></pre>"},{"location":"SQL/MySQL/adm/#log-timestamp","title":"log timestamp","text":"<p>change report time to local   [mysqld]   log_timestamps = SYSTEM</p>"},{"location":"SQL/MySQL/adm/#change-root-password","title":"change root password","text":"<p>in inti.txt add:\\ SET PASSWORD FOR 'root'@'localhost' = PASSWORD('MyNewPass');</p> <p>in cmd:\\ mysqld --init-file=C:\\init.txt --console</p> <p>update user set authentication_string=password('password') where user='root'; </p>"},{"location":"SQL/MySQL/adm/#dump-and-reload-method","title":"Dump and Reload Method","text":"<p>when dump all databases, please remember to exclude the system dbs, otherwise you will have issue to log into the server. <pre><code>--all dbs\nmysqldump --all-databases &gt; dump.sql\nmysql &lt; dump.sql\n\n--all tables\nmysqldump db_name &gt; dump.sql\nmysql db_name &lt; dump.sql\n\n--one table\nmysqldump db_name t1 &gt; dump.sql\nmysql db_name &lt; dump.sql\n\n--optimized\nmysqldump --opt mydatabase &gt; dump.sql\n\n--disable tables locking during the dump in InnoDB\nmysqldump dbname --single-transaction &gt; dump.sql\n\n--restore another way\nmysql -uroot -p --default-character-set=utf8 foo\nmysql&gt; SET names 'utf8'\nmysql&gt; SOURCE foo.dump\n</code></pre></p>"},{"location":"SQL/MySQL/adm/#rename-database-name","title":"Rename database name","text":"<pre><code>mysqldump -u username -P 3306 -p -R oldDbname &gt; oldDbname.sql\nmysqladmin -u username -P 3306 -p create newDbname\nmysql -u username -P 3306 -p newDbname &lt; oldDbname.sql\n</code></pre>"},{"location":"SQL/MySQL/adm/#repair-table-method","title":"REPAIR TABLE Method","text":"<p>the REPAIR TABLE method is only applicable to MyISAM, ARCHIVE, and CSV tables <pre><code>CHECK TABLE t1;\nREPAIR TABLE t1;\nmysqlcheck db_name [tbl_name]\nmysqlcheck --repair --databases db_name\nmysqlcheck --repair --all-databases\n</code></pre></p>"},{"location":"SQL/MySQL/adm/#repair-key","title":"repair key","text":"<p>Error: Incorrect key file for table '.\\db\\tbl_name.MYI'; try to repair it</p> <pre><code>REPAIR TABLE tbl_name USE_FRM;\n</code></pre> <p>The USE_FRM option is available for use if the .MYI index file is missing or if its header is corrupted. This option tells MySQL not to trust the information in the .MYI file header and to re-create it using information from the .frm file. This kind of repair cannot be done with myisamchk</p>"},{"location":"SQL/MySQL/adm/#save-a-few-db-tables","title":"save a few db tables","text":"<pre><code>@echo off\n\nset db=test\nset tbl=xxx_tbl\nset file=\"C:\\db_tbl.sql\"\n\nset mysqlexe=\"C:\\Program Files\\MySQL\\MySQL Server 5.7\\bin\\mysql.exe\"\nset mysqldumpexe=\"C:\\Program Files\\MySQL\\MySQL Server 5.7\\bin\\mysqldump.exe\"\n\nrem Dump db tables to sql file\n%mysqldumpexe% -h sql.server -P 3306 -u usr -p %1 %db% %tbl% &gt; %file%\n\necho all done!\n</code></pre>"},{"location":"SQL/MySQL/adm/#copy-db-to-another-server","title":"copy db to another server","text":"<pre><code>@echo off\n\nset dbnew=new_db\nset dbold=old_db\nset dbfile=\"C:\\db.sql\"\n\nset mysqlexe=\"C:\\Program Files\\MySQL\\MySQL Server 5.7\\bin\\mysql.exe\"\nset mysqldumpexe=\"C:\\Program Files\\MySQL\\MySQL Server 5.7\\bin\\mysqldump.exe\"\n\nrem Dump db to sql file\n%mysqldumpexe% -h sql.server -P 3306 -u usr -p %1 %dbold% &gt; %dbfile%\nrem only dump structure: %mysqldumpexe% -d -h sql.server -P 3306 -u usr -p %1 %dbold% &gt; %dbfile%\nrem Create an empty db\n%mysqlexe% -h sql.server -P 3306 -u usr -p %1 -e \"create database if not exists %dbnew%;\"\nrem Import the db\n%mysqlexe% -h sql.server -P 3306 -u usr -p %1 %dbnew% &lt; %dbfile%             \n\necho all done!\n</code></pre>"},{"location":"SQL/MySQL/adm/#alter-table-performance","title":"alter table performance","text":"<p>https://www.percona.com/forums/questions-discussions/mysql-and-percona-server/957-alter-table-performance-with-myisam</p> <p>The workaround which I found so far is really ugly, however I've seen users using it with good success.    - You can create table of the same structure without keys,    - Load data into it to get correct .MYD,    - Create table with all keys defined and copy over .frm and .MYI files from it,   - followed by FLUSH TABLES.    - Now you can use REPAIR TABLE to rebuild all keys by sort, including UNIQUE keys.</p>"},{"location":"SQL/MySQL/adm/#check-database-table-size","title":"check database / table size","text":"<pre><code>--sizes of all of dbname\nSELECT engine, table_schema AS db, \n       SUM(index_length) / 1024 / 1024 / 1024 AS ind_GB,\n       SUM(data_length) / 1024 / 1024 / 1024 AS dat_GB,\n       SUM(data_free) / 1024 / 1024 / 1024 AS dfr_GB\nFROM information_schema.TABLES \nwhere table_Type = 'BASE TABLE' and (TABLE_SCHEMA != 'sys' and TABLE_SCHEMA != 'mysql')\nGROUP BY table_schema\norder by dat_GB;\n\n--size of one database\nSELECT table_schema as dbname,\n       ROUND(SUM(data_length + index_length) / 1024 / 1024 / 1024, 2) as size_GB  \nFROM information_schema.tables \nwhere table_schema = 'database_name'\nGROUP BY table_schema; \n\n--sizes of all tables in a specific database\nSELECT table_name AS tb,\n(data_length / 1024 / 1024 / 1024) AS dat_GB, \n(index_length / 1024 / 1024 / 1024) AS ind_GB \nFROM information_schema.TABLES\nWHERE table_schema = 'database_name'\nORDER BY (data_length + index_length) DESC;\n</code></pre>"},{"location":"SQL/MySQL/adm/#list-tables-with-a-specific-colname","title":"list tables with a specific colname","text":"<pre><code>SELECT t.TABLE_NAME, t.data_length/1024/1024/1024 as dsz, t.index_length/1024/1024/1024 as isz\nFROM INFORMATION_SCHEMA.tables as t\nWHERE t.TABLE_SCHEMA = 'mydb' and t.table_type = 'BASE TABLE'\nand t.table_name in \n(\n  SELECT distinct c.TABLE_NAME\n  FROM INFORMATION_SCHEMA.COLUMNS as c\n  WHERE c.COLUMN_NAME = 'mycol' AND c.TABLE_SCHEMA = 'mydb' \n)\nORDER BY (t.data_length + t.index_length) asc;\n</code></pre>"},{"location":"SQL/MySQL/adm/#check-database-last-update-time","title":"check database last update time","text":"<pre><code>SELECT table_schema as db, max(update_time) as upd_time\nFROM   information_schema.tables\nWHERE  table_schema = 'mydb'\ngroup by table_schema;\n</code></pre>"},{"location":"SQL/MySQL/alter/","title":"alter table","text":"<p>directly using the alter command is very slow.</p> <pre><code>-- only for MYISAM\ncreate table tbl_tmp like tbl;\nalter table tbl_tmp add column colx int not null;\nalter table tbl_tmp disable keys;\ninsert into tble_tmp select *,0 from tbl;\nalter table tbl_tmp enable keys;\nrename table tbl to tbl_old, tbl_tmp to tbl;\n</code></pre>"},{"location":"SQL/MySQL/alter/#alter-table-method","title":"ALTER TABLE Method","text":"<pre><code>Alter TABLE my_table_name \nAdd fd1 float NULL, \nadd fd2 float NULL;\n\nALTER TABLE my_table_name \nMODIFY fld INTEGER [not null];\n\nALTER TABLE my_table_name \nMODIFY COLUMN name VARCHAR(255);\n\nALTER TABLE my_table_name\nADD INDEX `fld_name` (`fld_name`)\n\nalter table my_table_name \ndrop primary key, add primary key(k1, k2, k3);\n</code></pre>"},{"location":"SQL/MySQL/backup/","title":"backup","text":"<ul> <li>disable slow log</li> <li>change tmp dir to a large drive</li> <li>change repair params to large values</li> </ul>"},{"location":"SQL/MySQL/backup/#compress","title":"compress","text":"<p>Defragmentation will not only recover space, it will also help the queries run faster. After dumping and import the data can become larger, compress it <pre><code>optimize table tb1, tb2\nmysqlcheck -o db tb #mysqlcheck uses \u201cOPTIMIZE TABLE\u201d command\nmysqlcheck -o db    #all tbs\nmysqlcheck -o -A    #all dbs\n</code></pre></p>"},{"location":"SQL/MySQL/backup/#check-process","title":"check process","text":"<p>select * from INFORMATION_SCHEMA.PROCESSLIST where db = 'dbname';</p>"},{"location":"SQL/MySQL/backup/#log-slow-queries","title":"log-slow-queries","text":"<p>If you are importing using: <code>mysql -u user -p db_name &lt; db.sql</code>, check that you do NOT have \"log-slow-queries\" turned on. <pre><code>show global variables like '%slow%';\nset global slow_query_log = 'OFF';\n</code></pre></p>"},{"location":"SQL/MySQL/backup/#get-all-tables-in-db","title":"get all tables in db","text":"<pre><code>select table_name as 'tbl'\nfrom information_schema.tables\nwhere table_schema = 'database_name';\n</code></pre>"},{"location":"SQL/MySQL/backup/#lost-connection-during-query-when-dumping","title":"Lost connection during query when dumping","text":"<p>possible solutions, increase the variables values: <pre><code>mysqldump --net-read-timeout=7200 --net-write-timeout=7200\n          --net-buffer-length=32704 --max_allowed-packet=1024M\n</code></pre></p> <p>use --compress (does not work): <pre><code>mysqldump --defaults-extra-file=$dir/svr1.cnf $db $tb |\nmysql --defaults-extra-file=$dir/svr2.cnf --compress $db\n</code></pre></p>"},{"location":"SQL/MySQL/backup/#dump-to-tsv","title":"dump to tsv","text":"<p>when using LOAD DATA INFILE, the character set should be set to the character set of the source csv file. This again should be the same as the character set in the definition of the table. So when using mysqldump to create the csv files, the character set should also be set to the character set in the table.</p> <p>get table character set: <pre><code>SHOW FULL COLUMNS FROM db.tb;\n</code></pre></p> <pre><code>mysqldump -h host -P port -u user -ppass --tab=/dir mydb mytable\n$ mysql -h localhost -u root -p\nmysql&gt; LOAD DATA INFILE '/tmp/mytable.txt' INTO TABLE mytable;\n\nset qry=USE db1;LOAD DATA LOCAL INFILE '!file!' INTO TABLE !table!\nset pa1=FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"^\"\"'\nset pa2=LINES TERMINATED BY '\\n' character-set latin1 IGNORE 1 LINES;\nset sql_qry=%qry% %pa1% %pa2%\nmysql -u root --password=%password% -e \"sql_qry\" --verbose --show-warnings\n</code></pre>"},{"location":"SQL/MySQL/backup/#dump-to-csv","title":"dump to csv","text":"<p>LOAD DATA INFILE requires txt data path in the server data folder\\ LOAD DATA LOCAL INFILE is slower as a copy has to be made to the server disk <pre><code>mysqldump -h host -P port -u user -ppass --tab=/dir mydb mytable\nmysqldump --tab=/tmp --fields-terminated-by=, --fields-enclosed-by='\"' --lines-terminated-by=0x0d0a db1\n$ mysql -h localhost -u root -p\nmysql&gt; LOAD DATA INFILE '/tmp/mytable.txt' INTO TABLE mytable;\n\nLOAD DATA INFILE 'E:\\\\wamp\\\\tmp\\\\customer.csv' INTO TABLE customer\nCHARACTER SET 'utf8'\nFIELDS TERMINATED BY ',' ENCLOSED BY '\"'\nLINES TERMINATED BY '\\r\\n'\nIGNORE 1 LINES;\n</code></pre></p> <p>This does not work in windows: <pre><code>mysqldump --defaults-extra-file=D:\\sqlbackup\\sqlsvr.cnf --tab=\"D:\\sqlbackup\\dump_files\" db tb --fields-terminated-by=',' --fields-optionally-enclosed-by='\"' --fields-escaped-by='\\' --lines-terminated-by='\\n'\n</code></pre> Must use hex in windows <pre><code>mysqldump --defaults-extra-file=D:\\sqlbackup\\sqlsvr.cnf --tab=\"D:\\sqlbackup\\dump_files\" db tb --fields-terminated-by=0x2C --fields-optionally-enclosed-by=0x22 --fields-escaped-by=0x5C --lines-terminated-by=0x0a\n</code></pre></p>"},{"location":"SQL/MySQL/backup/#dump-db-with-each-table-in-a-separate-file","title":"dump db with each table in a separate file","text":"<p>redirect the query to another database <pre><code>#!/bin/bash\n\nfdb=dump_dbs.txt\ndir=\"/media/data/test\"\ntyp=\"BASE TABLE\"\n\ncreate_list=$(( $#==0 ? 1 : 0 ))\n\ntimestamp() {\n    date +\"%Y-%m-%d %H:%M:%S\"\n}\n\ni=0\necho $(timestamp) dump dbs\nwhile read db; do\n    ((++i))\n    ftb=$dir/$db\"_tbs.txt\"\n    if [ $create_list -eq 1 ]; then\n    qry='SELECT table_name FROM information_schema.tables WHERE table_schema='\\'${db}\\'' and table_type='\\'${typ}\\'' order by data_length asc;'\n        mysql --defaults-extra-file=$dir/svr_fr.cnf --skip-column-names -e \"${qry}\" &gt;$ftb\n    fi\n    echo $(timestamp) \"  \" db: $db\n    j=0\n    while read tb; do\n        ((++j))\n        echo $(timestamp) \"    \"$j table: $tb\n        mysqldump --defaults-extra-file=$dir/svr_fr.cnf $db $tb | mysql --defaults-extra-file=$dir/svr_to.cnf $db\n    done &lt;$ftb\ndone &lt;$fdb\n\necho $(timestamp) all done!\n</code></pre></p>"},{"location":"SQL/MySQL/backup/#dump-db-with-each-table-in-a-separate-file_1","title":"dump db with each table in a separate file","text":"<pre><code>#!/bin/bash\n\nfdb=fdt_dbs.txt\ndir=\"/media/data/test\"\nop1=\"--tab=$dir\"\nop2=\" --default-character-set=latin1 --fields-terminated-by=\\t --lines-terminated-by=\\r\\n\"\n\ncreate_list=$(( $#==0 ? 1 : 0 ))\n\ntimestamp() {\n  date +\"%Y-%m-%d %H:%M:%S\"\n}\n\necho $(timestamp) dump dbs\ni=0\nwhile read db; do\n    ((++i))\n    ftb=$dir/$db\"_tbs.txt\"\n    if [ $create_list -eq 1 ]; then\n        qry='select table_name from information_schema.tables where table_schema='\\'${db}\\'' order by table_name;'\n        mysql --defaults-extra-file=$dir/sqlsvr.cnf --skip-column-names -e \"${qry}\" &gt; $ftb\n    fi\n    echo $(timestamp) \"  \" db: $db\n    j=0\n    while read tb; do\n        ((++j))\n        echo $(timestamp) \"    \"$j table: $tb\n        mysqldump --defaults-extra-file=$dir/sqlsvr.cnf $op1 $op2 $db $tb\n        mv $tb\".sql\" $db\"_\"$tb\".sql\"\n        mv $tb\".txt\" $db\"_\"$tb\".txt\"\n    done &lt;$ftb\ndone &lt;$fdb\n\necho $(timestamp) all done!\n</code></pre>"},{"location":"SQL/MySQL/backup/#dump-db-with-each-table-in-a-separate-file_2","title":"dump db with each table in a separate file","text":"<pre><code>@echo off\n\nsetlocal EnableDelayedExpansion\n\nset fdb=fdt_dbs.txt\nset dirwd=%~dp0\nset dirwn=C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data\\#dump_tmp\nset dirto=%dirwn:\\=/%\nset dirfr=\\\\sql.server\\test\nset par=CHARACTER SET latin1 FIELDS TERMINATED BY '\\t' LINES TERMINATED BY '\\r\\n';\n\nrem net use \"\\\\sql.server\" pwd /user:workgroup\\usr\n\necho %dirto%/%fdb%\n\necho %date% %time% load dbs\nrem copy /z %dirfr%\\%fdb% %dirto%\\%fdb%\nset i=0\nfor /f \"usebackq\" %%a in (\"%dirto%/%fdb%\") do (\n    set /a i=!i!+1\n    echo !date! !time!  !i! db: %%a\n    rem copy /z %dirfr%\\%%a_tbs.txt %dirto%\\%%a%_tbs.txt\n    set j=0\n    for /f \"usebackq\" %%b in (\"%dirto%/%%a_tbs.txt\") do (\n        set /a j=!j!+1\n        echo !date! !time!    !j! table: %%b\n        rem copy /z %dirfr%\\%%a_%%b.sql %dirto%\\%%a_%%b.sql\n        mysql --defaults-extra-file=%dirwd%svr_to.cnf %%a &lt; \"%dirto%/%%a_%%b.sql\"\n        rem copy /z %dirfr%\\%%a_%%b.txt %dirto%\\%%a_%%b.txt\n        set sql_qry=USE %%a;LOAD DATA INFILE '!dirto!/%%a_%%b.txt' INTO TABLE %%b %par%\n        mysql --defaults-extra-file=%dirwd%sql_server.cnf -e \"!sql_qry!\" --show-warnings\n    )\n)\n\nrem net use \"\\\\xx-xx.xx.xx\" /delete\n\necho %date% %time% all done!\n</code></pre>"},{"location":"SQL/MySQL/backup/#disbale-key-checking","title":"disbale key checking","text":"<p>It sounds like your import is slowing down--processing fewer rows per second--as it progresses. That probably means MySQL is checking each new row to see whether it has key-conflicts with the rows already inserted.</p> <p>A few things you can do:</p> <p>Before starting, disable key checking:   SET FOREIGN_KEY_CHECKS = 0;   SET UNIQUE_CHECKS = 0;</p> <p>After ending restore your key checking:   SET UNIQUE_CHECKS = 1;   SET FOREIGN_KEY_CHECKS = 1;</p> <p>mysqldump can be made to create a dump with that disables keys:\\   mysqldump --disable-keys Similarly,   mysqldump --extended-insert --no-autocommit will make the dumped sql file contain a variant of my suggestion about using transactions.</p> <p>https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html#option_mysqldump_disable-keys</p> <p>For each table, surround the INSERT statements with /!40000 ALTER TABLE tbl_name DISABLE KEYS /; and /!40000 ALTER TABLE tbl_name ENABLE KEYS /; statements. This makes loading the dump file faster because the indexes are created after all rows are inserted. This option is effective only for nonunique indexes of MyISAM tables. By default, the key checking is disabled.</p>"},{"location":"SQL/MySQL/bakcode/","title":"backup code","text":""},{"location":"SQL/MySQL/bakcode/#mysql-extra-config","title":"mysql extra config","text":"<p>```cnf frto.cnf [client] host=xxx.xxx.xxx port=3306 user=usr password=pwd</p> <p>[mysqld] connect_timeout = 7200 net_read_timeout = 7200 net_write_timeout = 7200</p> <p>[mysql] net_buffer_length=32704 max_allowed_packet = 512M <pre><code>## backup databases\n```sh dts.sh\n#!/bin/bash\n\n#./dts.sh r 2&gt;&amp;1 | tee backup_db_dtr.log\n\n[ $# -eq 0 ] &amp;&amp; ty=\"s\" || ty=\"r\"\ncreate_list=$(( $#==0 ? 1 : 0 ))\n\nfdb=\"db\"$ty\"_dump.txt\"\ndir=\"/media/data/test\"\ntyp=\"BASE TABLE\"\n\ntimestamp() {\n    date +\"%Y-%m-%d %H:%M:%S\"\n}\n\ni=0 \necho $(timestamp) dump db$ty\nwhile read db; do\n    ((++i))  \n    ftb=$dir/\"tb\"$ty\"_\"$db\".txt\"\n    if [ $create_list -eq 1 ]; then\n    qry='SELECT table_name FROM information_schema.tables WHERE table_schema='\\'${db}\\'' and table_type='\\'${typ}\\'' order by data_length asc;'\n        mysql --defaults-extra-file=$dir/fr.cnf --skip-column-names -e \"${qry}\" &gt;$ftb\n    fi    \n    echo $(timestamp) \"  \" db: $db\n    j=0 \n    while read tb; do\n        ((++j))\n        echo $(timestamp) \"    \"$j table: $tb \n        mysqldump --defaults-extra-file=$dir/fr.cnf $db $tb | mysql --defaults-extra-file=$dir/to.cnf $db\n    done &lt;$ftb \ndone &lt;$fdb\n\necho $(timestamp) all done!\n</code></pre></p>"},{"location":"SQL/MySQL/bakcode/#backup-a-table","title":"backup a table","text":"<p>```sh dtr.sh</p>"},{"location":"SQL/MySQL/bakcode/#binbash","title":"!/bin/bash","text":""},{"location":"SQL/MySQL/bakcode/#dtrsh-db-21-tee-backup_xxx_db_dtrlog","title":"./dtr.sh db 2&gt;&amp;1 | tee backup_xxx_db_dtr.log","text":"<p>dir=\"/media/data/test\" db=$1</p> <p>timestamp() {     date +\"%Y-%m-%d %H:%M:%S\" }</p> <p>echo $(timestamp) dump dbr ftb=$dir/$db\"_tbr.txt\"   echo $(timestamp) \"  \" db: $db j=0  while read tb; do     ((++j))     echo $(timestamp) \"    \"$j table: $tb      mysqldump --defaults-extra-file=$dir/fr.cnf $db $tb | mysql --defaults-extra-file=$dir/to.cnf $db done &lt;$ftb </p> <p>echo $(timestamp) all done! ```</p>"},{"location":"SQL/MySQL/basic/","title":"Basic","text":""},{"location":"SQL/MySQL/basic/#delete-table-data","title":"Delete table data","text":"<pre><code>TRUNCATE TABLE my_table_name;\n</code></pre>"},{"location":"SQL/MySQL/basic/#create-db","title":"create db","text":"<pre><code>CREATE SCHEMA IF NOT EXISTS `dbname`;\nCREATE TABLE tbl_new LIKE tbl_old;\n</code></pre>"},{"location":"SQL/MySQL/basic/#copy-table","title":"copy table","text":"<pre><code>--copy structure and data\ncreate table tbl_new as select * from tbl_old;\n--copy structure with indexes and triggers\ncreate table tbl_new LIKE tbl_old; \ninsert into tbl_new select * from tbl_old;\n</code></pre>"},{"location":"SQL/MySQL/basic/#show-dbstabls","title":"show dbs/tabls","text":"<pre><code>SHOW SCHEMAS;\nSHOW DATABASES;\n\nSELECT table_name as tbl,\nFROM information_schema.TABLES\nWHERE table_schema = 'db_name'\nORDER BY table_name;\n</code></pre> <pre><code>--delete all from table\ndelete from db.tbl;\n\n--insert into another table\ninsert ignore into db.t2\nSELECT * \nFROM db.t1\nwhere datecol between '2017-06-30 00:00:00' and '2018-07-02 00:00:00';\n</code></pre>"},{"location":"SQL/MySQL/basic/#emulate-row_number","title":"Emulate ROW_NUMBER","text":"<pre><code>--add a row number for each row, reset it to 1 when customer number changes\nSELECT \n    @row_number:=CASE\n        WHEN @customer_no = customerNumber THEN @row_number + 1\n        ELSE 1\n    END AS num,\n    @customer_no:=customerNumber as CustomerNumber,\n    paymentDate,\n    amount\nFROM\n    payments,(SELECT @customer_no:=0,@row_number:=0) as t\nORDER BY customerNumber;\n</code></pre>"},{"location":"SQL/MySQL/basic/#insert-ignore-vs-replace","title":"insert ignore vs replace","text":"<p>insert ignore - if key/row exists or error, skip insertion</p> <p>replace - if key/row exists, delete the match row, and insert again; ON DELETE CASCADE will cause significant issues</p> <pre><code>INSERT INTO t1 (a,b,c) \nVALUES (1,2,3),(4,5,6)\n  ON DUPLICATE KEY \nUPDATE c=VALUES(a)+VALUES(b);\n</code></pre>"},{"location":"SQL/MySQL/basic/#group_concat-fields","title":"group_concat fields","text":"<pre><code>--default limit is 1024 characters\nSET group_concat_max_len=100000000\n--syntax\nGROUP_CONCAT([DISTINCT] expr [,expr ...]\n             [ORDER BY {unsigned_integer | col_name | expr}\n                 [ASC | DESC] [,col_name ...]]\n             [SEPARATOR str_val])\n--example             \nSELECT student_name,\n     GROUP_CONCAT(DISTINCT test_score \n                  ORDER BY test_score DESC SEPARATOR ';')\nFROM student\nGROUP BY student_name;             \n</code></pre>"},{"location":"SQL/MySQL/basic/#select-row-with-max-value-for-one-field-and-group-by-another-field","title":"Select row with max value for one field and group by another field","text":"<pre><code>--good solution\nSELECT t.nam, t.val, t.cnt\nFROM tbl as t\nINNER JOIN \n( SELECT nam, MAX(cnt) AS max_cnt\n  FROM tbl\n  GROUP BY nam \n) AS m\nON t.nam = m.nam AND t.cnt = m.max_cnt;\n</code></pre>"},{"location":"SQL/MySQL/deadlock/","title":"Deadlock","text":"<pre><code>--to see the last deadlock in an InnoDB user transaction\nSHOW ENGINE INNODB STATUS\n\n--enabled setting to print information about all deadlocks to the mysqld error log\ninnodb_print_all_deadlocks\n</code></pre>"},{"location":"SQL/MySQL/err/","title":"error","text":""},{"location":"SQL/MySQL/err/#the-host-does-not-support-ssl-connections","title":"the host does not support ssl connections","text":"<p>add 'SslMode=none'</p>"},{"location":"SQL/MySQL/err/#table-is-marked-as-crashed-and-should-be-repaired","title":"table is marked as crashed and should be repaired","text":"<p>https://www.a2hosting.com/kb/developer-corner/mysql/repairing-mysql-databases-and-tables</p> <p>This error can appear at any time, especially after a forced shutdown of MySQL database or due to the crash of the entire server.</p> <p>repair the table   mysqlcheck -u usr -ppsw -r db tb</p> <p>Recovering MySQL Crashed Tables:   # myisamchk -s /var/lib/mysql//.MYI Running this command will list the crashed MySQL tables, with a message as follows:   MyISAM-table '/var/lib/mysql/dbname/table_name.MYI' is marked as crashed and should be repaired</p>"},{"location":"SQL/MySQL/err/#unable-to-connect-to-any-of-the-specified-mysql-hosts","title":"Unable to connect to any of the specified MySQL hosts","text":"<p>https://www.codeproject.com/Questions/728680/Unable-to-connect-to-any-of-the-specified-MySQL-2</p> <p>change server to localhost OK!</p> <p>The error you are getting indicates it can't resolve the host name which is a dns issue. Whatever the server name in your connection string is, the name cannot be resolved.</p> <p>If the database is on the same machine as the web server you can use localhost for the server name.</p> <p>if its on a different machine you can try to ping it by name and it will probably give you the same error if it can't resolve the name. If you know the ip address of the machine with MySQL, you can edit your hosts file and add a row like</p> <p>192.168.0.x hostname</p> <p>On windows your hosts file should be in Windows\\System32\\Drivers\\etc folder</p>"},{"location":"SQL/MySQL/err/#wrong-query-results","title":"wrong query results","text":"<p>select * from db.tbl where datecol = '2020-04-30 07:05:00' and namecol = 'John';</p> <p>Repair the index fixed the issue</p>"},{"location":"SQL/MySQL/group/","title":"group","text":"<pre><code>--simulation time statistics\nselect t.sim_name, t.sim_id, avg(t.simtime_minutes), min(t.simtime_minutes), max(t.simtime_minutes) \nfrom (\n    select sim_name, sim_id, sim_start, sim_end, (time_to_sec(sim_start) - time_to_sec(sim_end))/60 as simtime_minutes\n    from db.sim\n    having simtime_minutes &gt; 1\n    order by sim_name, simtime_minutes\n) as t\ngroup by t.sim_name;\n</code></pre>"},{"location":"SQL/MySQL/io/","title":"IO","text":""},{"location":"SQL/MySQL/io/#loadsave-fromto-filetable","title":"load/save from/to file/table","text":"<pre><code>#load file to table\nLOAD DATA INFILE 'data.csv' INTO TABLE my_db.my_table;\n\n#save table to file\nSELECT a,b,a+b \nINTO OUTFILE '/tmp/result.csv'\n  FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"'\n  LINES TERMINATED BY '\\n'\nFROM test_table;\n</code></pre> <p>the file path:\\ /var/lib/mysql/\\ .\\bin\\mysql\\mysql5.6.17\\data\\db\\file.csv"},{"location":"SQL/MySQL/io/#csv-to-table","title":"csv to table","text":"<p>mysql \u2013u username \u2013p --local-infile somedatabase</p> <pre><code>USE database_name;\nLOAD DATA LOCAL INFILE '/home/export_file.csv'\nINTO TABLE table_name\nFIELDS TERMINATED BY ','\nENCLOSED BY '\"'\nLINES TERMINATED BY '/n'\nIGNORE 1 ROWS;\n</code></pre>"},{"location":"SQL/MySQL/keyrepair/","title":"key repair","text":"<p>Two modes for key repair: repair by Sorting (quick) or by Keycache (very slow)</p> <p>\"Repair by sorting\" doesn't work for UNIQUE or PRIMARY KEYs</p> <p>\"Repair by sorting\" uses the filesort routine, which in turn creates several temporary files (usually) in your tmpdir.</p> <p>If your tmpdir does not have enough space for them, it will revert to \"Repair by keycache\". This is extremely bad as it's much slower AND creates less optimal indexes.</p> <p>It chooses which method to use based on:   * myisam_sort_buffer_size,    * myisam_max_sort_file_size, and   * myisam_max_extra_sort_file_size [deprecated in v5] These are SESSION variables, so they can be set on the connection right before the LOAD DATA INFILE.</p> <p>The table can be repaired by sorting provided it meets the following requirements:</p> <ul> <li>table at least has one key (index)</li> <li>total size needed for individual key is less than myisam_max_sort_file_size</li> <li>tmpdir has enough spare space</li> </ul> <p>If it meets the above requirements, then it uses either regular sorting if myisam_repair_threads  = 1 (default) by building each key at a time or in parallel if myisam_repair_threads &gt; 1 by using \u2018n\u2019 threads in parallel (n = total keys in the table). If you have a table with more than one key and table needs a frequent key rebuild, then setting myisam_repair_threads = 2 can speedup the repair/alter process.</p> <p>If the table fails to satisfy the above two conditions, then it falls to expensive keycache repair mode.</p> <p>myisam_sort_buffer_size can be up to 90% of the system RAM.</p> <p>SET SESSION myisam_sort_buffer_size = 50010241024;</p> <p>Note that sort_buffer_size is per secession so should be too large. The default is 2 MB or 256 kB. Someone said that 256 is better. </p>"},{"location":"SQL/MySQL/keyrepair/#repair-keys","title":"repair keys","text":"<pre><code>@echo off\n\nsetlocal EnableDelayedExpansion\n\nset db=mydb\nset file=tbl_repair.txt\n\nset i=0\necho %date% %time% repair tbl\necho %date% %time% db: %db%\nfor /f %%a in (%file%) do (\n    set /a i=!i!+1\n    echo !date! !time!   !i! table: %%a \n    \"C:\\MySQL\\MySQLServer8.0\\bin\\mysqlcheck\" --defaults-extra-file=sql_svr.cnf -r %db% %%a\n)\n\necho %date% %time% all done!\n</code></pre>"},{"location":"SQL/MySQL/mem/","title":"memory","text":""},{"location":"SQL/MySQL/mem/#mysqld-high-ram","title":"mysqld high RAM","text":"<p>This will show the memory usage   select * from sys.memory_global_by_current_bytes;</p> <p>key_catch used 2GB memory after table repair because key_buffer_size is 2GB. Reduce key_buffer_size to a lower value   SET GLOBAL key_buffer_size = 1024 * 1024 * 1024;</p>"},{"location":"SQL/MySQL/myisam2innodb/","title":"MyISAM to InnoDB","text":""},{"location":"SQL/MySQL/myisam2innodb/#swap-tables","title":"swap tables","text":"<pre><code>RENAME TABLE old_table TO tmp_table, new_table TO old_table, tmp_table TO new_table;\n</code></pre>"},{"location":"SQL/MySQL/myisam2innodb/#alter-table","title":"alter table","text":"<pre><code>ALTER TABLE db.tbl\nDROP PRIMARY KEY,\nADD PRIMARY KEY (`dt`,`id`) USING BTREE,\nENGINE = InnoDB\nPARTITION BY RANGE (YEAR(`DATETIME`)) (\n  PARTITION l2016 VALUES LESS THAN (2016),\n  PARTITION l2019 VALUES LESS THAN (2019),\n  PARTITION l2021 VALUES LESS THAN (2021),\n  PARTITION l2022 VALUES LESS THAN (2022)\n);\n</code></pre>"},{"location":"SQL/MySQL/myisam2innodb/#dump-data-to-csv-file","title":"dump data to csv file","text":"<pre><code>@echo off\n\nset db=%1\nset tb=%2\n\nset curdir=%~dp0\nset tbdir=\"C:/ProgramData/MySQL/MySQL Server 8.0/Data/tmp\"\n\nset par=--fields-terminated-by=0x2C --fields-optionally-enclosed-by=0x22 --fields-escaped-by=0x5C --lines-terminated-by=0x0a\nset dumpexe=\"C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin\\mysqldump.exe\"\n\necho %date% %time% table: %db%.%tb%\n%dumpexe% --defaults-extra-file=%curdir%sql_svr.cnf --tab=%tbdir% %db% %tb% %par%\necho %date% %time% all done!\n</code></pre>"},{"location":"SQL/MySQL/myisam2innodb/#load-csv-file-to-innodb","title":"load csv file to InnoDB","text":"<pre><code>@echo off\n\nset db=%1\nset tb=%2\n\nset curdir=%~dp0\nset tbdef=C:/ProgramData/MySQL/MySQL Server 8.0/Data/tmp/%tb%.sql\nset tbfile=C:/ProgramData/MySQL/MySQL Server 8.0/Data/tmp/%tb%.txt\n\nset par=CHARACTER SET latin1 FIELDS TERMINATED BY 0x2C OPTIONALLY ENCLOSED BY 0x22 ESCAPED BY 0x5C LINES TERMINATED BY 0x0a\nset sqlexe=\"C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin\\mysql.exe\"\n\necho %date% %time% table: %db%.%tb%_myisam\n\n%sqlexe% --defaults-extra-file=%curdir%sql_svr.cnf %db% &lt; \"%tbdef%\"\necho %date% %time% created innodb table with partitions\n\nset sql_qry=\"LOAD DATA INFILE '%tbfile%' INTO TABLE %db%.%tb%_myisam %par%\"\n%sqlexe% --defaults-extra-file=%curdir%sql_svr.cnf --local-infile -e %sql_qry% --show-warnings\necho %date% %time% loaded data to new innodb table\n\necho %date% %time% all done!\n</code></pre>"},{"location":"SQL/MySQL/other/","title":"other","text":"<p>start mysql server: C:&gt; \"C:\\Program Files\\MySQL\\MySQL Server 5.7\\bin\\mysqld\" --console</p>"},{"location":"SQL/MySQL/other/#basic","title":"basic","text":"<pre><code>show full processlist;\n\nshow variables like \"%timeout%\";\n\n--full-text search\nSELECT * FROM schema.table\nWHERE MATCH (reason) AGAINST ('x-y-z' IN BOOLEAN MODE);\n\n--command line\nmysql -h host -P port -u user -p db\n</code></pre>"},{"location":"SQL/MySQL/other/#dba","title":"DBA","text":"<pre><code>--mysql server version\nSHOW VARIABLES LIKE '%version%';\n\n--show tables in db\nuse dbname;\nshow tables;\n\n--show table index\nSHOW INDEX FROM mytable;\n\n--show column info\n--obtain information about table structure\ndescribe tbl;\n--displays information about the columns in a given table. It also works for views\nshow columns from tbl;\n\n--get tbl size in db\nSELECT table_name AS 'Table',\n       ROUND((data_length + index_length) / 1024 / 1024, 2) AS 'SizeMB'\nFROM information_schema.tables\nWHERE table_schema = 'db_name'\nORDER BY SizeMB DESC\n;\n\n--get db size and free space\nSELECT table_schema AS 'DBName',\n       ROUND(SUM(data_length + index_length) / 1024 / 1024, 1) AS 'SizeMB',\n       ROUND(SUM(data_free)/ 1024 / 1024, 1) AS 'FreeSpaceMB'\nFROM information_schema.tables\nWHERE table_schema = 'db_name'\nGROUP BY table_schema\n;\n\n--find db tables with specific column names\nSELECT DISTINCT TABLE_NAME\nFROM INFORMATION_SCHEMA.COLUMNS\nWHERE COLUMN_NAME IN ('column1', 'column2') AND TABLE_SCHEMA='dbname'\n;\n\n--find db with specific table name\nselect table_schema, table_name\nfrom information_schema.tables\nwhere table_name like '%xyz%'\n;\n\n--get table number of columns\nSELECT COUNT(*) AS ncl\nFROM INFORMATION_SCHEMA.COLUMNS\nWHERE table_schema = 'db' AND table_name = 'tbl'\n;\n</code></pre>"},{"location":"SQL/MySQL/param/","title":"param","text":""},{"location":"SQL/MySQL/param/#tmpdir","title":"tmpdir","text":"<p>make sure it is on a disk with enough space. Otherwise it will affect the performance of many queries.</p>"},{"location":"SQL/MySQL/param/#max_execution_time","title":"max_execution_time","text":"<p>default value is 0, means no timeout. This applies to the select queries.</p>"},{"location":"SQL/MySQL/param/#range_optimizer_max_mem_size","title":"range_optimizer_max_mem_size","text":"<p>default value 8 MB. a value of 0 means no limits. to control the memory available to the range optimizer.</p>"},{"location":"SQL/MySQL/param/#code-example","title":"code example","text":"<pre><code>rows_affected = 0\ndo {\n   rows_affected = do_query(\n      \"DELETE FROM messages WHERE created &lt; DATE_SUB(NOW(),INTERVAL 3 MONTH)\n      LIMIT 10000\")\n} while rows_affected &gt; 0\n</code></pre>"},{"location":"SQL/MySQL/partition/","title":"Partition","text":"<p>Practical Partitioning</p> <p>http://www.arubin.org/files/PracticalPartitioning_Webinar.pdf</p> <p>https://dev.mysql.com/doc/refman/8.0/en/partitioning.html</p> <p>https://dev.mysql.com/doc/refman/8.0/en/partitioning-management-range-list.html</p> <p>MySQL 8.0 does not currently support partitioning of tables using any storage engine other than InnoDB or NDB, such as MyISAM.</p> <p>Partitioning and repartitioning operations involving InnoDB tables may be made more efficient by enabling innodb_file_per_table.</p> <p>MySQL supports horizontal partitioning, which means that all rows matching the partitioning function will be assigned to different physical partitions.</p> <p>All columns used in the partition expression must be present in every unique key in the table, including the primary key (which is UNIQUE by definition). In other words, all unique keys in the table must use all the columns in the partitioning expression.</p>"},{"location":"SQL/MySQL/partition/#check-if-mysql-supports-partition","title":"check if MySQL supports partition","text":"<pre><code>SHOW PLUGINS;\n\n--or like this\nSELECT PLUGIN_NAME as Name, PLUGIN_VERSION as Version, PLUGIN_STATUS as Status\nFROM INFORMATION_SCHEMA.PLUGINS\nWHERE PLUGIN_TYPE='STORAGE ENGINE';\n</code></pre>"},{"location":"SQL/MySQL/partition/#create-partition-table","title":"create partition table","text":"<p>There are four partition types available: RANGE, LIST, HASH and KEY.</p> <p>All columns used in the partition expression must be present in every unique key in the table, including the primary key. If the table does not have any unique keys (including primary keys), any column can be used in the partitioning expression that is compatible with the partitioning type.</p> <pre><code>--range: do not add the MAXVALUE partition when you want to add more partitions later\nCREATE TABLE tbl (\n    usr VARCHAR(20) NOT NULL,\n    created DATETIME NOT NULL,\n    PRIMARY KEY(usr, created)\n)\nPARTITION BY RANGE( YEAR(created) )(\n    PARTITION l2014 VALUES LESS THAN (2014),\n    PARTITION l2015 VALUES LESS THAN (2015),\n    PARTITION lmaxy VALUES LESS THAN MAXVALUE\n);\n\n--list\nPARTITION BY LIST(year(dt)) (\n    PARTITION p0 VALUES IN (2010,2011),\n    PARTITION p1 VALUES IN (2012,2013)\n);\n</code></pre>"},{"location":"SQL/MySQL/partition/#redefine-partition","title":"redefine partition","text":"<pre><code>ALTER TABLE tbl\nPARTITION BY HASH(id) PARTITIONS 4;\n\nALTER TABLE tbl\nPARTITION BY RANGE (YEAR(created)) (\n    PARTITION p0 VALUES LESS THAN (2014),\n    PARTITION p1 VALUES LESS THAN (2015),\n    PARTITION p2 VALUES LESS THAN MAXVALUE\n);\n</code></pre>"},{"location":"SQL/MySQL/partition/#add-partition","title":"add partition","text":"<pre><code>ALTER TABLE tbl ADD PARTITION (PARTITION p3 VALUES LESS THAN (2002));\n</code></pre>"},{"location":"SQL/MySQL/partition/#drop-partition","title":"drop partition","text":"<p>Note: drop the partition will drop the data in the partition as well <pre><code>ALTER TABLE tbl DROP PARTITION p0,p2;\n\n--will remove all partitions\nALTER TABLE tbl remove PARTITIONING;\n</code></pre></p>"},{"location":"SQL/MySQL/partition/#reorgnize-partitions","title":"reorgnize partitions","text":"<p>change the partitioning of a table without losing data <pre><code>--split\nALTER TABLE tbl\nREORGANIZE PARTITION l2014 INTO (\n        PARTITION n0 VALUES LESS THAN (2013),\n        PARTITION n1 VALUES LESS THAN (2014)\n);\n\n--merge\nALTER TABLE tbl\nREORGANIZE PARTITION n0,n1 INTO (\n    PARTITION l2012 VALUES LESS THAN (2012),\n    PARTITION l2014 VALUES LESS THAN (2014)\n);\n</code></pre></p>"},{"location":"SQL/MySQL/perf/","title":"perf","text":""},{"location":"SQL/MySQL/perf/#show-slow-queries","title":"show slow queries","text":"<pre><code>select start_time, time_to_sec(query_time) as qry_time_sec, \n       rows_sent, rows_examined, db, CONVERT(sql_text USING utf8) as qry_text\nfrom mysql.slow_log \norder by query_time desc;\n</code></pre> <p>when use OPTIMIZE TABLE?</p> <p>https://downloads.mysql.com/presentations/MySQL_Perfornance__Tuning_Overview_jp.pdf</p> <p>http://porthos.ist.utl.pt/docs/mySQL/optimization.html#optimizing-myisam https://www.liquidweb.com/kb/mysql-performance-myisam/</p>"},{"location":"SQL/MySQL/perf/#myisam-optimization","title":"MyISAM optimization","text":""},{"location":"SQL/MySQL/perf/#key_buffer_size","title":"key_buffer_size","text":"<p>usually 25% of the total RAM <pre><code>show status like 'key%';\n\nshow global variables like 'key%';\n\n--calc required key_buffer_size\nselect count(INDEX_LENGTH) as Indexes,\nfloor(sum(INDEX_LENGTH)/1024/1024) as Total_Index_Length_MB,\nfloor(sum(INDEX_LENGTH)*(1+0.05)/1024/1024) as key_buffer_size_MB\nFROM information_schema.TABLES WHERE ENGINE = 'MyISAM';\n\nSET GLOBAL key_buffer_size = 1024 * 1024 * 1024 * 2;\n\n--Calculate Key Cache Usage Rate\nselect VARIABLE_VALUE into @Key_blocks_unused \nFROM information_schema.global_status\nWHERE VARIABLE_NAME = \"Key_blocks_unused\";\nselect @Key_blocks_unused as Key_blocks_unused,\n    @@key_cache_block_size as key_cache_block_size,\n    @@key_buffer_size as key_buffer_size\\G\nselect truncate((1-((@Key_blocks_unused*@@key_cache_block_size)/@@key_buffer_size))*100, 2)\n   as \"(1-((Key_blocks_unused \u00d7 key_cache_block_size) / key_buffer_size)) \u00d7 100;\n</code></pre></p>"},{"location":"SQL/MySQL/perf/#optimize-myisam-settings","title":"optimize MyISAM settings","text":"<p>DISABLE KEYS / ENABLE KEYS only works for MyISAM</p> <p>read_rnd_buffer_size will affect query performance with sorted operations. As it is created per connection it is best to set it temporally</p> <p>key_buffer_size [affects exporting performance] is for MyISAM. Default size 8 MB. You can increase the value to get better index handling for all reads and multiple writes; on a system whose primary function is to run MySQL using the MyISAM storage engine, 25% of the machine's total memory is an acceptable value for this variable. Tune this to at least 30% of your RAM or the re-indexing process will probably be too slow. However, if you make the value too large (for example, more than 50% of the machine's total memory), your system might start to page and become extremely slow. </p> <p>check the usage of the buff: SHOW STATUS LIKE 'Key%';</p> <p>Key Cache Usage Rate: 1-(key_blocks_unused * key_cache_block_size / key_buffer_size)\\ This measures the percentage of the Key Cache that is filled, a helpful detail when determining if the key_buffer_size is large enough or if memory is being wasted.</p> <p>Key Cache Miss Rate: key_reads / key_read_requests\\ This shows how often items that could be in the Key Cache are being read from a physical disk.</p> <p>Key Cache Flush Rate: key_writes / key_write_requests\\ This value measures how frequently the Key Cache is written to disk. Frequent flushing can slow down MyISAM operations as writing data to a physical disk is much slower than writing to the Key Cache in memory. </p> <p>also consider to change 'bulk_insert_buffer_size' [affects importing performance] to at least 32M:   SET GLOBAL bulk_insert_buffer_size = 1024 * 1024 * 256;   SET bulk_insert_buffer_size = 1024 * 1024 * 256;</p> <p>show global variables like 'bulk_insert%';   show variables like 'bulk_insert%';</p>"},{"location":"SQL/MySQL/perf/#myisam-2","title":"MyISAM 2","text":"<p>For MyIsam i have some recommendations as</p> <p>key_buffer_size Size of the Key Buffer, used to cache index blocks for MyISAM tables. Do not set it larger than 30% of your available memory, as some memory is also required by the OS to cache rows. Even if you're not using MyISAM tables, you should still set it to 8-64M as it will also be used for internal temporary disk tables.So You should set it from 2GB to 3GB.</p> <p>read_buffer_size Size of the buffer used for doing full table scans of MyISAM tables. Allocated per thread, if a full scan is needed..You should set it to 8MB to 16MB .</p> <p>read_rnd_buffer_size When reading rows in sorted order after a sort, the rows are read through this buffer to avoid disk seeks. You can improve ORDER BY performance a lot, if set this to a high value. Allocated per thread, when needed.</p> <p>bulk_insert_buffer_size MyISAM uses special tree-like cache to make bulk inserts (that is, INSERT ... SELECT, INSERT ... VALUES (...), (...), ..., and LOAD DATA INFILE) faster. This variable limits the size of the cache tree in bytes per thread. Setting it to 0 will disable this optimisation. Do not set it larger than \"key_buffer_size\" for optimal performance. This buffer is allocated when a bulk insert is detected.Set it to 512 MB to 1GB.</p> <p>myisam_sort_buffer_size This buffer is allocated when MySQL needs to rebuild the index in REPAIR, OPTIMIZE, ALTER table statements as well as in LOAD DATA INFILE into an empty table. It is allocated per thread so be careful with large settings.So You should set it to 2M</p> <p>myisam_max_sort_file_size The maximum size of the temporary file MySQL is allowed to use while recreating the index (during REPAIR, ALTER TABLE or LOAD DATA INFILE. If the file-size would be bigger than this, the index will be created through the key cache (which is slower).Set it to as you need.</p> <p>myisam_repair_threads  If a table has more than one index, MyISAM can use more than one thread to repair them by sorting in parallel. This makes sense if you have multiple CPUs and plenty of memory.Set it to 1.</p> <p>myisam_recover Automatically check and repair not properly closed MyISAM tables.</p>"},{"location":"SQL/MySQL/perf/#optimize-innodb-settings","title":"optimize InnoDB settings","text":"<p>For Innodb we need to diable unique_checks and foreign_key_checks using <pre><code>set unique_checks=0; set foreign_key_checks=0; disable auto_commit.\ninserts......\nset unique_checks=1; set foreign_key_checks=1; enable auto_commit.\n</code></pre> Which improves bulk inserts performance.</p> <p>Your process is slow due to heavy disk I/O activity for SQL operations. You need to optimize innodb for intensive operations. Modify your mysql configuration file to have these lines:   innodb_buffer_pool_size = 4G   innodb_log_buffer_size = 256M   innodb_log_file_size = 1G   innodb_write_io_threads = 16</p> <p>innodb_buffer_pool_size: the memory area where InnoDB caches table and index data. When table data is cached in the InnoDB buffer pool, it can be accessed repeatedly by queries without requiring any disk I/O. Data changes are cached rather than immediately written to disk.</p> <p>A larger buffer pool requires less disk I/O to access the same table data more than once. On a dedicated database server, you might set the buffer pool size to 80% of the machine's physical memory else use 50 to 75 percent of system memory. The default buffer pool size is 128MB.</p> <p>To determine the optimal size, do: <pre><code>SELECT (PagesData*PageSize)/POWER(1024,3) DataGB \nFROM\n(SELECT variable_value PagesData\n FROM performance_schema.global_status\n WHERE variable_name='Innodb_buffer_pool_pages_data') A,\n(SELECT variable_value PageSize\n FROM performance_schema.global_status\n WHERE variable_name='Innodb_page_size') B;\n</code></pre></p> <pre><code>                    Duration / Fetch\n</code></pre> <p>353 row(s) returned 34.422 sec / 125.797 sec (8MB innodb buffer_pool_size)   353 row(s) returned  0.500 sec /   1.297 sec (1GB innodb buffer_pool_size)</p> <p>innodb_log_buffer_size: The size in bytes of the buffer that InnoDB uses to write to the log files on disk.</p> <p>A large log buffer enables large transactions to run without the need to write the log to disk before the transactions commit. Thus, if you have transactions that update, insert, or delete many rows, making the log buffer larger saves disk I/O.</p> <p>innodb_log_file_size: The size in bytes of each log file in a log group. A large size ensures that the server can smooth out peaks and troughs in workload activity, which often means that there is enough redo log space to handle more than an hour of write activity. The larger the value, the less checkpoint flush activity is required in the buffer pool, saving disk I/O.</p> <p>Innodb_write_io_threads: The number of I/O threads for write operations in InnoDB. The default value is 4. InnoDB uses background threads to service various types of I/O requests. You can configure the number of background threads that service read and write I/O on data pages using the innodb_read_io_threads and innodb_write_io_threads configuration parameters.</p> <p>These parameters signify the number of background threads used for read and write requests, respectively. Each background thread can handle up to 256 pending I/O requests.</p> <p>If you are importing to innodb the single most effective thing you can do is to put innodb_flush_log_at_trx_commit = 2 in your my.cnf, temporarily while the import is running. you can put it back to 1 if you need ACID. Your hint with innodb_flush_log_at_trx_commit = 2 saved my day. Importing a 600 MB dump (as a single big transaction) would have needed 6 hours, but with this temporary setting, it was done in 30 minutes! someone suggested a performance gain of 100x faster</p> <p>https://dev.mysql.com/doc/refman/5.7/en/optimizing-innodb-bulk-data-loading.html</p> <p>https://www.oreilly.com/library/view/high-performance-mysql/9780596101718/ch04.html</p> <p>replace In subquery with inner join will be much faster</p> <p>creating temporal table is faster than directly using inner join statement</p>"},{"location":"SQL/MySQL/perf/#innodb-optimization-example","title":"InnoDB optimization example","text":"<p>https://dba.stackexchange.com/questions/227742/extreme-performance-loss-due-to-moving-from-mysql-5-7-23-to-mysql-8-0-status-c Big update Spent half a day reading into internas, taking your suggestions in consideration and what I was told on IRC. I did the opposite of the professional: I made 10 changes at once, I can't afford so many restarts in a gradual change process: 1) Giving the system lots of parallel write opportunity without choking it   innodb_read_io_threads = 16   innodb_write_io_threads = 16   innodb_thread_concurrency=64  # cpus*2 </p> <p>2) Speeding up the background sync:   innodb_lru_scan_depth=100</p> <p>3) Disabling highest reliability settings that have a hard performance impact   performance_schema=OFF   skip-log-bin    sync_binlog=0    innodb_flush_log_at_trx_commit=0  # not crash safe, 0 is crash safe </p> <p>4) More multithreading in backend memory   innodb_buffer_pool_instances=12  </p> <p>5) Increasing logfiles significantly, increasing logfile buffer moderatly   innodb_log_file_size = 3G #    innodb_log_buffer_size = 64M</p> <p>What happened: About 10 times increase in read performance,1.5x times write performance, I am not where I want to be but it's 15 times faster than before! IOPS usage doubled from ~5-6k to 9k-12k, so I am at 60% IO usage CPU usage increased from 7% to 50%</p> <p>Update\\ I think the problem can be considered half solved, I've made another update after the previous successful one and the performance is acceptable now.</p> <p>The last changes involved write/read threads. I've put them to 32 each. write buffer increased to 128M (for my heavy workload higher might be better) logfiles increased to 8GB buffer_pool_instances increased to 64 (max) for better memory fragmentation page_cleaners increased to 64 (max) to have one for each buffer instance.</p> <p>write performance increased by another ~20%, read performance increased by another ~30%.</p> <p>It's been a 24 hour ride to get mysql perform acceptable, definitely not a simple upgrade.</p>"},{"location":"SQL/MySQL/perf/#limitations-of-the-mysql-query-optimizer","title":"Limitations of the MySQL Query Optimizer","text":""},{"location":"SQL/MySQL/perf/#correlated-subqueries","title":"Correlated subqueries","text":"<p>bad: <pre><code>SELECT * FROM sakila.film\nWHERE film_id IN\n(\n   SELECT film_id FROM sakila.film_actor WHERE actor_id = 1\n);\n</code></pre></p> <p>optimized to: <pre><code>SELECT * FROM sakila.film\nWHERE EXISTS \n(\n   SELECT * FROM sakila.film_actor WHERE actor_id = 1\n   AND film_actor.film_id = film.film_id\n);\n</code></pre></p> <p>good: <pre><code>SELECT film.* FROM sakila.film\nINNER JOIN sakila.film_actor USING(film_id)\nWHERE actor_id = 1;\n</code></pre></p> <p>Another good optimization is to manually generate the IN() list by executing the subquery as a separate query with GROUP_CONCAT(). Sometimes this can be faster than a JOIN.</p>"},{"location":"SQL/MySQL/perf/#slow-limit-query","title":"slow limit query","text":"<p>https://stackoverflow.com/questions/17747871/why-is-mysql-slow-when-using-limit-in-my-query</p> <p>Same problem happened in my project, I did some test, and found out that LIMIT is slow because of row lookups</p> <p>See: MySQL ORDER BY / LIMIT performance: late row lookups</p> <p>So, the solution is:   * when using LIMIT, select not all columns, but only the PK columns   * Select all columns you need, and then join with the result set of PK cols</p> <p>SQL should likes: <pre><code>SELECT  * FROM\n    orders t1   &lt;=== this is what you want\nJOIN (  SELECT ID FROM orders     &lt;== fetch the PK column only, this should be fast        \n        WHERE [your query condition]        &lt;== filter record by condition\n        ORDER BY [your order by condition]  &lt;== control the record order\n        LIMIT 2000, 50                      &lt;== filter record by paging condition\n) as t2\nON t1.ID = t2.ID\nORDER BY [your order by condition]          &lt;== control the record order\n</code></pre> in my DB, the old SQL which select all columns using \"LIMIT 21560, 20\", costs about 4.484s. the new sql costs only 0.063s. The new one is about 71 times faster</p>"},{"location":"SQL/MySQL/procedure/","title":"Procedure","text":"<p>https://stackoverflow.com/questions/5125096/for-loop-example-in-mysql\\ https://stackoverflow.com/questions/5817395/how-can-i-loop-through-all-rows-of-a-table-mysql\\ https://dba.stackexchange.com/questions/138549/mysql-loop-through-a-table-running-a-stored-procedure-on-each-entry</p> <pre><code>DELIMITER $$\n\nDROP PROCEDURE IF EXISTS `my_proc` $$\nCREATE PROCEDURE `my_proc`(arg1 INT)\nBEGIN\n\n-- declare the program variables where we'll hold the values we're sending into the procedure;\n-- declare as many of them as there are input arguments to the second procedure,\n-- with appropriate data types.\n\nDECLARE val1 INT DEFAULT NULL;\nDECLARE val2 INT DEFAULT NULL;\n\n-- we need a boolean variable to tell us when the cursor is out of data\n\nDECLARE done TINYINT DEFAULT FALSE;\n\n-- declare a cursor to select the desired columns from the desired source table1\n-- the input argument (which you might or might not need) is used in this example for row selection\n\nDECLARE cursor1 -- cursor1 is an arbitrary label, an identifier for the cursor\n CURSOR FOR\n SELECT t1.c1,\n        t1.c2\n   FROM table1 t1\n  WHERE c3 = arg1;\n\n-- this fancy spacing is of course not required; all of this could go on the same line.\n\n-- a cursor that runs out of data throws an exception; we need to catch this.\n-- when the NOT FOUND condition fires, \"done\" -- which defaults to FALSE -- will be set to true,\n-- and since this is a CONTINUE handler, execution continues with the next statement.\n\nDECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;\n\n-- open the cursor\n\nOPEN cursor1;\n\nmy_loop: -- loops have to have an arbitrary label; it's used to leave the loop\nLOOP\n\n  -- read the values from the next row that is available in the cursor\n\n  FETCH NEXT FROM cursor1 INTO val1, val2;\n\n  IF done THEN -- this will be true when we are out of rows to read, so we go to the statement after END LOOP.\n    LEAVE my_loop;\n  ELSE -- val1 and val2 will be the next values from c1 and c2 in table t1,\n       -- so now we call the procedure with them for this \"row\"\n    CALL the_other_procedure(val1,val2);\n    -- maybe do more stuff here\n  END IF;\nEND LOOP;\n\n-- execution continues here when LEAVE my_loop is encountered;\n-- you might have more things you want to do here\n\n-- the cursor is implicitly closed when it goes out of scope, or can be explicitly closed if desired\n\nCLOSE cursor1;\n\nEND $$\n\nDELIMITER ;\n</code></pre>"},{"location":"SQL/MySQL/profile/","title":"Profiling","text":""},{"location":"SQL/MySQL/profile/#settings","title":"settings","text":"<pre><code>SHOW PROCESSLIST;\nshow variables like 'innodb%'\nSHOW TABLE STATUS FROM `db` LIKE 'tbl';\n</code></pre>"},{"location":"SQL/MySQL/profile/#profiling_1","title":"profiling","text":"<p>https://dev.mysql.com/doc/refman/8.0/en/general-thread-states.html</p> <pre><code>mysql&gt; SET PROFILING=1;\nmysql&gt; SHOW TABLES;\nmysql&gt; SELECT * FROM foo;\nmysql&gt; SET PROFILING=0;   --deprecated from 5.6.7\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql&gt; SHOW PROFILES;\n+----------+------------+-------------------+\n| Query_ID | Duration   | Query             |\n+----------+------------+-------------------+\n|        1 | 0.09270400 | SHOW TABLES       |\n|        2 | 0.00026400 | SELECT * FROM foo |\n+----------+------------+-------------------+\n2 rows in set (0.05 sec)\n\nmysql&gt; SHOW PROFILE FOR QUERY 2;\n+----------------------+----------+\n| Status               | Duration |\n+----------------------+----------+\n| starting             | 0.000053 |\n| checking permissions | 0.000009 |\n| Opening tables       | 0.000032 |\n| System lock          | 0.000010 |\n| init                 | 0.000028 |\n| optimizing           | 0.000003 |\n| statistics           | 0.000012 |\n| preparing            | 0.000008 |\n| executing            | 0.000003 |\n| Sending data         | 0.000068 |\n| end                  | 0.000004 |\n| query end            | 0.000007 |\n| closing tables       | 0.000008 |\n| freeing items        | 0.000013 |\n| logging slow query   | 0.000003 |\n| cleaning up          | 0.000003 |\n+----------------------+----------+\n16 rows in set (0.04 sec)\n</code></pre>"},{"location":"SQL/MySQL/profile/#new-method","title":"new method","text":"<pre><code>#change statement settings (default YES)\nUPDATE performance_schema.setup_instruments\nSET ENABLED = 'YES', TIMED = 'YES'\nWHERE NAME LIKE '%statement/%';\n\n#change stage settings (default NO for properties != 'progress')\nUPDATE performance_schema.setup_instruments\nSET ENABLED = 'YES', TIMED = 'YES'\nWHERE NAME LIKE '%stage/%';\n\n#change consumers settings\nUPDATE performance_schema.setup_consumers\nSET ENABLED = 'YES'\nWHERE NAME LIKE '%events_statements_%';\n\nUPDATE performance_schema.setup_consumers\nSET ENABLED = 'YES'\nWHERE NAME LIKE '%events_stages_%';\n\n#disable other threads for instrument\nUPDATE performance_schema.threads\nSET INSTRUMENTED = 'NO'\nWHERE TYPE='FOREGROUND' AND PROCESSLIST_USER NOT LIKE 'user_john';\n\n#1. RUN QUERY\nselect * from mytable where col like '124%';\n\n#2. identify EVENT_ID \nSELECT @eid:=EVENT_ID, TRUNCATE(TIMER_WAIT/1000000000000,6) as Duration, SQL_TEXT\nFROM performance_schema.events_statements_history_long WHERE SQL_TEXT like '%10001%'\norder by EVENT_ID desc limit 1;\n\n#3. retrieve stage events\nSELECT event_name AS Stage, TRUNCATE(TIMER_WAIT/1000000000000,6) AS Duration\nFROM performance_schema.events_stages_history_long WHERE NESTING_EVENT_ID=@eid;\n\n#change settings back to default\nupdate performance_schema.setup_instruments\nSET ENABLED = 'NO', TIMED = 'NO'\nWHERE NAME LIKE '%stage/%' and properties != 'progress';  \n\nupdate performance_schema.setup_consumers\nset enabled = 'NO'\nWHERE NAME = 'events_statements_history_long';\n\nupdate performance_schema.setup_consumers\nset enabled = 'NO'\nWHERE NAME LIKE '%events_stages_%';\n</code></pre>"},{"location":"SQL/MySQL/qry/","title":"query","text":""},{"location":"SQL/MySQL/qry/#checksum","title":"checksum","text":"<p>CHECKSUM TABLE db.tbl;</p>"},{"location":"SQL/MySQL/qry/#par","title":"par","text":"<pre><code>SELECT variable_value FROM information_schema.global_variables \nWHERE variable_name IN ('slow_query_log','slow_query_log_file','long_query_time');\n</code></pre>"},{"location":"SQL/MySQL/qry/#move-records","title":"move records","text":"<p>wrap the insert and delete into one transaction to avoid data loss <pre><code>DECLARE EXIT HANDLER FOR SQLEXCEPTION, SQLWARNING, NOT FOUND\nBEGIN \n  ROLLBACK; \n  CALL ERROR_ROLLBACK_OCCURRED; \nEND;\n\nSTART TRANSACTION;\nINSERT INTO table_new SELECT a,b,c FROM table_old WHERE x = 'something';\nDELETE FROM table_old WHERE x = 'something';\nCOMMIT;\n</code></pre></p>"},{"location":"SQL/MySQL/shadding/","title":"Shadding","text":""},{"location":"SQL/MySQL/shadding/#sharding","title":"Sharding","text":""},{"location":"SQL/MySQL/shadding/#sharding-and-partitioning","title":"Sharding and Partitioning","text":"<p>used to increase the performance of the sql server by splitting the data into different subsets</p> <p> Example python code about how to insert records to a database table  <pre><code># connect\ncon = mdb.connect('DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=' + dbpath)\ncursor = con.cursor()\n\n# insert records\nqry = 'INSERT INTO tbl ('\nqry += ','.join(list(df.head(0))) + ') VALUES ('\nqry += ','.join(np.repeat('?',df.shape[1])) + ')'\nvals = list(df.itertuples(index=False, name=None))\ncursor.executemany(qry, vals)\ncursor.commit()\n\n# close connection\ncursor.close()\ncon.close()\n</code></pre></p>"},{"location":"SQL/MySQL/tip/","title":"tip","text":""},{"location":"SQL/MySQL/tip/#max-of-columns","title":"max of columns","text":"<pre><code>SELECT GREATEST(v1, v2 ...) FROM t\n</code></pre>"},{"location":"SQL/MySQL/tip/#use-variables-to-select-consecutive-rows-with-same-values","title":"use variables to select \"consecutive\" rows with same values","text":"<pre><code>SET @id = NULL, @val = NULL, @same = 0, @cnt = 0;\n\nSELECT @prev_id := @id AS prev_id,\n@prev_val := @val AS prev_val,\n@id := id AS id,\n@val := val AS val,\n@same := if(@prev_id = @id &amp;&amp; @prev_val = @val, 1, 0) AS same\n@cnt := if(@same = 0, 0, @cnt + 1) AS cnt\nFROM test \nORDER by id, reg_id;\n</code></pre>"},{"location":"SQL/MySQL/tip/#lag-function-to-update-with-previous-row-value","title":"lag function to update with previous row value","text":"<pre><code>-- update prev_v using lag(expr, offset, default)\n-- tbl(id, dt, v, prev_v) primary key (id,dt)\nwith a as(\n    select id, dt, v, \n    LAG(v, 1, 0) OVER (\n        PARTITION BY id\n        ORDER BY id, dt asc\n    ) as pv\n    from tbl\n)\nupdate tbl t\ninner join a on a.pk = t.pk  \nset t.pv = a.pv;\n</code></pre>"},{"location":"SQL/MySQL/user/","title":"user","text":""},{"location":"SQL/MySQL/user/#diff-between-localhost-and-127001","title":"diff between localhost and 127.0.0.1","text":"<p>when you use localhost, the socket connector is used (port is not used). Whereas, when you use 127.0.0.1, TCP/IP connector is used.</p>"},{"location":"SQL/MySQL/user/#show-grants","title":"show grants","text":"<p>SHOW GRANTS; #for current user   SHOW GRANTS FOR 'user_name';   SHOW GRANTS FOR 'root'@'localhost';   SHOW GRANTS FOR 'root'@'%';   SHOW GRANTS FOR 'admin'@'192.168.0.1';</p>"},{"location":"SQL/MySQL/user/#create-user","title":"create user","text":"<p>CREATE USER 'usr'@'%' IDENTIFIED BY 'new_password';   GRANT ALL PRIVILEGES ON . TO 'usr'@'%';   GRANT ALL PRIVILEGES ON db.* TO 'usr'@'%';    FLUSH PRIVILEGES;</p> <p>#new version   GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, RELOAD, SHUTDOWN, PROCESS, FILE, REFERENCES, INDEX, ALTER, SHOW DATABASES, SUPER, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, REPLICATION SLAVE, REPLICATION CLIENT, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, CREATE USER, EVENT, TRIGGER, CREATE TABLESPACE ON . TO <code>usr</code>@<code>%</code> WITH GRANT OPTION;</p>"},{"location":"SQL/MySQL/user/#delete-user","title":"delete user","text":"<p>DROP USER 'usr'@'%';</p>"},{"location":"SQL/MySQL/user/#select-user","title":"select user","text":"<p>SELECT user FROM mysql.user</p>"},{"location":"SQL/MySQL/user/#change-password","title":"change password","text":"<p>UPDATE mysql.user SET Password=PASSWORD('pwd') WHERE User='usr' and Host='%';   FLUSH PRIVILEGES; </p>"},{"location":"SQL/Oracle/Admin/","title":"Admin","text":""},{"location":"SQL/Oracle/Admin/#check-sessions","title":"check sessions","text":"<pre><code>select username, osuser, min(logon_time) as min_time, max(logon_time) as max_time, status, module, count(*) as sessions\nfrom v$session \nwhere status = 'active'\ngroup by username, osuser, status, module;\n</code></pre>"},{"location":"SQL/Oracle/Basic/","title":"Basic","text":""},{"location":"SQL/Oracle/Basic/#first-10","title":"first 10","text":"<pre><code>select * \nfrom tbl\nFETCH NEXT 10 ROWS ONLY;\n</code></pre>"},{"location":"SQL/Oracle/BindVariable/","title":"Bind Variable","text":"<p>The limit of 1000 is on the constructor of the table type (and in clauses), but if you build-up the table you don\u2019t need to worry about such limitation.</p> <p>Use table bnind variable in <code>in clauses</code> to avoid the limit of 1000 elements: https://cx-oracle.readthedocs.io/en/latest/user_guide/bind.html <pre><code>import cx_Oracle\ndef get_oracle_connection():\n    cnx = cx_Oracle.connect(\n        user='usr', \n        password='pwd',\n        dsn=\"svr.example.com\",\n        encoding=\"UTF-8\",\n    )\n    return cnx\n\nwith get_oracle_connection() as connection:\n    cursor = connection.cursor()\n    cursor.execute(\"insert into SomeTable values (:p1, :p2)\",\n                   {'p1':1, 'p2':\"Some string\"))\n    connection.commit()\n\n#CREATE OR REPLACE TYPE name_array AS TABLE OF VARCHAR2(25);\ntype_obj = connection.gettype(f'{MatappSchemaName}.name_array')\nobj = type_obj.newobject()\nobj.extend([\"Smith\", \"Taylor\"])\n</code></pre></p>"},{"location":"SQL/Oracle/Config/","title":"Config","text":""},{"location":"SQL/Oracle/Config/#set-tnsnamesora-and-sqlnetora","title":"set tnsnames.ora and sqlnet.ora","text":"<p>These two files must be set for connection to the oracle database for Oracle Client.</p>"},{"location":"SQL/Oracle/Config/#tns_admin","title":"TNS_ADMIN","text":"<p>This env var is used to point the folder for tnsnames.ora file. check it in cmd: <code>echo %TNS_ADMIN%</code></p> <p>If the <code>TNS_ADMIN</code> is not set, the Oracle <code>instantclient</code> path should be included in the <code>PATH</code> and the tnsnames folder should be mounted to <code>/network/admin/</code> under the <code>PATH</code>.</p>"},{"location":"SQL/Oracle/Connection/","title":"Connection","text":""},{"location":"SQL/Oracle/Connection/#oracle-instant-client","title":"Oracle Instant Client","text":"<p>https://www.oracle.com/au/database/technologies/instant-client/linux-x86-64-downloads.html</p> <p>https://docs.oracle.com/en/database/oracle/oracle-database/19/lnoci/instant-client.html#GUID-6895DB45-97AA-4738-9959-BD677D610186</p> <p>Oracle Instant Client provides the necessary Oracle Database client-side files to create and run OCI, OCCI, ODBC, and JDBC OCI applications.  Useful command-line utilities including SQLPlus, SQLLoader and Oracle Data Pump are also available. </p>"},{"location":"SQL/Oracle/DateTime/","title":"DateTime","text":""},{"location":"SQL/Oracle/DateTime/#define","title":"define","text":"<pre><code>define dt=\"TO_DATE ('2022-09-01', 'yyyy-mm-dd') + 1\";\nSELECT &amp;dt + 3 new_date\nFROM DUAL;\n</code></pre>"},{"location":"SQL/Oracle/DateTime/#add-5-minutes","title":"add 5 minutes","text":"<pre><code>select dt + INTERVAL '5' MINUTE as dt\nfrom my_table;\n</code></pre>"},{"location":"SQL/Oracle/DateTime/#string-to-datetime","title":"string to datetime","text":"<pre><code>SELECT TO_DATE('2024-01-01 12:00:00', 'YYYY-MM-DD HH24:MI:SS') AS datetime_value from dual\n</code></pre>"},{"location":"SQL/Oracle/Env/","title":"Env","text":""},{"location":"SQL/Oracle/Env/#naming-parameters","title":"Naming Parameters","text":"<p>By default, the <code>tnsnames.ora</code> file is located in the <code>ORACLE_HOME/network/admi</code>n directory.  Oracle Net will check the other directories for the configuration file. For example, the order checking the tnsnames.ora file is as follows:   - The directory specified by the <code>TNS_ADMIN</code> environment variable. If the file is not found in the directory specified, then it is assumed that the file does not exist.   - If the TNS_ADMIN environment variable is not set, then Oracle Net checks the <code>ORACLE_HOME/network/admin</code> directory.</p>"},{"location":"SQL/Oracle/Function/","title":"Function","text":""},{"location":"SQL/Oracle/Function/#find-functions-with-specific-string","title":"find functions with specific string","text":"<pre><code>SELECT * \nFROM ALL_OBJECTS \nWHERE OBJECT_TYPE IN ('FUNCTION','PROCEDURE','PACKAGE') and lower(object_name) like '%data%';\n</code></pre>"},{"location":"SQL/Oracle/Function/#show-function-definition","title":"show function definition","text":"<pre><code>select text \nfrom all_source \nwhere type = 'FUNCTION' and name = 'my_func_name'\norder by line;\n</code></pre>"},{"location":"SQL/Oracle/Index/","title":"Index","text":""},{"location":"SQL/Oracle/Index/#hints-of-key-for-optimizer","title":"hints of key for optimizer","text":"<pre><code>SELECT /*+ index(t my_tbl_pk)*/ DATETIME, QUALITY \nFROM schema.tbl t  \nWHERE datetime &gt; to_date('01-Oct-2021 00:00', 'DD-MON-YYYY HH24:MI')\nORDER BY 1;\n</code></pre>"},{"location":"SQL/Oracle/Issue/","title":"Issue","text":""},{"location":"SQL/Oracle/Issue/#ora-00907-missing-right-parenthesis","title":"ORA-00907: missing right parenthesis","text":"<p>can be cuased by other sytax error</p>"},{"location":"SQL/Oracle/Issue/#ora-00933-sql-command-not-properly-ended","title":"ORA-00933: SQL command not properly ended","text":"<p>The query should be ended with \";\"</p>"},{"location":"SQL/Oracle/ListLimit/","title":"List Limit","text":"<p>https://stackoverflow.com/questions/400255/how-to-put-more-than-1000-values-into-an-oracle-in-clause</p> <p>The limit of 1000 is on the constructor of the Oracle <code>table type</code> (and <code>in clauses</code>), but if you build-up the Oracle table you don\u2019t need to worry about such limitation. </p>"},{"location":"SQL/Oracle/Parallel/","title":"Parallel","text":"<p>How Parallel Execution Works https://docs.oracle.com/cd/E11882_01/server.112/e25523/parallel002.htm</p> <p>Using Parallel Execution https://docs.oracle.com/cd/B19306_01/server.102/b14223/usingpe.htm</p>"},{"location":"SQL/Oracle/Rank/","title":"Rank","text":""},{"location":"SQL/Oracle/Rank/#select-max-and-second-max","title":"select max and second max","text":"<p>do not use distinct - it's super slow.</p> <p>Note that this is not correct if we have multiple records with max datetime <pre><code>SELECT datetime_column\nFROM (\n    SELECT datetime_column,\n           ROW_NUMBER() OVER (ORDER BY datetime_column DESC) AS rn\n    FROM your_table\n) ranked\nWHERE rn &lt;= 2;\n</code></pre></p> <p>Use <code>dense_rank()</code> <pre><code>SELECT distinct datetime_column\nFROM (\n    SELECT datetime_column,\n           DENSE_RANK() OVER (ORDER BY datetime_column DESC) AS rn\n    FROM your_table\n) ranked\nWHERE rn &lt;= 2;\n</code></pre></p>"},{"location":"SQL/Oracle/Table/","title":"Table","text":""},{"location":"SQL/Oracle/Table/#find-table-with-specific-column","title":"find table with specific column","text":"<pre><code>select owner as schema, table_name, column_name \nfrom all_tab_columns \nwhere lower(column_name) like '%juri%';\n</code></pre>"},{"location":"SQL/Oracle/Table/#find-views-with-specific-string","title":"find views with specific string","text":"<pre><code>SELECT view_name, text\nFROM all_views\nwhere lower(view_name) like '%data%';\n</code></pre>"},{"location":"SQL/Oracle/TableInfo/","title":"Table info","text":""},{"location":"SQL/Oracle/TableInfo/#show-tables","title":"Show tables","text":"<p>right click &gt; Schema Browser &gt; sel3ct owner &gt; all tables under the schema will show</p>"},{"location":"SQL/Oracle/TableInfo/#describe-table","title":"describe table","text":"<pre><code>describe schema_name.table_name;\n</code></pre>"},{"location":"SQL/Oracle/TableInfo/#tables-accessible-to-the-current-user","title":"Tables accessible to the current user","text":"<pre><code>select \n    col.column_id, \n    col.owner as schema_name,\n    col.table_name, \n    col.column_name, \n    col.data_type, \n    col.data_length, \n    col.data_precision, \n    col.data_scale, \n    col.nullable\nfrom sys.all_tab_columns col\ninner join sys.all_tables t \n    on col.owner = t.owner and col.table_name = t.table_name\n-- excluding some Oracle maintained schemas\nwhere \n    col.owner not in ('ANONYMOUS','CTXSYS','DBSNMP','EXFSYS', 'LBACSYS', \n   'MDSYS', 'MGMT_VIEW','OLAPSYS','OWBSYS','ORDPLUGINS', 'ORDSYS','OUTLN', \n   'SI_INFORMTN_SCHEMA','SYS','SYSMAN','SYSTEM','TSMSYS','WK_TEST','WKSYS', \n   'WKPROXY','WMSYS','XDB','APEX_040000', 'APEX_PUBLIC_USER','DIP', \n   'FLOWS_30000','FLOWS_FILES','MDDATA', 'ORACLE_OCM', 'XS$NULL',\n   'SPATIAL_CSW_ADMIN_USR', 'SPATIAL_WFS_ADMIN_USR', 'PUBLIC')  \norder by col.owner, col.table_name, col.column_id;\n</code></pre>"},{"location":"SQL/Oracle/Variable/","title":"Variable","text":""},{"location":"SQL/Oracle/Variable/#define","title":"define","text":"<p>https://docs.oracle.com/cd/B19306_01/server.102/b14357/ch12017.htm</p> <p><code>DEFINE</code> statement for simple string substitution variables. <pre><code>DEFINE start_date = \"to_date('03/11/2011', 'dd/mm/yyyy')\"\nSELECT COUNT(*) from my_table tab where tab.some_date &lt; &amp;start_date;\n</code></pre></p> <pre><code>define col = 'date';\ndefine day = to_date('2022-07-01', 'YYYY-MM-DD');\nSELECT '&amp;&amp;col' from my_table where date &gt; &amp;&amp;day;\n</code></pre> <pre><code>define val = 12;\nSELECT * from my_table where value &gt; &amp;&amp;val;\n</code></pre>"},{"location":"SQL/Oracle/Variable/#declare","title":"declare","text":"<p>https://stackoverflow.com/questions/8039907/simple-oracle-variable-sql-assignment</p> <p>The <code>DECLARE</code> keyword is used to define variables scoped in a PL/SQL block (whose body is delimited by BEGIN and END;). <pre><code>DECLARE \n    startDate DATE := to_date('03/11/2011', 'dd/mm/yyyy');\n    reccount INTEGER;\nBEGIN\n    SELECT count(*) INTO reccount \n        FROM my_table tab \n        WHERE tab.somedate &lt; startDate;\n    dbms_output.put_line(reccount);\nEND;\n</code></pre></p>"},{"location":"SQL/Oracle/Variable/#print","title":"print","text":"<pre><code>DECLARE \n    dt DATE := to_date('2022-08-01', 'yyyy-mm-dd');\nBEGIN\n   DBMS_OUTPUT.put_line (\n        dt + 1\n   );\nEND;\n</code></pre>"},{"location":"SQL/PostgreSQL/Basic/","title":"Basic","text":"<ul> <li>PostgreSQL offers more complex data types than MySQL</li> <li>performance: Suited for applications with high volume of both reads and writes</li> </ul>"},{"location":"SQL/PostgreSQL/Error/","title":"Error","text":""},{"location":"SQL/PostgreSQL/Error/#cannot-connect","title":"cannot connect","text":"<p><pre><code>server closed the connection unexpectedly\nThis probably means the server terminated abnormally\nbefore or while processing the request.\n</code></pre> PostgreSQL server is on another machine and is not listening on external interfaces.</p>"},{"location":"SQL/PostgreSQL/Event/","title":"Event","text":""},{"location":"SQL/PostgreSQL/Event/#postgresql-listennotify","title":"PostgreSQL LISTEN/NOTIFY","text":"<p>Use to do something based on changes in db tables - https://stackoverflow.com/questions/24285563/waiting-for-a-row-update-or-row-insertion-with-sqlalchemy - https://tapoueh.org/blog/2018/07/postgresql-listen-notify</p> <p>need to create triggers in postgresql <pre><code>from sqlalchemy.pool.base import PoolProxiedConnection\nfrom psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\ndef main(conn: PoolProxiedConnection, channels: list[str]):\n    with conn.cursor() as cur:\n        for channel in channels:\n            cur.execute(f'LISTEN {channel};')\n    while True:\n      await trio.lowlevel.wait_readable(conn)\n      conn.poll()\n      while conn.notifies:\n          notify = conn.notifies.pop(0)\n          print 'Got NOTIFY:', notify.pid, notify.channel, notify.payload\n\nengine = sqlalchemy.Engine()\nconn = engine.raw_connection()\nconn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n\ntry:\n    trio.run(main, conn, channels)\nfinally:\n    conn.close()\n    engine.dispose()\n</code></pre></p>"},{"location":"SQL/PostgreSQL/Event/#event-trigger","title":"event trigger","text":"<p>https://www.postgresql.org/docs/current/event-triggers.html</p> <p>Python code use postgresql event triggers <pre><code>cnx = engine.raw_connection()\nawait trio.lowlevel.wait_readable(cnx)\ncnx.poll()\nwhile cnx.notifies:\n    notify = cnx.notifies.pop(0)\n    kwargs = json.loads(notify.payload)\n</code></pre></p> <p>This code uses Trio asynchronous I/O library and deals with PostgreSQL database notifications:</p> <ol> <li><code>await trio.lowlevel.wait_readable(cnx)</code>:</li> <li> <p>This line is using Trio's low-level API to wait until the <code>cnx</code> (presumably a network connection) becomes readable.      In asynchronous programming, this line of code allows the program to efficiently wait for data to be available for reading without blocking the entire thread.</p> </li> <li> <p><code>cnx.poll()</code>:</p> </li> <li> <p>This line is likely associated with a PostgreSQL connection (<code>cnx</code>).      The <code>poll</code> method is commonly used in asynchronous PostgreSQL libraries to check for any events or notifications on the connection.</p> </li> <li> <p><code>while cnx.notifies:</code>:</p> </li> <li> <p>This starts a <code>while</code> loop that iterates as long as there are notifications in the <code>cnx.notifies</code> list.      Notifications in PostgreSQL are messages sent by the database server to connected clients to signal changes or events.</p> </li> <li> <p><code>notify = cnx.notifies.pop(0)</code>:</p> </li> <li>Inside the loop, this line pops (removes and returns) the first notification from the list of notifications (<code>cnx.notifies</code>). </li> </ol> <p>Overall, the code is waiting for the <code>cnx</code> connection to become readable, polling for any notifications on the connection, and then processing each notification in a loop.  It suggests an asynchronous approach to handling PostgreSQL notifications, which is common in applications where you want to be notified of changes in the database in a non-blocking manner. </p>"},{"location":"SQL/PostgreSQL/Event/#triggers-for-event-handling","title":"Triggers for Event Handling","text":"<p>PostgreSQL provides the ability to use triggers, which can be a core component of an event-driven architecture. Triggers allow you to respond automatically to database changes (like insert, update, or delete operations) and execute functions or actions in reaction to those changes.</p> <p>Types of Triggers in PostgreSQL: - BEFORE Triggers: Fire before the actual data modification is applied. They can be used to validate data or modify the data before it's committed. - AFTER Triggers: Fire after the data modification is applied. These are often used for logging, auditing, or triggering additional actions (like updating other tables, calling external services, etc.). - INSTEAD OF Triggers: Overwrites the default behavior of certain actions, allowing more complex logic for data manipulation.</p>"},{"location":"SQL/PostgreSQL/Event/#logical-replication-and-change-data-capture-cdc","title":"Logical Replication and Change Data Capture (CDC)","text":""},{"location":"SQL/PostgreSQL/Event/#notifylisten-asynchronous-event-notification","title":"NOTIFY/LISTEN (Asynchronous Event Notification)","text":""},{"location":"SQL/PostgreSQL/Event/#polling-and-periodic-task-scheduling","title":"Polling and Periodic Task Scheduling","text":""},{"location":"SQL/PostgreSQL/Merge/","title":"Merge","text":""},{"location":"SQL/PostgreSQL/Merge/#ideas-and-code-example-for-merge-command","title":"ideas and code example for merge command","text":"<p>https://www.depesz.com/2022/03/31/waiting-for-postgresql-15-add-support-for-merge-sql-command/</p>"},{"location":"SQL/PostgreSQL/SSL/","title":"SSL","text":""},{"location":"SQL/PostgreSQL/SSL/#sslmode","title":"sslmode","text":"<p>https://www.postgresql.org/docs/current/libpq-ssl.html</p>"},{"location":"SQL/PostgreSQL/SSL/#gssencmode","title":"gssencmode","text":"<p>https://github.com/pgjdbc/pgjdbc/issues/1867</p> <p>if we already set <code>sslmode=require</code> then a GSSAPI encrypted connection increases the latency for nothing. <pre><code>gssencmode=disbale\n</code></pre></p>"},{"location":"SQL/PostgreSQL/pgcli/","title":"pgcli","text":""},{"location":"SQL/PostgreSQL/pgcli/#start-pgcli","title":"start pgcli","text":"<pre><code>pgcli \\d db_name\n</code></pre>"},{"location":"SQL/PostgreSQL/pgcli/#connect-to-db","title":"connect to db","text":"<pre><code>\\c db_name\n</code></pre>"},{"location":"SQL/PostgreSQL/pgcli/#list-all-schemas","title":"list all schemas","text":"<pre><code>\\dn\n</code></pre>"},{"location":"SQL/PostgreSQL/pgcli/#list-all-tables","title":"list all tables","text":"<pre><code>\\dt schema.*\n</code></pre>"},{"location":"SQL/PostgreSQL/pgcli/#show-details-of-table-cols","title":"show details of table cols","text":"<pre><code>\\d schema.table_name\n</code></pre>"},{"location":"SQL/PostgreSQL/pgcli/#expanded-mode","title":"expanded mode","text":"<p>Show all columns from a query result without truncation.  In this mode, each column value will be displayed on a new line, making it easier to view long values. <pre><code>\\x\n\\x off\n</code></pre></p>"},{"location":"SQL/Security/Encryption/","title":"Encryption","text":""},{"location":"SQL/Security/Encryption/#encryption-at-rest","title":"encryption at rest","text":"<p>Encrypt the total database - table files are encrypted.</p>"},{"location":"SQL/Security/Encryption/#row-level-encryption","title":"row level encryption","text":"<p>Encrypt each rows in a table - query results are encrypted.</p>"},{"location":"SQL/Security/Injection/","title":"Injection","text":"<p>https://cybr.com/</p> <p>https://slides.com/christophe-cybr/decks/sql-injections</p>"},{"location":"SQL/Security/MSSQL/","title":"MSSQL","text":""},{"location":"SQL/Security/MSSQL/#configure-sql-server-to-use-encrypted-connection","title":"Configure SQL Server to use encrypted connection","text":"<p>SQL Server Configuration Manager - SQL Server Network configuration - Protocols for <code>&lt;your-sql-server&gt;</code> - Force Encryption: Yes - And choose the certificate</p>"},{"location":"SQL/Security/MSSQL/#check-encryption-is-enabled-or-not","title":"Check encryption is enabled or not","text":"<ul> <li>On server machine use SQL Server Configuration Manager</li> <li>On server machine use PowerShell:   <code>Get-Item -Path HKLM:\\SOFTWARE\\Microsoft\\MSSQL\\Server\\Current Version\\Security</code></li> <li>Check in SQL Server Management Studio (SSMS):   ServerName -&gt; Properties -&gt; Security -&gt; Force Encryption</li> <li>Transact-SQL (T-SQL) Query   <pre><code>SELECT *\nFROM sys.dm_exec_connections;\n-- or\nSELECT session_id, encrypt_option\nFROM sys.dm_exec_connections;\n-- or (not work)\nSELECT encryption_level, cryptographic_provider, algorithm_name\nFROM sys.dm_cryptographic_providers;\n-- get more info\nSELECT s.host_name, s.program_name, s.login_name, s.session_id, s.total_elapsed_time, c.encrypt_option\nFROM sys.dm_exec_sessions as s \njoin sys.dm_exec_connections as c\non c.session_id = s.session_id\norder by s.host_name;\n</code></pre></li> </ul>"},{"location":"System/Behavior/","title":"Behavior","text":"<p>https://www.youtube.com/c/JeffHSipe</p>"},{"location":"System/API/BestPractice/","title":"Best practice","text":"<p>https://learn.microsoft.com/en-us/azure/architecture/best-practices/api-design</p>"},{"location":"System/API/Example/","title":"Example","text":"<p>https://stripe.com/docs/api https://developer.twitter.com/en/docs/api-reference-index</p>"},{"location":"System/API/Guideline/","title":"Guideline","text":"<p>https://github.com/Microsoft/api-guidelines/blob/master/Guidelines.md</p>"},{"location":"System/Test/ABTest/","title":"A/B Test","text":"<p>An A/B test, also known as a split test, is a statistical method used in marketing, product development,  and other fields to compare two versions of a variable (often referred to as A and B) to determine which one performs better.  The purpose of an A/B test is to identify which version leads to more desirable outcomes, such as higher conversion rates, increased sales, or better user engagement.</p> <p>Here's how an A/B test typically works: - Hypothesis: The test begins with formulating a clear hypothesis about the change you want to test. For example, you might wonder whether changing the color of a \"Buy Now\" button on a website will increase the click-through rate. - Variations: You create two versions (A and B) of the element you want to test. In our example, you would have two different colors for the \"Buy Now\" button: one for version A and another for version B. - Randomization: The participants or visitors are randomly divided into two groups. Group A is exposed to version A of the element, while Group B is exposed to version B. This randomization helps reduce bias and ensures that the two groups are as similar as possible in terms of demographics and preferences. - Data Collection: Both versions are shown simultaneously to their respective groups, and data is collected on the performance of each variation. For instance, you might measure the number of clicks on the \"Buy Now\" button for each version. - Analysis: After sufficient data is collected, statistical analysis is performed to determine which version performed better based on the predefined metrics. The statistical significance of the results is crucial to ensure that any observed differences are not due to chance. - Conclusion: Based on the analysis, you draw conclusions about which version performed better and whether the hypothesis is supported. If one version significantly outperforms the other, that version may be implemented as the new standard or used for further testing.</p> <p>A/B testing is a powerful tool for making data-driven decisions and optimizing various aspects of products and services. It allows businesses to validate ideas, improve user experience, and increase conversion rates by understanding what resonates better with their audience.</p>"}]}